{"path":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","commits":[{"id":"e7a005111928c661ab5d236ed6a3a079b438d2cf","date":1205411670,"type":0,"author":"Grant Ingersoll","isMerge":false,"pathNew":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Field field : (List<Field>) document.getDocument().getFields()) {\n      FieldSetting fieldSettings = fieldSettingsByFieldName.get(field.name());\n      if (fieldSettings == null) {\n        fieldSettings = new FieldSetting();\n        fieldSettings.fieldName = field.name().intern();\n        fieldSettingsByFieldName.put(fieldSettings.fieldName, fieldSettings);\n        fieldNameBuffer.add(fieldSettings.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSettings.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms() != fieldSettings.omitNorms) {\n        fieldSettings.omitNorms = true;\n      }\n      if (field.isIndexed() != fieldSettings.isIndexed) {\n        fieldSettings.isIndexed = true;\n      }\n      if (field.isTokenized() != fieldSettings.isTokenized) {\n        fieldSettings.isTokenized = true;\n      }\n      if (field.isCompressed() != fieldSettings.isCompressed) {\n        fieldSettings.isCompressed = true;\n      }\n      if (field.isStored() != fieldSettings.isStored) {\n        fieldSettings.isStored = true;\n      }\n      if (field.isBinary() != fieldSettings.isBinary) {\n        fieldSettings.isBinary = true;\n      }\n      if (field.isTermVectorStored() != fieldSettings.storeTermVector) {\n        fieldSettings.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector() != fieldSettings.storePositionWithTermVector) {\n        fieldSettings.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector() != fieldSettings.storeOffsetWithTermVector) {\n        fieldSettings.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Field, LinkedList<Token>> tokensByField = new LinkedHashMap<Field, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Field> it = (Iterator<Field>) document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Field field = it.next();\n\n      FieldSetting fieldSettings = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          int termCounter = 0;\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n          Token next = tokenStream.next();\n\n          while (next != null) {\n            next.setTermText(next.termText().intern()); // todo: not sure this needs to be interned?\n            tokens.add(next); // the vector will be built on commit.\n            next = tokenStream.next();\n            fieldSettings.fieldLength++;\n            if (fieldSettings.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n        } else {\n          // untokenized\n          tokens.add(new Token(field.stringValue().intern(), 0, field.stringValue().length(), \"untokenized\"));\n          fieldSettings.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Field, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSettings = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSettings.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSettings.position += analyzer.getPositionIncrementGap(fieldSettings.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.termText());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.termText(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSettings.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSettings.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSettings.offset + token.startOffset(), fieldSettings.offset + token.endOffset()));\n          lastOffset = fieldSettings.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSettings.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["317c809622a7a74e9257dd0eaf0b7c4dd7399bc7"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"317c809622a7a74e9257dd0eaf0b7c4dd7399bc7","date":1214673815,"type":3,"author":"Karl-Johan Wettin","isMerge":false,"pathNew":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","pathOld":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","sourceNew":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Field field : (List<Field>) document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = field.name().intern();\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isCompressed()) {\n        fieldSetting.compressed = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Field, LinkedList<Token>> tokensByField = new LinkedHashMap<Field, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Field> it = (Iterator<Field>) document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Field field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          int termCounter = 0;\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n          Token next = tokenStream.next();\n\n          while (next != null) {\n            next.setTermText(next.termText().intern()); // todo: not sure this needs to be interned?\n            tokens.add(next); // the vector will be built on commit.\n            next = tokenStream.next();\n            fieldSetting.fieldLength++;\n            if (fieldSetting.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n        } else {\n          // untokenized\n          tokens.add(new Token(field.stringValue().intern(), 0, field.stringValue().length(), \"untokenized\"));\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Field, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.termText());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.termText(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","sourceOld":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Field field : (List<Field>) document.getDocument().getFields()) {\n      FieldSetting fieldSettings = fieldSettingsByFieldName.get(field.name());\n      if (fieldSettings == null) {\n        fieldSettings = new FieldSetting();\n        fieldSettings.fieldName = field.name().intern();\n        fieldSettingsByFieldName.put(fieldSettings.fieldName, fieldSettings);\n        fieldNameBuffer.add(fieldSettings.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSettings.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms() != fieldSettings.omitNorms) {\n        fieldSettings.omitNorms = true;\n      }\n      if (field.isIndexed() != fieldSettings.isIndexed) {\n        fieldSettings.isIndexed = true;\n      }\n      if (field.isTokenized() != fieldSettings.isTokenized) {\n        fieldSettings.isTokenized = true;\n      }\n      if (field.isCompressed() != fieldSettings.isCompressed) {\n        fieldSettings.isCompressed = true;\n      }\n      if (field.isStored() != fieldSettings.isStored) {\n        fieldSettings.isStored = true;\n      }\n      if (field.isBinary() != fieldSettings.isBinary) {\n        fieldSettings.isBinary = true;\n      }\n      if (field.isTermVectorStored() != fieldSettings.storeTermVector) {\n        fieldSettings.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector() != fieldSettings.storePositionWithTermVector) {\n        fieldSettings.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector() != fieldSettings.storeOffsetWithTermVector) {\n        fieldSettings.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Field, LinkedList<Token>> tokensByField = new LinkedHashMap<Field, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Field> it = (Iterator<Field>) document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Field field = it.next();\n\n      FieldSetting fieldSettings = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          int termCounter = 0;\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n          Token next = tokenStream.next();\n\n          while (next != null) {\n            next.setTermText(next.termText().intern()); // todo: not sure this needs to be interned?\n            tokens.add(next); // the vector will be built on commit.\n            next = tokenStream.next();\n            fieldSettings.fieldLength++;\n            if (fieldSettings.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n        } else {\n          // untokenized\n          tokens.add(new Token(field.stringValue().intern(), 0, field.stringValue().length(), \"untokenized\"));\n          fieldSettings.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Field, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSettings = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSettings.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSettings.position += analyzer.getPositionIncrementGap(fieldSettings.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.termText());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.termText(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSettings.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSettings.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSettings.offset + token.startOffset(), fieldSettings.offset + token.endOffset()));\n          lastOffset = fieldSettings.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSettings.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","bugFix":["e7a005111928c661ab5d236ed6a3a079b438d2cf"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e2cb543b41c145f33390f460ee743d6693c9c6c","date":1219243087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","pathOld":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","sourceNew":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Field field : (List<Field>) document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = field.name().intern();\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isCompressed()) {\n        fieldSetting.compressed = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Field, LinkedList<Token>> tokensByField = new LinkedHashMap<Field, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Field> it = (Iterator<Field>) document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Field field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          int termCounter = 0;\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          final Token reusableToken = new Token();\n          for (Token nextToken = tokenStream.next(reusableToken); nextToken != null; nextToken = tokenStream.next(reusableToken)) {\n            tokens.add((Token) nextToken.clone()); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n            if (fieldSetting.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setTermBuffer(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Field, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.term());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.term(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","sourceOld":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Field field : (List<Field>) document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = field.name().intern();\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isCompressed()) {\n        fieldSetting.compressed = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Field, LinkedList<Token>> tokensByField = new LinkedHashMap<Field, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Field> it = (Iterator<Field>) document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Field field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          int termCounter = 0;\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n          Token next = tokenStream.next();\n\n          while (next != null) {\n            next.setTermText(next.termText().intern()); // todo: not sure this needs to be interned?\n            tokens.add(next); // the vector will be built on commit.\n            next = tokenStream.next();\n            fieldSetting.fieldLength++;\n            if (fieldSetting.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n        } else {\n          // untokenized\n          tokens.add(new Token(field.stringValue().intern(), 0, field.stringValue().length(), \"untokenized\"));\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Field, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.termText());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.termText(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e739da6f23da136433ea563eaa1ac4f1839d0241","date":1229033325,"type":3,"author":"Karl-Johan Wettin","isMerge":false,"pathNew":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","pathOld":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","sourceNew":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Field field : (List<Field>) document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = field.name().intern();\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isCompressed()) {\n        fieldSetting.compressed = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Field, LinkedList<Token>> tokensByField = new LinkedHashMap<Field, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Field> it = (Iterator<Field>) document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Field field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          int termCounter = 0;\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          final Token reusableToken = new Token();\n          for (Token nextToken = tokenStream.next(reusableToken); nextToken != null; nextToken = tokenStream.next(reusableToken)) {\n            tokens.add((Token) nextToken.clone()); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n            if (fieldSetting.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setTermBuffer(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Field, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.term());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.term(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","sourceOld":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Field field : (List<Field>) document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = field.name().intern();\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isCompressed()) {\n        fieldSetting.compressed = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Field, LinkedList<Token>> tokensByField = new LinkedHashMap<Field, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Field> it = (Iterator<Field>) document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Field field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          int termCounter = 0;\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          final Token reusableToken = new Token();\n          for (Token nextToken = tokenStream.next(reusableToken); nextToken != null; nextToken = tokenStream.next(reusableToken)) {\n            tokens.add((Token) nextToken.clone()); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n            if (fieldSetting.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setTermBuffer(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Field, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.term());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.term(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cf93f7a278746d4746fa3ebb3d53267b22fd040f","date":1249495506,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","pathOld":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","sourceNew":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Field field : (List<Field>) document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isCompressed()) {\n        fieldSetting.compressed = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Field, LinkedList<Token>> tokensByField = new LinkedHashMap<Field, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Field> it = (Iterator<Field>) document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Field field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          int termCounter = 0;\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          final Token reusableToken = new Token();\n          for (Token nextToken = tokenStream.next(reusableToken); nextToken != null; nextToken = tokenStream.next(reusableToken)) {\n            tokens.add((Token) nextToken.clone()); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n            if (fieldSetting.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setTermBuffer(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Field, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.term());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.term(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","sourceOld":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Field field : (List<Field>) document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = field.name().intern();\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isCompressed()) {\n        fieldSetting.compressed = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Field, LinkedList<Token>> tokensByField = new LinkedHashMap<Field, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Field> it = (Iterator<Field>) document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Field field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          int termCounter = 0;\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          final Token reusableToken = new Token();\n          for (Token nextToken = tokenStream.next(reusableToken); nextToken != null; nextToken = tokenStream.next(reusableToken)) {\n            tokens.add((Token) nextToken.clone()); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n            if (fieldSetting.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setTermBuffer(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Field, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.term());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.term(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a9bdeeeb073d19ab2bbfdd2b4cda4c9d5005b27","date":1254576225,"type":3,"author":"Karl-Johan Wettin","isMerge":false,"pathNew":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","pathOld":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","sourceNew":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : (List<Fieldable>) document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isCompressed()) {\n        fieldSetting.compressed = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = (Iterator<Fieldable>) document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          int termCounter = 0;\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          final Token reusableToken = new Token();\n          for (Token nextToken = tokenStream.next(reusableToken); nextToken != null; nextToken = tokenStream.next(reusableToken)) {\n            tokens.add((Token) nextToken.clone()); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n            if (fieldSetting.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setTermBuffer(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.term());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.term(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","sourceOld":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Field field : (List<Field>) document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isCompressed()) {\n        fieldSetting.compressed = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Field, LinkedList<Token>> tokensByField = new LinkedHashMap<Field, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Field> it = (Iterator<Field>) document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Field field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          int termCounter = 0;\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          final Token reusableToken = new Token();\n          for (Token nextToken = tokenStream.next(reusableToken); nextToken != null; nextToken = tokenStream.next(reusableToken)) {\n            tokens.add((Token) nextToken.clone()); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n            if (fieldSetting.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setTermBuffer(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Field, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.term());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.term(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e0c804f7aa477229414a7e12882af490c241f64d","date":1254963299,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","pathOld":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","sourceNew":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : (List<Fieldable>) document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = (Iterator<Fieldable>) document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          int termCounter = 0;\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          final Token reusableToken = new Token();\n          for (Token nextToken = tokenStream.next(reusableToken); nextToken != null; nextToken = tokenStream.next(reusableToken)) {\n            tokens.add((Token) nextToken.clone()); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n            if (fieldSetting.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setTermBuffer(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.term());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.term(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","sourceOld":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : (List<Fieldable>) document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isCompressed()) {\n        fieldSetting.compressed = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = (Iterator<Fieldable>) document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          int termCounter = 0;\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          final Token reusableToken = new Token();\n          for (Token nextToken = tokenStream.next(reusableToken); nextToken != null; nextToken = tokenStream.next(reusableToken)) {\n            tokens.add((Token) nextToken.clone()); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n            if (fieldSetting.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setTermBuffer(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.term());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.term(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"439b0fe2f799d1c722151e88e32bdefad8d34ebe","date":1255282509,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","pathOld":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","sourceNew":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : (List<Fieldable>) document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = (Iterator<Fieldable>) document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          int termCounter = 0;\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n            if (fieldSetting.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setTermBuffer(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.term());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.term(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","sourceOld":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : (List<Fieldable>) document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = (Iterator<Fieldable>) document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          int termCounter = 0;\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          final Token reusableToken = new Token();\n          for (Token nextToken = tokenStream.next(reusableToken); nextToken != null; nextToken = tokenStream.next(reusableToken)) {\n            tokens.add((Token) nextToken.clone()); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n            if (fieldSetting.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setTermBuffer(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.term());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.term(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a9ac13b5f0ce5ef1b2ce168367d993a79594b23a","date":1267298041,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","pathOld":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","sourceNew":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n            if (fieldSetting.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setTermBuffer(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.term());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.term(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","sourceOld":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : (List<Fieldable>) document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = (Iterator<Fieldable>) document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          int termCounter = 0;\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n            if (fieldSetting.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setTermBuffer(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.term());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.term(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","pathOld":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#addDocument(InstantiatedDocument,Analyzer).mjava","sourceNew":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n            if (fieldSetting.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setTermBuffer(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.term());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.term(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","sourceOld":"  /**\n   * Tokenizes a document and adds it to the buffer.\n   * Try to do all calculations in this method rather than in commit, as this is a non locking method.\n   * Remember, this index implementation expects unlimited memory for maximum speed.\n   *\n   * @param document\n   * @param analyzer\n   * @throws IOException\n   */\n  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {\n\n    if (document.getDocumentNumber() != null) {\n      throw new RuntimeException(\"Document number already set! Are you trying to add a document that already is bound to this or another index?\");\n    }\n\n    // todo: write lock\n\n    // normalize settings per field name in document\n\n    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (Fieldable field : document.getDocument().getFields()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n      if (fieldSetting == null) {\n        fieldSetting = new FieldSetting();\n        fieldSetting.fieldName = StringHelper.intern(field.name());\n        fieldSettingsByFieldName.put(fieldSetting.fieldName, fieldSetting);\n        fieldNameBuffer.add(fieldSetting.fieldName);\n      }\n\n      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.\n      fieldSetting.boost *= field.getBoost();\n      //fieldSettings.dimensions++;\n\n\n      // once fieldSettings, always fieldSettings.\n      if (field.getOmitNorms()) {\n        fieldSetting.omitNorms = true;\n      }\n      if (field.isIndexed() ) {\n        fieldSetting.indexed = true;\n      }\n      if (field.isTokenized()) {\n        fieldSetting.tokenized = true;\n      }\n      if (field.isStored()) {\n        fieldSetting.stored = true;\n      }\n      if (field.isBinary()) {\n        fieldSetting.isBinary = true;\n      }\n      if (field.isTermVectorStored()) {\n        fieldSetting.storeTermVector = true;\n      }\n      if (field.isStorePositionWithTermVector()) {\n        fieldSetting.storePositionWithTermVector = true;\n      }\n      if (field.isStoreOffsetWithTermVector()) {\n        fieldSetting.storeOffsetWithTermVector = true;\n      }\n    }\n\n    Map<Fieldable, LinkedList<Token>> tokensByField = new LinkedHashMap<Fieldable, LinkedList<Token>>(20);\n\n    // tokenize indexed fields.\n    for (Iterator<Fieldable> it = document.getDocument().getFields().iterator(); it.hasNext();) {\n\n      Fieldable field = it.next();\n\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(field.name());\n\n      if (field.isIndexed()) {\n\n        LinkedList<Token> tokens = new LinkedList<Token>();\n        tokensByField.put(field, tokens);\n\n        if (field.isTokenized()) {\n          final TokenStream tokenStream;\n          // todo readerValue(), binaryValue()\n          if (field.tokenStreamValue() != null) {\n            tokenStream = field.tokenStreamValue();\n          } else {\n            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n          }\n\n          // reset the TokenStream to the first token          \n          tokenStream.reset();\n\n          while (tokenStream.incrementToken()) {\n            // TODO: this is a simple workaround to still work with tokens, not very effective, but as far as I know, this writer should get removed soon:\n            final Token token = new Token();\n            for (Iterator<AttributeImpl> atts = tokenStream.getAttributeImplsIterator(); atts.hasNext();) {\n              final AttributeImpl att = atts.next();\n              try {\n                att.copyTo(token);\n              } catch (Exception e) {\n                // ignore unsupported attributes,\n                // this may fail to copy some attributes, if a special combined AttributeImpl is used, that\n                // implements basic attributes supported by Token and also other customized ones in one class.\n              }\n            }\n            tokens.add(token); // the vector will be built on commit.\n            fieldSetting.fieldLength++;\n            if (fieldSetting.fieldLength > maxFieldLength) {\n              break;\n            }\n          }\n          tokenStream.end();\n          tokenStream.close();\n        } else {\n          // untokenized\n          String fieldVal = field.stringValue();\n          Token token = new Token(0, fieldVal.length(), \"untokenized\");\n          token.setTermBuffer(fieldVal);\n          tokens.add(token);\n          fieldSetting.fieldLength++;\n        }\n      }\n\n      if (!field.isStored()) {\n        it.remove();\n      }\n    }\n\n\n    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();\n    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);\n\n    // build term vector, term positions and term offsets\n    for (Map.Entry<Fieldable, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {\n      FieldSetting fieldSetting = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());\n\n      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));\n      if (termDocumentInformationFactoryByTermText == null) {\n        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();\n        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);\n      }\n\n      int lastOffset = 0;\n\n      // for each new field, move positions a bunch.\n      if (fieldSetting.position > 0) {\n        // todo what if no analyzer set, multiple fields with same name and index without tokenization?\n        fieldSetting.position += analyzer.getPositionIncrementGap(fieldSetting.fieldName);\n      }\n\n      for (Token token : eField_Tokens.getValue()) {\n\n        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.term());\n        if (termDocumentInformationFactory == null) {\n          termDocumentInformationFactory = new TermDocumentInformationFactory();\n          termDocumentInformationFactoryByTermText.put(token.term(), termDocumentInformationFactory);\n        }\n        //termDocumentInformationFactory.termFrequency++;\n\n        fieldSetting.position += (token.getPositionIncrement() - 1);\n        termDocumentInformationFactory.termPositions.add(fieldSetting.position++);\n\n        if (token.getPayload() != null && token.getPayload().length() > 0) {\n          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());\n          fieldSetting.storePayloads = true;\n        } else {\n          termDocumentInformationFactory.payloads.add(null);\n        }\n\n        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n\n          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSetting.offset + token.startOffset(), fieldSetting.offset + token.endOffset()));\n          lastOffset = fieldSetting.offset + token.endOffset();\n        }\n\n\n      }\n\n      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {\n        fieldSetting.offset = lastOffset + 1;\n      }\n\n    }\n\n\n    unflushedDocuments.add(document);\n\n    // if too many documents in buffer, commit.\n    if (unflushedDocuments.size() >= getMergeFactor()) {\n      commit(/*lock*/);\n    }\n\n    // todo: unlock write lock\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"cf93f7a278746d4746fa3ebb3d53267b22fd040f":["e739da6f23da136433ea563eaa1ac4f1839d0241"],"7e2cb543b41c145f33390f460ee743d6693c9c6c":["317c809622a7a74e9257dd0eaf0b7c4dd7399bc7"],"3a9bdeeeb073d19ab2bbfdd2b4cda4c9d5005b27":["cf93f7a278746d4746fa3ebb3d53267b22fd040f"],"317c809622a7a74e9257dd0eaf0b7c4dd7399bc7":["e7a005111928c661ab5d236ed6a3a079b438d2cf"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"e7a005111928c661ab5d236ed6a3a079b438d2cf":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"439b0fe2f799d1c722151e88e32bdefad8d34ebe":["e0c804f7aa477229414a7e12882af490c241f64d"],"e739da6f23da136433ea563eaa1ac4f1839d0241":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"a9ac13b5f0ce5ef1b2ce168367d993a79594b23a":["439b0fe2f799d1c722151e88e32bdefad8d34ebe"],"e0c804f7aa477229414a7e12882af490c241f64d":["3a9bdeeeb073d19ab2bbfdd2b4cda4c9d5005b27"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a9ac13b5f0ce5ef1b2ce168367d993a79594b23a"]},"commit2Childs":{"cf93f7a278746d4746fa3ebb3d53267b22fd040f":["3a9bdeeeb073d19ab2bbfdd2b4cda4c9d5005b27"],"7e2cb543b41c145f33390f460ee743d6693c9c6c":["e739da6f23da136433ea563eaa1ac4f1839d0241"],"3a9bdeeeb073d19ab2bbfdd2b4cda4c9d5005b27":["e0c804f7aa477229414a7e12882af490c241f64d"],"317c809622a7a74e9257dd0eaf0b7c4dd7399bc7":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["e7a005111928c661ab5d236ed6a3a079b438d2cf"],"e7a005111928c661ab5d236ed6a3a079b438d2cf":["317c809622a7a74e9257dd0eaf0b7c4dd7399bc7"],"e739da6f23da136433ea563eaa1ac4f1839d0241":["cf93f7a278746d4746fa3ebb3d53267b22fd040f"],"439b0fe2f799d1c722151e88e32bdefad8d34ebe":["a9ac13b5f0ce5ef1b2ce168367d993a79594b23a"],"e0c804f7aa477229414a7e12882af490c241f64d":["439b0fe2f799d1c722151e88e32bdefad8d34ebe"],"a9ac13b5f0ce5ef1b2ce168367d993a79594b23a":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}