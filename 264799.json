{"path":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(SolrDocument,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","commits":[{"id":"1a9aeb4a98b03660f065aa31f6b3f2251a12b613","date":1581405488,"type":1,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(SolrDocument,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Document,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","sourceNew":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(SolrDocument doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS, DEFAULT_MAX_CHARS);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    try (OffsetWindowTokenFilter tvWindowStream = (tvStream != null && fieldValues.size() > 1) ? new OffsetWindowTokenFilter(tvStream) : null) {\n\n      for (String thisText : fieldValues) {\n        if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n          break;\n        }\n\n        TokenStream tstream;\n        if (tvWindowStream != null) {\n          // if we have a multi-valued field with term vectors, then get the next offset window\n          tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n        } else if (tvStream != null) {\n          tstream = tvStream; // single-valued with term vectors\n        } else {\n          // fall back to analyzer\n          tstream = createAnalyzerTStream(schemaField, thisText);\n        }\n\n        Highlighter highlighter;\n        if (params.getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n          // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n          // needs to implement reset() efficiently.\n\n          //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n          //  It should be okay if OffsetLimit won't get applied in this case.\n          final TokenStream tempTokenStream;\n          if (tstream != tvStream) {\n            if (maxCharsToAnalyze >= thisText.length()) {\n              tempTokenStream = new CachingTokenFilter(tstream);\n            } else {\n              tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n            }\n          } else {\n            tempTokenStream = tstream;\n          }\n\n          // get highlighter\n          highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n          // if the CachingTokenFilter was consumed then use it going forward.\n          if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n            tstream = tempTokenStream;\n          }\n          //tstream.reset(); not needed; getBestTextFragments will reset it.\n        } else {\n          // use \"the old way\"\n          highlighter = getHighlighter(query, fieldName, req);\n        }\n\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n        maxCharsToAnalyze -= thisText.length();\n\n        // Highlight!\n        try {\n          TextFragment[] bestTextFragments =\n              highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n          for (TextFragment bestTextFragment : bestTextFragments) {\n            if (bestTextFragment == null)//can happen via mergeContiguousFragments\n              continue;\n            // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n            if (bestTextFragment.getScore() > 0 || preserveMulti) {\n              frags.add(bestTextFragment);\n              if (bestTextFragment.getScore() > 0)\n                --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n            }\n          }\n        } catch (InvalidTokenOffsetsException e) {\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n        }\n      }//end field value loop\n    }\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, (arg0, arg1) -> Float.compare(arg1.getScore(), arg0.getScore()));\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","sourceOld":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(Document doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS, DEFAULT_MAX_CHARS);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    try (OffsetWindowTokenFilter tvWindowStream = (tvStream != null && fieldValues.size() > 1) ? new OffsetWindowTokenFilter(tvStream) : null) {\n\n      for (String thisText : fieldValues) {\n        if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n          break;\n        }\n\n        TokenStream tstream;\n        if (tvWindowStream != null) {\n          // if we have a multi-valued field with term vectors, then get the next offset window\n          tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n        } else if (tvStream != null) {\n          tstream = tvStream; // single-valued with term vectors\n        } else {\n          // fall back to analyzer\n          tstream = createAnalyzerTStream(schemaField, thisText);\n        }\n\n        Highlighter highlighter;\n        if (params.getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n          // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n          // needs to implement reset() efficiently.\n\n          //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n          //  It should be okay if OffsetLimit won't get applied in this case.\n          final TokenStream tempTokenStream;\n          if (tstream != tvStream) {\n            if (maxCharsToAnalyze >= thisText.length()) {\n              tempTokenStream = new CachingTokenFilter(tstream);\n            } else {\n              tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n            }\n          } else {\n            tempTokenStream = tstream;\n          }\n\n          // get highlighter\n          highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n          // if the CachingTokenFilter was consumed then use it going forward.\n          if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n            tstream = tempTokenStream;\n          }\n          //tstream.reset(); not needed; getBestTextFragments will reset it.\n        } else {\n          // use \"the old way\"\n          highlighter = getHighlighter(query, fieldName, req);\n        }\n\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n        maxCharsToAnalyze -= thisText.length();\n\n        // Highlight!\n        try {\n          TextFragment[] bestTextFragments =\n              highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n          for (TextFragment bestTextFragment : bestTextFragments) {\n            if (bestTextFragment == null)//can happen via mergeContiguousFragments\n              continue;\n            // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n            if (bestTextFragment.getScore() > 0 || preserveMulti) {\n              frags.add(bestTextFragment);\n              if (bestTextFragment.getScore() > 0)\n                --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n            }\n          }\n        } catch (InvalidTokenOffsetsException e) {\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n        }\n      }//end field value loop\n    }\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, (arg0, arg1) -> Float.compare(arg1.getScore(), arg0.getScore()));\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aa2585c33d5d66a1c837c312221eb55ddb3c4300","date":1592493170,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(SolrDocument,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(SolrDocument,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","sourceNew":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(SolrDocument doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n            params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n                    (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n            params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n            HighlightParams.MAX_CHARS, DEFAULT_MAX_CHARS);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n            TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    try (OffsetWindowTokenFilter tvWindowStream = (tvStream != null && fieldValues.size() > 1) ? new OffsetWindowTokenFilter(tvStream) : null) {\n\n      for (String thisText : fieldValues) {\n        if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n          break;\n        }\n\n        TokenStream tstream;\n        if (tvWindowStream != null) {\n          // if we have a multi-valued field with term vectors, then get the next offset window\n          tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n        } else if (tvStream != null) {\n          tstream = tvStream; // single-valued with term vectors\n        } else {\n          // fall back to analyzer\n          tstream = createAnalyzerTStream(schemaField, thisText);\n        }\n\n        Highlighter highlighter;\n        if (params.getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n          // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n          // needs to implement reset() efficiently.\n\n          //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n          //  It should be okay if OffsetLimit won't get applied in this case.\n          final TokenStream tempTokenStream;\n          if (tstream != tvStream) {\n            if (maxCharsToAnalyze >= thisText.length()) {\n              tempTokenStream = new CachingTokenFilter(tstream);\n            } else {\n              tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n            }\n          } else {\n            tempTokenStream = tstream;\n          }\n\n          // get highlighter\n          highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n          // if the CachingTokenFilter was consumed then use it going forward.\n          if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n            tstream = tempTokenStream;\n          }\n          //tstream.reset(); not needed; getBestTextFragments will reset it.\n        } else {\n          // use \"the old way\"\n          highlighter = getHighlighter(query, fieldName, req);\n        }\n\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n        maxCharsToAnalyze -= thisText.length();\n\n        // Highlight!\n        try {\n          TextFragment[] bestTextFragments =\n                  highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n          for (TextFragment bestTextFragment : bestTextFragments) {\n            if (bestTextFragment == null)//can happen via mergeContiguousFragments\n              continue;\n            // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n            if (bestTextFragment.getScore() > 0 || preserveMulti) {\n              frags.add(bestTextFragment);\n              if (bestTextFragment.getScore() > 0)\n                --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n            }\n          }\n        } catch (InvalidTokenOffsetsException e) {\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n        }\n      }//end field value loop\n    }\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, (arg0, arg1) -> Float.compare(arg1.getScore(), arg0.getScore()));\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","sourceOld":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(SolrDocument doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS, DEFAULT_MAX_CHARS);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    try (OffsetWindowTokenFilter tvWindowStream = (tvStream != null && fieldValues.size() > 1) ? new OffsetWindowTokenFilter(tvStream) : null) {\n\n      for (String thisText : fieldValues) {\n        if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n          break;\n        }\n\n        TokenStream tstream;\n        if (tvWindowStream != null) {\n          // if we have a multi-valued field with term vectors, then get the next offset window\n          tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n        } else if (tvStream != null) {\n          tstream = tvStream; // single-valued with term vectors\n        } else {\n          // fall back to analyzer\n          tstream = createAnalyzerTStream(schemaField, thisText);\n        }\n\n        Highlighter highlighter;\n        if (params.getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n          // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n          // needs to implement reset() efficiently.\n\n          //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n          //  It should be okay if OffsetLimit won't get applied in this case.\n          final TokenStream tempTokenStream;\n          if (tstream != tvStream) {\n            if (maxCharsToAnalyze >= thisText.length()) {\n              tempTokenStream = new CachingTokenFilter(tstream);\n            } else {\n              tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n            }\n          } else {\n            tempTokenStream = tstream;\n          }\n\n          // get highlighter\n          highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n          // if the CachingTokenFilter was consumed then use it going forward.\n          if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n            tstream = tempTokenStream;\n          }\n          //tstream.reset(); not needed; getBestTextFragments will reset it.\n        } else {\n          // use \"the old way\"\n          highlighter = getHighlighter(query, fieldName, req);\n        }\n\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n        maxCharsToAnalyze -= thisText.length();\n\n        // Highlight!\n        try {\n          TextFragment[] bestTextFragments =\n              highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n          for (TextFragment bestTextFragment : bestTextFragments) {\n            if (bestTextFragment == null)//can happen via mergeContiguousFragments\n              continue;\n            // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n            if (bestTextFragment.getScore() > 0 || preserveMulti) {\n              frags.add(bestTextFragment);\n              if (bestTextFragment.getScore() > 0)\n                --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n            }\n          }\n        } catch (InvalidTokenOffsetsException e) {\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n        }\n      }//end field value loop\n    }\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, (arg0, arg1) -> Float.compare(arg1.getScore(), arg0.getScore()));\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"aa2585c33d5d66a1c837c312221eb55ddb3c4300":["1a9aeb4a98b03660f065aa31f6b3f2251a12b613"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1a9aeb4a98b03660f065aa31f6b3f2251a12b613":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["aa2585c33d5d66a1c837c312221eb55ddb3c4300"]},"commit2Childs":{"aa2585c33d5d66a1c837c312221eb55ddb3c4300":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1a9aeb4a98b03660f065aa31f6b3f2251a12b613"],"1a9aeb4a98b03660f065aa31f6b3f2251a12b613":["aa2585c33d5d66a1c837c312221eb55ddb3c4300"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}