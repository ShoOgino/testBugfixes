{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random.nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random.nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random, tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    IndexReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random.nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random.nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random, tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    IndexReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    IndexReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random.nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random.nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random, tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    IndexReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","date":1338430031,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    IndexReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d19974432be9aed28ee7dca73bdf01d139e763a9","date":1342822166,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newMockDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":["ff6fd241dc6610f7f81b62e3ba4cedf105939623"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","date":1343059585,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newMockDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aba371508186796cc6151d8223a5b4e16d02e26e","date":1343474871,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newMockDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newMockDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newMockDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newMockDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newMockDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newMockDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.shutdown();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.shutdown();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newMockDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newMockDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(10)\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.shutdown();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setOpenMode(OpenMode.APPEND)\n            .setMaxBufferedDocs(10)\n            .setMergeScheduler(new SerialMergeScheduler())\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.shutdown();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newMockDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.shutdown();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.shutdown();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newMockDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(10)\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setOpenMode(OpenMode.APPEND)\n            .setMaxBufferedDocs(10)\n            .setMergeScheduler(new SerialMergeScheduler())\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newMockDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(10)\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.shutdown();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setOpenMode(OpenMode.APPEND)\n            .setMaxBufferedDocs(10)\n            .setMergeScheduler(new SerialMergeScheduler())\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.shutdown();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"98d2deb8c96c79ebef084a1f8e5a1a6c08608f13","date":1409346855,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newMockDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      // the virus scanner can use up too much disk space :)\n      // an alternative is to expose MDW.triedToDelete and discount it\n      dir.setEnableVirusScanner(false);\n    }\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(10)\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setOpenMode(OpenMode.APPEND)\n            .setMaxBufferedDocs(10)\n            .setMergeScheduler(new SerialMergeScheduler())\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newMockDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(10)\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setOpenMode(OpenMode.APPEND)\n            .setMaxBufferedDocs(10)\n            .setMergeScheduler(new SerialMergeScheduler())\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6bfe104fc023fadc9e709f8d17403d2cc61133fe","date":1454446396,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newMockDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(10)\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setOpenMode(OpenMode.APPEND)\n            .setMaxBufferedDocs(10)\n            .setMergeScheduler(new SerialMergeScheduler())\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newMockDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      // the virus scanner can use up too much disk space :)\n      // an alternative is to expose MDW.triedToDelete and discount it\n      dir.setEnableVirusScanner(false);\n    }\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(10)\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setOpenMode(OpenMode.APPEND)\n            .setMaxBufferedDocs(10)\n            .setMergeScheduler(new SerialMergeScheduler())\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b470f36a9372c97283360b1304eacbde22df6c0d","date":1454765175,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newMockDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(10)\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setOpenMode(OpenMode.APPEND)\n            .setMaxBufferedDocs(10)\n            .setMergeScheduler(new SerialMergeScheduler())\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newMockDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      // the virus scanner can use up too much disk space :)\n      // an alternative is to expose MDW.triedToDelete and discount it\n      dir.setEnableVirusScanner(false);\n    }\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(10)\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setOpenMode(OpenMode.APPEND)\n            .setMaxBufferedDocs(10)\n            .setMergeScheduler(new SerialMergeScheduler())\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1e6acbaae7af722f17204ceccf0f7db5753eccf3","date":1454775255,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newMockDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(10)\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setOpenMode(OpenMode.APPEND)\n            .setMaxBufferedDocs(10)\n            .setMergeScheduler(new SerialMergeScheduler())\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newMockDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      // the virus scanner can use up too much disk space :)\n      // an alternative is to expose MDW.triedToDelete and discount it\n      dir.setEnableVirusScanner(false);\n    }\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(10)\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setOpenMode(OpenMode.APPEND)\n            .setMaxBufferedDocs(10)\n            .setMergeScheduler(new SerialMergeScheduler())\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a207d19eac354d649c3f0e2cce070017c78125e","date":1454776470,"type":3,"author":"Erick Erickson","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newMockDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(10)\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setOpenMode(OpenMode.APPEND)\n            .setMaxBufferedDocs(10)\n            .setMergeScheduler(new SerialMergeScheduler())\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newMockDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      // the virus scanner can use up too much disk space :)\n      // an alternative is to expose MDW.triedToDelete and discount it\n      dir.setEnableVirusScanner(false);\n    }\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(10)\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setOpenMode(OpenMode.APPEND)\n            .setMaxBufferedDocs(10)\n            .setMergeScheduler(new SerialMergeScheduler())\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"24f89e8a6aac05753cde4c83d62a74356098200d","date":1525768331,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    MockDirectoryWrapper dir = newMockDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(10)\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setOpenMode(OpenMode.APPEND)\n            .setMaxBufferedDocs(10)\n            .setMergeScheduler(new SerialMergeScheduler())\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newMockDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(10)\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setOpenMode(OpenMode.APPEND)\n            .setMaxBufferedDocs(10)\n            .setMergeScheduler(new SerialMergeScheduler())\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"71da933d30aea361ccc224d6544c451cbf49916d","date":1579874339,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  // TODO: can this write less docs/indexes?\n  @Nightly\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    MockDirectoryWrapper dir = newMockDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(10)\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setOpenMode(OpenMode.APPEND)\n            .setMaxBufferedDocs(10)\n            .setMergeScheduler(new SerialMergeScheduler())\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    MockDirectoryWrapper dir = newMockDirectory();\n    Analyzer analyzer;\n    if (random().nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random().nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random(), tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(10)\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(analyzer)\n            .setOpenMode(OpenMode.APPEND)\n            .setMaxBufferedDocs(10)\n            .setMergeScheduler(new SerialMergeScheduler())\n            .setReaderPooling(false)\n            .setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    DirectoryReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["d19974432be9aed28ee7dca73bdf01d139e763a9"],"6613659748fe4411a7dcf85266e55db1f95f7315":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5a207d19eac354d649c3f0e2cce070017c78125e":["98d2deb8c96c79ebef084a1f8e5a1a6c08608f13","b470f36a9372c97283360b1304eacbde22df6c0d"],"1e6acbaae7af722f17204ceccf0f7db5753eccf3":["98d2deb8c96c79ebef084a1f8e5a1a6c08608f13","b470f36a9372c97283360b1304eacbde22df6c0d"],"98d2deb8c96c79ebef084a1f8e5a1a6c08608f13":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"aba371508186796cc6151d8223a5b4e16d02e26e":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","d19974432be9aed28ee7dca73bdf01d139e763a9"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"24f89e8a6aac05753cde4c83d62a74356098200d":["5a207d19eac354d649c3f0e2cce070017c78125e"],"d19974432be9aed28ee7dca73bdf01d139e763a9":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"b470f36a9372c97283360b1304eacbde22df6c0d":["98d2deb8c96c79ebef084a1f8e5a1a6c08608f13","6bfe104fc023fadc9e709f8d17403d2cc61133fe"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"6bfe104fc023fadc9e709f8d17403d2cc61133fe":["98d2deb8c96c79ebef084a1f8e5a1a6c08608f13"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["6613659748fe4411a7dcf85266e55db1f95f7315"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","d19974432be9aed28ee7dca73bdf01d139e763a9"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["71da933d30aea361ccc224d6544c451cbf49916d"],"71da933d30aea361ccc224d6544c451cbf49916d":["24f89e8a6aac05753cde4c83d62a74356098200d"]},"commit2Childs":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["6613659748fe4411a7dcf85266e55db1f95f7315"],"6613659748fe4411a7dcf85266e55db1f95f7315":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"5a207d19eac354d649c3f0e2cce070017c78125e":["24f89e8a6aac05753cde4c83d62a74356098200d"],"1e6acbaae7af722f17204ceccf0f7db5753eccf3":[],"98d2deb8c96c79ebef084a1f8e5a1a6c08608f13":["5a207d19eac354d649c3f0e2cce070017c78125e","1e6acbaae7af722f17204ceccf0f7db5753eccf3","b470f36a9372c97283360b1304eacbde22df6c0d","6bfe104fc023fadc9e709f8d17403d2cc61133fe"],"aba371508186796cc6151d8223a5b4e16d02e26e":[],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["aba371508186796cc6151d8223a5b4e16d02e26e","d19974432be9aed28ee7dca73bdf01d139e763a9","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7"],"d19974432be9aed28ee7dca73bdf01d139e763a9":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","aba371508186796cc6151d8223a5b4e16d02e26e","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7"],"24f89e8a6aac05753cde4c83d62a74356098200d":["71da933d30aea361ccc224d6544c451cbf49916d"],"b470f36a9372c97283360b1304eacbde22df6c0d":["5a207d19eac354d649c3f0e2cce070017c78125e","1e6acbaae7af722f17204ceccf0f7db5753eccf3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["98d2deb8c96c79ebef084a1f8e5a1a6c08608f13"],"6bfe104fc023fadc9e709f8d17403d2cc61133fe":["b470f36a9372c97283360b1304eacbde22df6c0d"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":[],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"71da933d30aea361ccc224d6544c451cbf49916d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["1e6acbaae7af722f17204ceccf0f7db5753eccf3","aba371508186796cc6151d8223a5b4e16d02e26e","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}