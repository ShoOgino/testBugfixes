{"path":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testBasicTextLogitStream().mjava","commits":[{"id":"f6b1e64caa933f6fb3c0494afd6ca2597f55cc91","date":1470238980,"type":0,"author":"jbernste","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testBasicTextLogitStream().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testBasicTextLogitStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"destinationCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"destinationCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n    for (int i = 0; i < 5000; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destinationCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    expression = StreamExpressionParser.parse(\"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4)\");\n    stream = new FeaturesSelectionStream(expression, factory);\n    tuples = getTuples(stream);\n\n    assert(tuples.size() == 4);\n    HashSet<String> terms = new HashSet<>();\n    for (Tuple tuple : tuples) {\n      terms.add((String) tuple.get(\"term_s\"));\n    }\n    assertTrue(terms.contains(\"d\"));\n    assertTrue(terms.contains(\"c\"));\n    assertTrue(terms.contains(\"e\"));\n    assertTrue(terms.contains(\"f\"));\n\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    stream = factory.constructStream(textLogitExpression);\n    tuples = getTuples(stream);\n    Tuple lastTuple = tuples.get(tuples.size() - 1);\n    List<Double> lastWeights = lastTuple.getDoubles(\"weights_ds\");\n    Double[] lastWeightsArray = lastWeights.toArray(new Double[lastWeights.size()]);\n\n    // first feature is bias value\n    Double[] testRecord = {1.0, 1.17, 0.691, 0.0, 0.0};\n    double d = sum(multiply(testRecord, lastWeightsArray));\n    double prob = sigmoid(d);\n    assertEquals(prob, 1.0, 0.1);\n\n    // first feature is bias value\n    Double[] testRecord2 = {1.0, 0.0, 0.0, 1.17, 0.691};\n    d = sum(multiply(testRecord2, lastWeightsArray));\n    prob = sigmoid(d);\n    assertEquals(prob, 0, 0.1);\n\n    stream = factory.constructStream(\"update(destinationCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(stream);\n    cluster.getSolrClient().commit(\"destinationCollection\");\n\n    stream = factory.constructStream(\"search(destinationCollection, \" +\n        \"q=*:*, \" +\n        \"fl=\\\"iteration_i,* \\\", \" +\n        \"rows=100, \" +\n        \"sort=\\\"iteration_i desc\\\")\");\n    tuples = getTuples(stream);\n    assertEquals(100, tuples.size());\n    Tuple lastModel = tuples.get(0);\n    ClassificationEvaluation evaluation = ClassificationEvaluation.create(lastModel.fields);\n    assertTrue(evaluation.getF1() >= 1.0);\n    assertEquals(Math.log( 5000.0 / (2500 + 1)), lastModel.getDoubles(\"idfs_ds\").get(0), 0.0001);\n    // make sure the tuples is retrieved in correct order\n    Tuple firstTuple = tuples.get(99);\n    assertEquals(1L, (long) firstTuple.getLong(\"iteration_i\"));\n\n    CollectionAdminRequest.deleteCollection(\"destinationCollection\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3b013574eedcdbac35dc7e35b0ee616ffc38895d","date":1470897818,"type":0,"author":"Noble Paul","isMerge":true,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testBasicTextLogitStream().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testBasicTextLogitStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"destinationCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"destinationCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n    for (int i = 0; i < 5000; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destinationCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    expression = StreamExpressionParser.parse(\"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4)\");\n    stream = new FeaturesSelectionStream(expression, factory);\n    tuples = getTuples(stream);\n\n    assert(tuples.size() == 4);\n    HashSet<String> terms = new HashSet<>();\n    for (Tuple tuple : tuples) {\n      terms.add((String) tuple.get(\"term_s\"));\n    }\n    assertTrue(terms.contains(\"d\"));\n    assertTrue(terms.contains(\"c\"));\n    assertTrue(terms.contains(\"e\"));\n    assertTrue(terms.contains(\"f\"));\n\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    stream = factory.constructStream(textLogitExpression);\n    tuples = getTuples(stream);\n    Tuple lastTuple = tuples.get(tuples.size() - 1);\n    List<Double> lastWeights = lastTuple.getDoubles(\"weights_ds\");\n    Double[] lastWeightsArray = lastWeights.toArray(new Double[lastWeights.size()]);\n\n    // first feature is bias value\n    Double[] testRecord = {1.0, 1.17, 0.691, 0.0, 0.0};\n    double d = sum(multiply(testRecord, lastWeightsArray));\n    double prob = sigmoid(d);\n    assertEquals(prob, 1.0, 0.1);\n\n    // first feature is bias value\n    Double[] testRecord2 = {1.0, 0.0, 0.0, 1.17, 0.691};\n    d = sum(multiply(testRecord2, lastWeightsArray));\n    prob = sigmoid(d);\n    assertEquals(prob, 0, 0.1);\n\n    stream = factory.constructStream(\"update(destinationCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(stream);\n    cluster.getSolrClient().commit(\"destinationCollection\");\n\n    stream = factory.constructStream(\"search(destinationCollection, \" +\n        \"q=*:*, \" +\n        \"fl=\\\"iteration_i,* \\\", \" +\n        \"rows=100, \" +\n        \"sort=\\\"iteration_i desc\\\")\");\n    tuples = getTuples(stream);\n    assertEquals(100, tuples.size());\n    Tuple lastModel = tuples.get(0);\n    ClassificationEvaluation evaluation = ClassificationEvaluation.create(lastModel.fields);\n    assertTrue(evaluation.getF1() >= 1.0);\n    assertEquals(Math.log( 5000.0 / (2500 + 1)), lastModel.getDoubles(\"idfs_ds\").get(0), 0.0001);\n    // make sure the tuples is retrieved in correct order\n    Tuple firstTuple = tuples.get(99);\n    assertEquals(1L, (long) firstTuple.getLong(\"iteration_i\"));\n\n    CollectionAdminRequest.deleteCollection(\"destinationCollection\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testBasicTextLogitStream().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testBasicTextLogitStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"destinationCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"destinationCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n    for (int i = 0; i < 5000; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destinationCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    expression = StreamExpressionParser.parse(\"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4)\");\n    stream = new FeaturesSelectionStream(expression, factory);\n    tuples = getTuples(stream);\n\n    assert(tuples.size() == 4);\n    HashSet<String> terms = new HashSet<>();\n    for (Tuple tuple : tuples) {\n      terms.add((String) tuple.get(\"term_s\"));\n    }\n    assertTrue(terms.contains(\"d\"));\n    assertTrue(terms.contains(\"c\"));\n    assertTrue(terms.contains(\"e\"));\n    assertTrue(terms.contains(\"f\"));\n\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    stream = factory.constructStream(textLogitExpression);\n    tuples = getTuples(stream);\n    Tuple lastTuple = tuples.get(tuples.size() - 1);\n    List<Double> lastWeights = lastTuple.getDoubles(\"weights_ds\");\n    Double[] lastWeightsArray = lastWeights.toArray(new Double[lastWeights.size()]);\n\n    // first feature is bias value\n    Double[] testRecord = {1.0, 1.17, 0.691, 0.0, 0.0};\n    double d = sum(multiply(testRecord, lastWeightsArray));\n    double prob = sigmoid(d);\n    assertEquals(prob, 1.0, 0.1);\n\n    // first feature is bias value\n    Double[] testRecord2 = {1.0, 0.0, 0.0, 1.17, 0.691};\n    d = sum(multiply(testRecord2, lastWeightsArray));\n    prob = sigmoid(d);\n    assertEquals(prob, 0, 0.1);\n\n    stream = factory.constructStream(\"update(destinationCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(stream);\n    cluster.getSolrClient().commit(\"destinationCollection\");\n\n    stream = factory.constructStream(\"search(destinationCollection, \" +\n        \"q=*:*, \" +\n        \"fl=\\\"iteration_i,* \\\", \" +\n        \"rows=100, \" +\n        \"sort=\\\"iteration_i desc\\\")\");\n    tuples = getTuples(stream);\n    assertEquals(100, tuples.size());\n    Tuple lastModel = tuples.get(0);\n    ClassificationEvaluation evaluation = ClassificationEvaluation.create(lastModel.fields);\n    assertTrue(evaluation.getF1() >= 1.0);\n    assertEquals(Math.log( 5000.0 / (2500 + 1)), lastModel.getDoubles(\"idfs_ds\").get(0), 0.0001);\n    // make sure the tuples is retrieved in correct order\n    Tuple firstTuple = tuples.get(99);\n    assertEquals(1L, (long) firstTuple.getLong(\"iteration_i\"));\n\n    CollectionAdminRequest.deleteCollection(\"destinationCollection\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8c969f15cd04d31e520319c619a445ae21f02d72","date":1479263638,"type":3,"author":"Kevin Risden","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testBasicTextLogitStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testBasicTextLogitStream().mjava","sourceNew":"  @Test\n  public void testBasicTextLogitStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"destinationCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"destinationCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n    for (int i = 0; i < 5000; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destinationCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    expression = StreamExpressionParser.parse(\"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4)\");\n    stream = new FeaturesSelectionStream(expression, factory);\n    tuples = getTuples(stream);\n\n    assert(tuples.size() == 4);\n    HashSet<String> terms = new HashSet<>();\n    for (Tuple tuple : tuples) {\n      terms.add((String) tuple.get(\"term_s\"));\n    }\n    assertTrue(terms.contains(\"d\"));\n    assertTrue(terms.contains(\"c\"));\n    assertTrue(terms.contains(\"e\"));\n    assertTrue(terms.contains(\"f\"));\n\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    stream = factory.constructStream(textLogitExpression);\n    tuples = getTuples(stream);\n    Tuple lastTuple = tuples.get(tuples.size() - 1);\n    List<Double> lastWeights = lastTuple.getDoubles(\"weights_ds\");\n    Double[] lastWeightsArray = lastWeights.toArray(new Double[lastWeights.size()]);\n\n    // first feature is bias value\n    Double[] testRecord = {1.0, 1.17, 0.691, 0.0, 0.0};\n    double d = sum(multiply(testRecord, lastWeightsArray));\n    double prob = sigmoid(d);\n    assertEquals(prob, 1.0, 0.1);\n\n    // first feature is bias value\n    Double[] testRecord2 = {1.0, 0.0, 0.0, 1.17, 0.691};\n    d = sum(multiply(testRecord2, lastWeightsArray));\n    prob = sigmoid(d);\n    assertEquals(prob, 0, 0.1);\n\n    stream = factory.constructStream(\"update(destinationCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(stream);\n    cluster.getSolrClient().commit(\"destinationCollection\");\n\n    stream = factory.constructStream(\"search(destinationCollection, \" +\n        \"q=*:*, \" +\n        \"fl=\\\"iteration_i,* \\\", \" +\n        \"rows=100, \" +\n        \"sort=\\\"iteration_i desc\\\")\");\n    tuples = getTuples(stream);\n    assertEquals(100, tuples.size());\n    Tuple lastModel = tuples.get(0);\n    ClassificationEvaluation evaluation = ClassificationEvaluation.create(lastModel.fields);\n    assertTrue(evaluation.getF1() >= 1.0);\n    assertEquals(Math.log( 5000.0 / (2500 + 1)), lastModel.getDoubles(\"idfs_ds\").get(0), 0.0001);\n    // make sure the tuples is retrieved in correct order\n    Tuple firstTuple = tuples.get(99);\n    assertEquals(1L, (long) firstTuple.getLong(\"iteration_i\"));\n\n    CollectionAdminRequest.deleteCollection(\"destinationCollection\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":"  @Test\n  public void testBasicTextLogitStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"destinationCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"destinationCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n    for (int i = 0; i < 5000; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destinationCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    expression = StreamExpressionParser.parse(\"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4)\");\n    stream = new FeaturesSelectionStream(expression, factory);\n    tuples = getTuples(stream);\n\n    assert(tuples.size() == 4);\n    HashSet<String> terms = new HashSet<>();\n    for (Tuple tuple : tuples) {\n      terms.add((String) tuple.get(\"term_s\"));\n    }\n    assertTrue(terms.contains(\"d\"));\n    assertTrue(terms.contains(\"c\"));\n    assertTrue(terms.contains(\"e\"));\n    assertTrue(terms.contains(\"f\"));\n\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    stream = factory.constructStream(textLogitExpression);\n    tuples = getTuples(stream);\n    Tuple lastTuple = tuples.get(tuples.size() - 1);\n    List<Double> lastWeights = lastTuple.getDoubles(\"weights_ds\");\n    Double[] lastWeightsArray = lastWeights.toArray(new Double[lastWeights.size()]);\n\n    // first feature is bias value\n    Double[] testRecord = {1.0, 1.17, 0.691, 0.0, 0.0};\n    double d = sum(multiply(testRecord, lastWeightsArray));\n    double prob = sigmoid(d);\n    assertEquals(prob, 1.0, 0.1);\n\n    // first feature is bias value\n    Double[] testRecord2 = {1.0, 0.0, 0.0, 1.17, 0.691};\n    d = sum(multiply(testRecord2, lastWeightsArray));\n    prob = sigmoid(d);\n    assertEquals(prob, 0, 0.1);\n\n    stream = factory.constructStream(\"update(destinationCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(stream);\n    cluster.getSolrClient().commit(\"destinationCollection\");\n\n    stream = factory.constructStream(\"search(destinationCollection, \" +\n        \"q=*:*, \" +\n        \"fl=\\\"iteration_i,* \\\", \" +\n        \"rows=100, \" +\n        \"sort=\\\"iteration_i desc\\\")\");\n    tuples = getTuples(stream);\n    assertEquals(100, tuples.size());\n    Tuple lastModel = tuples.get(0);\n    ClassificationEvaluation evaluation = ClassificationEvaluation.create(lastModel.fields);\n    assertTrue(evaluation.getF1() >= 1.0);\n    assertEquals(Math.log( 5000.0 / (2500 + 1)), lastModel.getDoubles(\"idfs_ds\").get(0), 0.0001);\n    // make sure the tuples is retrieved in correct order\n    Tuple firstTuple = tuples.get(99);\n    assertEquals(1L, (long) firstTuple.getLong(\"iteration_i\"));\n\n    CollectionAdminRequest.deleteCollection(\"destinationCollection\").process(cluster.getSolrClient());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a1ef55e1fff7ff44354432770ad8bc19be1fcc75","date":1479266056,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testBasicTextLogitStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testBasicTextLogitStream().mjava","sourceNew":"  @Test\n  public void testBasicTextLogitStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"destinationCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"destinationCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n    for (int i = 0; i < 5000; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destinationCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    expression = StreamExpressionParser.parse(\"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4)\");\n    stream = new FeaturesSelectionStream(expression, factory);\n    tuples = getTuples(stream);\n\n    assert(tuples.size() == 4);\n    HashSet<String> terms = new HashSet<>();\n    for (Tuple tuple : tuples) {\n      terms.add((String) tuple.get(\"term_s\"));\n    }\n    assertTrue(terms.contains(\"d\"));\n    assertTrue(terms.contains(\"c\"));\n    assertTrue(terms.contains(\"e\"));\n    assertTrue(terms.contains(\"f\"));\n\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    stream = factory.constructStream(textLogitExpression);\n    tuples = getTuples(stream);\n    Tuple lastTuple = tuples.get(tuples.size() - 1);\n    List<Double> lastWeights = lastTuple.getDoubles(\"weights_ds\");\n    Double[] lastWeightsArray = lastWeights.toArray(new Double[lastWeights.size()]);\n\n    // first feature is bias value\n    Double[] testRecord = {1.0, 1.17, 0.691, 0.0, 0.0};\n    double d = sum(multiply(testRecord, lastWeightsArray));\n    double prob = sigmoid(d);\n    assertEquals(prob, 1.0, 0.1);\n\n    // first feature is bias value\n    Double[] testRecord2 = {1.0, 0.0, 0.0, 1.17, 0.691};\n    d = sum(multiply(testRecord2, lastWeightsArray));\n    prob = sigmoid(d);\n    assertEquals(prob, 0, 0.1);\n\n    stream = factory.constructStream(\"update(destinationCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(stream);\n    cluster.getSolrClient().commit(\"destinationCollection\");\n\n    stream = factory.constructStream(\"search(destinationCollection, \" +\n        \"q=*:*, \" +\n        \"fl=\\\"iteration_i,* \\\", \" +\n        \"rows=100, \" +\n        \"sort=\\\"iteration_i desc\\\")\");\n    tuples = getTuples(stream);\n    assertEquals(100, tuples.size());\n    Tuple lastModel = tuples.get(0);\n    ClassificationEvaluation evaluation = ClassificationEvaluation.create(lastModel.fields);\n    assertTrue(evaluation.getF1() >= 1.0);\n    assertEquals(Math.log( 5000.0 / (2500 + 1)), lastModel.getDoubles(\"idfs_ds\").get(0), 0.0001);\n    // make sure the tuples is retrieved in correct order\n    Tuple firstTuple = tuples.get(99);\n    assertEquals(1L, (long) firstTuple.getLong(\"iteration_i\"));\n\n    CollectionAdminRequest.deleteCollection(\"destinationCollection\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":"  @Test\n  public void testBasicTextLogitStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"destinationCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"destinationCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n    for (int i = 0; i < 5000; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destinationCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    expression = StreamExpressionParser.parse(\"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4)\");\n    stream = new FeaturesSelectionStream(expression, factory);\n    tuples = getTuples(stream);\n\n    assert(tuples.size() == 4);\n    HashSet<String> terms = new HashSet<>();\n    for (Tuple tuple : tuples) {\n      terms.add((String) tuple.get(\"term_s\"));\n    }\n    assertTrue(terms.contains(\"d\"));\n    assertTrue(terms.contains(\"c\"));\n    assertTrue(terms.contains(\"e\"));\n    assertTrue(terms.contains(\"f\"));\n\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    stream = factory.constructStream(textLogitExpression);\n    tuples = getTuples(stream);\n    Tuple lastTuple = tuples.get(tuples.size() - 1);\n    List<Double> lastWeights = lastTuple.getDoubles(\"weights_ds\");\n    Double[] lastWeightsArray = lastWeights.toArray(new Double[lastWeights.size()]);\n\n    // first feature is bias value\n    Double[] testRecord = {1.0, 1.17, 0.691, 0.0, 0.0};\n    double d = sum(multiply(testRecord, lastWeightsArray));\n    double prob = sigmoid(d);\n    assertEquals(prob, 1.0, 0.1);\n\n    // first feature is bias value\n    Double[] testRecord2 = {1.0, 0.0, 0.0, 1.17, 0.691};\n    d = sum(multiply(testRecord2, lastWeightsArray));\n    prob = sigmoid(d);\n    assertEquals(prob, 0, 0.1);\n\n    stream = factory.constructStream(\"update(destinationCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(stream);\n    cluster.getSolrClient().commit(\"destinationCollection\");\n\n    stream = factory.constructStream(\"search(destinationCollection, \" +\n        \"q=*:*, \" +\n        \"fl=\\\"iteration_i,* \\\", \" +\n        \"rows=100, \" +\n        \"sort=\\\"iteration_i desc\\\")\");\n    tuples = getTuples(stream);\n    assertEquals(100, tuples.size());\n    Tuple lastModel = tuples.get(0);\n    ClassificationEvaluation evaluation = ClassificationEvaluation.create(lastModel.fields);\n    assertTrue(evaluation.getF1() >= 1.0);\n    assertEquals(Math.log( 5000.0 / (2500 + 1)), lastModel.getDoubles(\"idfs_ds\").get(0), 0.0001);\n    // make sure the tuples is retrieved in correct order\n    Tuple firstTuple = tuples.get(99);\n    assertEquals(1L, (long) firstTuple.getLong(\"iteration_i\"));\n\n    CollectionAdminRequest.deleteCollection(\"destinationCollection\").process(cluster.getSolrClient());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c405288c4553ffb50ab8ca5adbdde9881bcec4e4","date":1491938682,"type":3,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testBasicTextLogitStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testBasicTextLogitStream().mjava","sourceNew":"  @Test\n  public void testBasicTextLogitStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"destinationCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"destinationCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n    for (int i = 0; i < 5000; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n    StreamContext streamContext = new StreamContext();\n    SolrClientCache solrClientCache = new SolrClientCache();\n    streamContext.setSolrClientCache(solrClientCache);\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destinationCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n    try {\n      expression = StreamExpressionParser.parse(\"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4)\");\n      stream = new FeaturesSelectionStream(expression, factory);\n      stream.setStreamContext(streamContext);\n      tuples = getTuples(stream);\n\n      assert (tuples.size() == 4);\n      HashSet<String> terms = new HashSet<>();\n      for (Tuple tuple : tuples) {\n        terms.add((String) tuple.get(\"term_s\"));\n      }\n      assertTrue(terms.contains(\"d\"));\n      assertTrue(terms.contains(\"c\"));\n      assertTrue(terms.contains(\"e\"));\n      assertTrue(terms.contains(\"f\"));\n\n      String textLogitExpression = \"train(\" +\n          \"collection1, \" +\n          \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\" +\n          \"q=\\\"*:*\\\", \" +\n          \"name=\\\"model\\\", \" +\n          \"field=\\\"tv_text\\\", \" +\n          \"outcome=\\\"out_i\\\", \" +\n          \"maxIterations=100)\";\n      stream = factory.constructStream(textLogitExpression);\n      stream.setStreamContext(streamContext);\n      tuples = getTuples(stream);\n      Tuple lastTuple = tuples.get(tuples.size() - 1);\n      List<Double> lastWeights = lastTuple.getDoubles(\"weights_ds\");\n      Double[] lastWeightsArray = lastWeights.toArray(new Double[lastWeights.size()]);\n\n      // first feature is bias value\n      Double[] testRecord = {1.0, 1.17, 0.691, 0.0, 0.0};\n      double d = sum(multiply(testRecord, lastWeightsArray));\n      double prob = sigmoid(d);\n      assertEquals(prob, 1.0, 0.1);\n\n      // first feature is bias value\n      Double[] testRecord2 = {1.0, 0.0, 0.0, 1.17, 0.691};\n      d = sum(multiply(testRecord2, lastWeightsArray));\n      prob = sigmoid(d);\n      assertEquals(prob, 0, 0.1);\n\n      stream = factory.constructStream(\"update(destinationCollection, batchSize=5, \" + textLogitExpression + \")\");\n      getTuples(stream);\n      cluster.getSolrClient().commit(\"destinationCollection\");\n\n      stream = factory.constructStream(\"search(destinationCollection, \" +\n          \"q=*:*, \" +\n          \"fl=\\\"iteration_i,* \\\", \" +\n          \"rows=100, \" +\n          \"sort=\\\"iteration_i desc\\\")\");\n      stream.setStreamContext(streamContext);\n      tuples = getTuples(stream);\n      assertEquals(100, tuples.size());\n      Tuple lastModel = tuples.get(0);\n      ClassificationEvaluation evaluation = ClassificationEvaluation.create(lastModel.fields);\n      assertTrue(evaluation.getF1() >= 1.0);\n      assertEquals(Math.log(5000.0 / (2500 + 1)), lastModel.getDoubles(\"idfs_ds\").get(0), 0.0001);\n      // make sure the tuples is retrieved in correct order\n      Tuple firstTuple = tuples.get(99);\n      assertEquals(1L, (long) firstTuple.getLong(\"iteration_i\"));\n    } finally {\n      CollectionAdminRequest.deleteCollection(\"destinationCollection\").process(cluster.getSolrClient());\n      solrClientCache.close();\n    }\n  }\n\n","sourceOld":"  @Test\n  public void testBasicTextLogitStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"destinationCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"destinationCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n    for (int i = 0; i < 5000; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destinationCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    expression = StreamExpressionParser.parse(\"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4)\");\n    stream = new FeaturesSelectionStream(expression, factory);\n    tuples = getTuples(stream);\n\n    assert(tuples.size() == 4);\n    HashSet<String> terms = new HashSet<>();\n    for (Tuple tuple : tuples) {\n      terms.add((String) tuple.get(\"term_s\"));\n    }\n    assertTrue(terms.contains(\"d\"));\n    assertTrue(terms.contains(\"c\"));\n    assertTrue(terms.contains(\"e\"));\n    assertTrue(terms.contains(\"f\"));\n\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    stream = factory.constructStream(textLogitExpression);\n    tuples = getTuples(stream);\n    Tuple lastTuple = tuples.get(tuples.size() - 1);\n    List<Double> lastWeights = lastTuple.getDoubles(\"weights_ds\");\n    Double[] lastWeightsArray = lastWeights.toArray(new Double[lastWeights.size()]);\n\n    // first feature is bias value\n    Double[] testRecord = {1.0, 1.17, 0.691, 0.0, 0.0};\n    double d = sum(multiply(testRecord, lastWeightsArray));\n    double prob = sigmoid(d);\n    assertEquals(prob, 1.0, 0.1);\n\n    // first feature is bias value\n    Double[] testRecord2 = {1.0, 0.0, 0.0, 1.17, 0.691};\n    d = sum(multiply(testRecord2, lastWeightsArray));\n    prob = sigmoid(d);\n    assertEquals(prob, 0, 0.1);\n\n    stream = factory.constructStream(\"update(destinationCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(stream);\n    cluster.getSolrClient().commit(\"destinationCollection\");\n\n    stream = factory.constructStream(\"search(destinationCollection, \" +\n        \"q=*:*, \" +\n        \"fl=\\\"iteration_i,* \\\", \" +\n        \"rows=100, \" +\n        \"sort=\\\"iteration_i desc\\\")\");\n    tuples = getTuples(stream);\n    assertEquals(100, tuples.size());\n    Tuple lastModel = tuples.get(0);\n    ClassificationEvaluation evaluation = ClassificationEvaluation.create(lastModel.fields);\n    assertTrue(evaluation.getF1() >= 1.0);\n    assertEquals(Math.log( 5000.0 / (2500 + 1)), lastModel.getDoubles(\"idfs_ds\").get(0), 0.0001);\n    // make sure the tuples is retrieved in correct order\n    Tuple firstTuple = tuples.get(99);\n    assertEquals(1L, (long) firstTuple.getLong(\"iteration_i\"));\n\n    CollectionAdminRequest.deleteCollection(\"destinationCollection\").process(cluster.getSolrClient());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"54ca69905c5d9d1529286f06ab1d12c68f6c13cb","date":1492683554,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testBasicTextLogitStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testBasicTextLogitStream().mjava","sourceNew":"  @Test\n  public void testBasicTextLogitStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"destinationCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"destinationCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n    for (int i = 0; i < 5000; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n    StreamContext streamContext = new StreamContext();\n    SolrClientCache solrClientCache = new SolrClientCache();\n    streamContext.setSolrClientCache(solrClientCache);\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destinationCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n    try {\n      expression = StreamExpressionParser.parse(\"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4)\");\n      stream = new FeaturesSelectionStream(expression, factory);\n      stream.setStreamContext(streamContext);\n      tuples = getTuples(stream);\n\n      assert (tuples.size() == 4);\n      HashSet<String> terms = new HashSet<>();\n      for (Tuple tuple : tuples) {\n        terms.add((String) tuple.get(\"term_s\"));\n      }\n      assertTrue(terms.contains(\"d\"));\n      assertTrue(terms.contains(\"c\"));\n      assertTrue(terms.contains(\"e\"));\n      assertTrue(terms.contains(\"f\"));\n\n      String textLogitExpression = \"train(\" +\n          \"collection1, \" +\n          \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\" +\n          \"q=\\\"*:*\\\", \" +\n          \"name=\\\"model\\\", \" +\n          \"field=\\\"tv_text\\\", \" +\n          \"outcome=\\\"out_i\\\", \" +\n          \"maxIterations=100)\";\n      stream = factory.constructStream(textLogitExpression);\n      stream.setStreamContext(streamContext);\n      tuples = getTuples(stream);\n      Tuple lastTuple = tuples.get(tuples.size() - 1);\n      List<Double> lastWeights = lastTuple.getDoubles(\"weights_ds\");\n      Double[] lastWeightsArray = lastWeights.toArray(new Double[lastWeights.size()]);\n\n      // first feature is bias value\n      Double[] testRecord = {1.0, 1.17, 0.691, 0.0, 0.0};\n      double d = sum(multiply(testRecord, lastWeightsArray));\n      double prob = sigmoid(d);\n      assertEquals(prob, 1.0, 0.1);\n\n      // first feature is bias value\n      Double[] testRecord2 = {1.0, 0.0, 0.0, 1.17, 0.691};\n      d = sum(multiply(testRecord2, lastWeightsArray));\n      prob = sigmoid(d);\n      assertEquals(prob, 0, 0.1);\n\n      stream = factory.constructStream(\"update(destinationCollection, batchSize=5, \" + textLogitExpression + \")\");\n      getTuples(stream);\n      cluster.getSolrClient().commit(\"destinationCollection\");\n\n      stream = factory.constructStream(\"search(destinationCollection, \" +\n          \"q=*:*, \" +\n          \"fl=\\\"iteration_i,* \\\", \" +\n          \"rows=100, \" +\n          \"sort=\\\"iteration_i desc\\\")\");\n      stream.setStreamContext(streamContext);\n      tuples = getTuples(stream);\n      assertEquals(100, tuples.size());\n      Tuple lastModel = tuples.get(0);\n      ClassificationEvaluation evaluation = ClassificationEvaluation.create(lastModel.fields);\n      assertTrue(evaluation.getF1() >= 1.0);\n      assertEquals(Math.log(5000.0 / (2500 + 1)), lastModel.getDoubles(\"idfs_ds\").get(0), 0.0001);\n      // make sure the tuples is retrieved in correct order\n      Tuple firstTuple = tuples.get(99);\n      assertEquals(1L, (long) firstTuple.getLong(\"iteration_i\"));\n    } finally {\n      CollectionAdminRequest.deleteCollection(\"destinationCollection\").process(cluster.getSolrClient());\n      solrClientCache.close();\n    }\n  }\n\n","sourceOld":"  @Test\n  public void testBasicTextLogitStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"destinationCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"destinationCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n    for (int i = 0; i < 5000; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destinationCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    expression = StreamExpressionParser.parse(\"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4)\");\n    stream = new FeaturesSelectionStream(expression, factory);\n    tuples = getTuples(stream);\n\n    assert(tuples.size() == 4);\n    HashSet<String> terms = new HashSet<>();\n    for (Tuple tuple : tuples) {\n      terms.add((String) tuple.get(\"term_s\"));\n    }\n    assertTrue(terms.contains(\"d\"));\n    assertTrue(terms.contains(\"c\"));\n    assertTrue(terms.contains(\"e\"));\n    assertTrue(terms.contains(\"f\"));\n\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    stream = factory.constructStream(textLogitExpression);\n    tuples = getTuples(stream);\n    Tuple lastTuple = tuples.get(tuples.size() - 1);\n    List<Double> lastWeights = lastTuple.getDoubles(\"weights_ds\");\n    Double[] lastWeightsArray = lastWeights.toArray(new Double[lastWeights.size()]);\n\n    // first feature is bias value\n    Double[] testRecord = {1.0, 1.17, 0.691, 0.0, 0.0};\n    double d = sum(multiply(testRecord, lastWeightsArray));\n    double prob = sigmoid(d);\n    assertEquals(prob, 1.0, 0.1);\n\n    // first feature is bias value\n    Double[] testRecord2 = {1.0, 0.0, 0.0, 1.17, 0.691};\n    d = sum(multiply(testRecord2, lastWeightsArray));\n    prob = sigmoid(d);\n    assertEquals(prob, 0, 0.1);\n\n    stream = factory.constructStream(\"update(destinationCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(stream);\n    cluster.getSolrClient().commit(\"destinationCollection\");\n\n    stream = factory.constructStream(\"search(destinationCollection, \" +\n        \"q=*:*, \" +\n        \"fl=\\\"iteration_i,* \\\", \" +\n        \"rows=100, \" +\n        \"sort=\\\"iteration_i desc\\\")\");\n    tuples = getTuples(stream);\n    assertEquals(100, tuples.size());\n    Tuple lastModel = tuples.get(0);\n    ClassificationEvaluation evaluation = ClassificationEvaluation.create(lastModel.fields);\n    assertTrue(evaluation.getF1() >= 1.0);\n    assertEquals(Math.log( 5000.0 / (2500 + 1)), lastModel.getDoubles(\"idfs_ds\").get(0), 0.0001);\n    // make sure the tuples is retrieved in correct order\n    Tuple firstTuple = tuples.get(99);\n    assertEquals(1L, (long) firstTuple.getLong(\"iteration_i\"));\n\n    CollectionAdminRequest.deleteCollection(\"destinationCollection\").process(cluster.getSolrClient());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bb222a3f9d9421d5c95afce73013fbd8de07ea1f","date":1543514331,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testBasicTextLogitStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testBasicTextLogitStream().mjava","sourceNew":"  @Test\n  public void testBasicTextLogitStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"destinationCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    cluster.waitForActiveCollection(\"destinationCollection\", 2, 2);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n    for (int i = 0; i < 5000; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n    StreamContext streamContext = new StreamContext();\n    SolrClientCache solrClientCache = new SolrClientCache();\n    streamContext.setSolrClientCache(solrClientCache);\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destinationCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n    try {\n      expression = StreamExpressionParser.parse(\"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4)\");\n      stream = new FeaturesSelectionStream(expression, factory);\n      stream.setStreamContext(streamContext);\n      tuples = getTuples(stream);\n\n      assert (tuples.size() == 4);\n      HashSet<String> terms = new HashSet<>();\n      for (Tuple tuple : tuples) {\n        terms.add((String) tuple.get(\"term_s\"));\n      }\n      assertTrue(terms.contains(\"d\"));\n      assertTrue(terms.contains(\"c\"));\n      assertTrue(terms.contains(\"e\"));\n      assertTrue(terms.contains(\"f\"));\n\n      String textLogitExpression = \"train(\" +\n          \"collection1, \" +\n          \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\" +\n          \"q=\\\"*:*\\\", \" +\n          \"name=\\\"model\\\", \" +\n          \"field=\\\"tv_text\\\", \" +\n          \"outcome=\\\"out_i\\\", \" +\n          \"maxIterations=100)\";\n      stream = factory.constructStream(textLogitExpression);\n      stream.setStreamContext(streamContext);\n      tuples = getTuples(stream);\n      Tuple lastTuple = tuples.get(tuples.size() - 1);\n      List<Double> lastWeights = lastTuple.getDoubles(\"weights_ds\");\n      Double[] lastWeightsArray = lastWeights.toArray(new Double[lastWeights.size()]);\n\n      // first feature is bias value\n      Double[] testRecord = {1.0, 1.17, 0.691, 0.0, 0.0};\n      double d = sum(multiply(testRecord, lastWeightsArray));\n      double prob = sigmoid(d);\n      assertEquals(prob, 1.0, 0.1);\n\n      // first feature is bias value\n      Double[] testRecord2 = {1.0, 0.0, 0.0, 1.17, 0.691};\n      d = sum(multiply(testRecord2, lastWeightsArray));\n      prob = sigmoid(d);\n      assertEquals(prob, 0, 0.1);\n\n      stream = factory.constructStream(\"update(destinationCollection, batchSize=5, \" + textLogitExpression + \")\");\n      getTuples(stream);\n      cluster.getSolrClient().commit(\"destinationCollection\");\n\n      stream = factory.constructStream(\"search(destinationCollection, \" +\n          \"q=*:*, \" +\n          \"fl=\\\"iteration_i,* \\\", \" +\n          \"rows=100, \" +\n          \"sort=\\\"iteration_i desc\\\")\");\n      stream.setStreamContext(streamContext);\n      tuples = getTuples(stream);\n      assertEquals(100, tuples.size());\n      Tuple lastModel = tuples.get(0);\n      ClassificationEvaluation evaluation = ClassificationEvaluation.create(lastModel.fields);\n      assertTrue(evaluation.getF1() >= 1.0);\n      assertEquals(Math.log(5000.0 / (2500 + 1)), lastModel.getDoubles(\"idfs_ds\").get(0), 0.0001);\n      // make sure the tuples is retrieved in correct order\n      Tuple firstTuple = tuples.get(99);\n      assertEquals(1L, (long) firstTuple.getLong(\"iteration_i\"));\n    } finally {\n      CollectionAdminRequest.deleteCollection(\"destinationCollection\").process(cluster.getSolrClient());\n      solrClientCache.close();\n    }\n  }\n\n","sourceOld":"  @Test\n  public void testBasicTextLogitStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"destinationCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"destinationCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n    for (int i = 0; i < 5000; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n    StreamContext streamContext = new StreamContext();\n    SolrClientCache solrClientCache = new SolrClientCache();\n    streamContext.setSolrClientCache(solrClientCache);\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destinationCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n    try {\n      expression = StreamExpressionParser.parse(\"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4)\");\n      stream = new FeaturesSelectionStream(expression, factory);\n      stream.setStreamContext(streamContext);\n      tuples = getTuples(stream);\n\n      assert (tuples.size() == 4);\n      HashSet<String> terms = new HashSet<>();\n      for (Tuple tuple : tuples) {\n        terms.add((String) tuple.get(\"term_s\"));\n      }\n      assertTrue(terms.contains(\"d\"));\n      assertTrue(terms.contains(\"c\"));\n      assertTrue(terms.contains(\"e\"));\n      assertTrue(terms.contains(\"f\"));\n\n      String textLogitExpression = \"train(\" +\n          \"collection1, \" +\n          \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\" +\n          \"q=\\\"*:*\\\", \" +\n          \"name=\\\"model\\\", \" +\n          \"field=\\\"tv_text\\\", \" +\n          \"outcome=\\\"out_i\\\", \" +\n          \"maxIterations=100)\";\n      stream = factory.constructStream(textLogitExpression);\n      stream.setStreamContext(streamContext);\n      tuples = getTuples(stream);\n      Tuple lastTuple = tuples.get(tuples.size() - 1);\n      List<Double> lastWeights = lastTuple.getDoubles(\"weights_ds\");\n      Double[] lastWeightsArray = lastWeights.toArray(new Double[lastWeights.size()]);\n\n      // first feature is bias value\n      Double[] testRecord = {1.0, 1.17, 0.691, 0.0, 0.0};\n      double d = sum(multiply(testRecord, lastWeightsArray));\n      double prob = sigmoid(d);\n      assertEquals(prob, 1.0, 0.1);\n\n      // first feature is bias value\n      Double[] testRecord2 = {1.0, 0.0, 0.0, 1.17, 0.691};\n      d = sum(multiply(testRecord2, lastWeightsArray));\n      prob = sigmoid(d);\n      assertEquals(prob, 0, 0.1);\n\n      stream = factory.constructStream(\"update(destinationCollection, batchSize=5, \" + textLogitExpression + \")\");\n      getTuples(stream);\n      cluster.getSolrClient().commit(\"destinationCollection\");\n\n      stream = factory.constructStream(\"search(destinationCollection, \" +\n          \"q=*:*, \" +\n          \"fl=\\\"iteration_i,* \\\", \" +\n          \"rows=100, \" +\n          \"sort=\\\"iteration_i desc\\\")\");\n      stream.setStreamContext(streamContext);\n      tuples = getTuples(stream);\n      assertEquals(100, tuples.size());\n      Tuple lastModel = tuples.get(0);\n      ClassificationEvaluation evaluation = ClassificationEvaluation.create(lastModel.fields);\n      assertTrue(evaluation.getF1() >= 1.0);\n      assertEquals(Math.log(5000.0 / (2500 + 1)), lastModel.getDoubles(\"idfs_ds\").get(0), 0.0001);\n      // make sure the tuples is retrieved in correct order\n      Tuple firstTuple = tuples.get(99);\n      assertEquals(1L, (long) firstTuple.getLong(\"iteration_i\"));\n    } finally {\n      CollectionAdminRequest.deleteCollection(\"destinationCollection\").process(cluster.getSolrClient());\n      solrClientCache.close();\n    }\n  }\n\n","bugFix":["f6b1e64caa933f6fb3c0494afd6ca2597f55cc91"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"804a8d5358fe7b7563b85ee7838714d720b89272","date":1591624987,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testBasicTextLogitStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testBasicTextLogitStream().mjava","sourceNew":"  @Test\n  public void testBasicTextLogitStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"destinationCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    cluster.waitForActiveCollection(\"destinationCollection\", 2, 2);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n    for (int i = 0; i < 5000; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n    StreamContext streamContext = new StreamContext();\n    SolrClientCache solrClientCache = new SolrClientCache();\n    streamContext.setSolrClientCache(solrClientCache);\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destinationCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n    try {\n      expression = StreamExpressionParser.parse(\"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4)\");\n      stream = new FeaturesSelectionStream(expression, factory);\n      stream.setStreamContext(streamContext);\n      tuples = getTuples(stream);\n\n      assert (tuples.size() == 4);\n      HashSet<String> terms = new HashSet<>();\n      for (Tuple tuple : tuples) {\n        terms.add((String) tuple.get(\"term_s\"));\n      }\n      assertTrue(terms.contains(\"d\"));\n      assertTrue(terms.contains(\"c\"));\n      assertTrue(terms.contains(\"e\"));\n      assertTrue(terms.contains(\"f\"));\n\n      String textLogitExpression = \"train(\" +\n          \"collection1, \" +\n          \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\" +\n          \"q=\\\"*:*\\\", \" +\n          \"name=\\\"model\\\", \" +\n          \"field=\\\"tv_text\\\", \" +\n          \"outcome=\\\"out_i\\\", \" +\n          \"maxIterations=100)\";\n      stream = factory.constructStream(textLogitExpression);\n      stream.setStreamContext(streamContext);\n      tuples = getTuples(stream);\n      Tuple lastTuple = tuples.get(tuples.size() - 1);\n      List<Double> lastWeights = lastTuple.getDoubles(\"weights_ds\");\n      Double[] lastWeightsArray = lastWeights.toArray(new Double[lastWeights.size()]);\n\n      // first feature is bias value\n      Double[] testRecord = {1.0, 1.17, 0.691, 0.0, 0.0};\n      double d = sum(multiply(testRecord, lastWeightsArray));\n      double prob = sigmoid(d);\n      assertEquals(prob, 1.0, 0.1);\n\n      // first feature is bias value\n      Double[] testRecord2 = {1.0, 0.0, 0.0, 1.17, 0.691};\n      d = sum(multiply(testRecord2, lastWeightsArray));\n      prob = sigmoid(d);\n      assertEquals(prob, 0, 0.1);\n\n      stream = factory.constructStream(\"update(destinationCollection, batchSize=5, \" + textLogitExpression + \")\");\n      getTuples(stream);\n      cluster.getSolrClient().commit(\"destinationCollection\");\n\n      stream = factory.constructStream(\"search(destinationCollection, \" +\n          \"q=*:*, \" +\n          \"fl=\\\"iteration_i,* \\\", \" +\n          \"rows=100, \" +\n          \"sort=\\\"iteration_i desc\\\")\");\n      stream.setStreamContext(streamContext);\n      tuples = getTuples(stream);\n      assertEquals(100, tuples.size());\n      Tuple lastModel = tuples.get(0);\n      ClassificationEvaluation evaluation = ClassificationEvaluation.create(lastModel.getFields());\n      assertTrue(evaluation.getF1() >= 1.0);\n      assertEquals(Math.log(5000.0 / (2500 + 1)), lastModel.getDoubles(\"idfs_ds\").get(0), 0.0001);\n      // make sure the tuples is retrieved in correct order\n      Tuple firstTuple = tuples.get(99);\n      assertEquals(1L, (long) firstTuple.getLong(\"iteration_i\"));\n    } finally {\n      CollectionAdminRequest.deleteCollection(\"destinationCollection\").process(cluster.getSolrClient());\n      solrClientCache.close();\n    }\n  }\n\n","sourceOld":"  @Test\n  public void testBasicTextLogitStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"destinationCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    cluster.waitForActiveCollection(\"destinationCollection\", 2, 2);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n    for (int i = 0; i < 5000; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n    StreamContext streamContext = new StreamContext();\n    SolrClientCache solrClientCache = new SolrClientCache();\n    streamContext.setSolrClientCache(solrClientCache);\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destinationCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n    try {\n      expression = StreamExpressionParser.parse(\"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4)\");\n      stream = new FeaturesSelectionStream(expression, factory);\n      stream.setStreamContext(streamContext);\n      tuples = getTuples(stream);\n\n      assert (tuples.size() == 4);\n      HashSet<String> terms = new HashSet<>();\n      for (Tuple tuple : tuples) {\n        terms.add((String) tuple.get(\"term_s\"));\n      }\n      assertTrue(terms.contains(\"d\"));\n      assertTrue(terms.contains(\"c\"));\n      assertTrue(terms.contains(\"e\"));\n      assertTrue(terms.contains(\"f\"));\n\n      String textLogitExpression = \"train(\" +\n          \"collection1, \" +\n          \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\" +\n          \"q=\\\"*:*\\\", \" +\n          \"name=\\\"model\\\", \" +\n          \"field=\\\"tv_text\\\", \" +\n          \"outcome=\\\"out_i\\\", \" +\n          \"maxIterations=100)\";\n      stream = factory.constructStream(textLogitExpression);\n      stream.setStreamContext(streamContext);\n      tuples = getTuples(stream);\n      Tuple lastTuple = tuples.get(tuples.size() - 1);\n      List<Double> lastWeights = lastTuple.getDoubles(\"weights_ds\");\n      Double[] lastWeightsArray = lastWeights.toArray(new Double[lastWeights.size()]);\n\n      // first feature is bias value\n      Double[] testRecord = {1.0, 1.17, 0.691, 0.0, 0.0};\n      double d = sum(multiply(testRecord, lastWeightsArray));\n      double prob = sigmoid(d);\n      assertEquals(prob, 1.0, 0.1);\n\n      // first feature is bias value\n      Double[] testRecord2 = {1.0, 0.0, 0.0, 1.17, 0.691};\n      d = sum(multiply(testRecord2, lastWeightsArray));\n      prob = sigmoid(d);\n      assertEquals(prob, 0, 0.1);\n\n      stream = factory.constructStream(\"update(destinationCollection, batchSize=5, \" + textLogitExpression + \")\");\n      getTuples(stream);\n      cluster.getSolrClient().commit(\"destinationCollection\");\n\n      stream = factory.constructStream(\"search(destinationCollection, \" +\n          \"q=*:*, \" +\n          \"fl=\\\"iteration_i,* \\\", \" +\n          \"rows=100, \" +\n          \"sort=\\\"iteration_i desc\\\")\");\n      stream.setStreamContext(streamContext);\n      tuples = getTuples(stream);\n      assertEquals(100, tuples.size());\n      Tuple lastModel = tuples.get(0);\n      ClassificationEvaluation evaluation = ClassificationEvaluation.create(lastModel.fields);\n      assertTrue(evaluation.getF1() >= 1.0);\n      assertEquals(Math.log(5000.0 / (2500 + 1)), lastModel.getDoubles(\"idfs_ds\").get(0), 0.0001);\n      // make sure the tuples is retrieved in correct order\n      Tuple firstTuple = tuples.get(99);\n      assertEquals(1L, (long) firstTuple.getLong(\"iteration_i\"));\n    } finally {\n      CollectionAdminRequest.deleteCollection(\"destinationCollection\").process(cluster.getSolrClient());\n      solrClientCache.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"54ca69905c5d9d1529286f06ab1d12c68f6c13cb":["8c969f15cd04d31e520319c619a445ae21f02d72"],"bb222a3f9d9421d5c95afce73013fbd8de07ea1f":["c405288c4553ffb50ab8ca5adbdde9881bcec4e4"],"f6b1e64caa933f6fb3c0494afd6ca2597f55cc91":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a1ef55e1fff7ff44354432770ad8bc19be1fcc75":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","8c969f15cd04d31e520319c619a445ae21f02d72"],"c405288c4553ffb50ab8ca5adbdde9881bcec4e4":["8c969f15cd04d31e520319c619a445ae21f02d72"],"8c969f15cd04d31e520319c619a445ae21f02d72":["f6b1e64caa933f6fb3c0494afd6ca2597f55cc91"],"804a8d5358fe7b7563b85ee7838714d720b89272":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f6b1e64caa933f6fb3c0494afd6ca2597f55cc91"],"3b013574eedcdbac35dc7e35b0ee616ffc38895d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f6b1e64caa933f6fb3c0494afd6ca2597f55cc91"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["804a8d5358fe7b7563b85ee7838714d720b89272"]},"commit2Childs":{"54ca69905c5d9d1529286f06ab1d12c68f6c13cb":[],"bb222a3f9d9421d5c95afce73013fbd8de07ea1f":["804a8d5358fe7b7563b85ee7838714d720b89272"],"f6b1e64caa933f6fb3c0494afd6ca2597f55cc91":["8c969f15cd04d31e520319c619a445ae21f02d72","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","3b013574eedcdbac35dc7e35b0ee616ffc38895d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["f6b1e64caa933f6fb3c0494afd6ca2597f55cc91","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","3b013574eedcdbac35dc7e35b0ee616ffc38895d"],"a1ef55e1fff7ff44354432770ad8bc19be1fcc75":[],"c405288c4553ffb50ab8ca5adbdde9881bcec4e4":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"8c969f15cd04d31e520319c619a445ae21f02d72":["54ca69905c5d9d1529286f06ab1d12c68f6c13cb","a1ef55e1fff7ff44354432770ad8bc19be1fcc75","c405288c4553ffb50ab8ca5adbdde9881bcec4e4"],"804a8d5358fe7b7563b85ee7838714d720b89272":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a1ef55e1fff7ff44354432770ad8bc19be1fcc75"],"3b013574eedcdbac35dc7e35b0ee616ffc38895d":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["54ca69905c5d9d1529286f06ab1d12c68f6c13cb","a1ef55e1fff7ff44354432770ad8bc19be1fcc75","3b013574eedcdbac35dc7e35b0ee616ffc38895d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}