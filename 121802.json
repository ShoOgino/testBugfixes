{"path":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","commits":[{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":1,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3c71d502dea2f9d6ed3d8783f510ea3254435de9","date":1318266042,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      final Object arr;\n      if (ordReader.hasArray()) {\n        arr = ordReader.getArray();\n      } else {\n        arr = null;\n      }\n\n      if (arr instanceof int[]) {\n        int[] ords = (int[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (arr instanceof short[]) {\n        short[] ords = (short[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (arr instanceof byte[]) {\n        byte[] ords = (byte[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2dd6ecb8250c497ed227653279d6a4f470bfbb31","date":1326814483,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(new SlowMultiReaderWrapper(searcher.getIndexReader()), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      final Object arr;\n      if (ordReader.hasArray()) {\n        arr = ordReader.getArray();\n      } else {\n        arr = null;\n      }\n\n      if (arr instanceof int[]) {\n        int[] ords = (int[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (arr instanceof short[]) {\n        short[] ords = (short[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (arr instanceof byte[]) {\n        byte[] ords = (byte[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      final Object arr;\n      if (ordReader.hasArray()) {\n        arr = ordReader.getArray();\n      } else {\n        arr = null;\n      }\n\n      if (arr instanceof int[]) {\n        int[] ords = (int[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (arr instanceof short[]) {\n        short[] ords = (short[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (arr instanceof byte[]) {\n        byte[] ords = (byte[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"96d207426bd26fa5c1014e26d21d87603aea68b7","date":1327944562,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      final Object arr;\n      if (ordReader.hasArray()) {\n        arr = ordReader.getArray();\n      } else {\n        arr = null;\n      }\n\n      if (arr instanceof int[]) {\n        int[] ords = (int[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (arr instanceof short[]) {\n        short[] ords = (short[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (arr instanceof byte[]) {\n        byte[] ords = (byte[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(new SlowMultiReaderWrapper(searcher.getIndexReader()), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      final Object arr;\n      if (ordReader.hasArray()) {\n        arr = ordReader.getArray();\n      } else {\n        arr = null;\n      }\n\n      if (arr instanceof int[]) {\n        int[] ords = (int[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (arr instanceof short[]) {\n        short[] ords = (short[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (arr instanceof byte[]) {\n        byte[] ords = (byte[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      final Object arr;\n      if (ordReader.hasArray()) {\n        arr = ordReader.getArray();\n      } else {\n        arr = null;\n      }\n\n      if (arr instanceof int[]) {\n        int[] ords = (int[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (arr instanceof short[]) {\n        short[] ords = (short[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (arr instanceof byte[]) {\n        byte[] ords = (byte[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(new SlowMultiReaderWrapper(searcher.getIndexReader()), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      final Object arr;\n      if (ordReader.hasArray()) {\n        arr = ordReader.getArray();\n      } else {\n        arr = null;\n      }\n\n      if (arr instanceof int[]) {\n        int[] ords = (int[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (arr instanceof short[]) {\n        short[] ords = (short[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (arr instanceof byte[]) {\n        byte[] ords = (byte[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6e2893fd5349134af382d33ccc3d84840394c6c1","date":1353682567,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      while (iter.hasNext()) {\n        int term = si.getOrd(iter.nextDoc());\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      final Object arr;\n      if (ordReader.hasArray()) {\n        arr = ordReader.getArray();\n      } else {\n        arr = null;\n      }\n\n      if (arr instanceof int[]) {\n        int[] ords = (int[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (arr instanceof short[]) {\n        short[] ords = (short[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (arr instanceof byte[]) {\n        byte[] ords = (byte[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9d7e5f3aa5935964617824d1f9b2599ddb334464","date":1353762831,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    SortedDocValues si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.lookupTerm(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.lookupTerm(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      while (iter.hasNext()) {\n        int term = si.getOrd(iter.nextDoc());\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      while (iter.hasNext()) {\n        int term = si.getOrd(iter.nextDoc());\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3d5a5a419065ad4cdc70485f83fa9fe9373aaa8b","date":1359664357,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    SortedDocValues si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), fieldName);\n\n    final BytesRef br = new BytesRef();\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      while (iter.hasNext()) {\n        int term = si.getOrd(iter.nextDoc());\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    SortedDocValues si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.lookupTerm(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.lookupTerm(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      while (iter.hasNext()) {\n        int term = si.getOrd(iter.nextDoc());\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    SortedDocValues si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), fieldName);\n\n    final BytesRef br = new BytesRef();\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      while (iter.hasNext()) {\n        int term = si.getOrd(iter.nextDoc());\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      final Object arr;\n      if (ordReader.hasArray()) {\n        arr = ordReader.getArray();\n      } else {\n        arr = null;\n      }\n\n      if (arr instanceof int[]) {\n        int[] ords = (int[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (arr instanceof short[]) {\n        short[] ords = (short[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (arr instanceof byte[]) {\n        byte[] ords = (byte[]) arr;\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":["3c71d502dea2f9d6ed3d8783f510ea3254435de9","96d207426bd26fa5c1014e26d21d87603aea68b7","be20f9fed1d3edcb1c84abcc39df87a90fab22df","5a902fceaa7c10b5669d1ed631319bd619378ca7","e789a6ee3a5fa41394cbed2293ff68c3712c32a9","627ce218a5a68018115c2deb6559b41e3665b8ab","fd8b97815b7381ece49de90a42c654c24169a26c","a1b3a24d5d9b47345473ff564f5cc127a7b526b4"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<>();\n\n    SortedDocValues si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), fieldName);\n\n    final BytesRef br = new BytesRef();\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      while (iter.hasNext()) {\n        int term = si.getOrd(iter.nextDoc());\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    SortedDocValues si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), fieldName);\n\n    final BytesRef br = new BytesRef();\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      while (iter.hasNext()) {\n        int term = si.getOrd(iter.nextDoc());\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e28b14e7783d24ca69089f13ddadadbd2afdcb29","date":1399840701,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<>();\n\n    SortedDocValues si = DocValues.getSorted(searcher.getAtomicReader(), fieldName);\n\n    final BytesRef br = new BytesRef();\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      while (iter.hasNext()) {\n        int term = si.getOrd(iter.nextDoc());\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<>();\n\n    SortedDocValues si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), fieldName);\n\n    final BytesRef br = new BytesRef();\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      while (iter.hasNext()) {\n        int term = si.getOrd(iter.nextDoc());\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30b6ad849a21206db510322a3f583ca70ae20a2f","date":1399996150,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":null,"sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<>();\n\n    SortedDocValues si = DocValues.getSorted(searcher.getAtomicReader(), fieldName);\n\n    final BytesRef br = new BytesRef();\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      while (iter.hasNext()) {\n        int term = si.getOrd(iter.nextDoc());\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"93dd449115a9247533e44bab47e8429e5dccbc6d","date":1400258396,"type":4,"author":"Robert Muir","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":null,"sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<>();\n\n    SortedDocValues si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), fieldName);\n\n    final BytesRef br = new BytesRef();\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      while (iter.hasNext()) {\n        int term = si.getOrd(iter.nextDoc());\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"56572ec06f1407c066d6b7399413178b33176cd8","date":1400495675,"type":4,"author":"Michael McCandless","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":null,"sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<>();\n\n    SortedDocValues si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), fieldName);\n\n    final BytesRef br = new BytesRef();\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.lookupTerm(prefixRef);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.lookupTerm(prefixRef);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=-1;\n      endTermIndex=si.getValueCount();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      while (iter.hasNext()) {\n        int term = si.getOrd(iter.nextDoc());\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      if (startTermIndex == -1) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          si.lookupOrd(startTermIndex+tnum, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==-1)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          si.lookupOrd(startTermIndex+i, br);\n          ft.indexedToReadable(br, charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"6e2893fd5349134af382d33ccc3d84840394c6c1":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"c26f00b574427b55127e869b935845554afde1fa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"96d207426bd26fa5c1014e26d21d87603aea68b7":["2dd6ecb8250c497ed227653279d6a4f470bfbb31"],"3d5a5a419065ad4cdc70485f83fa9fe9373aaa8b":["9d7e5f3aa5935964617824d1f9b2599ddb334464"],"56572ec06f1407c066d6b7399413178b33176cd8":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","93dd449115a9247533e44bab47e8429e5dccbc6d"],"2dd6ecb8250c497ed227653279d6a4f470bfbb31":["3c71d502dea2f9d6ed3d8783f510ea3254435de9"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["2dd6ecb8250c497ed227653279d6a4f470bfbb31","96d207426bd26fa5c1014e26d21d87603aea68b7"],"3c71d502dea2f9d6ed3d8783f510ea3254435de9":["c26f00b574427b55127e869b935845554afde1fa"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["5cab9a86bd67202d20b6adc463008c8e982b070a","3d5a5a419065ad4cdc70485f83fa9fe9373aaa8b"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","30b6ad849a21206db510322a3f583ca70ae20a2f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9d7e5f3aa5935964617824d1f9b2599ddb334464":["6e2893fd5349134af382d33ccc3d84840394c6c1"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"30b6ad849a21206db510322a3f583ca70ae20a2f":["e28b14e7783d24ca69089f13ddadadbd2afdcb29"],"e28b14e7783d24ca69089f13ddadadbd2afdcb29":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["93dd449115a9247533e44bab47e8429e5dccbc6d"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["56572ec06f1407c066d6b7399413178b33176cd8","93dd449115a9247533e44bab47e8429e5dccbc6d","e28b14e7783d24ca69089f13ddadadbd2afdcb29"],"6e2893fd5349134af382d33ccc3d84840394c6c1":["9d7e5f3aa5935964617824d1f9b2599ddb334464"],"c26f00b574427b55127e869b935845554afde1fa":["3c71d502dea2f9d6ed3d8783f510ea3254435de9"],"96d207426bd26fa5c1014e26d21d87603aea68b7":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"3d5a5a419065ad4cdc70485f83fa9fe9373aaa8b":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"56572ec06f1407c066d6b7399413178b33176cd8":[],"5cab9a86bd67202d20b6adc463008c8e982b070a":["6e2893fd5349134af382d33ccc3d84840394c6c1","d4d69c535930b5cce125cff868d40f6373dc27d4"],"2dd6ecb8250c497ed227653279d6a4f470bfbb31":["96d207426bd26fa5c1014e26d21d87603aea68b7","5cab9a86bd67202d20b6adc463008c8e982b070a"],"3c71d502dea2f9d6ed3d8783f510ea3254435de9":["2dd6ecb8250c497ed227653279d6a4f470bfbb31"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["56572ec06f1407c066d6b7399413178b33176cd8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c26f00b574427b55127e869b935845554afde1fa","c903c3d15906a3da96b8c0c2fb704491005fdbdb","a258fbb26824fd104ed795e5d9033d2d040049ee"],"9d7e5f3aa5935964617824d1f9b2599ddb334464":["3d5a5a419065ad4cdc70485f83fa9fe9373aaa8b"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"30b6ad849a21206db510322a3f583ca70ae20a2f":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"e28b14e7783d24ca69089f13ddadadbd2afdcb29":["30b6ad849a21206db510322a3f583ca70ae20a2f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["56572ec06f1407c066d6b7399413178b33176cd8","a258fbb26824fd104ed795e5d9033d2d040049ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}