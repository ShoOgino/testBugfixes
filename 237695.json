{"path":"lucene/src/java/org/apache/lucene/index/SegmentMerger#setDocMaps().mjava","commits":[{"id":"ccad4bab070f323ce610caa0040346d4a87213dc","date":1327747432,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#setDocMaps().mjava","pathOld":"/dev/null","sourceNew":"  // NOTE: removes any \"all deleted\" readers from mergeState.readers\n  private void setDocMaps() throws IOException {\n    final int numReaders = mergeState.readers.size();\n\n    // Remap docIDs\n    mergeState.docMaps = new int[numReaders][];\n    mergeState.docBase = new int[numReaders];\n    mergeState.segmentDocCounts = new HashMap<SegmentInfo,Integer>();\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[numReaders];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[numReaders];\n\n    int docBase = 0;\n\n    int i = 0;\n    while(i < mergeState.readers.size()) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      final int maxDoc = reader.reader.maxDoc();\n      final int docCount;\n      final Bits liveDocs = reader.liveDocs;\n      final int[] docMap;\n      if (liveDocs != null) {\n        int delCount = 0;\n        docMap = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docCount = maxDoc - delCount;\n      } else {\n        docCount = maxDoc;\n        docMap = null;\n      }\n\n      if (reader.reader instanceof SegmentReader) {\n        mergeState.segmentDocCounts.put(((SegmentReader) reader.reader).getSegmentInfo(), docCount);\n      }\n\n      if (docCount == 0) {\n        // Skip this reader (all docs are deleted):\n        mergeState.readers.remove(i);\n        continue;\n      }\n\n      mergeState.docMaps[i] = docMap;\n      docBase += docCount;\n\n      if (mergeState.payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = mergeState.payloadProcessorProvider.getDirProcessor(reader.reader.directory());\n      }\n\n      i++;\n    }\n\n    final int numReadersLeft = mergeState.readers.size();\n\n    if (numReadersLeft < mergeState.docMaps.length) {\n      mergeState.docMaps = shrink(mergeState.docMaps, numReadersLeft);\n      mergeState.docBase = shrink(mergeState.docBase, numReadersLeft);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["c95a819869502635864dac0a788f874787e3395b"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d3d53ad7b9687e0dfaacaab099d3c7e98bfbe78e","date":1327757020,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#setDocMaps().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#setDocMaps().mjava","sourceNew":"  // NOTE: removes any \"all deleted\" readers from mergeState.readers\n  private void setDocMaps() throws IOException {\n    final int numReaders = mergeState.readers.size();\n\n    // Remap docIDs\n    mergeState.docMaps = new int[numReaders][];\n    mergeState.docBase = new int[numReaders];\n    mergeState.segmentDocCounts = new HashMap<SegmentInfo,Integer>();\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[numReaders];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[numReaders];\n\n    int docBase = 0;\n\n    int i = 0;\n    while(i < mergeState.readers.size()) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(i);\n\n      // nocommit -- assert that final doc count ==\n      // mergedDocCount from stored fields and term vectors\n      mergeState.docBase[i] = docBase;\n      final int maxDoc = reader.reader.maxDoc();\n      final int docCount;\n      final Bits liveDocs = reader.liveDocs;\n      final int[] docMap;\n      if (liveDocs != null) {\n        int delCount = 0;\n        docMap = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docCount = maxDoc - delCount;\n      } else {\n        docCount = maxDoc;\n        docMap = null;\n      }\n\n      if (reader.reader instanceof SegmentReader) {\n        mergeState.segmentDocCounts.put(((SegmentReader) reader.reader).getSegmentInfo(), docCount);\n      }\n\n      if (docCount == 0) {\n        // Skip this reader (all docs are deleted):\n        mergeState.readers.remove(i);\n        continue;\n      }\n\n      mergeState.docMaps[i] = docMap;\n      docBase += docCount;\n\n      if (mergeState.payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = mergeState.payloadProcessorProvider.getDirProcessor(reader.reader.directory());\n      }\n\n      i++;\n    }\n\n    final int numReadersLeft = mergeState.readers.size();\n\n    if (numReadersLeft < mergeState.docMaps.length) {\n      mergeState.docMaps = shrink(mergeState.docMaps, numReadersLeft);\n      mergeState.docBase = shrink(mergeState.docBase, numReadersLeft);\n    }\n  }\n\n","sourceOld":"  // NOTE: removes any \"all deleted\" readers from mergeState.readers\n  private void setDocMaps() throws IOException {\n    final int numReaders = mergeState.readers.size();\n\n    // Remap docIDs\n    mergeState.docMaps = new int[numReaders][];\n    mergeState.docBase = new int[numReaders];\n    mergeState.segmentDocCounts = new HashMap<SegmentInfo,Integer>();\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[numReaders];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[numReaders];\n\n    int docBase = 0;\n\n    int i = 0;\n    while(i < mergeState.readers.size()) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      final int maxDoc = reader.reader.maxDoc();\n      final int docCount;\n      final Bits liveDocs = reader.liveDocs;\n      final int[] docMap;\n      if (liveDocs != null) {\n        int delCount = 0;\n        docMap = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docCount = maxDoc - delCount;\n      } else {\n        docCount = maxDoc;\n        docMap = null;\n      }\n\n      if (reader.reader instanceof SegmentReader) {\n        mergeState.segmentDocCounts.put(((SegmentReader) reader.reader).getSegmentInfo(), docCount);\n      }\n\n      if (docCount == 0) {\n        // Skip this reader (all docs are deleted):\n        mergeState.readers.remove(i);\n        continue;\n      }\n\n      mergeState.docMaps[i] = docMap;\n      docBase += docCount;\n\n      if (mergeState.payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = mergeState.payloadProcessorProvider.getDirProcessor(reader.reader.directory());\n      }\n\n      i++;\n    }\n\n    final int numReadersLeft = mergeState.readers.size();\n\n    if (numReadersLeft < mergeState.docMaps.length) {\n      mergeState.docMaps = shrink(mergeState.docMaps, numReadersLeft);\n      mergeState.docBase = shrink(mergeState.docBase, numReadersLeft);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5a6eeb3bd5b45df414522e8c152b36db43eb18bf","date":1327761784,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#setDocMaps().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#setDocMaps().mjava","sourceNew":"  // NOTE: removes any \"all deleted\" readers from mergeState.readers\n  private int setDocMaps() throws IOException {\n    final int numReaders = mergeState.readers.size();\n\n    // Remap docIDs\n    mergeState.docMaps = new int[numReaders][];\n    mergeState.docBase = new int[numReaders];\n    mergeState.segmentDocCounts = new HashMap<SegmentInfo,Integer>();\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[numReaders];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[numReaders];\n\n    int docBase = 0;\n\n    int i = 0;\n    while(i < mergeState.readers.size()) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      final int maxDoc = reader.reader.maxDoc();\n      final int docCount;\n      final Bits liveDocs = reader.liveDocs;\n      final int[] docMap;\n      if (liveDocs != null) {\n        int delCount = 0;\n        docMap = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docCount = maxDoc - delCount;\n      } else {\n        docCount = maxDoc;\n        docMap = null;\n      }\n\n      if (reader.reader instanceof SegmentReader) {\n        mergeState.segmentDocCounts.put(((SegmentReader) reader.reader).getSegmentInfo(), docCount);\n      }\n\n      if (docCount == 0) {\n        // Skip this reader (all docs are deleted):\n        mergeState.readers.remove(i);\n        continue;\n      }\n\n      mergeState.docMaps[i] = docMap;\n      docBase += docCount;\n\n      if (mergeState.payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = mergeState.payloadProcessorProvider.getDirProcessor(reader.reader.directory());\n      }\n\n      i++;\n    }\n\n    final int numReadersLeft = mergeState.readers.size();\n\n    if (numReadersLeft < mergeState.docMaps.length) {\n      mergeState.docMaps = shrink(mergeState.docMaps, numReadersLeft);\n      mergeState.docBase = shrink(mergeState.docBase, numReadersLeft);\n    }\n    \n    return docBase;\n  }\n\n","sourceOld":"  // NOTE: removes any \"all deleted\" readers from mergeState.readers\n  private void setDocMaps() throws IOException {\n    final int numReaders = mergeState.readers.size();\n\n    // Remap docIDs\n    mergeState.docMaps = new int[numReaders][];\n    mergeState.docBase = new int[numReaders];\n    mergeState.segmentDocCounts = new HashMap<SegmentInfo,Integer>();\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[numReaders];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[numReaders];\n\n    int docBase = 0;\n\n    int i = 0;\n    while(i < mergeState.readers.size()) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(i);\n\n      // nocommit -- assert that final doc count ==\n      // mergedDocCount from stored fields and term vectors\n      mergeState.docBase[i] = docBase;\n      final int maxDoc = reader.reader.maxDoc();\n      final int docCount;\n      final Bits liveDocs = reader.liveDocs;\n      final int[] docMap;\n      if (liveDocs != null) {\n        int delCount = 0;\n        docMap = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docCount = maxDoc - delCount;\n      } else {\n        docCount = maxDoc;\n        docMap = null;\n      }\n\n      if (reader.reader instanceof SegmentReader) {\n        mergeState.segmentDocCounts.put(((SegmentReader) reader.reader).getSegmentInfo(), docCount);\n      }\n\n      if (docCount == 0) {\n        // Skip this reader (all docs are deleted):\n        mergeState.readers.remove(i);\n        continue;\n      }\n\n      mergeState.docMaps[i] = docMap;\n      docBase += docCount;\n\n      if (mergeState.payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = mergeState.payloadProcessorProvider.getDirProcessor(reader.reader.directory());\n      }\n\n      i++;\n    }\n\n    final int numReadersLeft = mergeState.readers.size();\n\n    if (numReadersLeft < mergeState.docMaps.length) {\n      mergeState.docMaps = shrink(mergeState.docMaps, numReadersLeft);\n      mergeState.docBase = shrink(mergeState.docBase, numReadersLeft);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"32feb7c2c571b402d2e231bd8e3b6add4af6d6eb","date":1327773585,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#setDocMaps().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#setDocMaps().mjava","sourceNew":"  // NOTE: removes any \"all deleted\" readers from mergeState.readers\n  private int setDocMaps() throws IOException {\n    final int numReaders = mergeState.readers.size();\n\n    // Remap docIDs\n    mergeState.docMaps = new int[numReaders][];\n    mergeState.docBase = new int[numReaders];\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[numReaders];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[numReaders];\n\n    int docBase = 0;\n\n    int i = 0;\n    while(i < mergeState.readers.size()) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      final int maxDoc = reader.reader.maxDoc();\n      final int docCount;\n      final Bits liveDocs = reader.liveDocs;\n      final int[] docMap;\n      if (liveDocs != null) {\n        int delCount = 0;\n        docMap = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docCount = maxDoc - delCount;\n      } else {\n        docCount = maxDoc;\n        docMap = null;\n      }\n\n      mergeState.docMaps[i] = docMap;\n      docBase += docCount;\n\n      if (mergeState.payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = mergeState.payloadProcessorProvider.getDirProcessor(reader.reader.directory());\n      }\n\n      i++;\n    }\n\n    return docBase;\n  }\n\n","sourceOld":"  // NOTE: removes any \"all deleted\" readers from mergeState.readers\n  private int setDocMaps() throws IOException {\n    final int numReaders = mergeState.readers.size();\n\n    // Remap docIDs\n    mergeState.docMaps = new int[numReaders][];\n    mergeState.docBase = new int[numReaders];\n    mergeState.segmentDocCounts = new HashMap<SegmentInfo,Integer>();\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[numReaders];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[numReaders];\n\n    int docBase = 0;\n\n    int i = 0;\n    while(i < mergeState.readers.size()) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      final int maxDoc = reader.reader.maxDoc();\n      final int docCount;\n      final Bits liveDocs = reader.liveDocs;\n      final int[] docMap;\n      if (liveDocs != null) {\n        int delCount = 0;\n        docMap = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docCount = maxDoc - delCount;\n      } else {\n        docCount = maxDoc;\n        docMap = null;\n      }\n\n      if (reader.reader instanceof SegmentReader) {\n        mergeState.segmentDocCounts.put(((SegmentReader) reader.reader).getSegmentInfo(), docCount);\n      }\n\n      if (docCount == 0) {\n        // Skip this reader (all docs are deleted):\n        mergeState.readers.remove(i);\n        continue;\n      }\n\n      mergeState.docMaps[i] = docMap;\n      docBase += docCount;\n\n      if (mergeState.payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = mergeState.payloadProcessorProvider.getDirProcessor(reader.reader.directory());\n      }\n\n      i++;\n    }\n\n    final int numReadersLeft = mergeState.readers.size();\n\n    if (numReadersLeft < mergeState.docMaps.length) {\n      mergeState.docMaps = shrink(mergeState.docMaps, numReadersLeft);\n      mergeState.docBase = shrink(mergeState.docBase, numReadersLeft);\n    }\n    \n    return docBase;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31","date":1327836826,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#setDocMaps().mjava","pathOld":"/dev/null","sourceNew":"  // NOTE: removes any \"all deleted\" readers from mergeState.readers\n  private int setDocMaps() throws IOException {\n    final int numReaders = mergeState.readers.size();\n\n    // Remap docIDs\n    mergeState.docMaps = new int[numReaders][];\n    mergeState.docBase = new int[numReaders];\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[numReaders];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[numReaders];\n\n    int docBase = 0;\n\n    int i = 0;\n    while(i < mergeState.readers.size()) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      final int maxDoc = reader.reader.maxDoc();\n      final int docCount;\n      final Bits liveDocs = reader.liveDocs;\n      final int[] docMap;\n      if (liveDocs != null) {\n        int delCount = 0;\n        docMap = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docCount = maxDoc - delCount;\n      } else {\n        docCount = maxDoc;\n        docMap = null;\n      }\n\n      mergeState.docMaps[i] = docMap;\n      docBase += docCount;\n\n      if (mergeState.payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = mergeState.payloadProcessorProvider.getDirProcessor(reader.reader.directory());\n      }\n\n      i++;\n    }\n\n    return docBase;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fd92b8bcc88e969302510acf77bd6970da3994c4","date":1327839530,"type":0,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#setDocMaps().mjava","pathOld":"/dev/null","sourceNew":"  // NOTE: removes any \"all deleted\" readers from mergeState.readers\n  private int setDocMaps() throws IOException {\n    final int numReaders = mergeState.readers.size();\n\n    // Remap docIDs\n    mergeState.docMaps = new int[numReaders][];\n    mergeState.docBase = new int[numReaders];\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[numReaders];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[numReaders];\n\n    int docBase = 0;\n\n    int i = 0;\n    while(i < mergeState.readers.size()) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      final int maxDoc = reader.reader.maxDoc();\n      final int docCount;\n      final Bits liveDocs = reader.liveDocs;\n      final int[] docMap;\n      if (liveDocs != null) {\n        int delCount = 0;\n        docMap = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docCount = maxDoc - delCount;\n      } else {\n        docCount = maxDoc;\n        docMap = null;\n      }\n\n      mergeState.docMaps[i] = docMap;\n      docBase += docCount;\n\n      if (mergeState.payloadProcessorProvider != null) {\n        // nocommit: this was original, is the change correct:\n        // mergeState.dirPayloadProcessor[i] = mergeState.payloadProcessorProvider.getDirProcessor(reader.reader.directory());\n        mergeState.dirPayloadProcessor[i] = mergeState.payloadProcessorProvider.getDirProcessor(directory);\n      }\n\n      i++;\n    }\n\n    return docBase;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9d5248bdfe89f464cce628bfb37cfa62a19ff022","date":1327876084,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#setDocMaps().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#setDocMaps().mjava","sourceNew":"  // NOTE: removes any \"all deleted\" readers from mergeState.readers\n  private int setDocMaps() throws IOException {\n    final int numReaders = mergeState.readers.size();\n\n    // Remap docIDs\n    mergeState.docMaps = new int[numReaders][];\n    mergeState.docBase = new int[numReaders];\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[numReaders];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[numReaders];\n\n    int docBase = 0;\n\n    int i = 0;\n    while(i < mergeState.readers.size()) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      final int maxDoc = reader.reader.maxDoc();\n      final int docCount;\n      final Bits liveDocs = reader.liveDocs;\n      final int[] docMap;\n      if (liveDocs != null) {\n        int delCount = 0;\n        docMap = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docCount = maxDoc - delCount;\n      } else {\n        docCount = maxDoc;\n        docMap = null;\n      }\n\n      mergeState.docMaps[i] = docMap;\n      docBase += docCount;\n\n      if (mergeState.payloadProcessorProvider != null) {\n        // nocommit: this does not work anymore as SR/AtomicIndexReader does not know the directory anymore:\n        // mergeState.dirPayloadProcessor[i] = mergeState.payloadProcessorProvider.getDirProcessor(reader.reader.directory());\n        throw new UnsupportedOperationException(\"PayloadProcessorProvider is not supported at the moment :(\");\n      }\n\n      i++;\n    }\n\n    return docBase;\n  }\n\n","sourceOld":"  // NOTE: removes any \"all deleted\" readers from mergeState.readers\n  private int setDocMaps() throws IOException {\n    final int numReaders = mergeState.readers.size();\n\n    // Remap docIDs\n    mergeState.docMaps = new int[numReaders][];\n    mergeState.docBase = new int[numReaders];\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[numReaders];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[numReaders];\n\n    int docBase = 0;\n\n    int i = 0;\n    while(i < mergeState.readers.size()) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      final int maxDoc = reader.reader.maxDoc();\n      final int docCount;\n      final Bits liveDocs = reader.liveDocs;\n      final int[] docMap;\n      if (liveDocs != null) {\n        int delCount = 0;\n        docMap = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docCount = maxDoc - delCount;\n      } else {\n        docCount = maxDoc;\n        docMap = null;\n      }\n\n      mergeState.docMaps[i] = docMap;\n      docBase += docCount;\n\n      if (mergeState.payloadProcessorProvider != null) {\n        // nocommit: this was original, is the change correct:\n        // mergeState.dirPayloadProcessor[i] = mergeState.payloadProcessorProvider.getDirProcessor(reader.reader.directory());\n        mergeState.dirPayloadProcessor[i] = mergeState.payloadProcessorProvider.getDirProcessor(directory);\n      }\n\n      i++;\n    }\n\n    return docBase;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fb4f1343ffbdd2b940c3cbcaa20c36c4c293e157","date":1327935362,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#setDocMaps().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#setDocMaps().mjava","sourceNew":"  // NOTE: removes any \"all deleted\" readers from mergeState.readers\n  private int setDocMaps() throws IOException {\n    final int numReaders = mergeState.readers.size();\n\n    // Remap docIDs\n    mergeState.docMaps = new int[numReaders][];\n    mergeState.docBase = new int[numReaders];\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[numReaders];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[numReaders];\n\n    int docBase = 0;\n\n    int i = 0;\n    while(i < mergeState.readers.size()) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      final int maxDoc = reader.reader.maxDoc();\n      final int docCount;\n      final Bits liveDocs = reader.liveDocs;\n      final int[] docMap;\n      if (liveDocs != null) {\n        int delCount = 0;\n        docMap = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docCount = maxDoc - delCount;\n      } else {\n        docCount = maxDoc;\n        docMap = null;\n      }\n\n      mergeState.docMaps[i] = docMap;\n      docBase += docCount;\n\n      if (mergeState.payloadProcessorProvider != null) {\n        // TODO: the PayloadProcessorProvider should take AtomicReader as parameter\n        // and find out by itself if it can provide a processor:\n        if (!(reader.reader instanceof SegmentReader))\n          throw new UnsupportedOperationException(\"Payload processing currently requires exclusively SegmentReaders to be merged.\");\n        final Directory dir = ((SegmentReader) reader.reader).directory();\n        mergeState.dirPayloadProcessor[i] = mergeState.payloadProcessorProvider.getDirProcessor(dir);\n      }\n\n      i++;\n    }\n\n    return docBase;\n  }\n\n","sourceOld":"  // NOTE: removes any \"all deleted\" readers from mergeState.readers\n  private int setDocMaps() throws IOException {\n    final int numReaders = mergeState.readers.size();\n\n    // Remap docIDs\n    mergeState.docMaps = new int[numReaders][];\n    mergeState.docBase = new int[numReaders];\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[numReaders];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[numReaders];\n\n    int docBase = 0;\n\n    int i = 0;\n    while(i < mergeState.readers.size()) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      final int maxDoc = reader.reader.maxDoc();\n      final int docCount;\n      final Bits liveDocs = reader.liveDocs;\n      final int[] docMap;\n      if (liveDocs != null) {\n        int delCount = 0;\n        docMap = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docCount = maxDoc - delCount;\n      } else {\n        docCount = maxDoc;\n        docMap = null;\n      }\n\n      mergeState.docMaps[i] = docMap;\n      docBase += docCount;\n\n      if (mergeState.payloadProcessorProvider != null) {\n        // nocommit: this does not work anymore as SR/AtomicIndexReader does not know the directory anymore:\n        // mergeState.dirPayloadProcessor[i] = mergeState.payloadProcessorProvider.getDirProcessor(reader.reader.directory());\n        throw new UnsupportedOperationException(\"PayloadProcessorProvider is not supported at the moment :(\");\n      }\n\n      i++;\n    }\n\n    return docBase;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#setDocMaps().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#setDocMaps().mjava","sourceNew":"  // NOTE: removes any \"all deleted\" readers from mergeState.readers\n  private int setDocMaps() throws IOException {\n    final int numReaders = mergeState.readers.size();\n\n    // Remap docIDs\n    mergeState.docMaps = new int[numReaders][];\n    mergeState.docBase = new int[numReaders];\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[numReaders];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[numReaders];\n\n    int docBase = 0;\n\n    int i = 0;\n    while(i < mergeState.readers.size()) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      final int maxDoc = reader.reader.maxDoc();\n      final int docCount;\n      final Bits liveDocs = reader.liveDocs;\n      final int[] docMap;\n      if (liveDocs != null) {\n        int delCount = 0;\n        docMap = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docCount = maxDoc - delCount;\n      } else {\n        docCount = maxDoc;\n        docMap = null;\n      }\n\n      mergeState.docMaps[i] = docMap;\n      docBase += docCount;\n\n      if (mergeState.payloadProcessorProvider != null) {\n        // TODO: the PayloadProcessorProvider should take AtomicReader as parameter\n        // and find out by itself if it can provide a processor:\n        if (!(reader.reader instanceof SegmentReader))\n          throw new UnsupportedOperationException(\"Payload processing currently requires exclusively SegmentReaders to be merged.\");\n        final Directory dir = ((SegmentReader) reader.reader).directory();\n        mergeState.dirPayloadProcessor[i] = mergeState.payloadProcessorProvider.getDirProcessor(dir);\n      }\n\n      i++;\n    }\n\n    return docBase;\n  }\n\n","sourceOld":"  // NOTE: removes any \"all deleted\" readers from mergeState.readers\n  private int setDocMaps() throws IOException {\n    final int numReaders = mergeState.readers.size();\n\n    // Remap docIDs\n    mergeState.docMaps = new int[numReaders][];\n    mergeState.docBase = new int[numReaders];\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[numReaders];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[numReaders];\n\n    int docBase = 0;\n\n    int i = 0;\n    while(i < mergeState.readers.size()) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      final int maxDoc = reader.reader.maxDoc();\n      final int docCount;\n      final Bits liveDocs = reader.liveDocs;\n      final int[] docMap;\n      if (liveDocs != null) {\n        int delCount = 0;\n        docMap = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docCount = maxDoc - delCount;\n      } else {\n        docCount = maxDoc;\n        docMap = null;\n      }\n\n      mergeState.docMaps[i] = docMap;\n      docBase += docCount;\n\n      if (mergeState.payloadProcessorProvider != null) {\n        mergeState.dirPayloadProcessor[i] = mergeState.payloadProcessorProvider.getDirProcessor(reader.reader.directory());\n      }\n\n      i++;\n    }\n\n    return docBase;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f25edd3dc1b39b51d0fe10053586711547ccce1d","date":1328093500,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#setDocMaps().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#setDocMaps().mjava","sourceNew":"  // NOTE: removes any \"all deleted\" readers from mergeState.readers\n  private int setDocMaps() throws IOException {\n    final int numReaders = mergeState.readers.size();\n\n    // Remap docIDs\n    mergeState.docMaps = new int[numReaders][];\n    mergeState.docBase = new int[numReaders];\n    mergeState.readerPayloadProcessor = new PayloadProcessorProvider.ReaderPayloadProcessor[numReaders];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[numReaders];\n\n    int docBase = 0;\n\n    int i = 0;\n    while(i < mergeState.readers.size()) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      final int maxDoc = reader.reader.maxDoc();\n      final int docCount;\n      final Bits liveDocs = reader.liveDocs;\n      final int[] docMap;\n      if (liveDocs != null) {\n        int delCount = 0;\n        docMap = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docCount = maxDoc - delCount;\n      } else {\n        docCount = maxDoc;\n        docMap = null;\n      }\n\n      mergeState.docMaps[i] = docMap;\n      docBase += docCount;\n\n      if (mergeState.payloadProcessorProvider != null) {\n        mergeState.readerPayloadProcessor[i] = mergeState.payloadProcessorProvider.getReaderProcessor(reader.reader);\n      }\n\n      i++;\n    }\n\n    return docBase;\n  }\n\n","sourceOld":"  // NOTE: removes any \"all deleted\" readers from mergeState.readers\n  private int setDocMaps() throws IOException {\n    final int numReaders = mergeState.readers.size();\n\n    // Remap docIDs\n    mergeState.docMaps = new int[numReaders][];\n    mergeState.docBase = new int[numReaders];\n    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[numReaders];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[numReaders];\n\n    int docBase = 0;\n\n    int i = 0;\n    while(i < mergeState.readers.size()) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      final int maxDoc = reader.reader.maxDoc();\n      final int docCount;\n      final Bits liveDocs = reader.liveDocs;\n      final int[] docMap;\n      if (liveDocs != null) {\n        int delCount = 0;\n        docMap = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docCount = maxDoc - delCount;\n      } else {\n        docCount = maxDoc;\n        docMap = null;\n      }\n\n      mergeState.docMaps[i] = docMap;\n      docBase += docCount;\n\n      if (mergeState.payloadProcessorProvider != null) {\n        // TODO: the PayloadProcessorProvider should take AtomicReader as parameter\n        // and find out by itself if it can provide a processor:\n        if (!(reader.reader instanceof SegmentReader))\n          throw new UnsupportedOperationException(\"Payload processing currently requires exclusively SegmentReaders to be merged.\");\n        final Directory dir = ((SegmentReader) reader.reader).directory();\n        mergeState.dirPayloadProcessor[i] = mergeState.payloadProcessorProvider.getDirProcessor(dir);\n      }\n\n      i++;\n    }\n\n    return docBase;\n  }\n\n","bugFix":null,"bugIntro":["c95a819869502635864dac0a788f874787e3395b"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/SegmentMerger#setDocMaps().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#setDocMaps().mjava","sourceNew":"  // NOTE: removes any \"all deleted\" readers from mergeState.readers\n  private int setDocMaps() throws IOException {\n    final int numReaders = mergeState.readers.size();\n\n    // Remap docIDs\n    mergeState.docMaps = new int[numReaders][];\n    mergeState.docBase = new int[numReaders];\n    mergeState.readerPayloadProcessor = new PayloadProcessorProvider.ReaderPayloadProcessor[numReaders];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[numReaders];\n\n    int docBase = 0;\n\n    int i = 0;\n    while(i < mergeState.readers.size()) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      final int maxDoc = reader.reader.maxDoc();\n      final int docCount;\n      final Bits liveDocs = reader.liveDocs;\n      final int[] docMap;\n      if (liveDocs != null) {\n        int delCount = 0;\n        docMap = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docCount = maxDoc - delCount;\n      } else {\n        docCount = maxDoc;\n        docMap = null;\n      }\n\n      mergeState.docMaps[i] = docMap;\n      docBase += docCount;\n\n      if (mergeState.payloadProcessorProvider != null) {\n        mergeState.readerPayloadProcessor[i] = mergeState.payloadProcessorProvider.getReaderProcessor(reader.reader);\n      }\n\n      i++;\n    }\n\n    return docBase;\n  }\n\n","sourceOld":"  // NOTE: removes any \"all deleted\" readers from mergeState.readers\n  private int setDocMaps() throws IOException {\n    final int numReaders = mergeState.readers.size();\n\n    // Remap docIDs\n    mergeState.docMaps = new int[numReaders][];\n    mergeState.docBase = new int[numReaders];\n    mergeState.readerPayloadProcessor = new PayloadProcessorProvider.ReaderPayloadProcessor[numReaders];\n    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[numReaders];\n\n    int docBase = 0;\n\n    int i = 0;\n    while(i < mergeState.readers.size()) {\n\n      final MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(i);\n\n      mergeState.docBase[i] = docBase;\n      final int maxDoc = reader.reader.maxDoc();\n      final int docCount;\n      final Bits liveDocs = reader.liveDocs;\n      final int[] docMap;\n      if (liveDocs != null) {\n        int delCount = 0;\n        docMap = new int[maxDoc];\n        int newDocID = 0;\n        for(int j=0;j<maxDoc;j++) {\n          if (!liveDocs.get(j)) {\n            docMap[j] = -1;\n            delCount++;\n          } else {\n            docMap[j] = newDocID++;\n          }\n        }\n        docCount = maxDoc - delCount;\n      } else {\n        docCount = maxDoc;\n        docMap = null;\n      }\n\n      mergeState.docMaps[i] = docMap;\n      docBase += docCount;\n\n      if (mergeState.payloadProcessorProvider != null) {\n        mergeState.readerPayloadProcessor[i] = mergeState.payloadProcessorProvider.getReaderProcessor(reader.reader);\n      }\n\n      i++;\n    }\n\n    return docBase;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"5a6eeb3bd5b45df414522e8c152b36db43eb18bf":["d3d53ad7b9687e0dfaacaab099d3c7e98bfbe78e"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["f25edd3dc1b39b51d0fe10053586711547ccce1d"],"d3d53ad7b9687e0dfaacaab099d3c7e98bfbe78e":["ccad4bab070f323ce610caa0040346d4a87213dc"],"fd92b8bcc88e969302510acf77bd6970da3994c4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31"],"f25edd3dc1b39b51d0fe10053586711547ccce1d":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"9d5248bdfe89f464cce628bfb37cfa62a19ff022":["fd92b8bcc88e969302510acf77bd6970da3994c4"],"fb4f1343ffbdd2b940c3cbcaa20c36c4c293e157":["9d5248bdfe89f464cce628bfb37cfa62a19ff022"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31","fb4f1343ffbdd2b940c3cbcaa20c36c4c293e157"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"32feb7c2c571b402d2e231bd8e3b6add4af6d6eb":["5a6eeb3bd5b45df414522e8c152b36db43eb18bf"],"c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","32feb7c2c571b402d2e231bd8e3b6add4af6d6eb"],"ccad4bab070f323ce610caa0040346d4a87213dc":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"5a6eeb3bd5b45df414522e8c152b36db43eb18bf":["32feb7c2c571b402d2e231bd8e3b6add4af6d6eb"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d3d53ad7b9687e0dfaacaab099d3c7e98bfbe78e":["5a6eeb3bd5b45df414522e8c152b36db43eb18bf"],"fd92b8bcc88e969302510acf77bd6970da3994c4":["9d5248bdfe89f464cce628bfb37cfa62a19ff022"],"f25edd3dc1b39b51d0fe10053586711547ccce1d":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"9d5248bdfe89f464cce628bfb37cfa62a19ff022":["fb4f1343ffbdd2b940c3cbcaa20c36c4c293e157"],"fb4f1343ffbdd2b940c3cbcaa20c36c4c293e157":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["f25edd3dc1b39b51d0fe10053586711547ccce1d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["fd92b8bcc88e969302510acf77bd6970da3994c4","c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31","ccad4bab070f323ce610caa0040346d4a87213dc"],"32feb7c2c571b402d2e231bd8e3b6add4af6d6eb":["c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31"],"ccad4bab070f323ce610caa0040346d4a87213dc":["d3d53ad7b9687e0dfaacaab099d3c7e98bfbe78e"],"c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31":["fd92b8bcc88e969302510acf77bd6970da3994c4","5cab9a86bd67202d20b6adc463008c8e982b070a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}