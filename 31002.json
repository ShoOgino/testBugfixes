{"path":"lucene/src/java/org/apache/lucene/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","commits":[{"id":"a0ae5e3ed1232483b7b8a014f175a5fe43595982","date":1324062192,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","sourceNew":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n    long sumDFsinceLastAbortCheck = 0;\n    FixedBitSet visitedDocs = new FixedBitSet(mergeState.mergedDocCount);\n\n    if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsEnumIn, false);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum, visitedDocs);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.docFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS) {\n      if (docsAndFreqsEnum == null) {\n        docsAndFreqsEnum = new MappingMultiDocsEnum();\n      }\n      docsAndFreqsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsAndFreqsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsAndFreqsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsAndFreqsEnumIn, true);\n        assert docsAndFreqsEnumIn != null;\n        docsAndFreqsEnum.reset(docsAndFreqsEnumIn);\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, docsAndFreqsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    } else {\n      assert mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn);\n        assert postingsEnumIn != null;\n        postingsEnum.reset(postingsEnumIn);\n        // set PayloadProcessor\n        if (mergeState.payloadProcessorProvider != null) {\n          for (int i = 0; i < mergeState.readers.size(); i++) {\n            if (mergeState.dirPayloadProcessor[i] != null) {\n              mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n            }\n          }\n        }\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n    long sumDFsinceLastAbortCheck = 0;\n    FixedBitSet visitedDocs = new FixedBitSet(mergeState.mergedDocCount);\n\n    if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsEnumIn, false);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum, visitedDocs);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.docFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS) {\n      if (docsAndFreqsEnum == null) {\n        docsAndFreqsEnum = new MappingMultiDocsEnum();\n      }\n      docsAndFreqsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsAndFreqsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsAndFreqsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsAndFreqsEnumIn, true);\n        assert docsAndFreqsEnumIn != null;\n        docsAndFreqsEnum.reset(docsAndFreqsEnumIn);\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, docsAndFreqsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    } else {\n      assert mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn);\n        assert postingsEnumIn != null;\n        postingsEnum.reset(postingsEnumIn);\n        // set PayloadProcessor\n        if (mergeState.payloadProcessorProvider != null) {\n          for (int i = 0; i < mergeState.readers.size(); i++) {\n            if (mergeState.dirPayloadProcessor[i] != null) {\n              mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n            }\n          }\n        }\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"31f025ae60076ae95274433f3fe8e6ace2857a87","date":1326669465,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","pathOld":"lucene/src/java/org/apache/lucene/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","sourceNew":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n    long sumDFsinceLastAbortCheck = 0;\n    FixedBitSet visitedDocs = new FixedBitSet(mergeState.mergedDocCount);\n\n    if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsEnumIn, false);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum, visitedDocs);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.docFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS) {\n      if (docsAndFreqsEnum == null) {\n        docsAndFreqsEnum = new MappingMultiDocsEnum();\n      }\n      docsAndFreqsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsAndFreqsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsAndFreqsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsAndFreqsEnumIn, true);\n        assert docsAndFreqsEnumIn != null;\n        docsAndFreqsEnum.reset(docsAndFreqsEnumIn);\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, docsAndFreqsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn, false);\n        assert postingsEnumIn != null;\n        postingsEnum.reset(postingsEnumIn);\n        // set PayloadProcessor\n        if (mergeState.payloadProcessorProvider != null) {\n          for (int i = 0; i < mergeState.readers.size(); i++) {\n            if (mergeState.dirPayloadProcessor[i] != null) {\n              mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n            }\n          }\n        }\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    } else {\n      assert mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn, true);\n        assert postingsEnumIn != null;\n        postingsEnum.reset(postingsEnumIn);\n        // set PayloadProcessor\n        if (mergeState.payloadProcessorProvider != null) {\n          for (int i = 0; i < mergeState.readers.size(); i++) {\n            if (mergeState.dirPayloadProcessor[i] != null) {\n              mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n            }\n          }\n        }\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    }\n    finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n    long sumDFsinceLastAbortCheck = 0;\n    FixedBitSet visitedDocs = new FixedBitSet(mergeState.mergedDocCount);\n\n    if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsEnumIn, false);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum, visitedDocs);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.docFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS) {\n      if (docsAndFreqsEnum == null) {\n        docsAndFreqsEnum = new MappingMultiDocsEnum();\n      }\n      docsAndFreqsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsAndFreqsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsAndFreqsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsAndFreqsEnumIn, true);\n        assert docsAndFreqsEnumIn != null;\n        docsAndFreqsEnum.reset(docsAndFreqsEnumIn);\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, docsAndFreqsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    } else {\n      assert mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn);\n        assert postingsEnumIn != null;\n        postingsEnum.reset(postingsEnumIn);\n        // set PayloadProcessor\n        if (mergeState.payloadProcessorProvider != null) {\n          for (int i = 0; i < mergeState.readers.size(); i++) {\n            if (mergeState.dirPayloadProcessor[i] != null) {\n              mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n            }\n          }\n        }\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f25edd3dc1b39b51d0fe10053586711547ccce1d","date":1328093500,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","pathOld":"lucene/src/java/org/apache/lucene/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","sourceNew":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n    long sumDFsinceLastAbortCheck = 0;\n    FixedBitSet visitedDocs = new FixedBitSet(mergeState.mergedDocCount);\n\n    if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsEnumIn, false);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum, visitedDocs);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.docFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS) {\n      if (docsAndFreqsEnum == null) {\n        docsAndFreqsEnum = new MappingMultiDocsEnum();\n      }\n      docsAndFreqsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsAndFreqsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsAndFreqsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsAndFreqsEnumIn, true);\n        assert docsAndFreqsEnumIn != null;\n        docsAndFreqsEnum.reset(docsAndFreqsEnumIn);\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, docsAndFreqsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn, false);\n        assert postingsEnumIn != null;\n        postingsEnum.reset(postingsEnumIn);\n        // set PayloadProcessor\n        if (mergeState.payloadProcessorProvider != null) {\n          for (int i = 0; i < mergeState.readers.size(); i++) {\n            if (mergeState.readerPayloadProcessor[i] != null) {\n              mergeState.currentPayloadProcessor[i] = mergeState.readerPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n            }\n          }\n        }\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    } else {\n      assert mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn, true);\n        assert postingsEnumIn != null;\n        postingsEnum.reset(postingsEnumIn);\n        // set PayloadProcessor\n        if (mergeState.payloadProcessorProvider != null) {\n          for (int i = 0; i < mergeState.readers.size(); i++) {\n            if (mergeState.readerPayloadProcessor[i] != null) {\n              mergeState.currentPayloadProcessor[i] = mergeState.readerPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n            }\n          }\n        }\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    }\n    finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n    long sumDFsinceLastAbortCheck = 0;\n    FixedBitSet visitedDocs = new FixedBitSet(mergeState.mergedDocCount);\n\n    if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsEnumIn, false);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum, visitedDocs);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.docFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS) {\n      if (docsAndFreqsEnum == null) {\n        docsAndFreqsEnum = new MappingMultiDocsEnum();\n      }\n      docsAndFreqsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsAndFreqsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsAndFreqsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsAndFreqsEnumIn, true);\n        assert docsAndFreqsEnumIn != null;\n        docsAndFreqsEnum.reset(docsAndFreqsEnumIn);\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, docsAndFreqsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn, false);\n        assert postingsEnumIn != null;\n        postingsEnum.reset(postingsEnumIn);\n        // set PayloadProcessor\n        if (mergeState.payloadProcessorProvider != null) {\n          for (int i = 0; i < mergeState.readers.size(); i++) {\n            if (mergeState.dirPayloadProcessor[i] != null) {\n              mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n            }\n          }\n        }\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    } else {\n      assert mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn, true);\n        assert postingsEnumIn != null;\n        postingsEnum.reset(postingsEnumIn);\n        // set PayloadProcessor\n        if (mergeState.payloadProcessorProvider != null) {\n          for (int i = 0; i < mergeState.readers.size(); i++) {\n            if (mergeState.dirPayloadProcessor[i] != null) {\n              mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n            }\n          }\n        }\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    }\n    finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","pathOld":"lucene/src/java/org/apache/lucene/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","sourceNew":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n    long sumDFsinceLastAbortCheck = 0;\n    FixedBitSet visitedDocs = new FixedBitSet(mergeState.mergedDocCount);\n\n    if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsEnumIn, false);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum, visitedDocs);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.docFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS) {\n      if (docsAndFreqsEnum == null) {\n        docsAndFreqsEnum = new MappingMultiDocsEnum();\n      }\n      docsAndFreqsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsAndFreqsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsAndFreqsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsAndFreqsEnumIn, true);\n        assert docsAndFreqsEnumIn != null;\n        docsAndFreqsEnum.reset(docsAndFreqsEnumIn);\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, docsAndFreqsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn, false);\n        assert postingsEnumIn != null;\n        postingsEnum.reset(postingsEnumIn);\n        // set PayloadProcessor\n        if (mergeState.payloadProcessorProvider != null) {\n          for (int i = 0; i < mergeState.readers.size(); i++) {\n            if (mergeState.readerPayloadProcessor[i] != null) {\n              mergeState.currentPayloadProcessor[i] = mergeState.readerPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n            }\n          }\n        }\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    } else {\n      assert mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn, true);\n        assert postingsEnumIn != null;\n        postingsEnum.reset(postingsEnumIn);\n        // set PayloadProcessor\n        if (mergeState.payloadProcessorProvider != null) {\n          for (int i = 0; i < mergeState.readers.size(); i++) {\n            if (mergeState.readerPayloadProcessor[i] != null) {\n              mergeState.currentPayloadProcessor[i] = mergeState.readerPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n            }\n          }\n        }\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    }\n    finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n    long sumDFsinceLastAbortCheck = 0;\n    FixedBitSet visitedDocs = new FixedBitSet(mergeState.mergedDocCount);\n\n    if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsEnumIn, false);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum, visitedDocs);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.docFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS) {\n      if (docsAndFreqsEnum == null) {\n        docsAndFreqsEnum = new MappingMultiDocsEnum();\n      }\n      docsAndFreqsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsAndFreqsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsAndFreqsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsAndFreqsEnumIn, true);\n        assert docsAndFreqsEnumIn != null;\n        docsAndFreqsEnum.reset(docsAndFreqsEnumIn);\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, docsAndFreqsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn, false);\n        assert postingsEnumIn != null;\n        postingsEnum.reset(postingsEnumIn);\n        // set PayloadProcessor\n        if (mergeState.payloadProcessorProvider != null) {\n          for (int i = 0; i < mergeState.readers.size(); i++) {\n            if (mergeState.readerPayloadProcessor[i] != null) {\n              mergeState.currentPayloadProcessor[i] = mergeState.readerPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n            }\n          }\n        }\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    } else {\n      assert mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn, true);\n        assert postingsEnumIn != null;\n        postingsEnum.reset(postingsEnumIn);\n        // set PayloadProcessor\n        if (mergeState.payloadProcessorProvider != null) {\n          for (int i = 0; i < mergeState.readers.size(); i++) {\n            if (mergeState.readerPayloadProcessor[i] != null) {\n              mergeState.currentPayloadProcessor[i] = mergeState.readerPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n            }\n          }\n        }\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    }\n    finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["f25edd3dc1b39b51d0fe10053586711547ccce1d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f25edd3dc1b39b51d0fe10053586711547ccce1d":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"a0ae5e3ed1232483b7b8a014f175a5fe43595982":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["a0ae5e3ed1232483b7b8a014f175a5fe43595982"]},"commit2Childs":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a0ae5e3ed1232483b7b8a014f175a5fe43595982"],"f25edd3dc1b39b51d0fe10053586711547ccce1d":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"a0ae5e3ed1232483b7b8a014f175a5fe43595982":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["f25edd3dc1b39b51d0fe10053586711547ccce1d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}