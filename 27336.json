{"path":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestFreeTextSuggester#testRandom().mjava","commits":[{"id":"d0af0c31a687dd847212ae59f661152896c76516","date":1380727430,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestFreeTextSuggester#testRandom().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom() throws IOException {\n    String[] terms = new String[_TestUtil.nextInt(random(), 2, 10)];\n    Set<String> seen = new HashSet<String>();\n    while (seen.size() < terms.length) {\n      String token = _TestUtil.randomSimpleString(random(), 1, 5);\n      if (!seen.contains(token)) {\n        terms[seen.size()] = token;\n        seen.add(token);\n      }\n    }\n\n    Analyzer a = new MockAnalyzer(random());\n\n    int numDocs = atLeast(10);\n    long totTokens = 0;\n    final String[][] docs = new String[numDocs][];\n    for(int i=0;i<numDocs;i++) {\n      docs[i] = new String[atLeast(100)];\n      if (VERBOSE) {\n        System.out.print(\"  doc \" + i + \":\");\n      }\n      for(int j=0;j<docs[i].length;j++) {\n        docs[i][j] = getZipfToken(terms);\n        if (VERBOSE) {\n          System.out.print(\" \" + docs[i][j]);\n        }\n      }\n      if (VERBOSE) {\n        System.out.println();\n      }\n      totTokens += docs[i].length;\n    }\n\n    int grams = _TestUtil.nextInt(random(), 1, 4);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + terms.length + \" terms; \" + numDocs + \" docs; \" + grams + \" grams\");\n    }\n\n    // Build suggester model:\n    FreeTextSuggester sug = new FreeTextSuggester(a, a, grams, (byte) 0x20);\n    sug.build(new TermFreqIterator() {\n        int upto;\n\n        @Override\n        public BytesRef next() {\n          if (upto == docs.length) {\n            return null;\n          } else {\n            StringBuilder b = new StringBuilder();\n            for(String token : docs[upto]) {\n              b.append(' ');\n              b.append(token);\n            }\n            upto++;\n            return new BytesRef(b.toString());\n          }\n        }\n\n        @Override\n        public long weight() {\n          return random().nextLong();\n        }\n      });\n\n    // Build inefficient but hopefully correct model:\n    List<Map<String,Integer>> gramCounts = new ArrayList<Map<String,Integer>>(grams);\n    for(int gram=0;gram<grams;gram++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: build model for gram=\" + gram);\n      }\n      Map<String,Integer> model = new HashMap<String,Integer>();\n      gramCounts.add(model);\n      for(String[] doc : docs) {\n        for(int i=0;i<doc.length-gram;i++) {\n          StringBuilder b = new StringBuilder();\n          for(int j=i;j<=i+gram;j++) {\n            if (j > i) {\n              b.append(' ');\n            }\n            b.append(doc[j]);\n          }\n          String token = b.toString();\n          Integer curCount = model.get(token);\n          if (curCount == null) {\n            model.put(token, 1);\n          } else {\n            model.put(token, 1 + curCount);\n          }\n          if (VERBOSE) {\n            System.out.println(\"  add '\" + token + \"' -> count=\" + model.get(token));\n          }\n        }\n      }\n    }\n\n    int lookups = atLeast(100);\n    for(int iter=0;iter<lookups;iter++) {\n      String[] tokens = new String[_TestUtil.nextInt(random(), 1, 5)];\n      for(int i=0;i<tokens.length;i++) {\n        tokens[i] = getZipfToken(terms);\n      }\n\n      // Maybe trim last token; be sure not to create the\n      // empty string:\n      int trimStart;\n      if (tokens.length == 1) {\n        trimStart = 1;\n      } else {\n        trimStart = 0;\n      }\n      int trimAt = _TestUtil.nextInt(random(), trimStart, tokens[tokens.length-1].length());\n      tokens[tokens.length-1] = tokens[tokens.length-1].substring(0, trimAt);\n\n      int num = _TestUtil.nextInt(random(), 1, 100);\n      StringBuilder b = new StringBuilder();\n      for(String token : tokens) {\n        b.append(' ');\n        b.append(token);\n      }\n      String query = b.toString();\n      query = query.substring(1);\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" query='\" + query + \"' num=\" + num);\n      }\n\n      // Expected:\n      List<LookupResult> expected = new ArrayList<LookupResult>();\n      double backoff = 1.0;\n      seen = new HashSet<String>();\n\n      if (VERBOSE) {\n        System.out.println(\"  compute expected\");\n      }\n      for(int i=grams-1;i>=0;i--) {\n        if (VERBOSE) {\n          System.out.println(\"    grams=\" + i);\n        }\n\n        if (tokens.length < i+1) {\n          // Don't have enough tokens to use this model\n          if (VERBOSE) {\n            System.out.println(\"      skip\");\n          }\n          continue;\n        }\n\n        if (i == 0 && tokens[tokens.length-1].length() == 0) {\n          // Never suggest unigrams from empty string:\n          if (VERBOSE) {\n            System.out.println(\"      skip unigram priors only\");\n          }\n          continue;\n        }\n\n        // Build up \"context\" ngram:\n        b = new StringBuilder();\n        for(int j=tokens.length-i-1;j<tokens.length-1;j++) {\n          b.append(' ');\n          b.append(tokens[j]);\n        }\n        String context = b.toString();\n        if (context.length() > 0) {\n          context = context.substring(1);\n        }\n        if (VERBOSE) {\n          System.out.println(\"      context='\" + context + \"'\");\n        }\n        long contextCount;\n        if (context.length() == 0) {\n          contextCount = totTokens;\n        } else {\n          Integer count = gramCounts.get(i-1).get(context);\n          if (count == null) {\n            // We never saw this context:\n            backoff *= FreeTextSuggester.ALPHA;\n            if (VERBOSE) {\n              System.out.println(\"      skip: never saw context\");\n            }\n            continue;\n          }\n          contextCount = count;\n        }\n        if (VERBOSE) {\n          System.out.println(\"      contextCount=\" + contextCount);\n        }\n        Map<String,Integer> model = gramCounts.get(i);\n\n        // First pass, gather all predictions for this model:\n        if (VERBOSE) {\n          System.out.println(\"      find terms w/ prefix=\" + tokens[tokens.length-1]);\n        }\n        List<LookupResult> tmp = new ArrayList<LookupResult>();\n        for(String term : terms) {\n          if (term.startsWith(tokens[tokens.length-1])) {\n            if (VERBOSE) {\n              System.out.println(\"        term=\" + term);\n            }\n            if (seen.contains(term)) {\n              if (VERBOSE) {\n                System.out.println(\"          skip seen\");\n              }\n              continue;\n            }\n            String ngram = (context + \" \" + term).trim();\n            Integer count = model.get(ngram);\n            if (count != null) {\n              LookupResult lr = new LookupResult(ngram, (long) (Long.MAX_VALUE * (backoff * (double) count / contextCount)));\n              tmp.add(lr);\n              if (VERBOSE) {\n                System.out.println(\"      add tmp key='\" + lr.key + \"' score=\" + lr.value);\n              }\n            }\n          }\n        }\n\n        // Second pass, trim to only top N, and fold those\n        // into overall suggestions:\n        Collections.sort(tmp, byScoreThenKey);\n        if (tmp.size() > num) {\n          tmp.subList(num, tmp.size()).clear();\n        }\n        for(LookupResult result : tmp) {\n          String key = result.key.toString();\n          int idx = key.lastIndexOf(' ');\n          String lastToken;\n          if (idx != -1) {\n            lastToken = key.substring(idx+1);\n          } else {\n            lastToken = key;\n          }\n          if (!seen.contains(lastToken)) {\n            seen.add(lastToken);\n            expected.add(result);\n            if (VERBOSE) {\n              System.out.println(\"      keep key='\" + result.key + \"' score=\" + result.value);\n            }\n          }\n        }\n        \n        backoff *= FreeTextSuggester.ALPHA;\n      }\n\n      Collections.sort(expected, byScoreThenKey);\n\n      if (expected.size() > num) {\n        expected.subList(num, expected.size()).clear();\n      }\n\n      // Actual:\n      List<LookupResult> actual = sug.lookup(query, num);\n\n      if (VERBOSE) {\n        System.out.println(\"  expected: \" + expected);\n        System.out.println(\"    actual: \" + actual);\n      }\n\n      assertEquals(expected.toString(), actual.toString());\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ada2f7352a7f964fe49bccd13227c4ec38563d39","date":1381659982,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestFreeTextSuggester#testRandom().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestFreeTextSuggester#testRandom().mjava","sourceNew":"  public void testRandom() throws IOException {\n    String[] terms = new String[_TestUtil.nextInt(random(), 2, 10)];\n    Set<String> seen = new HashSet<String>();\n    while (seen.size() < terms.length) {\n      String token = _TestUtil.randomSimpleString(random(), 1, 5);\n      if (!seen.contains(token)) {\n        terms[seen.size()] = token;\n        seen.add(token);\n      }\n    }\n\n    Analyzer a = new MockAnalyzer(random());\n\n    int numDocs = atLeast(10);\n    long totTokens = 0;\n    final String[][] docs = new String[numDocs][];\n    for(int i=0;i<numDocs;i++) {\n      docs[i] = new String[atLeast(100)];\n      if (VERBOSE) {\n        System.out.print(\"  doc \" + i + \":\");\n      }\n      for(int j=0;j<docs[i].length;j++) {\n        docs[i][j] = getZipfToken(terms);\n        if (VERBOSE) {\n          System.out.print(\" \" + docs[i][j]);\n        }\n      }\n      if (VERBOSE) {\n        System.out.println();\n      }\n      totTokens += docs[i].length;\n    }\n\n    int grams = _TestUtil.nextInt(random(), 1, 4);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + terms.length + \" terms; \" + numDocs + \" docs; \" + grams + \" grams\");\n    }\n\n    // Build suggester model:\n    FreeTextSuggester sug = new FreeTextSuggester(a, a, grams, (byte) 0x20);\n    sug.build(new TermFreqPayloadIterator() {\n        int upto;\n\n        @Override\n        public BytesRef next() {\n          if (upto == docs.length) {\n            return null;\n          } else {\n            StringBuilder b = new StringBuilder();\n            for(String token : docs[upto]) {\n              b.append(' ');\n              b.append(token);\n            }\n            upto++;\n            return new BytesRef(b.toString());\n          }\n        }\n\n        @Override\n        public long weight() {\n          return random().nextLong();\n        }\n\n        @Override\n        public BytesRef payload() {\n          return null;\n        }\n\n        @Override\n        public boolean hasPayloads() {\n          return false;\n        }\n      });\n\n    // Build inefficient but hopefully correct model:\n    List<Map<String,Integer>> gramCounts = new ArrayList<Map<String,Integer>>(grams);\n    for(int gram=0;gram<grams;gram++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: build model for gram=\" + gram);\n      }\n      Map<String,Integer> model = new HashMap<String,Integer>();\n      gramCounts.add(model);\n      for(String[] doc : docs) {\n        for(int i=0;i<doc.length-gram;i++) {\n          StringBuilder b = new StringBuilder();\n          for(int j=i;j<=i+gram;j++) {\n            if (j > i) {\n              b.append(' ');\n            }\n            b.append(doc[j]);\n          }\n          String token = b.toString();\n          Integer curCount = model.get(token);\n          if (curCount == null) {\n            model.put(token, 1);\n          } else {\n            model.put(token, 1 + curCount);\n          }\n          if (VERBOSE) {\n            System.out.println(\"  add '\" + token + \"' -> count=\" + model.get(token));\n          }\n        }\n      }\n    }\n\n    int lookups = atLeast(100);\n    for(int iter=0;iter<lookups;iter++) {\n      String[] tokens = new String[_TestUtil.nextInt(random(), 1, 5)];\n      for(int i=0;i<tokens.length;i++) {\n        tokens[i] = getZipfToken(terms);\n      }\n\n      // Maybe trim last token; be sure not to create the\n      // empty string:\n      int trimStart;\n      if (tokens.length == 1) {\n        trimStart = 1;\n      } else {\n        trimStart = 0;\n      }\n      int trimAt = _TestUtil.nextInt(random(), trimStart, tokens[tokens.length-1].length());\n      tokens[tokens.length-1] = tokens[tokens.length-1].substring(0, trimAt);\n\n      int num = _TestUtil.nextInt(random(), 1, 100);\n      StringBuilder b = new StringBuilder();\n      for(String token : tokens) {\n        b.append(' ');\n        b.append(token);\n      }\n      String query = b.toString();\n      query = query.substring(1);\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" query='\" + query + \"' num=\" + num);\n      }\n\n      // Expected:\n      List<LookupResult> expected = new ArrayList<LookupResult>();\n      double backoff = 1.0;\n      seen = new HashSet<String>();\n\n      if (VERBOSE) {\n        System.out.println(\"  compute expected\");\n      }\n      for(int i=grams-1;i>=0;i--) {\n        if (VERBOSE) {\n          System.out.println(\"    grams=\" + i);\n        }\n\n        if (tokens.length < i+1) {\n          // Don't have enough tokens to use this model\n          if (VERBOSE) {\n            System.out.println(\"      skip\");\n          }\n          continue;\n        }\n\n        if (i == 0 && tokens[tokens.length-1].length() == 0) {\n          // Never suggest unigrams from empty string:\n          if (VERBOSE) {\n            System.out.println(\"      skip unigram priors only\");\n          }\n          continue;\n        }\n\n        // Build up \"context\" ngram:\n        b = new StringBuilder();\n        for(int j=tokens.length-i-1;j<tokens.length-1;j++) {\n          b.append(' ');\n          b.append(tokens[j]);\n        }\n        String context = b.toString();\n        if (context.length() > 0) {\n          context = context.substring(1);\n        }\n        if (VERBOSE) {\n          System.out.println(\"      context='\" + context + \"'\");\n        }\n        long contextCount;\n        if (context.length() == 0) {\n          contextCount = totTokens;\n        } else {\n          Integer count = gramCounts.get(i-1).get(context);\n          if (count == null) {\n            // We never saw this context:\n            backoff *= FreeTextSuggester.ALPHA;\n            if (VERBOSE) {\n              System.out.println(\"      skip: never saw context\");\n            }\n            continue;\n          }\n          contextCount = count;\n        }\n        if (VERBOSE) {\n          System.out.println(\"      contextCount=\" + contextCount);\n        }\n        Map<String,Integer> model = gramCounts.get(i);\n\n        // First pass, gather all predictions for this model:\n        if (VERBOSE) {\n          System.out.println(\"      find terms w/ prefix=\" + tokens[tokens.length-1]);\n        }\n        List<LookupResult> tmp = new ArrayList<LookupResult>();\n        for(String term : terms) {\n          if (term.startsWith(tokens[tokens.length-1])) {\n            if (VERBOSE) {\n              System.out.println(\"        term=\" + term);\n            }\n            if (seen.contains(term)) {\n              if (VERBOSE) {\n                System.out.println(\"          skip seen\");\n              }\n              continue;\n            }\n            String ngram = (context + \" \" + term).trim();\n            Integer count = model.get(ngram);\n            if (count != null) {\n              LookupResult lr = new LookupResult(ngram, (long) (Long.MAX_VALUE * (backoff * (double) count / contextCount)));\n              tmp.add(lr);\n              if (VERBOSE) {\n                System.out.println(\"      add tmp key='\" + lr.key + \"' score=\" + lr.value);\n              }\n            }\n          }\n        }\n\n        // Second pass, trim to only top N, and fold those\n        // into overall suggestions:\n        Collections.sort(tmp, byScoreThenKey);\n        if (tmp.size() > num) {\n          tmp.subList(num, tmp.size()).clear();\n        }\n        for(LookupResult result : tmp) {\n          String key = result.key.toString();\n          int idx = key.lastIndexOf(' ');\n          String lastToken;\n          if (idx != -1) {\n            lastToken = key.substring(idx+1);\n          } else {\n            lastToken = key;\n          }\n          if (!seen.contains(lastToken)) {\n            seen.add(lastToken);\n            expected.add(result);\n            if (VERBOSE) {\n              System.out.println(\"      keep key='\" + result.key + \"' score=\" + result.value);\n            }\n          }\n        }\n        \n        backoff *= FreeTextSuggester.ALPHA;\n      }\n\n      Collections.sort(expected, byScoreThenKey);\n\n      if (expected.size() > num) {\n        expected.subList(num, expected.size()).clear();\n      }\n\n      // Actual:\n      List<LookupResult> actual = sug.lookup(query, num);\n\n      if (VERBOSE) {\n        System.out.println(\"  expected: \" + expected);\n        System.out.println(\"    actual: \" + actual);\n      }\n\n      assertEquals(expected.toString(), actual.toString());\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws IOException {\n    String[] terms = new String[_TestUtil.nextInt(random(), 2, 10)];\n    Set<String> seen = new HashSet<String>();\n    while (seen.size() < terms.length) {\n      String token = _TestUtil.randomSimpleString(random(), 1, 5);\n      if (!seen.contains(token)) {\n        terms[seen.size()] = token;\n        seen.add(token);\n      }\n    }\n\n    Analyzer a = new MockAnalyzer(random());\n\n    int numDocs = atLeast(10);\n    long totTokens = 0;\n    final String[][] docs = new String[numDocs][];\n    for(int i=0;i<numDocs;i++) {\n      docs[i] = new String[atLeast(100)];\n      if (VERBOSE) {\n        System.out.print(\"  doc \" + i + \":\");\n      }\n      for(int j=0;j<docs[i].length;j++) {\n        docs[i][j] = getZipfToken(terms);\n        if (VERBOSE) {\n          System.out.print(\" \" + docs[i][j]);\n        }\n      }\n      if (VERBOSE) {\n        System.out.println();\n      }\n      totTokens += docs[i].length;\n    }\n\n    int grams = _TestUtil.nextInt(random(), 1, 4);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + terms.length + \" terms; \" + numDocs + \" docs; \" + grams + \" grams\");\n    }\n\n    // Build suggester model:\n    FreeTextSuggester sug = new FreeTextSuggester(a, a, grams, (byte) 0x20);\n    sug.build(new TermFreqIterator() {\n        int upto;\n\n        @Override\n        public BytesRef next() {\n          if (upto == docs.length) {\n            return null;\n          } else {\n            StringBuilder b = new StringBuilder();\n            for(String token : docs[upto]) {\n              b.append(' ');\n              b.append(token);\n            }\n            upto++;\n            return new BytesRef(b.toString());\n          }\n        }\n\n        @Override\n        public long weight() {\n          return random().nextLong();\n        }\n      });\n\n    // Build inefficient but hopefully correct model:\n    List<Map<String,Integer>> gramCounts = new ArrayList<Map<String,Integer>>(grams);\n    for(int gram=0;gram<grams;gram++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: build model for gram=\" + gram);\n      }\n      Map<String,Integer> model = new HashMap<String,Integer>();\n      gramCounts.add(model);\n      for(String[] doc : docs) {\n        for(int i=0;i<doc.length-gram;i++) {\n          StringBuilder b = new StringBuilder();\n          for(int j=i;j<=i+gram;j++) {\n            if (j > i) {\n              b.append(' ');\n            }\n            b.append(doc[j]);\n          }\n          String token = b.toString();\n          Integer curCount = model.get(token);\n          if (curCount == null) {\n            model.put(token, 1);\n          } else {\n            model.put(token, 1 + curCount);\n          }\n          if (VERBOSE) {\n            System.out.println(\"  add '\" + token + \"' -> count=\" + model.get(token));\n          }\n        }\n      }\n    }\n\n    int lookups = atLeast(100);\n    for(int iter=0;iter<lookups;iter++) {\n      String[] tokens = new String[_TestUtil.nextInt(random(), 1, 5)];\n      for(int i=0;i<tokens.length;i++) {\n        tokens[i] = getZipfToken(terms);\n      }\n\n      // Maybe trim last token; be sure not to create the\n      // empty string:\n      int trimStart;\n      if (tokens.length == 1) {\n        trimStart = 1;\n      } else {\n        trimStart = 0;\n      }\n      int trimAt = _TestUtil.nextInt(random(), trimStart, tokens[tokens.length-1].length());\n      tokens[tokens.length-1] = tokens[tokens.length-1].substring(0, trimAt);\n\n      int num = _TestUtil.nextInt(random(), 1, 100);\n      StringBuilder b = new StringBuilder();\n      for(String token : tokens) {\n        b.append(' ');\n        b.append(token);\n      }\n      String query = b.toString();\n      query = query.substring(1);\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" query='\" + query + \"' num=\" + num);\n      }\n\n      // Expected:\n      List<LookupResult> expected = new ArrayList<LookupResult>();\n      double backoff = 1.0;\n      seen = new HashSet<String>();\n\n      if (VERBOSE) {\n        System.out.println(\"  compute expected\");\n      }\n      for(int i=grams-1;i>=0;i--) {\n        if (VERBOSE) {\n          System.out.println(\"    grams=\" + i);\n        }\n\n        if (tokens.length < i+1) {\n          // Don't have enough tokens to use this model\n          if (VERBOSE) {\n            System.out.println(\"      skip\");\n          }\n          continue;\n        }\n\n        if (i == 0 && tokens[tokens.length-1].length() == 0) {\n          // Never suggest unigrams from empty string:\n          if (VERBOSE) {\n            System.out.println(\"      skip unigram priors only\");\n          }\n          continue;\n        }\n\n        // Build up \"context\" ngram:\n        b = new StringBuilder();\n        for(int j=tokens.length-i-1;j<tokens.length-1;j++) {\n          b.append(' ');\n          b.append(tokens[j]);\n        }\n        String context = b.toString();\n        if (context.length() > 0) {\n          context = context.substring(1);\n        }\n        if (VERBOSE) {\n          System.out.println(\"      context='\" + context + \"'\");\n        }\n        long contextCount;\n        if (context.length() == 0) {\n          contextCount = totTokens;\n        } else {\n          Integer count = gramCounts.get(i-1).get(context);\n          if (count == null) {\n            // We never saw this context:\n            backoff *= FreeTextSuggester.ALPHA;\n            if (VERBOSE) {\n              System.out.println(\"      skip: never saw context\");\n            }\n            continue;\n          }\n          contextCount = count;\n        }\n        if (VERBOSE) {\n          System.out.println(\"      contextCount=\" + contextCount);\n        }\n        Map<String,Integer> model = gramCounts.get(i);\n\n        // First pass, gather all predictions for this model:\n        if (VERBOSE) {\n          System.out.println(\"      find terms w/ prefix=\" + tokens[tokens.length-1]);\n        }\n        List<LookupResult> tmp = new ArrayList<LookupResult>();\n        for(String term : terms) {\n          if (term.startsWith(tokens[tokens.length-1])) {\n            if (VERBOSE) {\n              System.out.println(\"        term=\" + term);\n            }\n            if (seen.contains(term)) {\n              if (VERBOSE) {\n                System.out.println(\"          skip seen\");\n              }\n              continue;\n            }\n            String ngram = (context + \" \" + term).trim();\n            Integer count = model.get(ngram);\n            if (count != null) {\n              LookupResult lr = new LookupResult(ngram, (long) (Long.MAX_VALUE * (backoff * (double) count / contextCount)));\n              tmp.add(lr);\n              if (VERBOSE) {\n                System.out.println(\"      add tmp key='\" + lr.key + \"' score=\" + lr.value);\n              }\n            }\n          }\n        }\n\n        // Second pass, trim to only top N, and fold those\n        // into overall suggestions:\n        Collections.sort(tmp, byScoreThenKey);\n        if (tmp.size() > num) {\n          tmp.subList(num, tmp.size()).clear();\n        }\n        for(LookupResult result : tmp) {\n          String key = result.key.toString();\n          int idx = key.lastIndexOf(' ');\n          String lastToken;\n          if (idx != -1) {\n            lastToken = key.substring(idx+1);\n          } else {\n            lastToken = key;\n          }\n          if (!seen.contains(lastToken)) {\n            seen.add(lastToken);\n            expected.add(result);\n            if (VERBOSE) {\n              System.out.println(\"      keep key='\" + result.key + \"' score=\" + result.value);\n            }\n          }\n        }\n        \n        backoff *= FreeTextSuggester.ALPHA;\n      }\n\n      Collections.sort(expected, byScoreThenKey);\n\n      if (expected.size() > num) {\n        expected.subList(num, expected.size()).clear();\n      }\n\n      // Actual:\n      List<LookupResult> actual = sug.lookup(query, num);\n\n      if (VERBOSE) {\n        System.out.println(\"  expected: \" + expected);\n        System.out.println(\"    actual: \" + actual);\n      }\n\n      assertEquals(expected.toString(), actual.toString());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"41aee74b5f91a096e3fd950f4a336bc763f0e7a7","date":1381772070,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestFreeTextSuggester#testRandom().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestFreeTextSuggester#testRandom().mjava","sourceNew":"  public void testRandom() throws IOException {\n    String[] terms = new String[_TestUtil.nextInt(random(), 2, 10)];\n    Set<String> seen = new HashSet<String>();\n    while (seen.size() < terms.length) {\n      String token = _TestUtil.randomSimpleString(random(), 1, 5);\n      if (!seen.contains(token)) {\n        terms[seen.size()] = token;\n        seen.add(token);\n      }\n    }\n\n    Analyzer a = new MockAnalyzer(random());\n\n    int numDocs = atLeast(10);\n    long totTokens = 0;\n    final String[][] docs = new String[numDocs][];\n    for(int i=0;i<numDocs;i++) {\n      docs[i] = new String[atLeast(100)];\n      if (VERBOSE) {\n        System.out.print(\"  doc \" + i + \":\");\n      }\n      for(int j=0;j<docs[i].length;j++) {\n        docs[i][j] = getZipfToken(terms);\n        if (VERBOSE) {\n          System.out.print(\" \" + docs[i][j]);\n        }\n      }\n      if (VERBOSE) {\n        System.out.println();\n      }\n      totTokens += docs[i].length;\n    }\n\n    int grams = _TestUtil.nextInt(random(), 1, 4);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + terms.length + \" terms; \" + numDocs + \" docs; \" + grams + \" grams\");\n    }\n\n    // Build suggester model:\n    FreeTextSuggester sug = new FreeTextSuggester(a, a, grams, (byte) 0x20);\n    sug.build(new InputIterator() {\n        int upto;\n\n        @Override\n        public BytesRef next() {\n          if (upto == docs.length) {\n            return null;\n          } else {\n            StringBuilder b = new StringBuilder();\n            for(String token : docs[upto]) {\n              b.append(' ');\n              b.append(token);\n            }\n            upto++;\n            return new BytesRef(b.toString());\n          }\n        }\n\n        @Override\n        public long weight() {\n          return random().nextLong();\n        }\n\n        @Override\n        public BytesRef payload() {\n          return null;\n        }\n\n        @Override\n        public boolean hasPayloads() {\n          return false;\n        }\n      });\n\n    // Build inefficient but hopefully correct model:\n    List<Map<String,Integer>> gramCounts = new ArrayList<Map<String,Integer>>(grams);\n    for(int gram=0;gram<grams;gram++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: build model for gram=\" + gram);\n      }\n      Map<String,Integer> model = new HashMap<String,Integer>();\n      gramCounts.add(model);\n      for(String[] doc : docs) {\n        for(int i=0;i<doc.length-gram;i++) {\n          StringBuilder b = new StringBuilder();\n          for(int j=i;j<=i+gram;j++) {\n            if (j > i) {\n              b.append(' ');\n            }\n            b.append(doc[j]);\n          }\n          String token = b.toString();\n          Integer curCount = model.get(token);\n          if (curCount == null) {\n            model.put(token, 1);\n          } else {\n            model.put(token, 1 + curCount);\n          }\n          if (VERBOSE) {\n            System.out.println(\"  add '\" + token + \"' -> count=\" + model.get(token));\n          }\n        }\n      }\n    }\n\n    int lookups = atLeast(100);\n    for(int iter=0;iter<lookups;iter++) {\n      String[] tokens = new String[_TestUtil.nextInt(random(), 1, 5)];\n      for(int i=0;i<tokens.length;i++) {\n        tokens[i] = getZipfToken(terms);\n      }\n\n      // Maybe trim last token; be sure not to create the\n      // empty string:\n      int trimStart;\n      if (tokens.length == 1) {\n        trimStart = 1;\n      } else {\n        trimStart = 0;\n      }\n      int trimAt = _TestUtil.nextInt(random(), trimStart, tokens[tokens.length-1].length());\n      tokens[tokens.length-1] = tokens[tokens.length-1].substring(0, trimAt);\n\n      int num = _TestUtil.nextInt(random(), 1, 100);\n      StringBuilder b = new StringBuilder();\n      for(String token : tokens) {\n        b.append(' ');\n        b.append(token);\n      }\n      String query = b.toString();\n      query = query.substring(1);\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" query='\" + query + \"' num=\" + num);\n      }\n\n      // Expected:\n      List<LookupResult> expected = new ArrayList<LookupResult>();\n      double backoff = 1.0;\n      seen = new HashSet<String>();\n\n      if (VERBOSE) {\n        System.out.println(\"  compute expected\");\n      }\n      for(int i=grams-1;i>=0;i--) {\n        if (VERBOSE) {\n          System.out.println(\"    grams=\" + i);\n        }\n\n        if (tokens.length < i+1) {\n          // Don't have enough tokens to use this model\n          if (VERBOSE) {\n            System.out.println(\"      skip\");\n          }\n          continue;\n        }\n\n        if (i == 0 && tokens[tokens.length-1].length() == 0) {\n          // Never suggest unigrams from empty string:\n          if (VERBOSE) {\n            System.out.println(\"      skip unigram priors only\");\n          }\n          continue;\n        }\n\n        // Build up \"context\" ngram:\n        b = new StringBuilder();\n        for(int j=tokens.length-i-1;j<tokens.length-1;j++) {\n          b.append(' ');\n          b.append(tokens[j]);\n        }\n        String context = b.toString();\n        if (context.length() > 0) {\n          context = context.substring(1);\n        }\n        if (VERBOSE) {\n          System.out.println(\"      context='\" + context + \"'\");\n        }\n        long contextCount;\n        if (context.length() == 0) {\n          contextCount = totTokens;\n        } else {\n          Integer count = gramCounts.get(i-1).get(context);\n          if (count == null) {\n            // We never saw this context:\n            backoff *= FreeTextSuggester.ALPHA;\n            if (VERBOSE) {\n              System.out.println(\"      skip: never saw context\");\n            }\n            continue;\n          }\n          contextCount = count;\n        }\n        if (VERBOSE) {\n          System.out.println(\"      contextCount=\" + contextCount);\n        }\n        Map<String,Integer> model = gramCounts.get(i);\n\n        // First pass, gather all predictions for this model:\n        if (VERBOSE) {\n          System.out.println(\"      find terms w/ prefix=\" + tokens[tokens.length-1]);\n        }\n        List<LookupResult> tmp = new ArrayList<LookupResult>();\n        for(String term : terms) {\n          if (term.startsWith(tokens[tokens.length-1])) {\n            if (VERBOSE) {\n              System.out.println(\"        term=\" + term);\n            }\n            if (seen.contains(term)) {\n              if (VERBOSE) {\n                System.out.println(\"          skip seen\");\n              }\n              continue;\n            }\n            String ngram = (context + \" \" + term).trim();\n            Integer count = model.get(ngram);\n            if (count != null) {\n              LookupResult lr = new LookupResult(ngram, (long) (Long.MAX_VALUE * (backoff * (double) count / contextCount)));\n              tmp.add(lr);\n              if (VERBOSE) {\n                System.out.println(\"      add tmp key='\" + lr.key + \"' score=\" + lr.value);\n              }\n            }\n          }\n        }\n\n        // Second pass, trim to only top N, and fold those\n        // into overall suggestions:\n        Collections.sort(tmp, byScoreThenKey);\n        if (tmp.size() > num) {\n          tmp.subList(num, tmp.size()).clear();\n        }\n        for(LookupResult result : tmp) {\n          String key = result.key.toString();\n          int idx = key.lastIndexOf(' ');\n          String lastToken;\n          if (idx != -1) {\n            lastToken = key.substring(idx+1);\n          } else {\n            lastToken = key;\n          }\n          if (!seen.contains(lastToken)) {\n            seen.add(lastToken);\n            expected.add(result);\n            if (VERBOSE) {\n              System.out.println(\"      keep key='\" + result.key + \"' score=\" + result.value);\n            }\n          }\n        }\n        \n        backoff *= FreeTextSuggester.ALPHA;\n      }\n\n      Collections.sort(expected, byScoreThenKey);\n\n      if (expected.size() > num) {\n        expected.subList(num, expected.size()).clear();\n      }\n\n      // Actual:\n      List<LookupResult> actual = sug.lookup(query, num);\n\n      if (VERBOSE) {\n        System.out.println(\"  expected: \" + expected);\n        System.out.println(\"    actual: \" + actual);\n      }\n\n      assertEquals(expected.toString(), actual.toString());\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws IOException {\n    String[] terms = new String[_TestUtil.nextInt(random(), 2, 10)];\n    Set<String> seen = new HashSet<String>();\n    while (seen.size() < terms.length) {\n      String token = _TestUtil.randomSimpleString(random(), 1, 5);\n      if (!seen.contains(token)) {\n        terms[seen.size()] = token;\n        seen.add(token);\n      }\n    }\n\n    Analyzer a = new MockAnalyzer(random());\n\n    int numDocs = atLeast(10);\n    long totTokens = 0;\n    final String[][] docs = new String[numDocs][];\n    for(int i=0;i<numDocs;i++) {\n      docs[i] = new String[atLeast(100)];\n      if (VERBOSE) {\n        System.out.print(\"  doc \" + i + \":\");\n      }\n      for(int j=0;j<docs[i].length;j++) {\n        docs[i][j] = getZipfToken(terms);\n        if (VERBOSE) {\n          System.out.print(\" \" + docs[i][j]);\n        }\n      }\n      if (VERBOSE) {\n        System.out.println();\n      }\n      totTokens += docs[i].length;\n    }\n\n    int grams = _TestUtil.nextInt(random(), 1, 4);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + terms.length + \" terms; \" + numDocs + \" docs; \" + grams + \" grams\");\n    }\n\n    // Build suggester model:\n    FreeTextSuggester sug = new FreeTextSuggester(a, a, grams, (byte) 0x20);\n    sug.build(new TermFreqPayloadIterator() {\n        int upto;\n\n        @Override\n        public BytesRef next() {\n          if (upto == docs.length) {\n            return null;\n          } else {\n            StringBuilder b = new StringBuilder();\n            for(String token : docs[upto]) {\n              b.append(' ');\n              b.append(token);\n            }\n            upto++;\n            return new BytesRef(b.toString());\n          }\n        }\n\n        @Override\n        public long weight() {\n          return random().nextLong();\n        }\n\n        @Override\n        public BytesRef payload() {\n          return null;\n        }\n\n        @Override\n        public boolean hasPayloads() {\n          return false;\n        }\n      });\n\n    // Build inefficient but hopefully correct model:\n    List<Map<String,Integer>> gramCounts = new ArrayList<Map<String,Integer>>(grams);\n    for(int gram=0;gram<grams;gram++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: build model for gram=\" + gram);\n      }\n      Map<String,Integer> model = new HashMap<String,Integer>();\n      gramCounts.add(model);\n      for(String[] doc : docs) {\n        for(int i=0;i<doc.length-gram;i++) {\n          StringBuilder b = new StringBuilder();\n          for(int j=i;j<=i+gram;j++) {\n            if (j > i) {\n              b.append(' ');\n            }\n            b.append(doc[j]);\n          }\n          String token = b.toString();\n          Integer curCount = model.get(token);\n          if (curCount == null) {\n            model.put(token, 1);\n          } else {\n            model.put(token, 1 + curCount);\n          }\n          if (VERBOSE) {\n            System.out.println(\"  add '\" + token + \"' -> count=\" + model.get(token));\n          }\n        }\n      }\n    }\n\n    int lookups = atLeast(100);\n    for(int iter=0;iter<lookups;iter++) {\n      String[] tokens = new String[_TestUtil.nextInt(random(), 1, 5)];\n      for(int i=0;i<tokens.length;i++) {\n        tokens[i] = getZipfToken(terms);\n      }\n\n      // Maybe trim last token; be sure not to create the\n      // empty string:\n      int trimStart;\n      if (tokens.length == 1) {\n        trimStart = 1;\n      } else {\n        trimStart = 0;\n      }\n      int trimAt = _TestUtil.nextInt(random(), trimStart, tokens[tokens.length-1].length());\n      tokens[tokens.length-1] = tokens[tokens.length-1].substring(0, trimAt);\n\n      int num = _TestUtil.nextInt(random(), 1, 100);\n      StringBuilder b = new StringBuilder();\n      for(String token : tokens) {\n        b.append(' ');\n        b.append(token);\n      }\n      String query = b.toString();\n      query = query.substring(1);\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" query='\" + query + \"' num=\" + num);\n      }\n\n      // Expected:\n      List<LookupResult> expected = new ArrayList<LookupResult>();\n      double backoff = 1.0;\n      seen = new HashSet<String>();\n\n      if (VERBOSE) {\n        System.out.println(\"  compute expected\");\n      }\n      for(int i=grams-1;i>=0;i--) {\n        if (VERBOSE) {\n          System.out.println(\"    grams=\" + i);\n        }\n\n        if (tokens.length < i+1) {\n          // Don't have enough tokens to use this model\n          if (VERBOSE) {\n            System.out.println(\"      skip\");\n          }\n          continue;\n        }\n\n        if (i == 0 && tokens[tokens.length-1].length() == 0) {\n          // Never suggest unigrams from empty string:\n          if (VERBOSE) {\n            System.out.println(\"      skip unigram priors only\");\n          }\n          continue;\n        }\n\n        // Build up \"context\" ngram:\n        b = new StringBuilder();\n        for(int j=tokens.length-i-1;j<tokens.length-1;j++) {\n          b.append(' ');\n          b.append(tokens[j]);\n        }\n        String context = b.toString();\n        if (context.length() > 0) {\n          context = context.substring(1);\n        }\n        if (VERBOSE) {\n          System.out.println(\"      context='\" + context + \"'\");\n        }\n        long contextCount;\n        if (context.length() == 0) {\n          contextCount = totTokens;\n        } else {\n          Integer count = gramCounts.get(i-1).get(context);\n          if (count == null) {\n            // We never saw this context:\n            backoff *= FreeTextSuggester.ALPHA;\n            if (VERBOSE) {\n              System.out.println(\"      skip: never saw context\");\n            }\n            continue;\n          }\n          contextCount = count;\n        }\n        if (VERBOSE) {\n          System.out.println(\"      contextCount=\" + contextCount);\n        }\n        Map<String,Integer> model = gramCounts.get(i);\n\n        // First pass, gather all predictions for this model:\n        if (VERBOSE) {\n          System.out.println(\"      find terms w/ prefix=\" + tokens[tokens.length-1]);\n        }\n        List<LookupResult> tmp = new ArrayList<LookupResult>();\n        for(String term : terms) {\n          if (term.startsWith(tokens[tokens.length-1])) {\n            if (VERBOSE) {\n              System.out.println(\"        term=\" + term);\n            }\n            if (seen.contains(term)) {\n              if (VERBOSE) {\n                System.out.println(\"          skip seen\");\n              }\n              continue;\n            }\n            String ngram = (context + \" \" + term).trim();\n            Integer count = model.get(ngram);\n            if (count != null) {\n              LookupResult lr = new LookupResult(ngram, (long) (Long.MAX_VALUE * (backoff * (double) count / contextCount)));\n              tmp.add(lr);\n              if (VERBOSE) {\n                System.out.println(\"      add tmp key='\" + lr.key + \"' score=\" + lr.value);\n              }\n            }\n          }\n        }\n\n        // Second pass, trim to only top N, and fold those\n        // into overall suggestions:\n        Collections.sort(tmp, byScoreThenKey);\n        if (tmp.size() > num) {\n          tmp.subList(num, tmp.size()).clear();\n        }\n        for(LookupResult result : tmp) {\n          String key = result.key.toString();\n          int idx = key.lastIndexOf(' ');\n          String lastToken;\n          if (idx != -1) {\n            lastToken = key.substring(idx+1);\n          } else {\n            lastToken = key;\n          }\n          if (!seen.contains(lastToken)) {\n            seen.add(lastToken);\n            expected.add(result);\n            if (VERBOSE) {\n              System.out.println(\"      keep key='\" + result.key + \"' score=\" + result.value);\n            }\n          }\n        }\n        \n        backoff *= FreeTextSuggester.ALPHA;\n      }\n\n      Collections.sort(expected, byScoreThenKey);\n\n      if (expected.size() > num) {\n        expected.subList(num, expected.size()).clear();\n      }\n\n      // Actual:\n      List<LookupResult> actual = sug.lookup(query, num);\n\n      if (VERBOSE) {\n        System.out.println(\"  expected: \" + expected);\n        System.out.println(\"    actual: \" + actual);\n      }\n\n      assertEquals(expected.toString(), actual.toString());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestFreeTextSuggester#testRandom().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestFreeTextSuggester#testRandom().mjava","sourceNew":"  public void testRandom() throws IOException {\n    String[] terms = new String[TestUtil.nextInt(random(), 2, 10)];\n    Set<String> seen = new HashSet<String>();\n    while (seen.size() < terms.length) {\n      String token = TestUtil.randomSimpleString(random(), 1, 5);\n      if (!seen.contains(token)) {\n        terms[seen.size()] = token;\n        seen.add(token);\n      }\n    }\n\n    Analyzer a = new MockAnalyzer(random());\n\n    int numDocs = atLeast(10);\n    long totTokens = 0;\n    final String[][] docs = new String[numDocs][];\n    for(int i=0;i<numDocs;i++) {\n      docs[i] = new String[atLeast(100)];\n      if (VERBOSE) {\n        System.out.print(\"  doc \" + i + \":\");\n      }\n      for(int j=0;j<docs[i].length;j++) {\n        docs[i][j] = getZipfToken(terms);\n        if (VERBOSE) {\n          System.out.print(\" \" + docs[i][j]);\n        }\n      }\n      if (VERBOSE) {\n        System.out.println();\n      }\n      totTokens += docs[i].length;\n    }\n\n    int grams = TestUtil.nextInt(random(), 1, 4);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + terms.length + \" terms; \" + numDocs + \" docs; \" + grams + \" grams\");\n    }\n\n    // Build suggester model:\n    FreeTextSuggester sug = new FreeTextSuggester(a, a, grams, (byte) 0x20);\n    sug.build(new InputIterator() {\n        int upto;\n\n        @Override\n        public BytesRef next() {\n          if (upto == docs.length) {\n            return null;\n          } else {\n            StringBuilder b = new StringBuilder();\n            for(String token : docs[upto]) {\n              b.append(' ');\n              b.append(token);\n            }\n            upto++;\n            return new BytesRef(b.toString());\n          }\n        }\n\n        @Override\n        public long weight() {\n          return random().nextLong();\n        }\n\n        @Override\n        public BytesRef payload() {\n          return null;\n        }\n\n        @Override\n        public boolean hasPayloads() {\n          return false;\n        }\n      });\n\n    // Build inefficient but hopefully correct model:\n    List<Map<String,Integer>> gramCounts = new ArrayList<Map<String,Integer>>(grams);\n    for(int gram=0;gram<grams;gram++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: build model for gram=\" + gram);\n      }\n      Map<String,Integer> model = new HashMap<String,Integer>();\n      gramCounts.add(model);\n      for(String[] doc : docs) {\n        for(int i=0;i<doc.length-gram;i++) {\n          StringBuilder b = new StringBuilder();\n          for(int j=i;j<=i+gram;j++) {\n            if (j > i) {\n              b.append(' ');\n            }\n            b.append(doc[j]);\n          }\n          String token = b.toString();\n          Integer curCount = model.get(token);\n          if (curCount == null) {\n            model.put(token, 1);\n          } else {\n            model.put(token, 1 + curCount);\n          }\n          if (VERBOSE) {\n            System.out.println(\"  add '\" + token + \"' -> count=\" + model.get(token));\n          }\n        }\n      }\n    }\n\n    int lookups = atLeast(100);\n    for(int iter=0;iter<lookups;iter++) {\n      String[] tokens = new String[TestUtil.nextInt(random(), 1, 5)];\n      for(int i=0;i<tokens.length;i++) {\n        tokens[i] = getZipfToken(terms);\n      }\n\n      // Maybe trim last token; be sure not to create the\n      // empty string:\n      int trimStart;\n      if (tokens.length == 1) {\n        trimStart = 1;\n      } else {\n        trimStart = 0;\n      }\n      int trimAt = TestUtil.nextInt(random(), trimStart, tokens[tokens.length - 1].length());\n      tokens[tokens.length-1] = tokens[tokens.length-1].substring(0, trimAt);\n\n      int num = TestUtil.nextInt(random(), 1, 100);\n      StringBuilder b = new StringBuilder();\n      for(String token : tokens) {\n        b.append(' ');\n        b.append(token);\n      }\n      String query = b.toString();\n      query = query.substring(1);\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" query='\" + query + \"' num=\" + num);\n      }\n\n      // Expected:\n      List<LookupResult> expected = new ArrayList<LookupResult>();\n      double backoff = 1.0;\n      seen = new HashSet<String>();\n\n      if (VERBOSE) {\n        System.out.println(\"  compute expected\");\n      }\n      for(int i=grams-1;i>=0;i--) {\n        if (VERBOSE) {\n          System.out.println(\"    grams=\" + i);\n        }\n\n        if (tokens.length < i+1) {\n          // Don't have enough tokens to use this model\n          if (VERBOSE) {\n            System.out.println(\"      skip\");\n          }\n          continue;\n        }\n\n        if (i == 0 && tokens[tokens.length-1].length() == 0) {\n          // Never suggest unigrams from empty string:\n          if (VERBOSE) {\n            System.out.println(\"      skip unigram priors only\");\n          }\n          continue;\n        }\n\n        // Build up \"context\" ngram:\n        b = new StringBuilder();\n        for(int j=tokens.length-i-1;j<tokens.length-1;j++) {\n          b.append(' ');\n          b.append(tokens[j]);\n        }\n        String context = b.toString();\n        if (context.length() > 0) {\n          context = context.substring(1);\n        }\n        if (VERBOSE) {\n          System.out.println(\"      context='\" + context + \"'\");\n        }\n        long contextCount;\n        if (context.length() == 0) {\n          contextCount = totTokens;\n        } else {\n          Integer count = gramCounts.get(i-1).get(context);\n          if (count == null) {\n            // We never saw this context:\n            backoff *= FreeTextSuggester.ALPHA;\n            if (VERBOSE) {\n              System.out.println(\"      skip: never saw context\");\n            }\n            continue;\n          }\n          contextCount = count;\n        }\n        if (VERBOSE) {\n          System.out.println(\"      contextCount=\" + contextCount);\n        }\n        Map<String,Integer> model = gramCounts.get(i);\n\n        // First pass, gather all predictions for this model:\n        if (VERBOSE) {\n          System.out.println(\"      find terms w/ prefix=\" + tokens[tokens.length-1]);\n        }\n        List<LookupResult> tmp = new ArrayList<LookupResult>();\n        for(String term : terms) {\n          if (term.startsWith(tokens[tokens.length-1])) {\n            if (VERBOSE) {\n              System.out.println(\"        term=\" + term);\n            }\n            if (seen.contains(term)) {\n              if (VERBOSE) {\n                System.out.println(\"          skip seen\");\n              }\n              continue;\n            }\n            String ngram = (context + \" \" + term).trim();\n            Integer count = model.get(ngram);\n            if (count != null) {\n              LookupResult lr = new LookupResult(ngram, (long) (Long.MAX_VALUE * (backoff * (double) count / contextCount)));\n              tmp.add(lr);\n              if (VERBOSE) {\n                System.out.println(\"      add tmp key='\" + lr.key + \"' score=\" + lr.value);\n              }\n            }\n          }\n        }\n\n        // Second pass, trim to only top N, and fold those\n        // into overall suggestions:\n        Collections.sort(tmp, byScoreThenKey);\n        if (tmp.size() > num) {\n          tmp.subList(num, tmp.size()).clear();\n        }\n        for(LookupResult result : tmp) {\n          String key = result.key.toString();\n          int idx = key.lastIndexOf(' ');\n          String lastToken;\n          if (idx != -1) {\n            lastToken = key.substring(idx+1);\n          } else {\n            lastToken = key;\n          }\n          if (!seen.contains(lastToken)) {\n            seen.add(lastToken);\n            expected.add(result);\n            if (VERBOSE) {\n              System.out.println(\"      keep key='\" + result.key + \"' score=\" + result.value);\n            }\n          }\n        }\n        \n        backoff *= FreeTextSuggester.ALPHA;\n      }\n\n      Collections.sort(expected, byScoreThenKey);\n\n      if (expected.size() > num) {\n        expected.subList(num, expected.size()).clear();\n      }\n\n      // Actual:\n      List<LookupResult> actual = sug.lookup(query, num);\n\n      if (VERBOSE) {\n        System.out.println(\"  expected: \" + expected);\n        System.out.println(\"    actual: \" + actual);\n      }\n\n      assertEquals(expected.toString(), actual.toString());\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws IOException {\n    String[] terms = new String[_TestUtil.nextInt(random(), 2, 10)];\n    Set<String> seen = new HashSet<String>();\n    while (seen.size() < terms.length) {\n      String token = _TestUtil.randomSimpleString(random(), 1, 5);\n      if (!seen.contains(token)) {\n        terms[seen.size()] = token;\n        seen.add(token);\n      }\n    }\n\n    Analyzer a = new MockAnalyzer(random());\n\n    int numDocs = atLeast(10);\n    long totTokens = 0;\n    final String[][] docs = new String[numDocs][];\n    for(int i=0;i<numDocs;i++) {\n      docs[i] = new String[atLeast(100)];\n      if (VERBOSE) {\n        System.out.print(\"  doc \" + i + \":\");\n      }\n      for(int j=0;j<docs[i].length;j++) {\n        docs[i][j] = getZipfToken(terms);\n        if (VERBOSE) {\n          System.out.print(\" \" + docs[i][j]);\n        }\n      }\n      if (VERBOSE) {\n        System.out.println();\n      }\n      totTokens += docs[i].length;\n    }\n\n    int grams = _TestUtil.nextInt(random(), 1, 4);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + terms.length + \" terms; \" + numDocs + \" docs; \" + grams + \" grams\");\n    }\n\n    // Build suggester model:\n    FreeTextSuggester sug = new FreeTextSuggester(a, a, grams, (byte) 0x20);\n    sug.build(new InputIterator() {\n        int upto;\n\n        @Override\n        public BytesRef next() {\n          if (upto == docs.length) {\n            return null;\n          } else {\n            StringBuilder b = new StringBuilder();\n            for(String token : docs[upto]) {\n              b.append(' ');\n              b.append(token);\n            }\n            upto++;\n            return new BytesRef(b.toString());\n          }\n        }\n\n        @Override\n        public long weight() {\n          return random().nextLong();\n        }\n\n        @Override\n        public BytesRef payload() {\n          return null;\n        }\n\n        @Override\n        public boolean hasPayloads() {\n          return false;\n        }\n      });\n\n    // Build inefficient but hopefully correct model:\n    List<Map<String,Integer>> gramCounts = new ArrayList<Map<String,Integer>>(grams);\n    for(int gram=0;gram<grams;gram++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: build model for gram=\" + gram);\n      }\n      Map<String,Integer> model = new HashMap<String,Integer>();\n      gramCounts.add(model);\n      for(String[] doc : docs) {\n        for(int i=0;i<doc.length-gram;i++) {\n          StringBuilder b = new StringBuilder();\n          for(int j=i;j<=i+gram;j++) {\n            if (j > i) {\n              b.append(' ');\n            }\n            b.append(doc[j]);\n          }\n          String token = b.toString();\n          Integer curCount = model.get(token);\n          if (curCount == null) {\n            model.put(token, 1);\n          } else {\n            model.put(token, 1 + curCount);\n          }\n          if (VERBOSE) {\n            System.out.println(\"  add '\" + token + \"' -> count=\" + model.get(token));\n          }\n        }\n      }\n    }\n\n    int lookups = atLeast(100);\n    for(int iter=0;iter<lookups;iter++) {\n      String[] tokens = new String[_TestUtil.nextInt(random(), 1, 5)];\n      for(int i=0;i<tokens.length;i++) {\n        tokens[i] = getZipfToken(terms);\n      }\n\n      // Maybe trim last token; be sure not to create the\n      // empty string:\n      int trimStart;\n      if (tokens.length == 1) {\n        trimStart = 1;\n      } else {\n        trimStart = 0;\n      }\n      int trimAt = _TestUtil.nextInt(random(), trimStart, tokens[tokens.length-1].length());\n      tokens[tokens.length-1] = tokens[tokens.length-1].substring(0, trimAt);\n\n      int num = _TestUtil.nextInt(random(), 1, 100);\n      StringBuilder b = new StringBuilder();\n      for(String token : tokens) {\n        b.append(' ');\n        b.append(token);\n      }\n      String query = b.toString();\n      query = query.substring(1);\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" query='\" + query + \"' num=\" + num);\n      }\n\n      // Expected:\n      List<LookupResult> expected = new ArrayList<LookupResult>();\n      double backoff = 1.0;\n      seen = new HashSet<String>();\n\n      if (VERBOSE) {\n        System.out.println(\"  compute expected\");\n      }\n      for(int i=grams-1;i>=0;i--) {\n        if (VERBOSE) {\n          System.out.println(\"    grams=\" + i);\n        }\n\n        if (tokens.length < i+1) {\n          // Don't have enough tokens to use this model\n          if (VERBOSE) {\n            System.out.println(\"      skip\");\n          }\n          continue;\n        }\n\n        if (i == 0 && tokens[tokens.length-1].length() == 0) {\n          // Never suggest unigrams from empty string:\n          if (VERBOSE) {\n            System.out.println(\"      skip unigram priors only\");\n          }\n          continue;\n        }\n\n        // Build up \"context\" ngram:\n        b = new StringBuilder();\n        for(int j=tokens.length-i-1;j<tokens.length-1;j++) {\n          b.append(' ');\n          b.append(tokens[j]);\n        }\n        String context = b.toString();\n        if (context.length() > 0) {\n          context = context.substring(1);\n        }\n        if (VERBOSE) {\n          System.out.println(\"      context='\" + context + \"'\");\n        }\n        long contextCount;\n        if (context.length() == 0) {\n          contextCount = totTokens;\n        } else {\n          Integer count = gramCounts.get(i-1).get(context);\n          if (count == null) {\n            // We never saw this context:\n            backoff *= FreeTextSuggester.ALPHA;\n            if (VERBOSE) {\n              System.out.println(\"      skip: never saw context\");\n            }\n            continue;\n          }\n          contextCount = count;\n        }\n        if (VERBOSE) {\n          System.out.println(\"      contextCount=\" + contextCount);\n        }\n        Map<String,Integer> model = gramCounts.get(i);\n\n        // First pass, gather all predictions for this model:\n        if (VERBOSE) {\n          System.out.println(\"      find terms w/ prefix=\" + tokens[tokens.length-1]);\n        }\n        List<LookupResult> tmp = new ArrayList<LookupResult>();\n        for(String term : terms) {\n          if (term.startsWith(tokens[tokens.length-1])) {\n            if (VERBOSE) {\n              System.out.println(\"        term=\" + term);\n            }\n            if (seen.contains(term)) {\n              if (VERBOSE) {\n                System.out.println(\"          skip seen\");\n              }\n              continue;\n            }\n            String ngram = (context + \" \" + term).trim();\n            Integer count = model.get(ngram);\n            if (count != null) {\n              LookupResult lr = new LookupResult(ngram, (long) (Long.MAX_VALUE * (backoff * (double) count / contextCount)));\n              tmp.add(lr);\n              if (VERBOSE) {\n                System.out.println(\"      add tmp key='\" + lr.key + \"' score=\" + lr.value);\n              }\n            }\n          }\n        }\n\n        // Second pass, trim to only top N, and fold those\n        // into overall suggestions:\n        Collections.sort(tmp, byScoreThenKey);\n        if (tmp.size() > num) {\n          tmp.subList(num, tmp.size()).clear();\n        }\n        for(LookupResult result : tmp) {\n          String key = result.key.toString();\n          int idx = key.lastIndexOf(' ');\n          String lastToken;\n          if (idx != -1) {\n            lastToken = key.substring(idx+1);\n          } else {\n            lastToken = key;\n          }\n          if (!seen.contains(lastToken)) {\n            seen.add(lastToken);\n            expected.add(result);\n            if (VERBOSE) {\n              System.out.println(\"      keep key='\" + result.key + \"' score=\" + result.value);\n            }\n          }\n        }\n        \n        backoff *= FreeTextSuggester.ALPHA;\n      }\n\n      Collections.sort(expected, byScoreThenKey);\n\n      if (expected.size() > num) {\n        expected.subList(num, expected.size()).clear();\n      }\n\n      // Actual:\n      List<LookupResult> actual = sug.lookup(query, num);\n\n      if (VERBOSE) {\n        System.out.println(\"  expected: \" + expected);\n        System.out.println(\"    actual: \" + actual);\n      }\n\n      assertEquals(expected.toString(), actual.toString());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestFreeTextSuggester#testRandom().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestFreeTextSuggester#testRandom().mjava","sourceNew":"  public void testRandom() throws IOException {\n    String[] terms = new String[TestUtil.nextInt(random(), 2, 10)];\n    Set<String> seen = new HashSet<>();\n    while (seen.size() < terms.length) {\n      String token = TestUtil.randomSimpleString(random(), 1, 5);\n      if (!seen.contains(token)) {\n        terms[seen.size()] = token;\n        seen.add(token);\n      }\n    }\n\n    Analyzer a = new MockAnalyzer(random());\n\n    int numDocs = atLeast(10);\n    long totTokens = 0;\n    final String[][] docs = new String[numDocs][];\n    for(int i=0;i<numDocs;i++) {\n      docs[i] = new String[atLeast(100)];\n      if (VERBOSE) {\n        System.out.print(\"  doc \" + i + \":\");\n      }\n      for(int j=0;j<docs[i].length;j++) {\n        docs[i][j] = getZipfToken(terms);\n        if (VERBOSE) {\n          System.out.print(\" \" + docs[i][j]);\n        }\n      }\n      if (VERBOSE) {\n        System.out.println();\n      }\n      totTokens += docs[i].length;\n    }\n\n    int grams = TestUtil.nextInt(random(), 1, 4);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + terms.length + \" terms; \" + numDocs + \" docs; \" + grams + \" grams\");\n    }\n\n    // Build suggester model:\n    FreeTextSuggester sug = new FreeTextSuggester(a, a, grams, (byte) 0x20);\n    sug.build(new InputIterator() {\n        int upto;\n\n        @Override\n        public BytesRef next() {\n          if (upto == docs.length) {\n            return null;\n          } else {\n            StringBuilder b = new StringBuilder();\n            for(String token : docs[upto]) {\n              b.append(' ');\n              b.append(token);\n            }\n            upto++;\n            return new BytesRef(b.toString());\n          }\n        }\n\n        @Override\n        public long weight() {\n          return random().nextLong();\n        }\n\n        @Override\n        public BytesRef payload() {\n          return null;\n        }\n\n        @Override\n        public boolean hasPayloads() {\n          return false;\n        }\n      });\n\n    // Build inefficient but hopefully correct model:\n    List<Map<String,Integer>> gramCounts = new ArrayList<>(grams);\n    for(int gram=0;gram<grams;gram++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: build model for gram=\" + gram);\n      }\n      Map<String,Integer> model = new HashMap<>();\n      gramCounts.add(model);\n      for(String[] doc : docs) {\n        for(int i=0;i<doc.length-gram;i++) {\n          StringBuilder b = new StringBuilder();\n          for(int j=i;j<=i+gram;j++) {\n            if (j > i) {\n              b.append(' ');\n            }\n            b.append(doc[j]);\n          }\n          String token = b.toString();\n          Integer curCount = model.get(token);\n          if (curCount == null) {\n            model.put(token, 1);\n          } else {\n            model.put(token, 1 + curCount);\n          }\n          if (VERBOSE) {\n            System.out.println(\"  add '\" + token + \"' -> count=\" + model.get(token));\n          }\n        }\n      }\n    }\n\n    int lookups = atLeast(100);\n    for(int iter=0;iter<lookups;iter++) {\n      String[] tokens = new String[TestUtil.nextInt(random(), 1, 5)];\n      for(int i=0;i<tokens.length;i++) {\n        tokens[i] = getZipfToken(terms);\n      }\n\n      // Maybe trim last token; be sure not to create the\n      // empty string:\n      int trimStart;\n      if (tokens.length == 1) {\n        trimStart = 1;\n      } else {\n        trimStart = 0;\n      }\n      int trimAt = TestUtil.nextInt(random(), trimStart, tokens[tokens.length - 1].length());\n      tokens[tokens.length-1] = tokens[tokens.length-1].substring(0, trimAt);\n\n      int num = TestUtil.nextInt(random(), 1, 100);\n      StringBuilder b = new StringBuilder();\n      for(String token : tokens) {\n        b.append(' ');\n        b.append(token);\n      }\n      String query = b.toString();\n      query = query.substring(1);\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" query='\" + query + \"' num=\" + num);\n      }\n\n      // Expected:\n      List<LookupResult> expected = new ArrayList<>();\n      double backoff = 1.0;\n      seen = new HashSet<>();\n\n      if (VERBOSE) {\n        System.out.println(\"  compute expected\");\n      }\n      for(int i=grams-1;i>=0;i--) {\n        if (VERBOSE) {\n          System.out.println(\"    grams=\" + i);\n        }\n\n        if (tokens.length < i+1) {\n          // Don't have enough tokens to use this model\n          if (VERBOSE) {\n            System.out.println(\"      skip\");\n          }\n          continue;\n        }\n\n        if (i == 0 && tokens[tokens.length-1].length() == 0) {\n          // Never suggest unigrams from empty string:\n          if (VERBOSE) {\n            System.out.println(\"      skip unigram priors only\");\n          }\n          continue;\n        }\n\n        // Build up \"context\" ngram:\n        b = new StringBuilder();\n        for(int j=tokens.length-i-1;j<tokens.length-1;j++) {\n          b.append(' ');\n          b.append(tokens[j]);\n        }\n        String context = b.toString();\n        if (context.length() > 0) {\n          context = context.substring(1);\n        }\n        if (VERBOSE) {\n          System.out.println(\"      context='\" + context + \"'\");\n        }\n        long contextCount;\n        if (context.length() == 0) {\n          contextCount = totTokens;\n        } else {\n          Integer count = gramCounts.get(i-1).get(context);\n          if (count == null) {\n            // We never saw this context:\n            backoff *= FreeTextSuggester.ALPHA;\n            if (VERBOSE) {\n              System.out.println(\"      skip: never saw context\");\n            }\n            continue;\n          }\n          contextCount = count;\n        }\n        if (VERBOSE) {\n          System.out.println(\"      contextCount=\" + contextCount);\n        }\n        Map<String,Integer> model = gramCounts.get(i);\n\n        // First pass, gather all predictions for this model:\n        if (VERBOSE) {\n          System.out.println(\"      find terms w/ prefix=\" + tokens[tokens.length-1]);\n        }\n        List<LookupResult> tmp = new ArrayList<>();\n        for(String term : terms) {\n          if (term.startsWith(tokens[tokens.length-1])) {\n            if (VERBOSE) {\n              System.out.println(\"        term=\" + term);\n            }\n            if (seen.contains(term)) {\n              if (VERBOSE) {\n                System.out.println(\"          skip seen\");\n              }\n              continue;\n            }\n            String ngram = (context + \" \" + term).trim();\n            Integer count = model.get(ngram);\n            if (count != null) {\n              LookupResult lr = new LookupResult(ngram, (long) (Long.MAX_VALUE * (backoff * (double) count / contextCount)));\n              tmp.add(lr);\n              if (VERBOSE) {\n                System.out.println(\"      add tmp key='\" + lr.key + \"' score=\" + lr.value);\n              }\n            }\n          }\n        }\n\n        // Second pass, trim to only top N, and fold those\n        // into overall suggestions:\n        Collections.sort(tmp, byScoreThenKey);\n        if (tmp.size() > num) {\n          tmp.subList(num, tmp.size()).clear();\n        }\n        for(LookupResult result : tmp) {\n          String key = result.key.toString();\n          int idx = key.lastIndexOf(' ');\n          String lastToken;\n          if (idx != -1) {\n            lastToken = key.substring(idx+1);\n          } else {\n            lastToken = key;\n          }\n          if (!seen.contains(lastToken)) {\n            seen.add(lastToken);\n            expected.add(result);\n            if (VERBOSE) {\n              System.out.println(\"      keep key='\" + result.key + \"' score=\" + result.value);\n            }\n          }\n        }\n        \n        backoff *= FreeTextSuggester.ALPHA;\n      }\n\n      Collections.sort(expected, byScoreThenKey);\n\n      if (expected.size() > num) {\n        expected.subList(num, expected.size()).clear();\n      }\n\n      // Actual:\n      List<LookupResult> actual = sug.lookup(query, num);\n\n      if (VERBOSE) {\n        System.out.println(\"  expected: \" + expected);\n        System.out.println(\"    actual: \" + actual);\n      }\n\n      assertEquals(expected.toString(), actual.toString());\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws IOException {\n    String[] terms = new String[TestUtil.nextInt(random(), 2, 10)];\n    Set<String> seen = new HashSet<String>();\n    while (seen.size() < terms.length) {\n      String token = TestUtil.randomSimpleString(random(), 1, 5);\n      if (!seen.contains(token)) {\n        terms[seen.size()] = token;\n        seen.add(token);\n      }\n    }\n\n    Analyzer a = new MockAnalyzer(random());\n\n    int numDocs = atLeast(10);\n    long totTokens = 0;\n    final String[][] docs = new String[numDocs][];\n    for(int i=0;i<numDocs;i++) {\n      docs[i] = new String[atLeast(100)];\n      if (VERBOSE) {\n        System.out.print(\"  doc \" + i + \":\");\n      }\n      for(int j=0;j<docs[i].length;j++) {\n        docs[i][j] = getZipfToken(terms);\n        if (VERBOSE) {\n          System.out.print(\" \" + docs[i][j]);\n        }\n      }\n      if (VERBOSE) {\n        System.out.println();\n      }\n      totTokens += docs[i].length;\n    }\n\n    int grams = TestUtil.nextInt(random(), 1, 4);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + terms.length + \" terms; \" + numDocs + \" docs; \" + grams + \" grams\");\n    }\n\n    // Build suggester model:\n    FreeTextSuggester sug = new FreeTextSuggester(a, a, grams, (byte) 0x20);\n    sug.build(new InputIterator() {\n        int upto;\n\n        @Override\n        public BytesRef next() {\n          if (upto == docs.length) {\n            return null;\n          } else {\n            StringBuilder b = new StringBuilder();\n            for(String token : docs[upto]) {\n              b.append(' ');\n              b.append(token);\n            }\n            upto++;\n            return new BytesRef(b.toString());\n          }\n        }\n\n        @Override\n        public long weight() {\n          return random().nextLong();\n        }\n\n        @Override\n        public BytesRef payload() {\n          return null;\n        }\n\n        @Override\n        public boolean hasPayloads() {\n          return false;\n        }\n      });\n\n    // Build inefficient but hopefully correct model:\n    List<Map<String,Integer>> gramCounts = new ArrayList<Map<String,Integer>>(grams);\n    for(int gram=0;gram<grams;gram++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: build model for gram=\" + gram);\n      }\n      Map<String,Integer> model = new HashMap<String,Integer>();\n      gramCounts.add(model);\n      for(String[] doc : docs) {\n        for(int i=0;i<doc.length-gram;i++) {\n          StringBuilder b = new StringBuilder();\n          for(int j=i;j<=i+gram;j++) {\n            if (j > i) {\n              b.append(' ');\n            }\n            b.append(doc[j]);\n          }\n          String token = b.toString();\n          Integer curCount = model.get(token);\n          if (curCount == null) {\n            model.put(token, 1);\n          } else {\n            model.put(token, 1 + curCount);\n          }\n          if (VERBOSE) {\n            System.out.println(\"  add '\" + token + \"' -> count=\" + model.get(token));\n          }\n        }\n      }\n    }\n\n    int lookups = atLeast(100);\n    for(int iter=0;iter<lookups;iter++) {\n      String[] tokens = new String[TestUtil.nextInt(random(), 1, 5)];\n      for(int i=0;i<tokens.length;i++) {\n        tokens[i] = getZipfToken(terms);\n      }\n\n      // Maybe trim last token; be sure not to create the\n      // empty string:\n      int trimStart;\n      if (tokens.length == 1) {\n        trimStart = 1;\n      } else {\n        trimStart = 0;\n      }\n      int trimAt = TestUtil.nextInt(random(), trimStart, tokens[tokens.length - 1].length());\n      tokens[tokens.length-1] = tokens[tokens.length-1].substring(0, trimAt);\n\n      int num = TestUtil.nextInt(random(), 1, 100);\n      StringBuilder b = new StringBuilder();\n      for(String token : tokens) {\n        b.append(' ');\n        b.append(token);\n      }\n      String query = b.toString();\n      query = query.substring(1);\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" query='\" + query + \"' num=\" + num);\n      }\n\n      // Expected:\n      List<LookupResult> expected = new ArrayList<LookupResult>();\n      double backoff = 1.0;\n      seen = new HashSet<String>();\n\n      if (VERBOSE) {\n        System.out.println(\"  compute expected\");\n      }\n      for(int i=grams-1;i>=0;i--) {\n        if (VERBOSE) {\n          System.out.println(\"    grams=\" + i);\n        }\n\n        if (tokens.length < i+1) {\n          // Don't have enough tokens to use this model\n          if (VERBOSE) {\n            System.out.println(\"      skip\");\n          }\n          continue;\n        }\n\n        if (i == 0 && tokens[tokens.length-1].length() == 0) {\n          // Never suggest unigrams from empty string:\n          if (VERBOSE) {\n            System.out.println(\"      skip unigram priors only\");\n          }\n          continue;\n        }\n\n        // Build up \"context\" ngram:\n        b = new StringBuilder();\n        for(int j=tokens.length-i-1;j<tokens.length-1;j++) {\n          b.append(' ');\n          b.append(tokens[j]);\n        }\n        String context = b.toString();\n        if (context.length() > 0) {\n          context = context.substring(1);\n        }\n        if (VERBOSE) {\n          System.out.println(\"      context='\" + context + \"'\");\n        }\n        long contextCount;\n        if (context.length() == 0) {\n          contextCount = totTokens;\n        } else {\n          Integer count = gramCounts.get(i-1).get(context);\n          if (count == null) {\n            // We never saw this context:\n            backoff *= FreeTextSuggester.ALPHA;\n            if (VERBOSE) {\n              System.out.println(\"      skip: never saw context\");\n            }\n            continue;\n          }\n          contextCount = count;\n        }\n        if (VERBOSE) {\n          System.out.println(\"      contextCount=\" + contextCount);\n        }\n        Map<String,Integer> model = gramCounts.get(i);\n\n        // First pass, gather all predictions for this model:\n        if (VERBOSE) {\n          System.out.println(\"      find terms w/ prefix=\" + tokens[tokens.length-1]);\n        }\n        List<LookupResult> tmp = new ArrayList<LookupResult>();\n        for(String term : terms) {\n          if (term.startsWith(tokens[tokens.length-1])) {\n            if (VERBOSE) {\n              System.out.println(\"        term=\" + term);\n            }\n            if (seen.contains(term)) {\n              if (VERBOSE) {\n                System.out.println(\"          skip seen\");\n              }\n              continue;\n            }\n            String ngram = (context + \" \" + term).trim();\n            Integer count = model.get(ngram);\n            if (count != null) {\n              LookupResult lr = new LookupResult(ngram, (long) (Long.MAX_VALUE * (backoff * (double) count / contextCount)));\n              tmp.add(lr);\n              if (VERBOSE) {\n                System.out.println(\"      add tmp key='\" + lr.key + \"' score=\" + lr.value);\n              }\n            }\n          }\n        }\n\n        // Second pass, trim to only top N, and fold those\n        // into overall suggestions:\n        Collections.sort(tmp, byScoreThenKey);\n        if (tmp.size() > num) {\n          tmp.subList(num, tmp.size()).clear();\n        }\n        for(LookupResult result : tmp) {\n          String key = result.key.toString();\n          int idx = key.lastIndexOf(' ');\n          String lastToken;\n          if (idx != -1) {\n            lastToken = key.substring(idx+1);\n          } else {\n            lastToken = key;\n          }\n          if (!seen.contains(lastToken)) {\n            seen.add(lastToken);\n            expected.add(result);\n            if (VERBOSE) {\n              System.out.println(\"      keep key='\" + result.key + \"' score=\" + result.value);\n            }\n          }\n        }\n        \n        backoff *= FreeTextSuggester.ALPHA;\n      }\n\n      Collections.sort(expected, byScoreThenKey);\n\n      if (expected.size() > num) {\n        expected.subList(num, expected.size()).clear();\n      }\n\n      // Actual:\n      List<LookupResult> actual = sug.lookup(query, num);\n\n      if (VERBOSE) {\n        System.out.println(\"  expected: \" + expected);\n        System.out.println(\"    actual: \" + actual);\n      }\n\n      assertEquals(expected.toString(), actual.toString());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b","date":1395588343,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestFreeTextSuggester#testRandom().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestFreeTextSuggester#testRandom().mjava","sourceNew":"  public void testRandom() throws IOException {\n    String[] terms = new String[TestUtil.nextInt(random(), 2, 10)];\n    Set<String> seen = new HashSet<>();\n    while (seen.size() < terms.length) {\n      String token = TestUtil.randomSimpleString(random(), 1, 5);\n      if (!seen.contains(token)) {\n        terms[seen.size()] = token;\n        seen.add(token);\n      }\n    }\n\n    Analyzer a = new MockAnalyzer(random());\n\n    int numDocs = atLeast(10);\n    long totTokens = 0;\n    final String[][] docs = new String[numDocs][];\n    for(int i=0;i<numDocs;i++) {\n      docs[i] = new String[atLeast(100)];\n      if (VERBOSE) {\n        System.out.print(\"  doc \" + i + \":\");\n      }\n      for(int j=0;j<docs[i].length;j++) {\n        docs[i][j] = getZipfToken(terms);\n        if (VERBOSE) {\n          System.out.print(\" \" + docs[i][j]);\n        }\n      }\n      if (VERBOSE) {\n        System.out.println();\n      }\n      totTokens += docs[i].length;\n    }\n\n    int grams = TestUtil.nextInt(random(), 1, 4);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + terms.length + \" terms; \" + numDocs + \" docs; \" + grams + \" grams\");\n    }\n\n    // Build suggester model:\n    FreeTextSuggester sug = new FreeTextSuggester(a, a, grams, (byte) 0x20);\n    sug.build(new InputIterator() {\n        int upto;\n\n        @Override\n        public BytesRef next() {\n          if (upto == docs.length) {\n            return null;\n          } else {\n            StringBuilder b = new StringBuilder();\n            for(String token : docs[upto]) {\n              b.append(' ');\n              b.append(token);\n            }\n            upto++;\n            return new BytesRef(b.toString());\n          }\n        }\n\n        @Override\n        public long weight() {\n          return random().nextLong();\n        }\n\n        @Override\n        public BytesRef payload() {\n          return null;\n        }\n\n        @Override\n        public boolean hasPayloads() {\n          return false;\n        }\n\n        @Override\n        public Set<BytesRef> contexts() {\n          return null;\n        }\n\n        @Override\n        public boolean hasContexts() {\n          return false;\n        }\n      });\n\n    // Build inefficient but hopefully correct model:\n    List<Map<String,Integer>> gramCounts = new ArrayList<>(grams);\n    for(int gram=0;gram<grams;gram++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: build model for gram=\" + gram);\n      }\n      Map<String,Integer> model = new HashMap<>();\n      gramCounts.add(model);\n      for(String[] doc : docs) {\n        for(int i=0;i<doc.length-gram;i++) {\n          StringBuilder b = new StringBuilder();\n          for(int j=i;j<=i+gram;j++) {\n            if (j > i) {\n              b.append(' ');\n            }\n            b.append(doc[j]);\n          }\n          String token = b.toString();\n          Integer curCount = model.get(token);\n          if (curCount == null) {\n            model.put(token, 1);\n          } else {\n            model.put(token, 1 + curCount);\n          }\n          if (VERBOSE) {\n            System.out.println(\"  add '\" + token + \"' -> count=\" + model.get(token));\n          }\n        }\n      }\n    }\n\n    int lookups = atLeast(100);\n    for(int iter=0;iter<lookups;iter++) {\n      String[] tokens = new String[TestUtil.nextInt(random(), 1, 5)];\n      for(int i=0;i<tokens.length;i++) {\n        tokens[i] = getZipfToken(terms);\n      }\n\n      // Maybe trim last token; be sure not to create the\n      // empty string:\n      int trimStart;\n      if (tokens.length == 1) {\n        trimStart = 1;\n      } else {\n        trimStart = 0;\n      }\n      int trimAt = TestUtil.nextInt(random(), trimStart, tokens[tokens.length - 1].length());\n      tokens[tokens.length-1] = tokens[tokens.length-1].substring(0, trimAt);\n\n      int num = TestUtil.nextInt(random(), 1, 100);\n      StringBuilder b = new StringBuilder();\n      for(String token : tokens) {\n        b.append(' ');\n        b.append(token);\n      }\n      String query = b.toString();\n      query = query.substring(1);\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" query='\" + query + \"' num=\" + num);\n      }\n\n      // Expected:\n      List<LookupResult> expected = new ArrayList<>();\n      double backoff = 1.0;\n      seen = new HashSet<>();\n\n      if (VERBOSE) {\n        System.out.println(\"  compute expected\");\n      }\n      for(int i=grams-1;i>=0;i--) {\n        if (VERBOSE) {\n          System.out.println(\"    grams=\" + i);\n        }\n\n        if (tokens.length < i+1) {\n          // Don't have enough tokens to use this model\n          if (VERBOSE) {\n            System.out.println(\"      skip\");\n          }\n          continue;\n        }\n\n        if (i == 0 && tokens[tokens.length-1].length() == 0) {\n          // Never suggest unigrams from empty string:\n          if (VERBOSE) {\n            System.out.println(\"      skip unigram priors only\");\n          }\n          continue;\n        }\n\n        // Build up \"context\" ngram:\n        b = new StringBuilder();\n        for(int j=tokens.length-i-1;j<tokens.length-1;j++) {\n          b.append(' ');\n          b.append(tokens[j]);\n        }\n        String context = b.toString();\n        if (context.length() > 0) {\n          context = context.substring(1);\n        }\n        if (VERBOSE) {\n          System.out.println(\"      context='\" + context + \"'\");\n        }\n        long contextCount;\n        if (context.length() == 0) {\n          contextCount = totTokens;\n        } else {\n          Integer count = gramCounts.get(i-1).get(context);\n          if (count == null) {\n            // We never saw this context:\n            backoff *= FreeTextSuggester.ALPHA;\n            if (VERBOSE) {\n              System.out.println(\"      skip: never saw context\");\n            }\n            continue;\n          }\n          contextCount = count;\n        }\n        if (VERBOSE) {\n          System.out.println(\"      contextCount=\" + contextCount);\n        }\n        Map<String,Integer> model = gramCounts.get(i);\n\n        // First pass, gather all predictions for this model:\n        if (VERBOSE) {\n          System.out.println(\"      find terms w/ prefix=\" + tokens[tokens.length-1]);\n        }\n        List<LookupResult> tmp = new ArrayList<>();\n        for(String term : terms) {\n          if (term.startsWith(tokens[tokens.length-1])) {\n            if (VERBOSE) {\n              System.out.println(\"        term=\" + term);\n            }\n            if (seen.contains(term)) {\n              if (VERBOSE) {\n                System.out.println(\"          skip seen\");\n              }\n              continue;\n            }\n            String ngram = (context + \" \" + term).trim();\n            Integer count = model.get(ngram);\n            if (count != null) {\n              LookupResult lr = new LookupResult(ngram, (long) (Long.MAX_VALUE * (backoff * (double) count / contextCount)));\n              tmp.add(lr);\n              if (VERBOSE) {\n                System.out.println(\"      add tmp key='\" + lr.key + \"' score=\" + lr.value);\n              }\n            }\n          }\n        }\n\n        // Second pass, trim to only top N, and fold those\n        // into overall suggestions:\n        Collections.sort(tmp, byScoreThenKey);\n        if (tmp.size() > num) {\n          tmp.subList(num, tmp.size()).clear();\n        }\n        for(LookupResult result : tmp) {\n          String key = result.key.toString();\n          int idx = key.lastIndexOf(' ');\n          String lastToken;\n          if (idx != -1) {\n            lastToken = key.substring(idx+1);\n          } else {\n            lastToken = key;\n          }\n          if (!seen.contains(lastToken)) {\n            seen.add(lastToken);\n            expected.add(result);\n            if (VERBOSE) {\n              System.out.println(\"      keep key='\" + result.key + \"' score=\" + result.value);\n            }\n          }\n        }\n        \n        backoff *= FreeTextSuggester.ALPHA;\n      }\n\n      Collections.sort(expected, byScoreThenKey);\n\n      if (expected.size() > num) {\n        expected.subList(num, expected.size()).clear();\n      }\n\n      // Actual:\n      List<LookupResult> actual = sug.lookup(query, num);\n\n      if (VERBOSE) {\n        System.out.println(\"  expected: \" + expected);\n        System.out.println(\"    actual: \" + actual);\n      }\n\n      assertEquals(expected.toString(), actual.toString());\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws IOException {\n    String[] terms = new String[TestUtil.nextInt(random(), 2, 10)];\n    Set<String> seen = new HashSet<>();\n    while (seen.size() < terms.length) {\n      String token = TestUtil.randomSimpleString(random(), 1, 5);\n      if (!seen.contains(token)) {\n        terms[seen.size()] = token;\n        seen.add(token);\n      }\n    }\n\n    Analyzer a = new MockAnalyzer(random());\n\n    int numDocs = atLeast(10);\n    long totTokens = 0;\n    final String[][] docs = new String[numDocs][];\n    for(int i=0;i<numDocs;i++) {\n      docs[i] = new String[atLeast(100)];\n      if (VERBOSE) {\n        System.out.print(\"  doc \" + i + \":\");\n      }\n      for(int j=0;j<docs[i].length;j++) {\n        docs[i][j] = getZipfToken(terms);\n        if (VERBOSE) {\n          System.out.print(\" \" + docs[i][j]);\n        }\n      }\n      if (VERBOSE) {\n        System.out.println();\n      }\n      totTokens += docs[i].length;\n    }\n\n    int grams = TestUtil.nextInt(random(), 1, 4);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + terms.length + \" terms; \" + numDocs + \" docs; \" + grams + \" grams\");\n    }\n\n    // Build suggester model:\n    FreeTextSuggester sug = new FreeTextSuggester(a, a, grams, (byte) 0x20);\n    sug.build(new InputIterator() {\n        int upto;\n\n        @Override\n        public BytesRef next() {\n          if (upto == docs.length) {\n            return null;\n          } else {\n            StringBuilder b = new StringBuilder();\n            for(String token : docs[upto]) {\n              b.append(' ');\n              b.append(token);\n            }\n            upto++;\n            return new BytesRef(b.toString());\n          }\n        }\n\n        @Override\n        public long weight() {\n          return random().nextLong();\n        }\n\n        @Override\n        public BytesRef payload() {\n          return null;\n        }\n\n        @Override\n        public boolean hasPayloads() {\n          return false;\n        }\n      });\n\n    // Build inefficient but hopefully correct model:\n    List<Map<String,Integer>> gramCounts = new ArrayList<>(grams);\n    for(int gram=0;gram<grams;gram++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: build model for gram=\" + gram);\n      }\n      Map<String,Integer> model = new HashMap<>();\n      gramCounts.add(model);\n      for(String[] doc : docs) {\n        for(int i=0;i<doc.length-gram;i++) {\n          StringBuilder b = new StringBuilder();\n          for(int j=i;j<=i+gram;j++) {\n            if (j > i) {\n              b.append(' ');\n            }\n            b.append(doc[j]);\n          }\n          String token = b.toString();\n          Integer curCount = model.get(token);\n          if (curCount == null) {\n            model.put(token, 1);\n          } else {\n            model.put(token, 1 + curCount);\n          }\n          if (VERBOSE) {\n            System.out.println(\"  add '\" + token + \"' -> count=\" + model.get(token));\n          }\n        }\n      }\n    }\n\n    int lookups = atLeast(100);\n    for(int iter=0;iter<lookups;iter++) {\n      String[] tokens = new String[TestUtil.nextInt(random(), 1, 5)];\n      for(int i=0;i<tokens.length;i++) {\n        tokens[i] = getZipfToken(terms);\n      }\n\n      // Maybe trim last token; be sure not to create the\n      // empty string:\n      int trimStart;\n      if (tokens.length == 1) {\n        trimStart = 1;\n      } else {\n        trimStart = 0;\n      }\n      int trimAt = TestUtil.nextInt(random(), trimStart, tokens[tokens.length - 1].length());\n      tokens[tokens.length-1] = tokens[tokens.length-1].substring(0, trimAt);\n\n      int num = TestUtil.nextInt(random(), 1, 100);\n      StringBuilder b = new StringBuilder();\n      for(String token : tokens) {\n        b.append(' ');\n        b.append(token);\n      }\n      String query = b.toString();\n      query = query.substring(1);\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" query='\" + query + \"' num=\" + num);\n      }\n\n      // Expected:\n      List<LookupResult> expected = new ArrayList<>();\n      double backoff = 1.0;\n      seen = new HashSet<>();\n\n      if (VERBOSE) {\n        System.out.println(\"  compute expected\");\n      }\n      for(int i=grams-1;i>=0;i--) {\n        if (VERBOSE) {\n          System.out.println(\"    grams=\" + i);\n        }\n\n        if (tokens.length < i+1) {\n          // Don't have enough tokens to use this model\n          if (VERBOSE) {\n            System.out.println(\"      skip\");\n          }\n          continue;\n        }\n\n        if (i == 0 && tokens[tokens.length-1].length() == 0) {\n          // Never suggest unigrams from empty string:\n          if (VERBOSE) {\n            System.out.println(\"      skip unigram priors only\");\n          }\n          continue;\n        }\n\n        // Build up \"context\" ngram:\n        b = new StringBuilder();\n        for(int j=tokens.length-i-1;j<tokens.length-1;j++) {\n          b.append(' ');\n          b.append(tokens[j]);\n        }\n        String context = b.toString();\n        if (context.length() > 0) {\n          context = context.substring(1);\n        }\n        if (VERBOSE) {\n          System.out.println(\"      context='\" + context + \"'\");\n        }\n        long contextCount;\n        if (context.length() == 0) {\n          contextCount = totTokens;\n        } else {\n          Integer count = gramCounts.get(i-1).get(context);\n          if (count == null) {\n            // We never saw this context:\n            backoff *= FreeTextSuggester.ALPHA;\n            if (VERBOSE) {\n              System.out.println(\"      skip: never saw context\");\n            }\n            continue;\n          }\n          contextCount = count;\n        }\n        if (VERBOSE) {\n          System.out.println(\"      contextCount=\" + contextCount);\n        }\n        Map<String,Integer> model = gramCounts.get(i);\n\n        // First pass, gather all predictions for this model:\n        if (VERBOSE) {\n          System.out.println(\"      find terms w/ prefix=\" + tokens[tokens.length-1]);\n        }\n        List<LookupResult> tmp = new ArrayList<>();\n        for(String term : terms) {\n          if (term.startsWith(tokens[tokens.length-1])) {\n            if (VERBOSE) {\n              System.out.println(\"        term=\" + term);\n            }\n            if (seen.contains(term)) {\n              if (VERBOSE) {\n                System.out.println(\"          skip seen\");\n              }\n              continue;\n            }\n            String ngram = (context + \" \" + term).trim();\n            Integer count = model.get(ngram);\n            if (count != null) {\n              LookupResult lr = new LookupResult(ngram, (long) (Long.MAX_VALUE * (backoff * (double) count / contextCount)));\n              tmp.add(lr);\n              if (VERBOSE) {\n                System.out.println(\"      add tmp key='\" + lr.key + \"' score=\" + lr.value);\n              }\n            }\n          }\n        }\n\n        // Second pass, trim to only top N, and fold those\n        // into overall suggestions:\n        Collections.sort(tmp, byScoreThenKey);\n        if (tmp.size() > num) {\n          tmp.subList(num, tmp.size()).clear();\n        }\n        for(LookupResult result : tmp) {\n          String key = result.key.toString();\n          int idx = key.lastIndexOf(' ');\n          String lastToken;\n          if (idx != -1) {\n            lastToken = key.substring(idx+1);\n          } else {\n            lastToken = key;\n          }\n          if (!seen.contains(lastToken)) {\n            seen.add(lastToken);\n            expected.add(result);\n            if (VERBOSE) {\n              System.out.println(\"      keep key='\" + result.key + \"' score=\" + result.value);\n            }\n          }\n        }\n        \n        backoff *= FreeTextSuggester.ALPHA;\n      }\n\n      Collections.sort(expected, byScoreThenKey);\n\n      if (expected.size() > num) {\n        expected.subList(num, expected.size()).clear();\n      }\n\n      // Actual:\n      List<LookupResult> actual = sug.lookup(query, num);\n\n      if (VERBOSE) {\n        System.out.println(\"  expected: \" + expected);\n        System.out.println(\"    actual: \" + actual);\n      }\n\n      assertEquals(expected.toString(), actual.toString());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a56958d7f71a28824f20031ffbb2e13502a0274e","date":1425573902,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestFreeTextSuggester#testRandom().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestFreeTextSuggester#testRandom().mjava","sourceNew":"  public void testRandom() throws IOException {\n    String[] terms = new String[TestUtil.nextInt(random(), 2, 10)];\n    Set<String> seen = new HashSet<>();\n    while (seen.size() < terms.length) {\n      String token = TestUtil.randomSimpleString(random(), 1, 5);\n      if (!seen.contains(token)) {\n        terms[seen.size()] = token;\n        seen.add(token);\n      }\n    }\n\n    Analyzer a = new MockAnalyzer(random());\n\n    int numDocs = atLeast(10);\n    long totTokens = 0;\n    final String[][] docs = new String[numDocs][];\n    for(int i=0;i<numDocs;i++) {\n      docs[i] = new String[atLeast(100)];\n      if (VERBOSE) {\n        System.out.print(\"  doc \" + i + \":\");\n      }\n      for(int j=0;j<docs[i].length;j++) {\n        docs[i][j] = getZipfToken(terms);\n        if (VERBOSE) {\n          System.out.print(\" \" + docs[i][j]);\n        }\n      }\n      if (VERBOSE) {\n        System.out.println();\n      }\n      totTokens += docs[i].length;\n    }\n\n    int grams = TestUtil.nextInt(random(), 1, 4);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + terms.length + \" terms; \" + numDocs + \" docs; \" + grams + \" grams\");\n    }\n\n    // Build suggester model:\n    FreeTextSuggester sug = new FreeTextSuggester(a, a, grams, (byte) 0x20);\n    sug.build(new InputIterator() {\n        int upto;\n\n        @Override\n        public BytesRef next() {\n          if (upto == docs.length) {\n            return null;\n          } else {\n            StringBuilder b = new StringBuilder();\n            for(String token : docs[upto]) {\n              b.append(' ');\n              b.append(token);\n            }\n            upto++;\n            return new BytesRef(b.toString());\n          }\n        }\n\n        @Override\n        public long weight() {\n          return random().nextLong();\n        }\n\n        @Override\n        public BytesRef payload() {\n          return null;\n        }\n\n        @Override\n        public boolean hasPayloads() {\n          return false;\n        }\n\n        @Override\n        public Set<BytesRef> contexts() {\n          return null;\n        }\n\n        @Override\n        public boolean hasContexts() {\n          return false;\n        }\n      });\n\n    // Build inefficient but hopefully correct model:\n    List<Map<String,Integer>> gramCounts = new ArrayList<>(grams);\n    for(int gram=0;gram<grams;gram++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: build model for gram=\" + gram);\n      }\n      Map<String,Integer> model = new HashMap<>();\n      gramCounts.add(model);\n      for(String[] doc : docs) {\n        for(int i=0;i<doc.length-gram;i++) {\n          StringBuilder b = new StringBuilder();\n          for(int j=i;j<=i+gram;j++) {\n            if (j > i) {\n              b.append(' ');\n            }\n            b.append(doc[j]);\n          }\n          String token = b.toString();\n          Integer curCount = model.get(token);\n          if (curCount == null) {\n            model.put(token, 1);\n          } else {\n            model.put(token, 1 + curCount);\n          }\n          if (VERBOSE) {\n            System.out.println(\"  add '\" + token + \"' -> count=\" + model.get(token));\n          }\n        }\n      }\n    }\n\n    int lookups = atLeast(100);\n    for(int iter=0;iter<lookups;iter++) {\n      String[] tokens = new String[TestUtil.nextInt(random(), 1, 5)];\n      for(int i=0;i<tokens.length;i++) {\n        tokens[i] = getZipfToken(terms);\n      }\n\n      // Maybe trim last token; be sure not to create the\n      // empty string:\n      int trimStart;\n      if (tokens.length == 1) {\n        trimStart = 1;\n      } else {\n        trimStart = 0;\n      }\n      int trimAt = TestUtil.nextInt(random(), trimStart, tokens[tokens.length - 1].length());\n      tokens[tokens.length-1] = tokens[tokens.length-1].substring(0, trimAt);\n\n      int num = TestUtil.nextInt(random(), 1, 100);\n      StringBuilder b = new StringBuilder();\n      for(String token : tokens) {\n        b.append(' ');\n        b.append(token);\n      }\n      String query = b.toString();\n      query = query.substring(1);\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" query='\" + query + \"' num=\" + num);\n      }\n\n      // Expected:\n      List<LookupResult> expected = new ArrayList<>();\n      double backoff = 1.0;\n      seen = new HashSet<>();\n\n      if (VERBOSE) {\n        System.out.println(\"  compute expected\");\n      }\n      for(int i=grams-1;i>=0;i--) {\n        if (VERBOSE) {\n          System.out.println(\"    grams=\" + i);\n        }\n\n        if (tokens.length < i+1) {\n          // Don't have enough tokens to use this model\n          if (VERBOSE) {\n            System.out.println(\"      skip\");\n          }\n          continue;\n        }\n\n        if (i == 0 && tokens[tokens.length-1].length() == 0) {\n          // Never suggest unigrams from empty string:\n          if (VERBOSE) {\n            System.out.println(\"      skip unigram priors only\");\n          }\n          continue;\n        }\n\n        // Build up \"context\" ngram:\n        b = new StringBuilder();\n        for(int j=tokens.length-i-1;j<tokens.length-1;j++) {\n          b.append(' ');\n          b.append(tokens[j]);\n        }\n        String context = b.toString();\n        if (context.length() > 0) {\n          context = context.substring(1);\n        }\n        if (VERBOSE) {\n          System.out.println(\"      context='\" + context + \"'\");\n        }\n        long contextCount;\n        if (context.length() == 0) {\n          contextCount = totTokens;\n        } else {\n          Integer count = gramCounts.get(i-1).get(context);\n          if (count == null) {\n            // We never saw this context:\n            backoff *= FreeTextSuggester.ALPHA;\n            if (VERBOSE) {\n              System.out.println(\"      skip: never saw context\");\n            }\n            continue;\n          }\n          contextCount = count;\n        }\n        if (VERBOSE) {\n          System.out.println(\"      contextCount=\" + contextCount);\n        }\n        Map<String,Integer> model = gramCounts.get(i);\n\n        // First pass, gather all predictions for this model:\n        if (VERBOSE) {\n          System.out.println(\"      find terms w/ prefix=\" + tokens[tokens.length-1]);\n        }\n        List<LookupResult> tmp = new ArrayList<>();\n        for(String term : terms) {\n          if (term.startsWith(tokens[tokens.length-1])) {\n            if (VERBOSE) {\n              System.out.println(\"        term=\" + term);\n            }\n            if (seen.contains(term)) {\n              if (VERBOSE) {\n                System.out.println(\"          skip seen\");\n              }\n              continue;\n            }\n            String ngram = (context + \" \" + term).trim();\n            Integer count = model.get(ngram);\n            if (count != null) {\n              LookupResult lr = new LookupResult(ngram, (long) (Long.MAX_VALUE * (backoff * (double) count / contextCount)));\n              tmp.add(lr);\n              if (VERBOSE) {\n                System.out.println(\"      add tmp key='\" + lr.key + \"' score=\" + lr.value);\n              }\n            }\n          }\n        }\n\n        // Second pass, trim to only top N, and fold those\n        // into overall suggestions:\n        Collections.sort(tmp, byScoreThenKey);\n        if (tmp.size() > num) {\n          tmp.subList(num, tmp.size()).clear();\n        }\n        for(LookupResult result : tmp) {\n          String key = result.key.toString();\n          int idx = key.lastIndexOf(' ');\n          String lastToken;\n          if (idx != -1) {\n            lastToken = key.substring(idx+1);\n          } else {\n            lastToken = key;\n          }\n          if (!seen.contains(lastToken)) {\n            seen.add(lastToken);\n            expected.add(result);\n            if (VERBOSE) {\n              System.out.println(\"      keep key='\" + result.key + \"' score=\" + result.value);\n            }\n          }\n        }\n        \n        backoff *= FreeTextSuggester.ALPHA;\n      }\n\n      Collections.sort(expected, byScoreThenKey);\n\n      if (expected.size() > num) {\n        expected.subList(num, expected.size()).clear();\n      }\n\n      // Actual:\n      List<LookupResult> actual = sug.lookup(query, num);\n\n      if (VERBOSE) {\n        System.out.println(\"  expected: \" + expected);\n        System.out.println(\"    actual: \" + actual);\n      }\n\n      assertEquals(expected.toString(), actual.toString());\n    }\n    a.close();\n  }\n\n","sourceOld":"  public void testRandom() throws IOException {\n    String[] terms = new String[TestUtil.nextInt(random(), 2, 10)];\n    Set<String> seen = new HashSet<>();\n    while (seen.size() < terms.length) {\n      String token = TestUtil.randomSimpleString(random(), 1, 5);\n      if (!seen.contains(token)) {\n        terms[seen.size()] = token;\n        seen.add(token);\n      }\n    }\n\n    Analyzer a = new MockAnalyzer(random());\n\n    int numDocs = atLeast(10);\n    long totTokens = 0;\n    final String[][] docs = new String[numDocs][];\n    for(int i=0;i<numDocs;i++) {\n      docs[i] = new String[atLeast(100)];\n      if (VERBOSE) {\n        System.out.print(\"  doc \" + i + \":\");\n      }\n      for(int j=0;j<docs[i].length;j++) {\n        docs[i][j] = getZipfToken(terms);\n        if (VERBOSE) {\n          System.out.print(\" \" + docs[i][j]);\n        }\n      }\n      if (VERBOSE) {\n        System.out.println();\n      }\n      totTokens += docs[i].length;\n    }\n\n    int grams = TestUtil.nextInt(random(), 1, 4);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + terms.length + \" terms; \" + numDocs + \" docs; \" + grams + \" grams\");\n    }\n\n    // Build suggester model:\n    FreeTextSuggester sug = new FreeTextSuggester(a, a, grams, (byte) 0x20);\n    sug.build(new InputIterator() {\n        int upto;\n\n        @Override\n        public BytesRef next() {\n          if (upto == docs.length) {\n            return null;\n          } else {\n            StringBuilder b = new StringBuilder();\n            for(String token : docs[upto]) {\n              b.append(' ');\n              b.append(token);\n            }\n            upto++;\n            return new BytesRef(b.toString());\n          }\n        }\n\n        @Override\n        public long weight() {\n          return random().nextLong();\n        }\n\n        @Override\n        public BytesRef payload() {\n          return null;\n        }\n\n        @Override\n        public boolean hasPayloads() {\n          return false;\n        }\n\n        @Override\n        public Set<BytesRef> contexts() {\n          return null;\n        }\n\n        @Override\n        public boolean hasContexts() {\n          return false;\n        }\n      });\n\n    // Build inefficient but hopefully correct model:\n    List<Map<String,Integer>> gramCounts = new ArrayList<>(grams);\n    for(int gram=0;gram<grams;gram++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: build model for gram=\" + gram);\n      }\n      Map<String,Integer> model = new HashMap<>();\n      gramCounts.add(model);\n      for(String[] doc : docs) {\n        for(int i=0;i<doc.length-gram;i++) {\n          StringBuilder b = new StringBuilder();\n          for(int j=i;j<=i+gram;j++) {\n            if (j > i) {\n              b.append(' ');\n            }\n            b.append(doc[j]);\n          }\n          String token = b.toString();\n          Integer curCount = model.get(token);\n          if (curCount == null) {\n            model.put(token, 1);\n          } else {\n            model.put(token, 1 + curCount);\n          }\n          if (VERBOSE) {\n            System.out.println(\"  add '\" + token + \"' -> count=\" + model.get(token));\n          }\n        }\n      }\n    }\n\n    int lookups = atLeast(100);\n    for(int iter=0;iter<lookups;iter++) {\n      String[] tokens = new String[TestUtil.nextInt(random(), 1, 5)];\n      for(int i=0;i<tokens.length;i++) {\n        tokens[i] = getZipfToken(terms);\n      }\n\n      // Maybe trim last token; be sure not to create the\n      // empty string:\n      int trimStart;\n      if (tokens.length == 1) {\n        trimStart = 1;\n      } else {\n        trimStart = 0;\n      }\n      int trimAt = TestUtil.nextInt(random(), trimStart, tokens[tokens.length - 1].length());\n      tokens[tokens.length-1] = tokens[tokens.length-1].substring(0, trimAt);\n\n      int num = TestUtil.nextInt(random(), 1, 100);\n      StringBuilder b = new StringBuilder();\n      for(String token : tokens) {\n        b.append(' ');\n        b.append(token);\n      }\n      String query = b.toString();\n      query = query.substring(1);\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" query='\" + query + \"' num=\" + num);\n      }\n\n      // Expected:\n      List<LookupResult> expected = new ArrayList<>();\n      double backoff = 1.0;\n      seen = new HashSet<>();\n\n      if (VERBOSE) {\n        System.out.println(\"  compute expected\");\n      }\n      for(int i=grams-1;i>=0;i--) {\n        if (VERBOSE) {\n          System.out.println(\"    grams=\" + i);\n        }\n\n        if (tokens.length < i+1) {\n          // Don't have enough tokens to use this model\n          if (VERBOSE) {\n            System.out.println(\"      skip\");\n          }\n          continue;\n        }\n\n        if (i == 0 && tokens[tokens.length-1].length() == 0) {\n          // Never suggest unigrams from empty string:\n          if (VERBOSE) {\n            System.out.println(\"      skip unigram priors only\");\n          }\n          continue;\n        }\n\n        // Build up \"context\" ngram:\n        b = new StringBuilder();\n        for(int j=tokens.length-i-1;j<tokens.length-1;j++) {\n          b.append(' ');\n          b.append(tokens[j]);\n        }\n        String context = b.toString();\n        if (context.length() > 0) {\n          context = context.substring(1);\n        }\n        if (VERBOSE) {\n          System.out.println(\"      context='\" + context + \"'\");\n        }\n        long contextCount;\n        if (context.length() == 0) {\n          contextCount = totTokens;\n        } else {\n          Integer count = gramCounts.get(i-1).get(context);\n          if (count == null) {\n            // We never saw this context:\n            backoff *= FreeTextSuggester.ALPHA;\n            if (VERBOSE) {\n              System.out.println(\"      skip: never saw context\");\n            }\n            continue;\n          }\n          contextCount = count;\n        }\n        if (VERBOSE) {\n          System.out.println(\"      contextCount=\" + contextCount);\n        }\n        Map<String,Integer> model = gramCounts.get(i);\n\n        // First pass, gather all predictions for this model:\n        if (VERBOSE) {\n          System.out.println(\"      find terms w/ prefix=\" + tokens[tokens.length-1]);\n        }\n        List<LookupResult> tmp = new ArrayList<>();\n        for(String term : terms) {\n          if (term.startsWith(tokens[tokens.length-1])) {\n            if (VERBOSE) {\n              System.out.println(\"        term=\" + term);\n            }\n            if (seen.contains(term)) {\n              if (VERBOSE) {\n                System.out.println(\"          skip seen\");\n              }\n              continue;\n            }\n            String ngram = (context + \" \" + term).trim();\n            Integer count = model.get(ngram);\n            if (count != null) {\n              LookupResult lr = new LookupResult(ngram, (long) (Long.MAX_VALUE * (backoff * (double) count / contextCount)));\n              tmp.add(lr);\n              if (VERBOSE) {\n                System.out.println(\"      add tmp key='\" + lr.key + \"' score=\" + lr.value);\n              }\n            }\n          }\n        }\n\n        // Second pass, trim to only top N, and fold those\n        // into overall suggestions:\n        Collections.sort(tmp, byScoreThenKey);\n        if (tmp.size() > num) {\n          tmp.subList(num, tmp.size()).clear();\n        }\n        for(LookupResult result : tmp) {\n          String key = result.key.toString();\n          int idx = key.lastIndexOf(' ');\n          String lastToken;\n          if (idx != -1) {\n            lastToken = key.substring(idx+1);\n          } else {\n            lastToken = key;\n          }\n          if (!seen.contains(lastToken)) {\n            seen.add(lastToken);\n            expected.add(result);\n            if (VERBOSE) {\n              System.out.println(\"      keep key='\" + result.key + \"' score=\" + result.value);\n            }\n          }\n        }\n        \n        backoff *= FreeTextSuggester.ALPHA;\n      }\n\n      Collections.sort(expected, byScoreThenKey);\n\n      if (expected.size() > num) {\n        expected.subList(num, expected.size()).clear();\n      }\n\n      // Actual:\n      List<LookupResult> actual = sug.lookup(query, num);\n\n      if (VERBOSE) {\n        System.out.println(\"  expected: \" + expected);\n        System.out.println(\"    actual: \" + actual);\n      }\n\n      assertEquals(expected.toString(), actual.toString());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestFreeTextSuggester#testRandom().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestFreeTextSuggester#testRandom().mjava","sourceNew":"  public void testRandom() throws IOException {\n    String[] terms = new String[TestUtil.nextInt(random(), 2, 10)];\n    Set<String> seen = new HashSet<>();\n    while (seen.size() < terms.length) {\n      String token = TestUtil.randomSimpleString(random(), 1, 5);\n      if (!seen.contains(token)) {\n        terms[seen.size()] = token;\n        seen.add(token);\n      }\n    }\n\n    Analyzer a = new MockAnalyzer(random());\n\n    int numDocs = atLeast(10);\n    long totTokens = 0;\n    final String[][] docs = new String[numDocs][];\n    for(int i=0;i<numDocs;i++) {\n      docs[i] = new String[atLeast(100)];\n      if (VERBOSE) {\n        System.out.print(\"  doc \" + i + \":\");\n      }\n      for(int j=0;j<docs[i].length;j++) {\n        docs[i][j] = getZipfToken(terms);\n        if (VERBOSE) {\n          System.out.print(\" \" + docs[i][j]);\n        }\n      }\n      if (VERBOSE) {\n        System.out.println();\n      }\n      totTokens += docs[i].length;\n    }\n\n    int grams = TestUtil.nextInt(random(), 1, 4);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + terms.length + \" terms; \" + numDocs + \" docs; \" + grams + \" grams\");\n    }\n\n    // Build suggester model:\n    FreeTextSuggester sug = new FreeTextSuggester(a, a, grams, (byte) 0x20);\n    sug.build(new InputIterator() {\n        int upto;\n\n        @Override\n        public BytesRef next() {\n          if (upto == docs.length) {\n            return null;\n          } else {\n            StringBuilder b = new StringBuilder();\n            for(String token : docs[upto]) {\n              b.append(' ');\n              b.append(token);\n            }\n            upto++;\n            return new BytesRef(b.toString());\n          }\n        }\n\n        @Override\n        public long weight() {\n          return random().nextLong();\n        }\n\n        @Override\n        public BytesRef payload() {\n          return null;\n        }\n\n        @Override\n        public boolean hasPayloads() {\n          return false;\n        }\n\n        @Override\n        public Set<BytesRef> contexts() {\n          return null;\n        }\n\n        @Override\n        public boolean hasContexts() {\n          return false;\n        }\n      });\n\n    // Build inefficient but hopefully correct model:\n    List<Map<String,Integer>> gramCounts = new ArrayList<>(grams);\n    for(int gram=0;gram<grams;gram++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: build model for gram=\" + gram);\n      }\n      Map<String,Integer> model = new HashMap<>();\n      gramCounts.add(model);\n      for(String[] doc : docs) {\n        for(int i=0;i<doc.length-gram;i++) {\n          StringBuilder b = new StringBuilder();\n          for(int j=i;j<=i+gram;j++) {\n            if (j > i) {\n              b.append(' ');\n            }\n            b.append(doc[j]);\n          }\n          String token = b.toString();\n          Integer curCount = model.get(token);\n          if (curCount == null) {\n            model.put(token, 1);\n          } else {\n            model.put(token, 1 + curCount);\n          }\n          if (VERBOSE) {\n            System.out.println(\"  add '\" + token + \"' -> count=\" + model.get(token));\n          }\n        }\n      }\n    }\n\n    int lookups = atLeast(100);\n    for(int iter=0;iter<lookups;iter++) {\n      String[] tokens = new String[TestUtil.nextInt(random(), 1, 5)];\n      for(int i=0;i<tokens.length;i++) {\n        tokens[i] = getZipfToken(terms);\n      }\n\n      // Maybe trim last token; be sure not to create the\n      // empty string:\n      int trimStart;\n      if (tokens.length == 1) {\n        trimStart = 1;\n      } else {\n        trimStart = 0;\n      }\n      int trimAt = TestUtil.nextInt(random(), trimStart, tokens[tokens.length - 1].length());\n      tokens[tokens.length-1] = tokens[tokens.length-1].substring(0, trimAt);\n\n      int num = TestUtil.nextInt(random(), 1, 100);\n      StringBuilder b = new StringBuilder();\n      for(String token : tokens) {\n        b.append(' ');\n        b.append(token);\n      }\n      String query = b.toString();\n      query = query.substring(1);\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" query='\" + query + \"' num=\" + num);\n      }\n\n      // Expected:\n      List<LookupResult> expected = new ArrayList<>();\n      double backoff = 1.0;\n      seen = new HashSet<>();\n\n      if (VERBOSE) {\n        System.out.println(\"  compute expected\");\n      }\n      for(int i=grams-1;i>=0;i--) {\n        if (VERBOSE) {\n          System.out.println(\"    grams=\" + i);\n        }\n\n        if (tokens.length < i+1) {\n          // Don't have enough tokens to use this model\n          if (VERBOSE) {\n            System.out.println(\"      skip\");\n          }\n          continue;\n        }\n\n        if (i == 0 && tokens[tokens.length-1].length() == 0) {\n          // Never suggest unigrams from empty string:\n          if (VERBOSE) {\n            System.out.println(\"      skip unigram priors only\");\n          }\n          continue;\n        }\n\n        // Build up \"context\" ngram:\n        b = new StringBuilder();\n        for(int j=tokens.length-i-1;j<tokens.length-1;j++) {\n          b.append(' ');\n          b.append(tokens[j]);\n        }\n        String context = b.toString();\n        if (context.length() > 0) {\n          context = context.substring(1);\n        }\n        if (VERBOSE) {\n          System.out.println(\"      context='\" + context + \"'\");\n        }\n        long contextCount;\n        if (context.length() == 0) {\n          contextCount = totTokens;\n        } else {\n          Integer count = gramCounts.get(i-1).get(context);\n          if (count == null) {\n            // We never saw this context:\n            backoff *= FreeTextSuggester.ALPHA;\n            if (VERBOSE) {\n              System.out.println(\"      skip: never saw context\");\n            }\n            continue;\n          }\n          contextCount = count;\n        }\n        if (VERBOSE) {\n          System.out.println(\"      contextCount=\" + contextCount);\n        }\n        Map<String,Integer> model = gramCounts.get(i);\n\n        // First pass, gather all predictions for this model:\n        if (VERBOSE) {\n          System.out.println(\"      find terms w/ prefix=\" + tokens[tokens.length-1]);\n        }\n        List<LookupResult> tmp = new ArrayList<>();\n        for(String term : terms) {\n          if (term.startsWith(tokens[tokens.length-1])) {\n            if (VERBOSE) {\n              System.out.println(\"        term=\" + term);\n            }\n            if (seen.contains(term)) {\n              if (VERBOSE) {\n                System.out.println(\"          skip seen\");\n              }\n              continue;\n            }\n            String ngram = (context + \" \" + term).trim();\n            Integer count = model.get(ngram);\n            if (count != null) {\n              LookupResult lr = new LookupResult(ngram, (long) (Long.MAX_VALUE * (backoff * (double) count / contextCount)));\n              tmp.add(lr);\n              if (VERBOSE) {\n                System.out.println(\"      add tmp key='\" + lr.key + \"' score=\" + lr.value);\n              }\n            }\n          }\n        }\n\n        // Second pass, trim to only top N, and fold those\n        // into overall suggestions:\n        Collections.sort(tmp, byScoreThenKey);\n        if (tmp.size() > num) {\n          tmp.subList(num, tmp.size()).clear();\n        }\n        for(LookupResult result : tmp) {\n          String key = result.key.toString();\n          int idx = key.lastIndexOf(' ');\n          String lastToken;\n          if (idx != -1) {\n            lastToken = key.substring(idx+1);\n          } else {\n            lastToken = key;\n          }\n          if (!seen.contains(lastToken)) {\n            seen.add(lastToken);\n            expected.add(result);\n            if (VERBOSE) {\n              System.out.println(\"      keep key='\" + result.key + \"' score=\" + result.value);\n            }\n          }\n        }\n        \n        backoff *= FreeTextSuggester.ALPHA;\n      }\n\n      Collections.sort(expected, byScoreThenKey);\n\n      if (expected.size() > num) {\n        expected.subList(num, expected.size()).clear();\n      }\n\n      // Actual:\n      List<LookupResult> actual = sug.lookup(query, num);\n\n      if (VERBOSE) {\n        System.out.println(\"  expected: \" + expected);\n        System.out.println(\"    actual: \" + actual);\n      }\n\n      assertEquals(expected.toString(), actual.toString());\n    }\n    a.close();\n  }\n\n","sourceOld":"  public void testRandom() throws IOException {\n    String[] terms = new String[TestUtil.nextInt(random(), 2, 10)];\n    Set<String> seen = new HashSet<>();\n    while (seen.size() < terms.length) {\n      String token = TestUtil.randomSimpleString(random(), 1, 5);\n      if (!seen.contains(token)) {\n        terms[seen.size()] = token;\n        seen.add(token);\n      }\n    }\n\n    Analyzer a = new MockAnalyzer(random());\n\n    int numDocs = atLeast(10);\n    long totTokens = 0;\n    final String[][] docs = new String[numDocs][];\n    for(int i=0;i<numDocs;i++) {\n      docs[i] = new String[atLeast(100)];\n      if (VERBOSE) {\n        System.out.print(\"  doc \" + i + \":\");\n      }\n      for(int j=0;j<docs[i].length;j++) {\n        docs[i][j] = getZipfToken(terms);\n        if (VERBOSE) {\n          System.out.print(\" \" + docs[i][j]);\n        }\n      }\n      if (VERBOSE) {\n        System.out.println();\n      }\n      totTokens += docs[i].length;\n    }\n\n    int grams = TestUtil.nextInt(random(), 1, 4);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + terms.length + \" terms; \" + numDocs + \" docs; \" + grams + \" grams\");\n    }\n\n    // Build suggester model:\n    FreeTextSuggester sug = new FreeTextSuggester(a, a, grams, (byte) 0x20);\n    sug.build(new InputIterator() {\n        int upto;\n\n        @Override\n        public BytesRef next() {\n          if (upto == docs.length) {\n            return null;\n          } else {\n            StringBuilder b = new StringBuilder();\n            for(String token : docs[upto]) {\n              b.append(' ');\n              b.append(token);\n            }\n            upto++;\n            return new BytesRef(b.toString());\n          }\n        }\n\n        @Override\n        public long weight() {\n          return random().nextLong();\n        }\n\n        @Override\n        public BytesRef payload() {\n          return null;\n        }\n\n        @Override\n        public boolean hasPayloads() {\n          return false;\n        }\n\n        @Override\n        public Set<BytesRef> contexts() {\n          return null;\n        }\n\n        @Override\n        public boolean hasContexts() {\n          return false;\n        }\n      });\n\n    // Build inefficient but hopefully correct model:\n    List<Map<String,Integer>> gramCounts = new ArrayList<>(grams);\n    for(int gram=0;gram<grams;gram++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: build model for gram=\" + gram);\n      }\n      Map<String,Integer> model = new HashMap<>();\n      gramCounts.add(model);\n      for(String[] doc : docs) {\n        for(int i=0;i<doc.length-gram;i++) {\n          StringBuilder b = new StringBuilder();\n          for(int j=i;j<=i+gram;j++) {\n            if (j > i) {\n              b.append(' ');\n            }\n            b.append(doc[j]);\n          }\n          String token = b.toString();\n          Integer curCount = model.get(token);\n          if (curCount == null) {\n            model.put(token, 1);\n          } else {\n            model.put(token, 1 + curCount);\n          }\n          if (VERBOSE) {\n            System.out.println(\"  add '\" + token + \"' -> count=\" + model.get(token));\n          }\n        }\n      }\n    }\n\n    int lookups = atLeast(100);\n    for(int iter=0;iter<lookups;iter++) {\n      String[] tokens = new String[TestUtil.nextInt(random(), 1, 5)];\n      for(int i=0;i<tokens.length;i++) {\n        tokens[i] = getZipfToken(terms);\n      }\n\n      // Maybe trim last token; be sure not to create the\n      // empty string:\n      int trimStart;\n      if (tokens.length == 1) {\n        trimStart = 1;\n      } else {\n        trimStart = 0;\n      }\n      int trimAt = TestUtil.nextInt(random(), trimStart, tokens[tokens.length - 1].length());\n      tokens[tokens.length-1] = tokens[tokens.length-1].substring(0, trimAt);\n\n      int num = TestUtil.nextInt(random(), 1, 100);\n      StringBuilder b = new StringBuilder();\n      for(String token : tokens) {\n        b.append(' ');\n        b.append(token);\n      }\n      String query = b.toString();\n      query = query.substring(1);\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" query='\" + query + \"' num=\" + num);\n      }\n\n      // Expected:\n      List<LookupResult> expected = new ArrayList<>();\n      double backoff = 1.0;\n      seen = new HashSet<>();\n\n      if (VERBOSE) {\n        System.out.println(\"  compute expected\");\n      }\n      for(int i=grams-1;i>=0;i--) {\n        if (VERBOSE) {\n          System.out.println(\"    grams=\" + i);\n        }\n\n        if (tokens.length < i+1) {\n          // Don't have enough tokens to use this model\n          if (VERBOSE) {\n            System.out.println(\"      skip\");\n          }\n          continue;\n        }\n\n        if (i == 0 && tokens[tokens.length-1].length() == 0) {\n          // Never suggest unigrams from empty string:\n          if (VERBOSE) {\n            System.out.println(\"      skip unigram priors only\");\n          }\n          continue;\n        }\n\n        // Build up \"context\" ngram:\n        b = new StringBuilder();\n        for(int j=tokens.length-i-1;j<tokens.length-1;j++) {\n          b.append(' ');\n          b.append(tokens[j]);\n        }\n        String context = b.toString();\n        if (context.length() > 0) {\n          context = context.substring(1);\n        }\n        if (VERBOSE) {\n          System.out.println(\"      context='\" + context + \"'\");\n        }\n        long contextCount;\n        if (context.length() == 0) {\n          contextCount = totTokens;\n        } else {\n          Integer count = gramCounts.get(i-1).get(context);\n          if (count == null) {\n            // We never saw this context:\n            backoff *= FreeTextSuggester.ALPHA;\n            if (VERBOSE) {\n              System.out.println(\"      skip: never saw context\");\n            }\n            continue;\n          }\n          contextCount = count;\n        }\n        if (VERBOSE) {\n          System.out.println(\"      contextCount=\" + contextCount);\n        }\n        Map<String,Integer> model = gramCounts.get(i);\n\n        // First pass, gather all predictions for this model:\n        if (VERBOSE) {\n          System.out.println(\"      find terms w/ prefix=\" + tokens[tokens.length-1]);\n        }\n        List<LookupResult> tmp = new ArrayList<>();\n        for(String term : terms) {\n          if (term.startsWith(tokens[tokens.length-1])) {\n            if (VERBOSE) {\n              System.out.println(\"        term=\" + term);\n            }\n            if (seen.contains(term)) {\n              if (VERBOSE) {\n                System.out.println(\"          skip seen\");\n              }\n              continue;\n            }\n            String ngram = (context + \" \" + term).trim();\n            Integer count = model.get(ngram);\n            if (count != null) {\n              LookupResult lr = new LookupResult(ngram, (long) (Long.MAX_VALUE * (backoff * (double) count / contextCount)));\n              tmp.add(lr);\n              if (VERBOSE) {\n                System.out.println(\"      add tmp key='\" + lr.key + \"' score=\" + lr.value);\n              }\n            }\n          }\n        }\n\n        // Second pass, trim to only top N, and fold those\n        // into overall suggestions:\n        Collections.sort(tmp, byScoreThenKey);\n        if (tmp.size() > num) {\n          tmp.subList(num, tmp.size()).clear();\n        }\n        for(LookupResult result : tmp) {\n          String key = result.key.toString();\n          int idx = key.lastIndexOf(' ');\n          String lastToken;\n          if (idx != -1) {\n            lastToken = key.substring(idx+1);\n          } else {\n            lastToken = key;\n          }\n          if (!seen.contains(lastToken)) {\n            seen.add(lastToken);\n            expected.add(result);\n            if (VERBOSE) {\n              System.out.println(\"      keep key='\" + result.key + \"' score=\" + result.value);\n            }\n          }\n        }\n        \n        backoff *= FreeTextSuggester.ALPHA;\n      }\n\n      Collections.sort(expected, byScoreThenKey);\n\n      if (expected.size() > num) {\n        expected.subList(num, expected.size()).clear();\n      }\n\n      // Actual:\n      List<LookupResult> actual = sug.lookup(query, num);\n\n      if (VERBOSE) {\n        System.out.println(\"  expected: \" + expected);\n        System.out.println(\"    actual: \" + actual);\n      }\n\n      assertEquals(expected.toString(), actual.toString());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["6613659748fe4411a7dcf85266e55db1f95f7315"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b","a56958d7f71a28824f20031ffbb2e13502a0274e"],"58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"41aee74b5f91a096e3fd950f4a336bc763f0e7a7":["ada2f7352a7f964fe49bccd13227c4ec38563d39"],"d0af0c31a687dd847212ae59f661152896c76516":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"6613659748fe4411a7dcf85266e55db1f95f7315":["41aee74b5f91a096e3fd950f4a336bc763f0e7a7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a56958d7f71a28824f20031ffbb2e13502a0274e":["58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a56958d7f71a28824f20031ffbb2e13502a0274e"],"ada2f7352a7f964fe49bccd13227c4ec38563d39":["d0af0c31a687dd847212ae59f661152896c76516"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","a56958d7f71a28824f20031ffbb2e13502a0274e"],"41aee74b5f91a096e3fd950f4a336bc763f0e7a7":["6613659748fe4411a7dcf85266e55db1f95f7315"],"d0af0c31a687dd847212ae59f661152896c76516":["ada2f7352a7f964fe49bccd13227c4ec38563d39"],"6613659748fe4411a7dcf85266e55db1f95f7315":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d0af0c31a687dd847212ae59f661152896c76516"],"a56958d7f71a28824f20031ffbb2e13502a0274e":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ada2f7352a7f964fe49bccd13227c4ec38563d39":["41aee74b5f91a096e3fd950f4a336bc763f0e7a7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}