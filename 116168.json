{"path":"solr/core/src/java/org/apache/solr/handler/admin/SplitOp#getHashHistogramFromId(SolrIndexSearcher,String,DocRouter,DocCollection).mjava","commits":[{"id":"36bdabc04743acfe0e82c9cf8208b1111b2b193a","date":1565115020,"type":0,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/admin/SplitOp#getHashHistogramFromId(SolrIndexSearcher,String,DocRouter,DocCollection).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   *   Returns a list of range counts sorted by the range lower bound, using the indexed \"id\" field (i.e. the terms are full IDs, not just prefixes)\n   */\n  static Collection<RangeCount> getHashHistogramFromId(SolrIndexSearcher searcher, String idField, DocRouter router, DocCollection collection) throws IOException {\n    RTimer timer = new RTimer();\n\n    TreeMap<DocRouter.Range, RangeCount> counts = new TreeMap<>();\n\n    Terms terms = MultiTerms.getTerms(searcher.getIndexReader(), idField);\n    if (terms == null) {\n      return counts.values();\n    }\n\n    int numPrefixes = 0;\n    int numCollisions = 0;\n    long sumBuckets = 0;\n\n\n    byte sep = (byte) CompositeIdRouter.SEPARATOR.charAt(0);\n    TermsEnum termsEnum = terms.iterator();\n    BytesRef currPrefix = new BytesRef();  // prefix of the previous \"id\" term\n    int bucketCount = 0; // count of the number of docs in the current bucket\n\n    // We're going to iterate over all terms, so do the minimum amount of work per term.\n    // Terms are sorted, so all terms sharing a prefix will be grouped together.  The extra work\n    // is really just limited to stepping over all the terms in the id field.\n    for (;;) {\n      BytesRef term = termsEnum.next();\n\n      // compare to current prefix bucket and see if this new term shares the same prefix\n      if (term != null && term.length >= currPrefix.length && currPrefix.length > 0) {\n        int i = 0;\n        for (; i < currPrefix.length; i++) {\n          if (currPrefix.bytes[i] != term.bytes[term.offset + i]) {\n            break;\n          }\n        }\n\n        if (i == currPrefix.length) {\n          // prefix was the same (common-case fast path)\n          // int count = termsEnum.docFreq();\n          bucketCount++;  // use 1 since we are dealing with unique ids\n          continue;\n        }\n      }\n\n      // At this point the prefix did not match, so if we had a bucket we were working on, record it.\n      if (currPrefix.length > 0) {\n        numPrefixes++;\n        sumBuckets += bucketCount;\n        String currPrefixStr = currPrefix.utf8ToString();\n        DocRouter.Range range = router.getSearchRangeSingle(currPrefixStr, null, collection);\n\n        RangeCount rangeCount = new RangeCount(range, bucketCount);\n        bucketCount = 0;\n\n        RangeCount prev = counts.put(rangeCount.range, rangeCount);\n        if (prev != null) {\n          // we hit a hash collision, so add the buckets together.\n          rangeCount.count += prev.count;\n          numCollisions++;\n        }\n      }\n\n      // if the current term is null, we ran out of values\n      if (term == null) break;\n\n      // find the new prefix (if any)\n\n      // resize if needed\n      if (currPrefix.length < term.length) {\n        currPrefix.bytes = new byte[term.length+10];\n      }\n\n      // Copy the bytes up to and including the separator, and set the length if the separator is found.\n      // If there was no separator, then length remains 0 and it's the indicator that we have no prefix bucket\n      currPrefix.length = 0;\n      for (int i=0; i<term.length; i++) {\n        byte b = term.bytes[i + term.offset];\n        currPrefix.bytes[i] = b;\n        if (b == sep) {\n          currPrefix.length = i + 1;\n          bucketCount++;\n          break;\n        }\n      }\n    }\n\n    log.info(\"Split histogram from idField {}: ms={}, numBuckets={} sumBuckets={} numPrefixes={}numCollisions={}\", idField, timer.getTime(), counts.size(), sumBuckets, numPrefixes, numCollisions);\n\n    return counts.values();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a97a72dc16d01fda8ca5c9e0264b3604e30ab539","date":1565639985,"type":3,"author":"Megan Carey","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/admin/SplitOp#getHashHistogramFromId(SolrIndexSearcher,String,DocRouter,DocCollection).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/admin/SplitOp#getHashHistogramFromId(SolrIndexSearcher,String,DocRouter,DocCollection).mjava","sourceNew":"  /**\n   *   Returns a list of range counts sorted by the range lower bound, using the indexed \"id\" field (i.e. the terms are full IDs, not just prefixes)\n   */\n  static Collection<RangeCount> getHashHistogramFromId(SolrIndexSearcher searcher, String idField, DocRouter router, DocCollection collection) throws IOException {\n    RTimer timer = new RTimer();\n\n    TreeMap<DocRouter.Range, RangeCount> counts = new TreeMap<>();\n\n    Terms terms = MultiTerms.getTerms(searcher.getIndexReader(), idField);\n    if (terms == null) {\n      return counts.values();\n    }\n\n    int numPrefixes = 0;\n    int numCollisions = 0;\n    long sumBuckets = 0;\n\n\n    byte sep = (byte) CompositeIdRouter.SEPARATOR.charAt(0);\n    TermsEnum termsEnum = terms.iterator();\n    BytesRef currPrefix = new BytesRef();  // prefix of the previous \"id\" term\n    int bucketCount = 0; // count of the number of docs in the current bucket\n\n    // We're going to iterate over all terms, so do the minimum amount of work per term.\n    // Terms are sorted, so all terms sharing a prefix will be grouped together.  The extra work\n    // is really just limited to stepping over all the terms in the id field.\n    for (;;) {\n      BytesRef term = termsEnum.next();\n\n      // compare to current prefix bucket and see if this new term shares the same prefix\n      if (term != null && term.length >= currPrefix.length && currPrefix.length > 0) {\n        if (StringHelper.startsWith(term, currPrefix)) {\n          bucketCount++;  // use 1 since we are dealing with unique ids\n          continue;\n        }\n      }\n\n      // At this point the prefix did not match, so if we had a bucket we were working on, record it.\n      if (currPrefix.length > 0) {\n        numPrefixes++;\n        sumBuckets += bucketCount;\n        String currPrefixStr = currPrefix.utf8ToString();\n        DocRouter.Range range = router.getSearchRangeSingle(currPrefixStr, null, collection);\n\n        RangeCount rangeCount = new RangeCount(range, bucketCount);\n        bucketCount = 0;\n\n        RangeCount prev = counts.put(rangeCount.range, rangeCount);\n        if (prev != null) {\n          // we hit a hash collision, so add the buckets together.\n          rangeCount.count += prev.count;\n          numCollisions++;\n        }\n      }\n\n      // if the current term is null, we ran out of values\n      if (term == null) break;\n\n      // find the new prefix (if any)\n\n      // resize if needed\n      if (currPrefix.length < term.length) {\n        currPrefix.bytes = new byte[term.length+10];\n      }\n\n      // Copy the bytes up to and including the separator, and set the length if the separator is found.\n      // If there was no separator, then length remains 0 and it's the indicator that we have no prefix bucket\n      currPrefix.length = 0;\n      for (int i=0; i<term.length; i++) {\n        byte b = term.bytes[i + term.offset];\n        currPrefix.bytes[i] = b;\n        if (b == sep) {\n          currPrefix.length = i + 1;\n          bucketCount++;\n          break;\n        }\n      }\n    }\n\n    log.info(\"Split histogram from idField {}: ms={}, numBuckets={} sumBuckets={} numPrefixes={} numCollisions={}\", idField, timer.getTime(), counts.size(), sumBuckets, numPrefixes, numCollisions);\n\n    return counts.values();\n  }\n\n","sourceOld":"  /**\n   *   Returns a list of range counts sorted by the range lower bound, using the indexed \"id\" field (i.e. the terms are full IDs, not just prefixes)\n   */\n  static Collection<RangeCount> getHashHistogramFromId(SolrIndexSearcher searcher, String idField, DocRouter router, DocCollection collection) throws IOException {\n    RTimer timer = new RTimer();\n\n    TreeMap<DocRouter.Range, RangeCount> counts = new TreeMap<>();\n\n    Terms terms = MultiTerms.getTerms(searcher.getIndexReader(), idField);\n    if (terms == null) {\n      return counts.values();\n    }\n\n    int numPrefixes = 0;\n    int numCollisions = 0;\n    long sumBuckets = 0;\n\n\n    byte sep = (byte) CompositeIdRouter.SEPARATOR.charAt(0);\n    TermsEnum termsEnum = terms.iterator();\n    BytesRef currPrefix = new BytesRef();  // prefix of the previous \"id\" term\n    int bucketCount = 0; // count of the number of docs in the current bucket\n\n    // We're going to iterate over all terms, so do the minimum amount of work per term.\n    // Terms are sorted, so all terms sharing a prefix will be grouped together.  The extra work\n    // is really just limited to stepping over all the terms in the id field.\n    for (;;) {\n      BytesRef term = termsEnum.next();\n\n      // compare to current prefix bucket and see if this new term shares the same prefix\n      if (term != null && term.length >= currPrefix.length && currPrefix.length > 0) {\n        int i = 0;\n        for (; i < currPrefix.length; i++) {\n          if (currPrefix.bytes[i] != term.bytes[term.offset + i]) {\n            break;\n          }\n        }\n\n        if (i == currPrefix.length) {\n          // prefix was the same (common-case fast path)\n          // int count = termsEnum.docFreq();\n          bucketCount++;  // use 1 since we are dealing with unique ids\n          continue;\n        }\n      }\n\n      // At this point the prefix did not match, so if we had a bucket we were working on, record it.\n      if (currPrefix.length > 0) {\n        numPrefixes++;\n        sumBuckets += bucketCount;\n        String currPrefixStr = currPrefix.utf8ToString();\n        DocRouter.Range range = router.getSearchRangeSingle(currPrefixStr, null, collection);\n\n        RangeCount rangeCount = new RangeCount(range, bucketCount);\n        bucketCount = 0;\n\n        RangeCount prev = counts.put(rangeCount.range, rangeCount);\n        if (prev != null) {\n          // we hit a hash collision, so add the buckets together.\n          rangeCount.count += prev.count;\n          numCollisions++;\n        }\n      }\n\n      // if the current term is null, we ran out of values\n      if (term == null) break;\n\n      // find the new prefix (if any)\n\n      // resize if needed\n      if (currPrefix.length < term.length) {\n        currPrefix.bytes = new byte[term.length+10];\n      }\n\n      // Copy the bytes up to and including the separator, and set the length if the separator is found.\n      // If there was no separator, then length remains 0 and it's the indicator that we have no prefix bucket\n      currPrefix.length = 0;\n      for (int i=0; i<term.length; i++) {\n        byte b = term.bytes[i + term.offset];\n        currPrefix.bytes[i] = b;\n        if (b == sep) {\n          currPrefix.length = i + 1;\n          bucketCount++;\n          break;\n        }\n      }\n    }\n\n    log.info(\"Split histogram from idField {}: ms={}, numBuckets={} sumBuckets={} numPrefixes={}numCollisions={}\", idField, timer.getTime(), counts.size(), sumBuckets, numPrefixes, numCollisions);\n\n    return counts.values();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"575e66bd4b2349209027f6801184da7fc3cba13f","date":1587609169,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/admin/SplitOp#getHashHistogramFromId(SolrIndexSearcher,String,DocRouter,DocCollection).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/admin/SplitOp#getHashHistogramFromId(SolrIndexSearcher,String,DocRouter,DocCollection).mjava","sourceNew":"  /**\n   *   Returns a list of range counts sorted by the range lower bound, using the indexed \"id\" field (i.e. the terms are full IDs, not just prefixes)\n   */\n  static Collection<RangeCount> getHashHistogramFromId(SolrIndexSearcher searcher, String idField, DocRouter router, DocCollection collection) throws IOException {\n    RTimer timer = new RTimer();\n\n    TreeMap<DocRouter.Range, RangeCount> counts = new TreeMap<>();\n\n    Terms terms = MultiTerms.getTerms(searcher.getIndexReader(), idField);\n    if (terms == null) {\n      return counts.values();\n    }\n\n    int numPrefixes = 0;\n    int numCollisions = 0;\n    long sumBuckets = 0;\n\n\n    byte sep = (byte) CompositeIdRouter.SEPARATOR.charAt(0);\n    TermsEnum termsEnum = terms.iterator();\n    BytesRef currPrefix = new BytesRef();  // prefix of the previous \"id\" term\n    int bucketCount = 0; // count of the number of docs in the current bucket\n\n    // We're going to iterate over all terms, so do the minimum amount of work per term.\n    // Terms are sorted, so all terms sharing a prefix will be grouped together.  The extra work\n    // is really just limited to stepping over all the terms in the id field.\n    for (;;) {\n      BytesRef term = termsEnum.next();\n\n      // compare to current prefix bucket and see if this new term shares the same prefix\n      if (term != null && term.length >= currPrefix.length && currPrefix.length > 0) {\n        if (StringHelper.startsWith(term, currPrefix)) {\n          bucketCount++;  // use 1 since we are dealing with unique ids\n          continue;\n        }\n      }\n\n      // At this point the prefix did not match, so if we had a bucket we were working on, record it.\n      if (currPrefix.length > 0) {\n        numPrefixes++;\n        sumBuckets += bucketCount;\n        String currPrefixStr = currPrefix.utf8ToString();\n        DocRouter.Range range = router.getSearchRangeSingle(currPrefixStr, null, collection);\n\n        RangeCount rangeCount = new RangeCount(range, bucketCount);\n        bucketCount = 0;\n\n        RangeCount prev = counts.put(rangeCount.range, rangeCount);\n        if (prev != null) {\n          // we hit a hash collision, so add the buckets together.\n          rangeCount.count += prev.count;\n          numCollisions++;\n        }\n      }\n\n      // if the current term is null, we ran out of values\n      if (term == null) break;\n\n      // find the new prefix (if any)\n\n      // resize if needed\n      if (currPrefix.length < term.length) {\n        currPrefix.bytes = new byte[term.length+10];\n      }\n\n      // Copy the bytes up to and including the separator, and set the length if the separator is found.\n      // If there was no separator, then length remains 0 and it's the indicator that we have no prefix bucket\n      currPrefix.length = 0;\n      for (int i=0; i<term.length; i++) {\n        byte b = term.bytes[i + term.offset];\n        currPrefix.bytes[i] = b;\n        if (b == sep) {\n          currPrefix.length = i + 1;\n          bucketCount++;\n          break;\n        }\n      }\n    }\n\n    if (log.isInfoEnabled()) {\n      log.info(\"Split histogram from idField {}: ms={}, numBuckets={} sumBuckets={} numPrefixes={} numCollisions={}\"\n          , idField, timer.getTime(), counts.size(), sumBuckets, numPrefixes, numCollisions);\n    }\n\n    return counts.values();\n  }\n\n","sourceOld":"  /**\n   *   Returns a list of range counts sorted by the range lower bound, using the indexed \"id\" field (i.e. the terms are full IDs, not just prefixes)\n   */\n  static Collection<RangeCount> getHashHistogramFromId(SolrIndexSearcher searcher, String idField, DocRouter router, DocCollection collection) throws IOException {\n    RTimer timer = new RTimer();\n\n    TreeMap<DocRouter.Range, RangeCount> counts = new TreeMap<>();\n\n    Terms terms = MultiTerms.getTerms(searcher.getIndexReader(), idField);\n    if (terms == null) {\n      return counts.values();\n    }\n\n    int numPrefixes = 0;\n    int numCollisions = 0;\n    long sumBuckets = 0;\n\n\n    byte sep = (byte) CompositeIdRouter.SEPARATOR.charAt(0);\n    TermsEnum termsEnum = terms.iterator();\n    BytesRef currPrefix = new BytesRef();  // prefix of the previous \"id\" term\n    int bucketCount = 0; // count of the number of docs in the current bucket\n\n    // We're going to iterate over all terms, so do the minimum amount of work per term.\n    // Terms are sorted, so all terms sharing a prefix will be grouped together.  The extra work\n    // is really just limited to stepping over all the terms in the id field.\n    for (;;) {\n      BytesRef term = termsEnum.next();\n\n      // compare to current prefix bucket and see if this new term shares the same prefix\n      if (term != null && term.length >= currPrefix.length && currPrefix.length > 0) {\n        if (StringHelper.startsWith(term, currPrefix)) {\n          bucketCount++;  // use 1 since we are dealing with unique ids\n          continue;\n        }\n      }\n\n      // At this point the prefix did not match, so if we had a bucket we were working on, record it.\n      if (currPrefix.length > 0) {\n        numPrefixes++;\n        sumBuckets += bucketCount;\n        String currPrefixStr = currPrefix.utf8ToString();\n        DocRouter.Range range = router.getSearchRangeSingle(currPrefixStr, null, collection);\n\n        RangeCount rangeCount = new RangeCount(range, bucketCount);\n        bucketCount = 0;\n\n        RangeCount prev = counts.put(rangeCount.range, rangeCount);\n        if (prev != null) {\n          // we hit a hash collision, so add the buckets together.\n          rangeCount.count += prev.count;\n          numCollisions++;\n        }\n      }\n\n      // if the current term is null, we ran out of values\n      if (term == null) break;\n\n      // find the new prefix (if any)\n\n      // resize if needed\n      if (currPrefix.length < term.length) {\n        currPrefix.bytes = new byte[term.length+10];\n      }\n\n      // Copy the bytes up to and including the separator, and set the length if the separator is found.\n      // If there was no separator, then length remains 0 and it's the indicator that we have no prefix bucket\n      currPrefix.length = 0;\n      for (int i=0; i<term.length; i++) {\n        byte b = term.bytes[i + term.offset];\n        currPrefix.bytes[i] = b;\n        if (b == sep) {\n          currPrefix.length = i + 1;\n          bucketCount++;\n          break;\n        }\n      }\n    }\n\n    log.info(\"Split histogram from idField {}: ms={}, numBuckets={} sumBuckets={} numPrefixes={} numCollisions={}\", idField, timer.getTime(), counts.size(), sumBuckets, numPrefixes, numCollisions);\n\n    return counts.values();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"575e66bd4b2349209027f6801184da7fc3cba13f":["a97a72dc16d01fda8ca5c9e0264b3604e30ab539"],"36bdabc04743acfe0e82c9cf8208b1111b2b193a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["575e66bd4b2349209027f6801184da7fc3cba13f"],"a97a72dc16d01fda8ca5c9e0264b3604e30ab539":["36bdabc04743acfe0e82c9cf8208b1111b2b193a"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["36bdabc04743acfe0e82c9cf8208b1111b2b193a"],"575e66bd4b2349209027f6801184da7fc3cba13f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"36bdabc04743acfe0e82c9cf8208b1111b2b193a":["a97a72dc16d01fda8ca5c9e0264b3604e30ab539"],"a97a72dc16d01fda8ca5c9e0264b3604e30ab539":["575e66bd4b2349209027f6801184da7fc3cba13f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}