{"path":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#doTest().mjava","commits":[{"id":"70f91c8322fbffe3a3a897ef20ea19119cac10cd","date":1386170124,"type":1,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#doTest().mjava","pathOld":"solr/contrib/solr-morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    File file = new File(RESOURCES_DIR + \"/test-documents/sample-statuses-20120906-141433-medium.avro\");\n    \n    waitForRecoveriesToFinish(false);\n    \n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines/tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.toByteArray(file);    \n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    commit();\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cloudClient.query(new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));   \n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), new Comparator<Record>() {\n      @Override\n      public int compare(Record r1, Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList();\n    FileReader<GenericData.Record> reader = new DataFileReader(file, new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, new Comparator<GenericData.Record>() {\n      @Override\n      public int compare(GenericData.Record r1, GenericData.Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n    cloudClient.shutdown();\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    File file = new File(RESOURCES_DIR + \"/test-documents/sample-statuses-20120906-141433-medium.avro\");\n    \n    waitForRecoveriesToFinish(false);\n    \n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines/tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.toByteArray(file);    \n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    commit();\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cloudClient.query(new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));   \n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), new Comparator<Record>() {\n      @Override\n      public int compare(Record r1, Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList();\n    FileReader<GenericData.Record> reader = new DataFileReader(file, new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, new Comparator<GenericData.Record>() {\n      @Override\n      public int compare(GenericData.Record r1, GenericData.Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n    cloudClient.shutdown();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"74f45af4339b0daf7a95c820ab88c1aea74fbce0","date":1387475327,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#doTest().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    File file = new File(RESOURCES_DIR + \"/test-documents/sample-statuses-20120906-141433-medium.avro\");\n    \n    waitForRecoveriesToFinish(false);\n    \n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines/tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.toByteArray(file);    \n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    commit();\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cloudClient.query(new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));   \n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), new Comparator<Record>() {\n      @Override\n      public int compare(Record r1, Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList();\n    FileReader<GenericData.Record> reader = new DataFileReader(file, new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, new Comparator<GenericData.Record>() {\n      @Override\n      public int compare(GenericData.Record r1, GenericData.Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n    cloudClient.shutdown();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c170e9d4c93c47801b611c5f124a91c5d27e0d73","date":1392824784,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#doTest().mjava","pathOld":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    Joiner joiner = Joiner.on(File.separator);\n    File file = new File(joiner.join(RESOURCES_DIR, \"test-documents\", \"sample-statuses-20120906-141433-medium.avro\"));\n    \n    waitForRecoveriesToFinish(false);\n    \n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.toByteArray(file);    \n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    commit();\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cloudClient.query(new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));   \n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), new Comparator<Record>() {\n      @Override\n      public int compare(Record r1, Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList();\n    FileReader<GenericData.Record> reader = new DataFileReader(file, new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, new Comparator<GenericData.Record>() {\n      @Override\n      public int compare(GenericData.Record r1, GenericData.Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n    cloudClient.shutdown();\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    File file = new File(RESOURCES_DIR + \"/test-documents/sample-statuses-20120906-141433-medium.avro\");\n    \n    waitForRecoveriesToFinish(false);\n    \n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines/tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.toByteArray(file);    \n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    commit();\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cloudClient.query(new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));   \n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), new Comparator<Record>() {\n      @Override\n      public int compare(Record r1, Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList();\n    FileReader<GenericData.Record> reader = new DataFileReader(file, new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, new Comparator<GenericData.Record>() {\n      @Override\n      public int compare(GenericData.Record r1, GenericData.Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n    cloudClient.shutdown();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"abb23fcc2461782ab204e61213240feb77d355aa","date":1422029612,"type":5,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#test().mjava","pathOld":"solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest#doTest().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    Joiner joiner = Joiner.on(File.separator);\n    File file = new File(joiner.join(RESOURCES_DIR, \"test-documents\", \"sample-statuses-20120906-141433-medium.avro\"));\n    \n    waitForRecoveriesToFinish(false);\n    \n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.toByteArray(file);    \n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    commit();\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cloudClient.query(new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));   \n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), new Comparator<Record>() {\n      @Override\n      public int compare(Record r1, Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList();\n    FileReader<GenericData.Record> reader = new DataFileReader(file, new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, new Comparator<GenericData.Record>() {\n      @Override\n      public int compare(GenericData.Record r1, GenericData.Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n    cloudClient.shutdown();\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    Joiner joiner = Joiner.on(File.separator);\n    File file = new File(joiner.join(RESOURCES_DIR, \"test-documents\", \"sample-statuses-20120906-141433-medium.avro\"));\n    \n    waitForRecoveriesToFinish(false);\n    \n    // load avro records via morphline and zk into solr\n    morphline = parse(\"test-morphlines\" + File.separator + \"tutorialReadAvroContainer\");    \n    Record record = new Record();\n    byte[] body = Files.toByteArray(file);    \n    record.put(Fields.ATTACHMENT_BODY, body);\n    startSession();\n    Notifications.notifyBeginTransaction(morphline);\n    assertTrue(morphline.process(record));\n    assertEquals(1, collector.getNumStartEvents());\n    \n    commit();\n    \n    // fetch sorted result set from solr\n    QueryResponse rsp = cloudClient.query(new SolrQuery(\"*:*\").setRows(100000).addSort(\"id\", SolrQuery.ORDER.asc));   \n    assertEquals(2104, collector.getRecords().size());\n    assertEquals(collector.getRecords().size(), rsp.getResults().size());\n    \n    Collections.sort(collector.getRecords(), new Comparator<Record>() {\n      @Override\n      public int compare(Record r1, Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n\n    // fetch test input data and sort like solr result set\n    List<GenericData.Record> records = new ArrayList();\n    FileReader<GenericData.Record> reader = new DataFileReader(file, new GenericDatumReader());\n    while (reader.hasNext()) {\n      GenericData.Record expected = reader.next();\n      records.add(expected);\n    }\n    assertEquals(collector.getRecords().size(), records.size());    \n    Collections.sort(records, new Comparator<GenericData.Record>() {\n      @Override\n      public int compare(GenericData.Record r1, GenericData.Record r2) {\n        return r1.get(\"id\").toString().compareTo(r2.get(\"id\").toString());\n      }      \n    });   \n    \n    Object lastId = null;\n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrec\" + i + \":\" + records.get(i));      \n      Object id = records.get(i);\n      if (id != null && id.equals(lastId)) {\n        throw new IllegalStateException(\"Detected duplicate id. Test input data must not contain duplicate ids!\");        \n      }\n      lastId = id;\n    }\n    \n    for (int i = 0; i < records.size(); i++) {  \n      //System.out.println(\"myrsp\" + i + \":\" + rsp.getResults().get(i));      \n    }    \n\n    Iterator<SolrDocument> rspIter = rsp.getResults().iterator();\n    for (int i = 0; i < records.size(); i++) {  \n      // verify morphline spat out expected data\n      Record actual = collector.getRecords().get(i);\n      GenericData.Record expected = records.get(i);\n      Preconditions.checkNotNull(expected);\n      assertTweetEquals(expected, actual, i);\n      \n      // verify Solr result set contains expected data\n      actual = new Record();\n      actual.getFields().putAll(next(rspIter));\n      assertTweetEquals(expected, actual, i);\n    }\n    \n    Notifications.notifyRollbackTransaction(morphline);\n    Notifications.notifyShutdown(morphline);\n    cloudClient.shutdown();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c170e9d4c93c47801b611c5f124a91c5d27e0d73":["70f91c8322fbffe3a3a897ef20ea19119cac10cd"],"abb23fcc2461782ab204e61213240feb77d355aa":["c170e9d4c93c47801b611c5f124a91c5d27e0d73"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","70f91c8322fbffe3a3a897ef20ea19119cac10cd"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["abb23fcc2461782ab204e61213240feb77d355aa"]},"commit2Childs":{"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["c170e9d4c93c47801b611c5f124a91c5d27e0d73","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"c170e9d4c93c47801b611c5f124a91c5d27e0d73":["abb23fcc2461782ab204e61213240feb77d355aa"],"abb23fcc2461782ab204e61213240feb77d355aa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["70f91c8322fbffe3a3a897ef20ea19119cac10cd","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["74f45af4339b0daf7a95c820ab88c1aea74fbce0","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}