{"path":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testLocalityReporter().mjava","commits":[{"id":"1ce8283f367b946e5dd6300887294d7d115f2b9f","date":1433955116,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testLocalityReporter().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testLocalityReporter() throws Exception {\n    Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    \n    HdfsDirectoryFactory factory = new HdfsDirectoryFactory();\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(HdfsDirectoryFactory.HDFS_HOME, HdfsTestUtil.getURI(dfsCluster) + \"/solr\");\n    props.put(HdfsDirectoryFactory.BLOCKCACHE_ENABLED, \"false\");\n    props.put(HdfsDirectoryFactory.NRTCACHINGDIRECTORY_ENABLE, \"false\");\n    factory.init(new NamedList<>(props));\n    \n    Iterator<SolrInfoMBean> it = factory.offerMBeans().iterator();\n    it.next(); // skip\n    SolrInfoMBean localityBean = it.next(); // brittle, but it's ok\n    \n    // Make sure we have the right bean.\n    assertEquals(\"hdfs-locality\", localityBean.getName());\n    \n    // We haven't done anything, so there should be no data\n    NamedList<?> statistics = localityBean.getStatistics();\n    assertEquals(0l, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(0, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO));\n    \n    // create a directory and a file\n    String path = HdfsTestUtil.getURI(dfsCluster) + \"/solr3/\";\n    Directory dir = factory.create(path, NoLockFactory.INSTANCE, DirContext.DEFAULT);\n    try(IndexOutput writer = dir.createOutput(\"output\", null)) {\n      writer.writeLong(42l);\n    }\n    \n    final long long_bytes = Long.SIZE / Byte.SIZE;\n    \n    // no locality because hostname not set\n    statistics = localityBean.getStatistics();\n    assertEquals(long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(1, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL));\n    assertEquals(0, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL));\n    \n    // set hostname and check again\n    factory.setHost(\"127.0.0.1\");\n    statistics = localityBean.getStatistics();\n    assertEquals(long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL));\n    \n    factory.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["add53de9835b2cd1a7a80b4e0036afee171c9fdf"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"215d4d9447bf3e08c95b7f13cb3681b82aa15602","date":1434465189,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testLocalityReporter().mjava","pathOld":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testLocalityReporter().mjava","sourceNew":"  @Test\n  public void testLocalityReporter() throws Exception {\n    Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    \n    HdfsDirectoryFactory factory = new HdfsDirectoryFactory();\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(HdfsDirectoryFactory.HDFS_HOME, HdfsTestUtil.getURI(dfsCluster) + \"/solr\");\n    props.put(HdfsDirectoryFactory.BLOCKCACHE_ENABLED, \"false\");\n    props.put(HdfsDirectoryFactory.NRTCACHINGDIRECTORY_ENABLE, \"false\");\n    factory.init(new NamedList<>(props));\n    \n    Iterator<SolrInfoMBean> it = factory.offerMBeans().iterator();\n    it.next(); // skip\n    SolrInfoMBean localityBean = it.next(); // brittle, but it's ok\n    \n    // Make sure we have the right bean.\n    assertEquals(\"Got the wrong bean: \" + localityBean.getName(), \"hdfs-locality\", localityBean.getName());\n    \n    // We haven't done anything, so there should be no data\n    NamedList<?> statistics = localityBean.getStatistics();\n    assertEquals(\"Saw bytes that were not written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL), 0l,\n        statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(\n        \"Counted bytes as local when none written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO), 0,\n        statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO));\n    \n    // create a directory and a file\n    String path = HdfsTestUtil.getURI(dfsCluster) + \"/solr3/\";\n    Directory dir = factory.create(path, NoLockFactory.INSTANCE, DirContext.DEFAULT);\n    try(IndexOutput writer = dir.createOutput(\"output\", null)) {\n      writer.writeLong(42l);\n    }\n    \n    final long long_bytes = Long.SIZE / Byte.SIZE;\n    \n    // no locality because hostname not set\n    factory.setHost(\"bogus\");\n    statistics = localityBean.getStatistics();\n    assertEquals(\"Wrong number of total bytes counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL),\n        long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(\"Wrong number of total blocks counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL),\n        1, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL));\n    assertEquals(\n        \"Counted block as local when bad hostname set: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL),\n        0, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL));\n        \n    // set hostname and check again\n    factory.setHost(\"127.0.0.1\");\n    statistics = localityBean.getStatistics();\n    assertEquals(\n        \"Did not count block as local after setting hostname: \"\n            + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL),\n        long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL));\n        \n    factory.close();\n  }\n\n","sourceOld":"  @Test\n  public void testLocalityReporter() throws Exception {\n    Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    \n    HdfsDirectoryFactory factory = new HdfsDirectoryFactory();\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(HdfsDirectoryFactory.HDFS_HOME, HdfsTestUtil.getURI(dfsCluster) + \"/solr\");\n    props.put(HdfsDirectoryFactory.BLOCKCACHE_ENABLED, \"false\");\n    props.put(HdfsDirectoryFactory.NRTCACHINGDIRECTORY_ENABLE, \"false\");\n    factory.init(new NamedList<>(props));\n    \n    Iterator<SolrInfoMBean> it = factory.offerMBeans().iterator();\n    it.next(); // skip\n    SolrInfoMBean localityBean = it.next(); // brittle, but it's ok\n    \n    // Make sure we have the right bean.\n    assertEquals(\"hdfs-locality\", localityBean.getName());\n    \n    // We haven't done anything, so there should be no data\n    NamedList<?> statistics = localityBean.getStatistics();\n    assertEquals(0l, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(0, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO));\n    \n    // create a directory and a file\n    String path = HdfsTestUtil.getURI(dfsCluster) + \"/solr3/\";\n    Directory dir = factory.create(path, NoLockFactory.INSTANCE, DirContext.DEFAULT);\n    try(IndexOutput writer = dir.createOutput(\"output\", null)) {\n      writer.writeLong(42l);\n    }\n    \n    final long long_bytes = Long.SIZE / Byte.SIZE;\n    \n    // no locality because hostname not set\n    statistics = localityBean.getStatistics();\n    assertEquals(long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(1, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL));\n    assertEquals(0, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL));\n    \n    // set hostname and check again\n    factory.setHost(\"127.0.0.1\");\n    statistics = localityBean.getStatistics();\n    assertEquals(long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL));\n    \n    factory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"87df8312996aa71322478e54c0c1c43233e0b491","date":1443723405,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testLocalityReporter().mjava","pathOld":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testLocalityReporter().mjava","sourceNew":"  @Test\n  public void testLocalityReporter() throws Exception {\n    Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    \n    HdfsDirectoryFactory factory = new HdfsDirectoryFactory();\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(HdfsDirectoryFactory.HDFS_HOME, HdfsTestUtil.getURI(dfsCluster) + \"/solr\");\n    props.put(HdfsDirectoryFactory.BLOCKCACHE_ENABLED, \"false\");\n    props.put(HdfsDirectoryFactory.NRTCACHINGDIRECTORY_ENABLE, \"false\");\n    props.put(HdfsDirectoryFactory.LOCALITYMETRICS_ENABLED, \"true\");\n    factory.init(new NamedList<>(props));\n    \n    Iterator<SolrInfoMBean> it = factory.offerMBeans().iterator();\n    it.next(); // skip\n    SolrInfoMBean localityBean = it.next(); // brittle, but it's ok\n    \n    // Make sure we have the right bean.\n    assertEquals(\"Got the wrong bean: \" + localityBean.getName(), \"hdfs-locality\", localityBean.getName());\n    \n    // We haven't done anything, so there should be no data\n    NamedList<?> statistics = localityBean.getStatistics();\n    assertEquals(\"Saw bytes that were not written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL), 0l,\n        statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(\n        \"Counted bytes as local when none written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO), 0,\n        statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO));\n    \n    // create a directory and a file\n    String path = HdfsTestUtil.getURI(dfsCluster) + \"/solr3/\";\n    Directory dir = factory.create(path, NoLockFactory.INSTANCE, DirContext.DEFAULT);\n    try(IndexOutput writer = dir.createOutput(\"output\", null)) {\n      writer.writeLong(42l);\n    }\n    \n    final long long_bytes = Long.SIZE / Byte.SIZE;\n    \n    // no locality because hostname not set\n    factory.setHost(\"bogus\");\n    statistics = localityBean.getStatistics();\n    assertEquals(\"Wrong number of total bytes counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL),\n        long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(\"Wrong number of total blocks counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL),\n        1, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL));\n    assertEquals(\n        \"Counted block as local when bad hostname set: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL),\n        0, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL));\n        \n    // set hostname and check again\n    factory.setHost(\"127.0.0.1\");\n    statistics = localityBean.getStatistics();\n    assertEquals(\n        \"Did not count block as local after setting hostname: \"\n            + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL),\n        long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL));\n        \n    factory.close();\n  }\n\n","sourceOld":"  @Test\n  public void testLocalityReporter() throws Exception {\n    Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    \n    HdfsDirectoryFactory factory = new HdfsDirectoryFactory();\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(HdfsDirectoryFactory.HDFS_HOME, HdfsTestUtil.getURI(dfsCluster) + \"/solr\");\n    props.put(HdfsDirectoryFactory.BLOCKCACHE_ENABLED, \"false\");\n    props.put(HdfsDirectoryFactory.NRTCACHINGDIRECTORY_ENABLE, \"false\");\n    factory.init(new NamedList<>(props));\n    \n    Iterator<SolrInfoMBean> it = factory.offerMBeans().iterator();\n    it.next(); // skip\n    SolrInfoMBean localityBean = it.next(); // brittle, but it's ok\n    \n    // Make sure we have the right bean.\n    assertEquals(\"Got the wrong bean: \" + localityBean.getName(), \"hdfs-locality\", localityBean.getName());\n    \n    // We haven't done anything, so there should be no data\n    NamedList<?> statistics = localityBean.getStatistics();\n    assertEquals(\"Saw bytes that were not written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL), 0l,\n        statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(\n        \"Counted bytes as local when none written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO), 0,\n        statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO));\n    \n    // create a directory and a file\n    String path = HdfsTestUtil.getURI(dfsCluster) + \"/solr3/\";\n    Directory dir = factory.create(path, NoLockFactory.INSTANCE, DirContext.DEFAULT);\n    try(IndexOutput writer = dir.createOutput(\"output\", null)) {\n      writer.writeLong(42l);\n    }\n    \n    final long long_bytes = Long.SIZE / Byte.SIZE;\n    \n    // no locality because hostname not set\n    factory.setHost(\"bogus\");\n    statistics = localityBean.getStatistics();\n    assertEquals(\"Wrong number of total bytes counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL),\n        long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(\"Wrong number of total blocks counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL),\n        1, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL));\n    assertEquals(\n        \"Counted block as local when bad hostname set: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL),\n        0, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL));\n        \n    // set hostname and check again\n    factory.setHost(\"127.0.0.1\");\n    statistics = localityBean.getStatistics();\n    assertEquals(\n        \"Did not count block as local after setting hostname: \"\n            + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL),\n        long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL));\n        \n    factory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"816521ebaad5add9cb96bb88c577394e2938c40b","date":1491931343,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testLocalityReporter().mjava","pathOld":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testLocalityReporter().mjava","sourceNew":"  @Test\n  public void testLocalityReporter() throws Exception {\n    Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n\n    Random r = random();\n    HdfsDirectoryFactory factory = new HdfsDirectoryFactory();\n    SolrMetricManager metricManager = new SolrMetricManager();\n    String registry = TestUtil.randomSimpleString(r, 2, 10);\n    String scope = TestUtil.randomSimpleString(r,2, 10);\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(HdfsDirectoryFactory.HDFS_HOME, HdfsTestUtil.getURI(dfsCluster) + \"/solr\");\n    props.put(HdfsDirectoryFactory.BLOCKCACHE_ENABLED, \"false\");\n    props.put(HdfsDirectoryFactory.NRTCACHINGDIRECTORY_ENABLE, \"false\");\n    props.put(HdfsDirectoryFactory.LOCALITYMETRICS_ENABLED, \"true\");\n    factory.init(new NamedList<>(props));\n    factory.initializeMetrics(metricManager, registry, scope);\n\n    // get the metrics map for the locality bean\n    MetricsMap metrics = (MetricsMap)metricManager.registry(registry).getMetrics().get(\"OTHER.\" + scope + \".hdfsLocality\");\n    // We haven't done anything, so there should be no data\n    Map<String,Object> statistics = metrics.getValue();\n    assertEquals(\"Saw bytes that were not written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL), 0l,\n        statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(\n        \"Counted bytes as local when none written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO), 0,\n        statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO));\n    \n    // create a directory and a file\n    String path = HdfsTestUtil.getURI(dfsCluster) + \"/solr3/\";\n    Directory dir = factory.create(path, NoLockFactory.INSTANCE, DirContext.DEFAULT);\n    try(IndexOutput writer = dir.createOutput(\"output\", null)) {\n      writer.writeLong(42l);\n    }\n    \n    final long long_bytes = Long.SIZE / Byte.SIZE;\n    \n    // no locality because hostname not set\n    factory.setHost(\"bogus\");\n    statistics = metrics.getValue();\n    assertEquals(\"Wrong number of total bytes counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL),\n        long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(\"Wrong number of total blocks counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL),\n        1, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL));\n    assertEquals(\n        \"Counted block as local when bad hostname set: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL),\n        0, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL));\n        \n    // set hostname and check again\n    factory.setHost(\"127.0.0.1\");\n    statistics = metrics.getValue();\n    assertEquals(\n        \"Did not count block as local after setting hostname: \"\n            + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL),\n        long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL));\n        \n    factory.close();\n  }\n\n","sourceOld":"  @Test\n  public void testLocalityReporter() throws Exception {\n    Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    \n    HdfsDirectoryFactory factory = new HdfsDirectoryFactory();\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(HdfsDirectoryFactory.HDFS_HOME, HdfsTestUtil.getURI(dfsCluster) + \"/solr\");\n    props.put(HdfsDirectoryFactory.BLOCKCACHE_ENABLED, \"false\");\n    props.put(HdfsDirectoryFactory.NRTCACHINGDIRECTORY_ENABLE, \"false\");\n    props.put(HdfsDirectoryFactory.LOCALITYMETRICS_ENABLED, \"true\");\n    factory.init(new NamedList<>(props));\n    \n    Iterator<SolrInfoMBean> it = factory.offerMBeans().iterator();\n    it.next(); // skip\n    SolrInfoMBean localityBean = it.next(); // brittle, but it's ok\n    \n    // Make sure we have the right bean.\n    assertEquals(\"Got the wrong bean: \" + localityBean.getName(), \"hdfs-locality\", localityBean.getName());\n    \n    // We haven't done anything, so there should be no data\n    NamedList<?> statistics = localityBean.getStatistics();\n    assertEquals(\"Saw bytes that were not written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL), 0l,\n        statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(\n        \"Counted bytes as local when none written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO), 0,\n        statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO));\n    \n    // create a directory and a file\n    String path = HdfsTestUtil.getURI(dfsCluster) + \"/solr3/\";\n    Directory dir = factory.create(path, NoLockFactory.INSTANCE, DirContext.DEFAULT);\n    try(IndexOutput writer = dir.createOutput(\"output\", null)) {\n      writer.writeLong(42l);\n    }\n    \n    final long long_bytes = Long.SIZE / Byte.SIZE;\n    \n    // no locality because hostname not set\n    factory.setHost(\"bogus\");\n    statistics = localityBean.getStatistics();\n    assertEquals(\"Wrong number of total bytes counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL),\n        long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(\"Wrong number of total blocks counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL),\n        1, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL));\n    assertEquals(\n        \"Counted block as local when bad hostname set: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL),\n        0, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL));\n        \n    // set hostname and check again\n    factory.setHost(\"127.0.0.1\");\n    statistics = localityBean.getStatistics();\n    assertEquals(\n        \"Did not count block as local after setting hostname: \"\n            + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL),\n        long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL));\n        \n    factory.close();\n  }\n\n","bugFix":null,"bugIntro":["bfc52860e6d13d034226a760813c59d984c6817a","add53de9835b2cd1a7a80b4e0036afee171c9fdf"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"54ca69905c5d9d1529286f06ab1d12c68f6c13cb","date":1492683554,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testLocalityReporter().mjava","pathOld":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testLocalityReporter().mjava","sourceNew":"  @Test\n  public void testLocalityReporter() throws Exception {\n    Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n\n    Random r = random();\n    HdfsDirectoryFactory factory = new HdfsDirectoryFactory();\n    SolrMetricManager metricManager = new SolrMetricManager();\n    String registry = TestUtil.randomSimpleString(r, 2, 10);\n    String scope = TestUtil.randomSimpleString(r,2, 10);\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(HdfsDirectoryFactory.HDFS_HOME, HdfsTestUtil.getURI(dfsCluster) + \"/solr\");\n    props.put(HdfsDirectoryFactory.BLOCKCACHE_ENABLED, \"false\");\n    props.put(HdfsDirectoryFactory.NRTCACHINGDIRECTORY_ENABLE, \"false\");\n    props.put(HdfsDirectoryFactory.LOCALITYMETRICS_ENABLED, \"true\");\n    factory.init(new NamedList<>(props));\n    factory.initializeMetrics(metricManager, registry, scope);\n\n    // get the metrics map for the locality bean\n    MetricsMap metrics = (MetricsMap)metricManager.registry(registry).getMetrics().get(\"OTHER.\" + scope + \".hdfsLocality\");\n    // We haven't done anything, so there should be no data\n    Map<String,Object> statistics = metrics.getValue();\n    assertEquals(\"Saw bytes that were not written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL), 0l,\n        statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(\n        \"Counted bytes as local when none written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO), 0,\n        statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO));\n    \n    // create a directory and a file\n    String path = HdfsTestUtil.getURI(dfsCluster) + \"/solr3/\";\n    Directory dir = factory.create(path, NoLockFactory.INSTANCE, DirContext.DEFAULT);\n    try(IndexOutput writer = dir.createOutput(\"output\", null)) {\n      writer.writeLong(42l);\n    }\n    \n    final long long_bytes = Long.SIZE / Byte.SIZE;\n    \n    // no locality because hostname not set\n    factory.setHost(\"bogus\");\n    statistics = metrics.getValue();\n    assertEquals(\"Wrong number of total bytes counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL),\n        long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(\"Wrong number of total blocks counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL),\n        1, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL));\n    assertEquals(\n        \"Counted block as local when bad hostname set: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL),\n        0, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL));\n        \n    // set hostname and check again\n    factory.setHost(\"127.0.0.1\");\n    statistics = metrics.getValue();\n    assertEquals(\n        \"Did not count block as local after setting hostname: \"\n            + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL),\n        long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL));\n        \n    factory.close();\n  }\n\n","sourceOld":"  @Test\n  public void testLocalityReporter() throws Exception {\n    Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    \n    HdfsDirectoryFactory factory = new HdfsDirectoryFactory();\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(HdfsDirectoryFactory.HDFS_HOME, HdfsTestUtil.getURI(dfsCluster) + \"/solr\");\n    props.put(HdfsDirectoryFactory.BLOCKCACHE_ENABLED, \"false\");\n    props.put(HdfsDirectoryFactory.NRTCACHINGDIRECTORY_ENABLE, \"false\");\n    props.put(HdfsDirectoryFactory.LOCALITYMETRICS_ENABLED, \"true\");\n    factory.init(new NamedList<>(props));\n    \n    Iterator<SolrInfoMBean> it = factory.offerMBeans().iterator();\n    it.next(); // skip\n    SolrInfoMBean localityBean = it.next(); // brittle, but it's ok\n    \n    // Make sure we have the right bean.\n    assertEquals(\"Got the wrong bean: \" + localityBean.getName(), \"hdfs-locality\", localityBean.getName());\n    \n    // We haven't done anything, so there should be no data\n    NamedList<?> statistics = localityBean.getStatistics();\n    assertEquals(\"Saw bytes that were not written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL), 0l,\n        statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(\n        \"Counted bytes as local when none written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO), 0,\n        statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO));\n    \n    // create a directory and a file\n    String path = HdfsTestUtil.getURI(dfsCluster) + \"/solr3/\";\n    Directory dir = factory.create(path, NoLockFactory.INSTANCE, DirContext.DEFAULT);\n    try(IndexOutput writer = dir.createOutput(\"output\", null)) {\n      writer.writeLong(42l);\n    }\n    \n    final long long_bytes = Long.SIZE / Byte.SIZE;\n    \n    // no locality because hostname not set\n    factory.setHost(\"bogus\");\n    statistics = localityBean.getStatistics();\n    assertEquals(\"Wrong number of total bytes counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL),\n        long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(\"Wrong number of total blocks counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL),\n        1, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL));\n    assertEquals(\n        \"Counted block as local when bad hostname set: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL),\n        0, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL));\n        \n    // set hostname and check again\n    factory.setHost(\"127.0.0.1\");\n    statistics = localityBean.getStatistics();\n    assertEquals(\n        \"Did not count block as local after setting hostname: \"\n            + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL),\n        long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL));\n        \n    factory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bfc52860e6d13d034226a760813c59d984c6817a","date":1522229027,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testLocalityReporter().mjava","pathOld":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testLocalityReporter().mjava","sourceNew":"  @Test\n  public void testLocalityReporter() throws Exception {\n    Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n\n    Random r = random();\n    HdfsDirectoryFactory factory = new HdfsDirectoryFactory();\n    SolrMetricManager metricManager = new SolrMetricManager();\n    String registry = TestUtil.randomSimpleString(r, 2, 10);\n    String scope = TestUtil.randomSimpleString(r,2, 10);\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(HdfsDirectoryFactory.HDFS_HOME, HdfsTestUtil.getURI(dfsCluster) + \"/solr\");\n    props.put(HdfsDirectoryFactory.BLOCKCACHE_ENABLED, \"false\");\n    props.put(HdfsDirectoryFactory.NRTCACHINGDIRECTORY_ENABLE, \"false\");\n    props.put(HdfsDirectoryFactory.LOCALITYMETRICS_ENABLED, \"true\");\n    factory.init(new NamedList<>(props));\n    factory.initializeMetrics(metricManager, registry, \"foo\", scope);\n\n    // get the metrics map for the locality bean\n    MetricsMap metrics = (MetricsMap)((SolrMetricManager.GaugeWrapper)metricManager.registry(registry).getMetrics().get(\"OTHER.\" + scope + \".hdfsLocality\")).getGauge();\n    // We haven't done anything, so there should be no data\n    Map<String,Object> statistics = metrics.getValue();\n    assertEquals(\"Saw bytes that were not written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL), 0l,\n        statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(\n        \"Counted bytes as local when none written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO), 0,\n        statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO));\n    \n    // create a directory and a file\n    String path = HdfsTestUtil.getURI(dfsCluster) + \"/solr3/\";\n    Directory dir = factory.create(path, NoLockFactory.INSTANCE, DirContext.DEFAULT);\n    try(IndexOutput writer = dir.createOutput(\"output\", null)) {\n      writer.writeLong(42l);\n    }\n    \n    final long long_bytes = Long.SIZE / Byte.SIZE;\n    \n    // no locality because hostname not set\n    factory.setHost(\"bogus\");\n    statistics = metrics.getValue();\n    assertEquals(\"Wrong number of total bytes counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL),\n        long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(\"Wrong number of total blocks counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL),\n        1, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL));\n    assertEquals(\n        \"Counted block as local when bad hostname set: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL),\n        0, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL));\n        \n    // set hostname and check again\n    factory.setHost(\"127.0.0.1\");\n    statistics = metrics.getValue();\n    assertEquals(\n        \"Did not count block as local after setting hostname: \"\n            + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL),\n        long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL));\n        \n    factory.close();\n  }\n\n","sourceOld":"  @Test\n  public void testLocalityReporter() throws Exception {\n    Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n\n    Random r = random();\n    HdfsDirectoryFactory factory = new HdfsDirectoryFactory();\n    SolrMetricManager metricManager = new SolrMetricManager();\n    String registry = TestUtil.randomSimpleString(r, 2, 10);\n    String scope = TestUtil.randomSimpleString(r,2, 10);\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(HdfsDirectoryFactory.HDFS_HOME, HdfsTestUtil.getURI(dfsCluster) + \"/solr\");\n    props.put(HdfsDirectoryFactory.BLOCKCACHE_ENABLED, \"false\");\n    props.put(HdfsDirectoryFactory.NRTCACHINGDIRECTORY_ENABLE, \"false\");\n    props.put(HdfsDirectoryFactory.LOCALITYMETRICS_ENABLED, \"true\");\n    factory.init(new NamedList<>(props));\n    factory.initializeMetrics(metricManager, registry, scope);\n\n    // get the metrics map for the locality bean\n    MetricsMap metrics = (MetricsMap)metricManager.registry(registry).getMetrics().get(\"OTHER.\" + scope + \".hdfsLocality\");\n    // We haven't done anything, so there should be no data\n    Map<String,Object> statistics = metrics.getValue();\n    assertEquals(\"Saw bytes that were not written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL), 0l,\n        statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(\n        \"Counted bytes as local when none written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO), 0,\n        statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO));\n    \n    // create a directory and a file\n    String path = HdfsTestUtil.getURI(dfsCluster) + \"/solr3/\";\n    Directory dir = factory.create(path, NoLockFactory.INSTANCE, DirContext.DEFAULT);\n    try(IndexOutput writer = dir.createOutput(\"output\", null)) {\n      writer.writeLong(42l);\n    }\n    \n    final long long_bytes = Long.SIZE / Byte.SIZE;\n    \n    // no locality because hostname not set\n    factory.setHost(\"bogus\");\n    statistics = metrics.getValue();\n    assertEquals(\"Wrong number of total bytes counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL),\n        long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(\"Wrong number of total blocks counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL),\n        1, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL));\n    assertEquals(\n        \"Counted block as local when bad hostname set: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL),\n        0, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL));\n        \n    // set hostname and check again\n    factory.setHost(\"127.0.0.1\");\n    statistics = metrics.getValue();\n    assertEquals(\n        \"Did not count block as local after setting hostname: \"\n            + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL),\n        long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL));\n        \n    factory.close();\n  }\n\n","bugFix":["816521ebaad5add9cb96bb88c577394e2938c40b"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"43564cbb30b064675027cfb569564e8531096e97","date":1522334265,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testLocalityReporter().mjava","pathOld":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testLocalityReporter().mjava","sourceNew":"  @Test\n  public void testLocalityReporter() throws Exception {\n    Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n\n    Random r = random();\n    HdfsDirectoryFactory factory = new HdfsDirectoryFactory();\n    SolrMetricManager metricManager = new SolrMetricManager();\n    String registry = TestUtil.randomSimpleString(r, 2, 10);\n    String scope = TestUtil.randomSimpleString(r,2, 10);\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(HdfsDirectoryFactory.HDFS_HOME, HdfsTestUtil.getURI(dfsCluster) + \"/solr\");\n    props.put(HdfsDirectoryFactory.BLOCKCACHE_ENABLED, \"false\");\n    props.put(HdfsDirectoryFactory.NRTCACHINGDIRECTORY_ENABLE, \"false\");\n    props.put(HdfsDirectoryFactory.LOCALITYMETRICS_ENABLED, \"true\");\n    factory.init(new NamedList<>(props));\n    factory.initializeMetrics(metricManager, registry, \"foo\", scope);\n\n    // get the metrics map for the locality bean\n    MetricsMap metrics = (MetricsMap)((SolrMetricManager.GaugeWrapper)metricManager.registry(registry).getMetrics().get(\"OTHER.\" + scope + \".hdfsLocality\")).getGauge();\n    // We haven't done anything, so there should be no data\n    Map<String,Object> statistics = metrics.getValue();\n    assertEquals(\"Saw bytes that were not written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL), 0l,\n        statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(\n        \"Counted bytes as local when none written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO), 0,\n        statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO));\n    \n    // create a directory and a file\n    String path = HdfsTestUtil.getURI(dfsCluster) + \"/solr3/\";\n    Directory dir = factory.create(path, NoLockFactory.INSTANCE, DirContext.DEFAULT);\n    try(IndexOutput writer = dir.createOutput(\"output\", null)) {\n      writer.writeLong(42l);\n    }\n    \n    final long long_bytes = Long.SIZE / Byte.SIZE;\n    \n    // no locality because hostname not set\n    factory.setHost(\"bogus\");\n    statistics = metrics.getValue();\n    assertEquals(\"Wrong number of total bytes counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL),\n        long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(\"Wrong number of total blocks counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL),\n        1, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL));\n    assertEquals(\n        \"Counted block as local when bad hostname set: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL),\n        0, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL));\n        \n    // set hostname and check again\n    factory.setHost(\"127.0.0.1\");\n    statistics = metrics.getValue();\n    assertEquals(\n        \"Did not count block as local after setting hostname: \"\n            + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL),\n        long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL));\n        \n    factory.close();\n  }\n\n","sourceOld":"  @Test\n  public void testLocalityReporter() throws Exception {\n    Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n\n    Random r = random();\n    HdfsDirectoryFactory factory = new HdfsDirectoryFactory();\n    SolrMetricManager metricManager = new SolrMetricManager();\n    String registry = TestUtil.randomSimpleString(r, 2, 10);\n    String scope = TestUtil.randomSimpleString(r,2, 10);\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(HdfsDirectoryFactory.HDFS_HOME, HdfsTestUtil.getURI(dfsCluster) + \"/solr\");\n    props.put(HdfsDirectoryFactory.BLOCKCACHE_ENABLED, \"false\");\n    props.put(HdfsDirectoryFactory.NRTCACHINGDIRECTORY_ENABLE, \"false\");\n    props.put(HdfsDirectoryFactory.LOCALITYMETRICS_ENABLED, \"true\");\n    factory.init(new NamedList<>(props));\n    factory.initializeMetrics(metricManager, registry, scope);\n\n    // get the metrics map for the locality bean\n    MetricsMap metrics = (MetricsMap)metricManager.registry(registry).getMetrics().get(\"OTHER.\" + scope + \".hdfsLocality\");\n    // We haven't done anything, so there should be no data\n    Map<String,Object> statistics = metrics.getValue();\n    assertEquals(\"Saw bytes that were not written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL), 0l,\n        statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(\n        \"Counted bytes as local when none written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO), 0,\n        statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO));\n    \n    // create a directory and a file\n    String path = HdfsTestUtil.getURI(dfsCluster) + \"/solr3/\";\n    Directory dir = factory.create(path, NoLockFactory.INSTANCE, DirContext.DEFAULT);\n    try(IndexOutput writer = dir.createOutput(\"output\", null)) {\n      writer.writeLong(42l);\n    }\n    \n    final long long_bytes = Long.SIZE / Byte.SIZE;\n    \n    // no locality because hostname not set\n    factory.setHost(\"bogus\");\n    statistics = metrics.getValue();\n    assertEquals(\"Wrong number of total bytes counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL),\n        long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(\"Wrong number of total blocks counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL),\n        1, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL));\n    assertEquals(\n        \"Counted block as local when bad hostname set: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL),\n        0, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL));\n        \n    // set hostname and check again\n    factory.setHost(\"127.0.0.1\");\n    statistics = metrics.getValue();\n    assertEquals(\n        \"Did not count block as local after setting hostname: \"\n            + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL),\n        long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL));\n        \n    factory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f","date":1552317217,"type":3,"author":"Kevin Risden","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testLocalityReporter().mjava","pathOld":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testLocalityReporter().mjava","sourceNew":"  @Test\n  public void testLocalityReporter() throws Exception {\n    Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n\n    Random r = random();\n    try(HdfsDirectoryFactory factory = new HdfsDirectoryFactory()) {\n      SolrMetricManager metricManager = new SolrMetricManager();\n      String registry = TestUtil.randomSimpleString(r, 2, 10);\n      String scope = TestUtil.randomSimpleString(r, 2, 10);\n      Map<String, String> props = new HashMap<String, String>();\n      props.put(HdfsDirectoryFactory.HDFS_HOME, HdfsTestUtil.getURI(dfsCluster) + \"/solr\");\n      props.put(HdfsDirectoryFactory.BLOCKCACHE_ENABLED, \"false\");\n      props.put(HdfsDirectoryFactory.NRTCACHINGDIRECTORY_ENABLE, \"false\");\n      props.put(HdfsDirectoryFactory.LOCALITYMETRICS_ENABLED, \"true\");\n      factory.init(new NamedList<>(props));\n      factory.initializeMetrics(metricManager, registry, \"foo\", scope);\n\n      // get the metrics map for the locality bean\n      MetricsMap metrics = (MetricsMap) ((SolrMetricManager.GaugeWrapper) metricManager.registry(registry).getMetrics().get(\"OTHER.\" + scope + \".hdfsLocality\")).getGauge();\n      // We haven't done anything, so there should be no data\n      Map<String, Object> statistics = metrics.getValue();\n      assertEquals(\"Saw bytes that were not written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL), 0l,\n          statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n      assertEquals(\n          \"Counted bytes as local when none written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO), 0,\n          statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO));\n\n      // create a directory and a file\n      String path = HdfsTestUtil.getURI(dfsCluster) + \"/solr3/\";\n      try (Directory dir = factory.create(path, NoLockFactory.INSTANCE, DirContext.DEFAULT)) {\n        try (IndexOutput writer = dir.createOutput(\"output\", null)) {\n          writer.writeLong(42L);\n        }\n\n        final long long_bytes = Long.SIZE / Byte.SIZE;\n\n        // no locality because hostname not set\n        factory.setHost(\"bogus\");\n        statistics = metrics.getValue();\n        assertEquals(\"Wrong number of total bytes counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL),\n            long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n        assertEquals(\"Wrong number of total blocks counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL),\n            1, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL));\n        assertEquals(\n            \"Counted block as local when bad hostname set: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL),\n            0, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL));\n\n        // set hostname and check again\n        factory.setHost(\"127.0.0.1\");\n        statistics = metrics.getValue();\n        assertEquals(\n            \"Did not count block as local after setting hostname: \"\n                + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL),\n            long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL));\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void testLocalityReporter() throws Exception {\n    Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n\n    Random r = random();\n    HdfsDirectoryFactory factory = new HdfsDirectoryFactory();\n    SolrMetricManager metricManager = new SolrMetricManager();\n    String registry = TestUtil.randomSimpleString(r, 2, 10);\n    String scope = TestUtil.randomSimpleString(r,2, 10);\n    Map<String,String> props = new HashMap<String,String>();\n    props.put(HdfsDirectoryFactory.HDFS_HOME, HdfsTestUtil.getURI(dfsCluster) + \"/solr\");\n    props.put(HdfsDirectoryFactory.BLOCKCACHE_ENABLED, \"false\");\n    props.put(HdfsDirectoryFactory.NRTCACHINGDIRECTORY_ENABLE, \"false\");\n    props.put(HdfsDirectoryFactory.LOCALITYMETRICS_ENABLED, \"true\");\n    factory.init(new NamedList<>(props));\n    factory.initializeMetrics(metricManager, registry, \"foo\", scope);\n\n    // get the metrics map for the locality bean\n    MetricsMap metrics = (MetricsMap)((SolrMetricManager.GaugeWrapper)metricManager.registry(registry).getMetrics().get(\"OTHER.\" + scope + \".hdfsLocality\")).getGauge();\n    // We haven't done anything, so there should be no data\n    Map<String,Object> statistics = metrics.getValue();\n    assertEquals(\"Saw bytes that were not written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL), 0l,\n        statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(\n        \"Counted bytes as local when none written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO), 0,\n        statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO));\n    \n    // create a directory and a file\n    String path = HdfsTestUtil.getURI(dfsCluster) + \"/solr3/\";\n    Directory dir = factory.create(path, NoLockFactory.INSTANCE, DirContext.DEFAULT);\n    try(IndexOutput writer = dir.createOutput(\"output\", null)) {\n      writer.writeLong(42l);\n    }\n    \n    final long long_bytes = Long.SIZE / Byte.SIZE;\n    \n    // no locality because hostname not set\n    factory.setHost(\"bogus\");\n    statistics = metrics.getValue();\n    assertEquals(\"Wrong number of total bytes counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL),\n        long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n    assertEquals(\"Wrong number of total blocks counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL),\n        1, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL));\n    assertEquals(\n        \"Counted block as local when bad hostname set: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL),\n        0, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL));\n        \n    // set hostname and check again\n    factory.setHost(\"127.0.0.1\");\n    statistics = metrics.getValue();\n    assertEquals(\n        \"Did not count block as local after setting hostname: \"\n            + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL),\n        long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL));\n        \n    factory.close();\n  }\n\n","bugFix":null,"bugIntro":["add53de9835b2cd1a7a80b4e0036afee171c9fdf"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"add53de9835b2cd1a7a80b4e0036afee171c9fdf","date":1552937136,"type":3,"author":"Kevin Risden","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testLocalityReporter().mjava","pathOld":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testLocalityReporter().mjava","sourceNew":"  @Test\n  public void testLocalityReporter() throws Exception {\n    Random r = random();\n    try(HdfsDirectoryFactory factory = new HdfsDirectoryFactory()) {\n      SolrMetricManager metricManager = new SolrMetricManager();\n      String registry = TestUtil.randomSimpleString(r, 2, 10);\n      String scope = TestUtil.randomSimpleString(r, 2, 10);\n      Map<String, String> props = new HashMap<>();\n      props.put(HdfsDirectoryFactory.HDFS_HOME, HdfsTestUtil.getURI(dfsCluster) + \"/solr\");\n      props.put(HdfsDirectoryFactory.BLOCKCACHE_ENABLED, \"false\");\n      props.put(HdfsDirectoryFactory.NRTCACHINGDIRECTORY_ENABLE, \"false\");\n      props.put(HdfsDirectoryFactory.LOCALITYMETRICS_ENABLED, \"true\");\n      factory.init(new NamedList<>(props));\n      factory.initializeMetrics(metricManager, registry, \"foo\", scope);\n\n      // get the metrics map for the locality bean\n      MetricsMap metrics = (MetricsMap) ((SolrMetricManager.GaugeWrapper) metricManager.registry(registry).getMetrics().get(\"OTHER.\" + scope + \".hdfsLocality\")).getGauge();\n      // We haven't done anything, so there should be no data\n      Map<String, Object> statistics = metrics.getValue();\n      assertEquals(\"Saw bytes that were not written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL), 0L,\n          statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n      assertEquals(\n          \"Counted bytes as local when none written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO), 0,\n          statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO));\n\n      // create a directory and a file\n      String path = HdfsTestUtil.getURI(dfsCluster) + \"/solr3/\";\n      try (Directory dir = factory.create(path, NoLockFactory.INSTANCE, DirContext.DEFAULT)) {\n        try (IndexOutput writer = dir.createOutput(\"output\", null)) {\n          writer.writeLong(42L);\n        }\n\n        final long long_bytes = Long.SIZE / Byte.SIZE;\n\n        // no locality because hostname not set\n        factory.setHost(\"bogus\");\n        statistics = metrics.getValue();\n        assertEquals(\"Wrong number of total bytes counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL),\n            long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n        assertEquals(\"Wrong number of total blocks counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL),\n            1, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL));\n        assertEquals(\n            \"Counted block as local when bad hostname set: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL),\n            0, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL));\n\n        // set hostname and check again\n        factory.setHost(\"127.0.0.1\");\n        statistics = metrics.getValue();\n        assertEquals(\n            \"Did not count block as local after setting hostname: \"\n                + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL),\n            long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL));\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void testLocalityReporter() throws Exception {\n    Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n\n    Random r = random();\n    try(HdfsDirectoryFactory factory = new HdfsDirectoryFactory()) {\n      SolrMetricManager metricManager = new SolrMetricManager();\n      String registry = TestUtil.randomSimpleString(r, 2, 10);\n      String scope = TestUtil.randomSimpleString(r, 2, 10);\n      Map<String, String> props = new HashMap<String, String>();\n      props.put(HdfsDirectoryFactory.HDFS_HOME, HdfsTestUtil.getURI(dfsCluster) + \"/solr\");\n      props.put(HdfsDirectoryFactory.BLOCKCACHE_ENABLED, \"false\");\n      props.put(HdfsDirectoryFactory.NRTCACHINGDIRECTORY_ENABLE, \"false\");\n      props.put(HdfsDirectoryFactory.LOCALITYMETRICS_ENABLED, \"true\");\n      factory.init(new NamedList<>(props));\n      factory.initializeMetrics(metricManager, registry, \"foo\", scope);\n\n      // get the metrics map for the locality bean\n      MetricsMap metrics = (MetricsMap) ((SolrMetricManager.GaugeWrapper) metricManager.registry(registry).getMetrics().get(\"OTHER.\" + scope + \".hdfsLocality\")).getGauge();\n      // We haven't done anything, so there should be no data\n      Map<String, Object> statistics = metrics.getValue();\n      assertEquals(\"Saw bytes that were not written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL), 0l,\n          statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n      assertEquals(\n          \"Counted bytes as local when none written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO), 0,\n          statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO));\n\n      // create a directory and a file\n      String path = HdfsTestUtil.getURI(dfsCluster) + \"/solr3/\";\n      try (Directory dir = factory.create(path, NoLockFactory.INSTANCE, DirContext.DEFAULT)) {\n        try (IndexOutput writer = dir.createOutput(\"output\", null)) {\n          writer.writeLong(42L);\n        }\n\n        final long long_bytes = Long.SIZE / Byte.SIZE;\n\n        // no locality because hostname not set\n        factory.setHost(\"bogus\");\n        statistics = metrics.getValue();\n        assertEquals(\"Wrong number of total bytes counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL),\n            long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n        assertEquals(\"Wrong number of total blocks counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL),\n            1, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL));\n        assertEquals(\n            \"Counted block as local when bad hostname set: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL),\n            0, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL));\n\n        // set hostname and check again\n        factory.setHost(\"127.0.0.1\");\n        statistics = metrics.getValue();\n        assertEquals(\n            \"Did not count block as local after setting hostname: \"\n                + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL),\n            long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL));\n      }\n    }\n  }\n\n","bugFix":["1ce8283f367b946e5dd6300887294d7d115f2b9f","2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f","816521ebaad5add9cb96bb88c577394e2938c40b"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2e9e5eaf280a6aa21423126b8232aa157a9b7366","date":1571772228,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testLocalityReporter().mjava","pathOld":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testLocalityReporter().mjava","sourceNew":"  @Test\n  public void testLocalityReporter() throws Exception {\n    Random r = random();\n    try(HdfsDirectoryFactory factory = new HdfsDirectoryFactory()) {\n      SolrMetricManager metricManager = new SolrMetricManager();\n      String registry = TestUtil.randomSimpleString(r, 2, 10);\n      String scope = TestUtil.randomSimpleString(r, 2, 10);\n      Map<String, String> props = new HashMap<>();\n      props.put(HdfsDirectoryFactory.HDFS_HOME, HdfsTestUtil.getURI(dfsCluster) + \"/solr\");\n      props.put(HdfsDirectoryFactory.BLOCKCACHE_ENABLED, \"false\");\n      props.put(HdfsDirectoryFactory.NRTCACHINGDIRECTORY_ENABLE, \"false\");\n      props.put(HdfsDirectoryFactory.LOCALITYMETRICS_ENABLED, \"true\");\n      factory.init(new NamedList<>(props));\n      factory.initializeMetrics(new SolrMetricsContext(metricManager, registry, \"foo\"), scope);\n\n      // get the metrics map for the locality bean\n      MetricsMap metrics = (MetricsMap) ((SolrMetricManager.GaugeWrapper) metricManager.registry(registry).getMetrics().get(\"OTHER.\" + scope + \".hdfsLocality\")).getGauge();\n      // We haven't done anything, so there should be no data\n      Map<String, Object> statistics = metrics.getValue();\n      assertEquals(\"Saw bytes that were not written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL), 0L,\n          statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n      assertEquals(\n          \"Counted bytes as local when none written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO), 0,\n          statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO));\n\n      // create a directory and a file\n      String path = HdfsTestUtil.getURI(dfsCluster) + \"/solr3/\";\n      try (Directory dir = factory.create(path, NoLockFactory.INSTANCE, DirContext.DEFAULT)) {\n        try (IndexOutput writer = dir.createOutput(\"output\", null)) {\n          writer.writeLong(42L);\n        }\n\n        final long long_bytes = Long.SIZE / Byte.SIZE;\n\n        // no locality because hostname not set\n        factory.setHost(\"bogus\");\n        statistics = metrics.getValue();\n        assertEquals(\"Wrong number of total bytes counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL),\n            long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n        assertEquals(\"Wrong number of total blocks counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL),\n            1, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL));\n        assertEquals(\n            \"Counted block as local when bad hostname set: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL),\n            0, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL));\n\n        // set hostname and check again\n        factory.setHost(\"127.0.0.1\");\n        statistics = metrics.getValue();\n        assertEquals(\n            \"Did not count block as local after setting hostname: \"\n                + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL),\n            long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL));\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void testLocalityReporter() throws Exception {\n    Random r = random();\n    try(HdfsDirectoryFactory factory = new HdfsDirectoryFactory()) {\n      SolrMetricManager metricManager = new SolrMetricManager();\n      String registry = TestUtil.randomSimpleString(r, 2, 10);\n      String scope = TestUtil.randomSimpleString(r, 2, 10);\n      Map<String, String> props = new HashMap<>();\n      props.put(HdfsDirectoryFactory.HDFS_HOME, HdfsTestUtil.getURI(dfsCluster) + \"/solr\");\n      props.put(HdfsDirectoryFactory.BLOCKCACHE_ENABLED, \"false\");\n      props.put(HdfsDirectoryFactory.NRTCACHINGDIRECTORY_ENABLE, \"false\");\n      props.put(HdfsDirectoryFactory.LOCALITYMETRICS_ENABLED, \"true\");\n      factory.init(new NamedList<>(props));\n      factory.initializeMetrics(metricManager, registry, \"foo\", scope);\n\n      // get the metrics map for the locality bean\n      MetricsMap metrics = (MetricsMap) ((SolrMetricManager.GaugeWrapper) metricManager.registry(registry).getMetrics().get(\"OTHER.\" + scope + \".hdfsLocality\")).getGauge();\n      // We haven't done anything, so there should be no data\n      Map<String, Object> statistics = metrics.getValue();\n      assertEquals(\"Saw bytes that were not written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL), 0L,\n          statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n      assertEquals(\n          \"Counted bytes as local when none written: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO), 0,\n          statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_RATIO));\n\n      // create a directory and a file\n      String path = HdfsTestUtil.getURI(dfsCluster) + \"/solr3/\";\n      try (Directory dir = factory.create(path, NoLockFactory.INSTANCE, DirContext.DEFAULT)) {\n        try (IndexOutput writer = dir.createOutput(\"output\", null)) {\n          writer.writeLong(42L);\n        }\n\n        final long long_bytes = Long.SIZE / Byte.SIZE;\n\n        // no locality because hostname not set\n        factory.setHost(\"bogus\");\n        statistics = metrics.getValue();\n        assertEquals(\"Wrong number of total bytes counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL),\n            long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_TOTAL));\n        assertEquals(\"Wrong number of total blocks counted: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL),\n            1, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_TOTAL));\n        assertEquals(\n            \"Counted block as local when bad hostname set: \" + statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL),\n            0, statistics.get(HdfsLocalityReporter.LOCALITY_BLOCKS_LOCAL));\n\n        // set hostname and check again\n        factory.setHost(\"127.0.0.1\");\n        statistics = metrics.getValue();\n        assertEquals(\n            \"Did not count block as local after setting hostname: \"\n                + statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL),\n            long_bytes, statistics.get(HdfsLocalityReporter.LOCALITY_BYTES_LOCAL));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"215d4d9447bf3e08c95b7f13cb3681b82aa15602":["1ce8283f367b946e5dd6300887294d7d115f2b9f"],"54ca69905c5d9d1529286f06ab1d12c68f6c13cb":["87df8312996aa71322478e54c0c1c43233e0b491"],"2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f":["43564cbb30b064675027cfb569564e8531096e97"],"bfc52860e6d13d034226a760813c59d984c6817a":["816521ebaad5add9cb96bb88c577394e2938c40b"],"43564cbb30b064675027cfb569564e8531096e97":["816521ebaad5add9cb96bb88c577394e2938c40b","bfc52860e6d13d034226a760813c59d984c6817a"],"2e9e5eaf280a6aa21423126b8232aa157a9b7366":["add53de9835b2cd1a7a80b4e0036afee171c9fdf"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"87df8312996aa71322478e54c0c1c43233e0b491":["215d4d9447bf3e08c95b7f13cb3681b82aa15602"],"1ce8283f367b946e5dd6300887294d7d115f2b9f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"816521ebaad5add9cb96bb88c577394e2938c40b":["87df8312996aa71322478e54c0c1c43233e0b491"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["2e9e5eaf280a6aa21423126b8232aa157a9b7366"],"add53de9835b2cd1a7a80b4e0036afee171c9fdf":["2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f"]},"commit2Childs":{"215d4d9447bf3e08c95b7f13cb3681b82aa15602":["87df8312996aa71322478e54c0c1c43233e0b491"],"54ca69905c5d9d1529286f06ab1d12c68f6c13cb":[],"2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f":["add53de9835b2cd1a7a80b4e0036afee171c9fdf"],"bfc52860e6d13d034226a760813c59d984c6817a":["43564cbb30b064675027cfb569564e8531096e97"],"43564cbb30b064675027cfb569564e8531096e97":["2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f"],"2e9e5eaf280a6aa21423126b8232aa157a9b7366":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1ce8283f367b946e5dd6300887294d7d115f2b9f"],"87df8312996aa71322478e54c0c1c43233e0b491":["54ca69905c5d9d1529286f06ab1d12c68f6c13cb","816521ebaad5add9cb96bb88c577394e2938c40b"],"1ce8283f367b946e5dd6300887294d7d115f2b9f":["215d4d9447bf3e08c95b7f13cb3681b82aa15602"],"816521ebaad5add9cb96bb88c577394e2938c40b":["bfc52860e6d13d034226a760813c59d984c6817a","43564cbb30b064675027cfb569564e8531096e97"],"add53de9835b2cd1a7a80b4e0036afee171c9fdf":["2e9e5eaf280a6aa21423126b8232aa157a9b7366"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["54ca69905c5d9d1529286f06ab1d12c68f6c13cb","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}