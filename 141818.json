{"path":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","commits":[{"id":"06805da26538ed636bd89b10c2699cc3834032ae","date":1395132972,"type":1,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,Map[String,NumericFieldUpdates]).mjava","sourceNew":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          builder.addOrUpdate(f, NumericDocValuesField.TYPE);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          builder.addOrUpdate(f, BinaryDocValuesField.TYPE);\n        }\n        \n        fieldInfos = builder.finish();\n        final long nextFieldInfosGen = info.getNextFieldInfosGen();\n        final String segmentSuffix = Long.toString(nextFieldInfosGen, Character.MAX_RADIX);\n        final SegmentWriteState state = new SegmentWriteState(null, trackingDir, info.info, fieldInfos, null, IOContext.DEFAULT, segmentSuffix);\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        final DocValuesConsumer fieldsConsumer = docValuesFormat.fieldsConsumer(state);\n        boolean fieldsConsumerSuccess = false;\n        try {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n          for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n            final String field = e.getKey();\n            final NumericDocValuesFieldUpdates fieldUpdates = e.getValue();\n            final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n            assert fieldInfo != null;\n\n            fieldInfo.setDocValuesGen(nextFieldInfosGen);\n            // write the numeric updates to a new gen'd docvalues file\n            fieldsConsumer.addNumericField(fieldInfo, new Iterable<Number>() {\n              final NumericDocValues currentValues = reader.getNumericDocValues(field);\n              final Bits docsWithField = reader.getDocsWithField(field);\n              final int maxDoc = reader.maxDoc();\n              final NumericDocValuesFieldUpdates.Iterator updatesIter = fieldUpdates.iterator();\n              @Override\n              public Iterator<Number> iterator() {\n                updatesIter.reset();\n                return new Iterator<Number>() {\n\n                  int curDoc = -1;\n                  int updateDoc = updatesIter.nextDoc();\n                  \n                  @Override\n                  public boolean hasNext() {\n                    return curDoc < maxDoc - 1;\n                  }\n\n                  @Override\n                  public Number next() {\n                    if (++curDoc >= maxDoc) {\n                      throw new NoSuchElementException(\"no more documents to return values for\");\n                    }\n                    if (curDoc == updateDoc) { // this document has an updated value\n                      Long value = updatesIter.value(); // either null (unset value) or updated value\n                      updateDoc = updatesIter.nextDoc(); // prepare for next round\n                      return value;\n                    } else {\n                      // no update for this document\n                      assert curDoc < updateDoc;\n                      if (currentValues != null && docsWithField.get(curDoc)) {\n                        // only read the current value if the document had a value before\n                        return currentValues.get(curDoc);\n                      } else {\n                        return null;\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void remove() {\n                    throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                  }\n                };\n              }\n            });\n          }\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n          final String field = e.getKey();\n          final BinaryDocValuesFieldUpdates dvFieldUpdates = e.getValue();\n          final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n          assert fieldInfo != null;\n\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" f=\" + dvFieldUpdates + \", updates=\" + dvFieldUpdates);\n\n          fieldInfo.setDocValuesGen(nextFieldInfosGen);\n          // write the numeric updates to a new gen'd docvalues file\n          fieldsConsumer.addBinaryField(fieldInfo, new Iterable<BytesRef>() {\n            final BinaryDocValues currentValues = reader.getBinaryDocValues(field);\n            final Bits docsWithField = reader.getDocsWithField(field);\n            final int maxDoc = reader.maxDoc();\n            final BinaryDocValuesFieldUpdates.Iterator updatesIter = dvFieldUpdates.iterator();\n            @Override\n            public Iterator<BytesRef> iterator() {\n              updatesIter.reset();\n              return new Iterator<BytesRef>() {\n\n                int curDoc = -1;\n                int updateDoc = updatesIter.nextDoc();\n                BytesRef scratch = new BytesRef();\n                \n                @Override\n                public boolean hasNext() {\n                  return curDoc < maxDoc - 1;\n                }\n\n                @Override\n                public BytesRef next() {\n                  if (++curDoc >= maxDoc) {\n                    throw new NoSuchElementException(\"no more documents to return values for\");\n                  }\n                  if (curDoc == updateDoc) { // this document has an updated value\n                    BytesRef value = updatesIter.value(); // either null (unset value) or updated value\n                    updateDoc = updatesIter.nextDoc(); // prepare for next round\n                    return value;\n                  } else {\n                    // no update for this document\n                    assert curDoc < updateDoc;\n                    if (currentValues != null && docsWithField.get(curDoc)) {\n                      // only read the current value if the document had a value before\n                      currentValues.get(curDoc, scratch);\n                      return scratch;\n                    } else {\n                      return null;\n                    }\n                  }\n                }\n\n                @Override\n                public void remove() {\n                  throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                }\n              };\n            }\n          });\n        }\n\n          codec.fieldInfosFormat().getFieldInfosWriter().write(trackingDir, info.info.name, segmentSuffix, fieldInfos, IOContext.DEFAULT);\n          fieldsConsumerSuccess = true;\n        } finally {\n          if (fieldsConsumerSuccess) {\n            fieldsConsumer.close();\n          } else {\n            IOUtils.closeWhileHandlingException(fieldsConsumer);\n          }\n        }\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteDocValuesGen so that a 2nd\n        // attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          try {\n            dir.deleteFile(fileName);\n          } catch (Throwable t) {\n            // Ignore so we throw only the first exc\n          }\n        }\n      }\n    }\n    \n    info.advanceFieldInfosGen();\n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // create a new map, keeping only the gens that are in use\n    Map<Long,Set<String>> genUpdatesFiles = info.getUpdatesFiles();\n    Map<Long,Set<String>> newGenUpdatesFiles = new HashMap<>();\n    final long fieldInfosGen = info.getFieldInfosGen();\n    for (FieldInfo fi : fieldInfos) {\n      long dvGen = fi.getDocValuesGen();\n      if (dvGen != -1 && !newGenUpdatesFiles.containsKey(dvGen)) {\n        if (dvGen == fieldInfosGen) {\n          newGenUpdatesFiles.put(fieldInfosGen, trackingDir.getCreatedFiles());\n        } else {\n          newGenUpdatesFiles.put(dvGen, genUpdatesFiles.get(dvGen));\n        }\n      }\n    }\n    \n    info.setGenUpdatesFiles(newGenUpdatesFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, Map<String,NumericFieldUpdates> numericFieldUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert numericFieldUpdates != null && !numericFieldUpdates.isEmpty();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : numericFieldUpdates.keySet()) {\n          builder.addOrUpdate(f, NumericDocValuesField.TYPE);\n        }\n        \n        fieldInfos = builder.finish();\n        final long nextFieldInfosGen = info.getNextFieldInfosGen();\n        final String segmentSuffix = Long.toString(nextFieldInfosGen, Character.MAX_RADIX);\n        final SegmentWriteState state = new SegmentWriteState(null, trackingDir, info.info, fieldInfos, null, IOContext.DEFAULT, segmentSuffix);\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        final DocValuesConsumer fieldsConsumer = docValuesFormat.fieldsConsumer(state);\n        boolean fieldsConsumerSuccess = false;\n        try {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: applying updates; seg=\" + info + \" updates=\" + numericUpdates);\n          for (Entry<String,NumericFieldUpdates> e : numericFieldUpdates.entrySet()) {\n            final String field = e.getKey();\n            final NumericFieldUpdates fieldUpdates = e.getValue();\n            final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n            assert fieldInfo != null;\n\n            fieldInfo.setDocValuesGen(nextFieldInfosGen);\n            // write the numeric updates to a new gen'd docvalues file\n            fieldsConsumer.addNumericField(fieldInfo, new Iterable<Number>() {\n              final NumericDocValues currentValues = reader.getNumericDocValues(field);\n              final Bits docsWithField = reader.getDocsWithField(field);\n              final int maxDoc = reader.maxDoc();\n              final UpdatesIterator updatesIter = fieldUpdates.getUpdates();\n              @Override\n              public Iterator<Number> iterator() {\n                updatesIter.reset();\n                return new Iterator<Number>() {\n\n                  int curDoc = -1;\n                  int updateDoc = updatesIter.nextDoc();\n                  \n                  @Override\n                  public boolean hasNext() {\n                    return curDoc < maxDoc - 1;\n                  }\n\n                  @Override\n                  public Number next() {\n                    if (++curDoc >= maxDoc) {\n                      throw new NoSuchElementException(\"no more documents to return values for\");\n                    }\n                    if (curDoc == updateDoc) { // this document has an updated value\n                      Long value = updatesIter.value(); // either null (unset value) or updated value\n                      updateDoc = updatesIter.nextDoc(); // prepare for next round\n                      return value;\n                    } else {\n                      // no update for this document\n                      assert curDoc < updateDoc;\n                      if (currentValues != null && docsWithField.get(curDoc)) {\n                        // only read the current value if the document had a value before\n                        return currentValues.get(curDoc);\n                      } else {\n                        return null;\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void remove() {\n                    throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                  }\n                };\n              }\n            });\n          }\n          \n          codec.fieldInfosFormat().getFieldInfosWriter().write(trackingDir, info.info.name, segmentSuffix, fieldInfos, IOContext.DEFAULT);\n          fieldsConsumerSuccess = true;\n        } finally {\n          if (fieldsConsumerSuccess) {\n            fieldsConsumer.close();\n          } else {\n            IOUtils.closeWhileHandlingException(fieldsConsumer);\n          }\n        }\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteDocValuesGen so that a 2nd\n        // attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          try {\n            dir.deleteFile(fileName);\n          } catch (Throwable t) {\n            // Ignore so we throw only the first exc\n          }\n        }\n      }\n    }\n    \n    info.advanceFieldInfosGen();\n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericFieldUpdates> e : numericFieldUpdates.entrySet()) {\n        NumericFieldUpdates fieldUpdates = mergingNumericUpdates.get(e.getKey());\n        if (fieldUpdates == null) {\n          mergingNumericUpdates.put(e.getKey(), e.getValue());\n        } else {\n          fieldUpdates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // create a new map, keeping only the gens that are in use\n    Map<Long,Set<String>> genUpdatesFiles = info.getUpdatesFiles();\n    Map<Long,Set<String>> newGenUpdatesFiles = new HashMap<>();\n    final long fieldInfosGen = info.getFieldInfosGen();\n    for (FieldInfo fi : fieldInfos) {\n      long dvGen = fi.getDocValuesGen();\n      if (dvGen != -1 && !newGenUpdatesFiles.containsKey(dvGen)) {\n        if (dvGen == fieldInfosGen) {\n          newGenUpdatesFiles.put(fieldInfosGen, trackingDir.getCreatedFiles());\n        } else {\n          newGenUpdatesFiles.put(dvGen, genUpdatesFiles.get(dvGen));\n        }\n      }\n    }\n    \n    info.setGenUpdatesFiles(newGenUpdatesFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c1d0b4a65bc57bd59cef05619306aa8ee7431a0f","date":1398696007,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","sourceNew":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          builder.addOrUpdate(f, NumericDocValuesField.TYPE);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          builder.addOrUpdate(f, BinaryDocValuesField.TYPE);\n        }\n        \n        fieldInfos = builder.finish();\n        final long nextFieldInfosGen = info.getNextFieldInfosGen();\n        final String segmentSuffix = Long.toString(nextFieldInfosGen, Character.MAX_RADIX);\n        final SegmentWriteState state = new SegmentWriteState(null, trackingDir, info.info, fieldInfos, null, IOContext.DEFAULT, segmentSuffix);\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        final DocValuesConsumer fieldsConsumer = docValuesFormat.fieldsConsumer(state);\n        boolean fieldsConsumerSuccess = false;\n        try {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n          for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n            final String field = e.getKey();\n            final NumericDocValuesFieldUpdates fieldUpdates = e.getValue();\n            final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n            assert fieldInfo != null;\n\n            fieldInfo.setDocValuesGen(nextFieldInfosGen);\n            // write the numeric updates to a new gen'd docvalues file\n            fieldsConsumer.addNumericField(fieldInfo, new Iterable<Number>() {\n              final NumericDocValues currentValues = reader.getNumericDocValues(field);\n              final Bits docsWithField = reader.getDocsWithField(field);\n              final int maxDoc = reader.maxDoc();\n              final NumericDocValuesFieldUpdates.Iterator updatesIter = fieldUpdates.iterator();\n              @Override\n              public Iterator<Number> iterator() {\n                updatesIter.reset();\n                return new Iterator<Number>() {\n\n                  int curDoc = -1;\n                  int updateDoc = updatesIter.nextDoc();\n                  \n                  @Override\n                  public boolean hasNext() {\n                    return curDoc < maxDoc - 1;\n                  }\n\n                  @Override\n                  public Number next() {\n                    if (++curDoc >= maxDoc) {\n                      throw new NoSuchElementException(\"no more documents to return values for\");\n                    }\n                    if (curDoc == updateDoc) { // this document has an updated value\n                      Long value = updatesIter.value(); // either null (unset value) or updated value\n                      updateDoc = updatesIter.nextDoc(); // prepare for next round\n                      return value;\n                    } else {\n                      // no update for this document\n                      assert curDoc < updateDoc;\n                      if (currentValues != null && docsWithField.get(curDoc)) {\n                        // only read the current value if the document had a value before\n                        return currentValues.get(curDoc);\n                      } else {\n                        return null;\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void remove() {\n                    throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                  }\n                };\n              }\n            });\n          }\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n          final String field = e.getKey();\n          final BinaryDocValuesFieldUpdates dvFieldUpdates = e.getValue();\n          final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n          assert fieldInfo != null;\n\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" f=\" + dvFieldUpdates + \", updates=\" + dvFieldUpdates);\n\n          fieldInfo.setDocValuesGen(nextFieldInfosGen);\n          // write the numeric updates to a new gen'd docvalues file\n          fieldsConsumer.addBinaryField(fieldInfo, new Iterable<BytesRef>() {\n            final BinaryDocValues currentValues = reader.getBinaryDocValues(field);\n            final Bits docsWithField = reader.getDocsWithField(field);\n            final int maxDoc = reader.maxDoc();\n            final BinaryDocValuesFieldUpdates.Iterator updatesIter = dvFieldUpdates.iterator();\n            @Override\n            public Iterator<BytesRef> iterator() {\n              updatesIter.reset();\n              return new Iterator<BytesRef>() {\n\n                int curDoc = -1;\n                int updateDoc = updatesIter.nextDoc();\n                BytesRef scratch = new BytesRef();\n                \n                @Override\n                public boolean hasNext() {\n                  return curDoc < maxDoc - 1;\n                }\n\n                @Override\n                public BytesRef next() {\n                  if (++curDoc >= maxDoc) {\n                    throw new NoSuchElementException(\"no more documents to return values for\");\n                  }\n                  if (curDoc == updateDoc) { // this document has an updated value\n                    BytesRef value = updatesIter.value(); // either null (unset value) or updated value\n                    updateDoc = updatesIter.nextDoc(); // prepare for next round\n                    return value;\n                  } else {\n                    // no update for this document\n                    assert curDoc < updateDoc;\n                    if (currentValues != null && docsWithField.get(curDoc)) {\n                      // only read the current value if the document had a value before\n                      currentValues.get(curDoc, scratch);\n                      return scratch;\n                    } else {\n                      return null;\n                    }\n                  }\n                }\n\n                @Override\n                public void remove() {\n                  throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                }\n              };\n            }\n          });\n        }\n\n          codec.fieldInfosFormat().getFieldInfosWriter().write(trackingDir, info.info.name, segmentSuffix, fieldInfos, IOContext.DEFAULT);\n          fieldsConsumerSuccess = true;\n        } finally {\n          if (fieldsConsumerSuccess) {\n            fieldsConsumer.close();\n          } else {\n            IOUtils.closeWhileHandlingException(fieldsConsumer);\n          }\n        }\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen so that a 2nd\n        // attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          try {\n            dir.deleteFile(fileName);\n          } catch (Throwable t) {\n            // Ignore so we throw only the first exc\n          }\n        }\n      }\n    }\n    \n    info.advanceFieldInfosGen();\n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // create a new map, keeping only the gens that are in use\n    Map<Long,Set<String>> genUpdatesFiles = info.getUpdatesFiles();\n    Map<Long,Set<String>> newGenUpdatesFiles = new HashMap<>();\n    final long fieldInfosGen = info.getFieldInfosGen();\n    for (FieldInfo fi : fieldInfos) {\n      long dvGen = fi.getDocValuesGen();\n      if (dvGen != -1 && !newGenUpdatesFiles.containsKey(dvGen)) {\n        if (dvGen == fieldInfosGen) {\n          newGenUpdatesFiles.put(fieldInfosGen, trackingDir.getCreatedFiles());\n        } else {\n          newGenUpdatesFiles.put(dvGen, genUpdatesFiles.get(dvGen));\n        }\n      }\n    }\n    \n    info.setGenUpdatesFiles(newGenUpdatesFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          builder.addOrUpdate(f, NumericDocValuesField.TYPE);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          builder.addOrUpdate(f, BinaryDocValuesField.TYPE);\n        }\n        \n        fieldInfos = builder.finish();\n        final long nextFieldInfosGen = info.getNextFieldInfosGen();\n        final String segmentSuffix = Long.toString(nextFieldInfosGen, Character.MAX_RADIX);\n        final SegmentWriteState state = new SegmentWriteState(null, trackingDir, info.info, fieldInfos, null, IOContext.DEFAULT, segmentSuffix);\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        final DocValuesConsumer fieldsConsumer = docValuesFormat.fieldsConsumer(state);\n        boolean fieldsConsumerSuccess = false;\n        try {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n          for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n            final String field = e.getKey();\n            final NumericDocValuesFieldUpdates fieldUpdates = e.getValue();\n            final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n            assert fieldInfo != null;\n\n            fieldInfo.setDocValuesGen(nextFieldInfosGen);\n            // write the numeric updates to a new gen'd docvalues file\n            fieldsConsumer.addNumericField(fieldInfo, new Iterable<Number>() {\n              final NumericDocValues currentValues = reader.getNumericDocValues(field);\n              final Bits docsWithField = reader.getDocsWithField(field);\n              final int maxDoc = reader.maxDoc();\n              final NumericDocValuesFieldUpdates.Iterator updatesIter = fieldUpdates.iterator();\n              @Override\n              public Iterator<Number> iterator() {\n                updatesIter.reset();\n                return new Iterator<Number>() {\n\n                  int curDoc = -1;\n                  int updateDoc = updatesIter.nextDoc();\n                  \n                  @Override\n                  public boolean hasNext() {\n                    return curDoc < maxDoc - 1;\n                  }\n\n                  @Override\n                  public Number next() {\n                    if (++curDoc >= maxDoc) {\n                      throw new NoSuchElementException(\"no more documents to return values for\");\n                    }\n                    if (curDoc == updateDoc) { // this document has an updated value\n                      Long value = updatesIter.value(); // either null (unset value) or updated value\n                      updateDoc = updatesIter.nextDoc(); // prepare for next round\n                      return value;\n                    } else {\n                      // no update for this document\n                      assert curDoc < updateDoc;\n                      if (currentValues != null && docsWithField.get(curDoc)) {\n                        // only read the current value if the document had a value before\n                        return currentValues.get(curDoc);\n                      } else {\n                        return null;\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void remove() {\n                    throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                  }\n                };\n              }\n            });\n          }\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n          final String field = e.getKey();\n          final BinaryDocValuesFieldUpdates dvFieldUpdates = e.getValue();\n          final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n          assert fieldInfo != null;\n\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" f=\" + dvFieldUpdates + \", updates=\" + dvFieldUpdates);\n\n          fieldInfo.setDocValuesGen(nextFieldInfosGen);\n          // write the numeric updates to a new gen'd docvalues file\n          fieldsConsumer.addBinaryField(fieldInfo, new Iterable<BytesRef>() {\n            final BinaryDocValues currentValues = reader.getBinaryDocValues(field);\n            final Bits docsWithField = reader.getDocsWithField(field);\n            final int maxDoc = reader.maxDoc();\n            final BinaryDocValuesFieldUpdates.Iterator updatesIter = dvFieldUpdates.iterator();\n            @Override\n            public Iterator<BytesRef> iterator() {\n              updatesIter.reset();\n              return new Iterator<BytesRef>() {\n\n                int curDoc = -1;\n                int updateDoc = updatesIter.nextDoc();\n                BytesRef scratch = new BytesRef();\n                \n                @Override\n                public boolean hasNext() {\n                  return curDoc < maxDoc - 1;\n                }\n\n                @Override\n                public BytesRef next() {\n                  if (++curDoc >= maxDoc) {\n                    throw new NoSuchElementException(\"no more documents to return values for\");\n                  }\n                  if (curDoc == updateDoc) { // this document has an updated value\n                    BytesRef value = updatesIter.value(); // either null (unset value) or updated value\n                    updateDoc = updatesIter.nextDoc(); // prepare for next round\n                    return value;\n                  } else {\n                    // no update for this document\n                    assert curDoc < updateDoc;\n                    if (currentValues != null && docsWithField.get(curDoc)) {\n                      // only read the current value if the document had a value before\n                      currentValues.get(curDoc, scratch);\n                      return scratch;\n                    } else {\n                      return null;\n                    }\n                  }\n                }\n\n                @Override\n                public void remove() {\n                  throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                }\n              };\n            }\n          });\n        }\n\n          codec.fieldInfosFormat().getFieldInfosWriter().write(trackingDir, info.info.name, segmentSuffix, fieldInfos, IOContext.DEFAULT);\n          fieldsConsumerSuccess = true;\n        } finally {\n          if (fieldsConsumerSuccess) {\n            fieldsConsumer.close();\n          } else {\n            IOUtils.closeWhileHandlingException(fieldsConsumer);\n          }\n        }\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteDocValuesGen so that a 2nd\n        // attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          try {\n            dir.deleteFile(fileName);\n          } catch (Throwable t) {\n            // Ignore so we throw only the first exc\n          }\n        }\n      }\n    }\n    \n    info.advanceFieldInfosGen();\n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // create a new map, keeping only the gens that are in use\n    Map<Long,Set<String>> genUpdatesFiles = info.getUpdatesFiles();\n    Map<Long,Set<String>> newGenUpdatesFiles = new HashMap<>();\n    final long fieldInfosGen = info.getFieldInfosGen();\n    for (FieldInfo fi : fieldInfos) {\n      long dvGen = fi.getDocValuesGen();\n      if (dvGen != -1 && !newGenUpdatesFiles.containsKey(dvGen)) {\n        if (dvGen == fieldInfosGen) {\n          newGenUpdatesFiles.put(fieldInfosGen, trackingDir.getCreatedFiles());\n        } else {\n          newGenUpdatesFiles.put(dvGen, genUpdatesFiles.get(dvGen));\n        }\n      }\n    }\n    \n    info.setGenUpdatesFiles(newGenUpdatesFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["0567bdc5c86c94ced64201187cfcef2417d76dda"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","date":1398844771,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","sourceNew":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          builder.addOrUpdate(f, NumericDocValuesField.TYPE);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          builder.addOrUpdate(f, BinaryDocValuesField.TYPE);\n        }\n        \n        fieldInfos = builder.finish();\n        final long nextFieldInfosGen = info.getNextFieldInfosGen();\n        final String segmentSuffix = Long.toString(nextFieldInfosGen, Character.MAX_RADIX);\n        final SegmentWriteState state = new SegmentWriteState(null, trackingDir, info.info, fieldInfos, null, IOContext.DEFAULT, segmentSuffix);\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        final DocValuesConsumer fieldsConsumer = docValuesFormat.fieldsConsumer(state);\n        boolean fieldsConsumerSuccess = false;\n        try {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n          for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n            final String field = e.getKey();\n            final NumericDocValuesFieldUpdates fieldUpdates = e.getValue();\n            final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n            assert fieldInfo != null;\n\n            fieldInfo.setDocValuesGen(nextFieldInfosGen);\n            // write the numeric updates to a new gen'd docvalues file\n            fieldsConsumer.addNumericField(fieldInfo, new Iterable<Number>() {\n              final NumericDocValues currentValues = reader.getNumericDocValues(field);\n              final Bits docsWithField = reader.getDocsWithField(field);\n              final int maxDoc = reader.maxDoc();\n              final NumericDocValuesFieldUpdates.Iterator updatesIter = fieldUpdates.iterator();\n              @Override\n              public Iterator<Number> iterator() {\n                updatesIter.reset();\n                return new Iterator<Number>() {\n\n                  int curDoc = -1;\n                  int updateDoc = updatesIter.nextDoc();\n                  \n                  @Override\n                  public boolean hasNext() {\n                    return curDoc < maxDoc - 1;\n                  }\n\n                  @Override\n                  public Number next() {\n                    if (++curDoc >= maxDoc) {\n                      throw new NoSuchElementException(\"no more documents to return values for\");\n                    }\n                    if (curDoc == updateDoc) { // this document has an updated value\n                      Long value = updatesIter.value(); // either null (unset value) or updated value\n                      updateDoc = updatesIter.nextDoc(); // prepare for next round\n                      return value;\n                    } else {\n                      // no update for this document\n                      assert curDoc < updateDoc;\n                      if (currentValues != null && docsWithField.get(curDoc)) {\n                        // only read the current value if the document had a value before\n                        return currentValues.get(curDoc);\n                      } else {\n                        return null;\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void remove() {\n                    throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                  }\n                };\n              }\n            });\n          }\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n          final String field = e.getKey();\n          final BinaryDocValuesFieldUpdates dvFieldUpdates = e.getValue();\n          final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n          assert fieldInfo != null;\n\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" f=\" + dvFieldUpdates + \", updates=\" + dvFieldUpdates);\n\n          fieldInfo.setDocValuesGen(nextFieldInfosGen);\n          // write the numeric updates to a new gen'd docvalues file\n          fieldsConsumer.addBinaryField(fieldInfo, new Iterable<BytesRef>() {\n            final BinaryDocValues currentValues = reader.getBinaryDocValues(field);\n            final Bits docsWithField = reader.getDocsWithField(field);\n            final int maxDoc = reader.maxDoc();\n            final BinaryDocValuesFieldUpdates.Iterator updatesIter = dvFieldUpdates.iterator();\n            @Override\n            public Iterator<BytesRef> iterator() {\n              updatesIter.reset();\n              return new Iterator<BytesRef>() {\n\n                int curDoc = -1;\n                int updateDoc = updatesIter.nextDoc();\n                BytesRef scratch = new BytesRef();\n                \n                @Override\n                public boolean hasNext() {\n                  return curDoc < maxDoc - 1;\n                }\n\n                @Override\n                public BytesRef next() {\n                  if (++curDoc >= maxDoc) {\n                    throw new NoSuchElementException(\"no more documents to return values for\");\n                  }\n                  if (curDoc == updateDoc) { // this document has an updated value\n                    BytesRef value = updatesIter.value(); // either null (unset value) or updated value\n                    updateDoc = updatesIter.nextDoc(); // prepare for next round\n                    return value;\n                  } else {\n                    // no update for this document\n                    assert curDoc < updateDoc;\n                    if (currentValues != null && docsWithField.get(curDoc)) {\n                      // only read the current value if the document had a value before\n                      currentValues.get(curDoc, scratch);\n                      return scratch;\n                    } else {\n                      return null;\n                    }\n                  }\n                }\n\n                @Override\n                public void remove() {\n                  throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                }\n              };\n            }\n          });\n        }\n\n          codec.fieldInfosFormat().getFieldInfosWriter().write(trackingDir, info.info.name, segmentSuffix, fieldInfos, IOContext.DEFAULT);\n          fieldsConsumerSuccess = true;\n        } finally {\n          if (fieldsConsumerSuccess) {\n            fieldsConsumer.close();\n          } else {\n            IOUtils.closeWhileHandlingException(fieldsConsumer);\n          }\n        }\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen so that a 2nd\n        // attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          try {\n            dir.deleteFile(fileName);\n          } catch (Throwable t) {\n            // Ignore so we throw only the first exc\n          }\n        }\n      }\n    }\n    \n    info.advanceFieldInfosGen();\n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // create a new map, keeping only the gens that are in use\n    Map<Long,Set<String>> genUpdatesFiles = info.getUpdatesFiles();\n    Map<Long,Set<String>> newGenUpdatesFiles = new HashMap<>();\n    final long fieldInfosGen = info.getFieldInfosGen();\n    for (FieldInfo fi : fieldInfos) {\n      long dvGen = fi.getDocValuesGen();\n      if (dvGen != -1 && !newGenUpdatesFiles.containsKey(dvGen)) {\n        if (dvGen == fieldInfosGen) {\n          newGenUpdatesFiles.put(fieldInfosGen, trackingDir.getCreatedFiles());\n        } else {\n          newGenUpdatesFiles.put(dvGen, genUpdatesFiles.get(dvGen));\n        }\n      }\n    }\n    \n    info.setGenUpdatesFiles(newGenUpdatesFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          builder.addOrUpdate(f, NumericDocValuesField.TYPE);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          builder.addOrUpdate(f, BinaryDocValuesField.TYPE);\n        }\n        \n        fieldInfos = builder.finish();\n        final long nextFieldInfosGen = info.getNextFieldInfosGen();\n        final String segmentSuffix = Long.toString(nextFieldInfosGen, Character.MAX_RADIX);\n        final SegmentWriteState state = new SegmentWriteState(null, trackingDir, info.info, fieldInfos, null, IOContext.DEFAULT, segmentSuffix);\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        final DocValuesConsumer fieldsConsumer = docValuesFormat.fieldsConsumer(state);\n        boolean fieldsConsumerSuccess = false;\n        try {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n          for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n            final String field = e.getKey();\n            final NumericDocValuesFieldUpdates fieldUpdates = e.getValue();\n            final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n            assert fieldInfo != null;\n\n            fieldInfo.setDocValuesGen(nextFieldInfosGen);\n            // write the numeric updates to a new gen'd docvalues file\n            fieldsConsumer.addNumericField(fieldInfo, new Iterable<Number>() {\n              final NumericDocValues currentValues = reader.getNumericDocValues(field);\n              final Bits docsWithField = reader.getDocsWithField(field);\n              final int maxDoc = reader.maxDoc();\n              final NumericDocValuesFieldUpdates.Iterator updatesIter = fieldUpdates.iterator();\n              @Override\n              public Iterator<Number> iterator() {\n                updatesIter.reset();\n                return new Iterator<Number>() {\n\n                  int curDoc = -1;\n                  int updateDoc = updatesIter.nextDoc();\n                  \n                  @Override\n                  public boolean hasNext() {\n                    return curDoc < maxDoc - 1;\n                  }\n\n                  @Override\n                  public Number next() {\n                    if (++curDoc >= maxDoc) {\n                      throw new NoSuchElementException(\"no more documents to return values for\");\n                    }\n                    if (curDoc == updateDoc) { // this document has an updated value\n                      Long value = updatesIter.value(); // either null (unset value) or updated value\n                      updateDoc = updatesIter.nextDoc(); // prepare for next round\n                      return value;\n                    } else {\n                      // no update for this document\n                      assert curDoc < updateDoc;\n                      if (currentValues != null && docsWithField.get(curDoc)) {\n                        // only read the current value if the document had a value before\n                        return currentValues.get(curDoc);\n                      } else {\n                        return null;\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void remove() {\n                    throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                  }\n                };\n              }\n            });\n          }\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n          final String field = e.getKey();\n          final BinaryDocValuesFieldUpdates dvFieldUpdates = e.getValue();\n          final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n          assert fieldInfo != null;\n\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" f=\" + dvFieldUpdates + \", updates=\" + dvFieldUpdates);\n\n          fieldInfo.setDocValuesGen(nextFieldInfosGen);\n          // write the numeric updates to a new gen'd docvalues file\n          fieldsConsumer.addBinaryField(fieldInfo, new Iterable<BytesRef>() {\n            final BinaryDocValues currentValues = reader.getBinaryDocValues(field);\n            final Bits docsWithField = reader.getDocsWithField(field);\n            final int maxDoc = reader.maxDoc();\n            final BinaryDocValuesFieldUpdates.Iterator updatesIter = dvFieldUpdates.iterator();\n            @Override\n            public Iterator<BytesRef> iterator() {\n              updatesIter.reset();\n              return new Iterator<BytesRef>() {\n\n                int curDoc = -1;\n                int updateDoc = updatesIter.nextDoc();\n                BytesRef scratch = new BytesRef();\n                \n                @Override\n                public boolean hasNext() {\n                  return curDoc < maxDoc - 1;\n                }\n\n                @Override\n                public BytesRef next() {\n                  if (++curDoc >= maxDoc) {\n                    throw new NoSuchElementException(\"no more documents to return values for\");\n                  }\n                  if (curDoc == updateDoc) { // this document has an updated value\n                    BytesRef value = updatesIter.value(); // either null (unset value) or updated value\n                    updateDoc = updatesIter.nextDoc(); // prepare for next round\n                    return value;\n                  } else {\n                    // no update for this document\n                    assert curDoc < updateDoc;\n                    if (currentValues != null && docsWithField.get(curDoc)) {\n                      // only read the current value if the document had a value before\n                      currentValues.get(curDoc, scratch);\n                      return scratch;\n                    } else {\n                      return null;\n                    }\n                  }\n                }\n\n                @Override\n                public void remove() {\n                  throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                }\n              };\n            }\n          });\n        }\n\n          codec.fieldInfosFormat().getFieldInfosWriter().write(trackingDir, info.info.name, segmentSuffix, fieldInfos, IOContext.DEFAULT);\n          fieldsConsumerSuccess = true;\n        } finally {\n          if (fieldsConsumerSuccess) {\n            fieldsConsumer.close();\n          } else {\n            IOUtils.closeWhileHandlingException(fieldsConsumer);\n          }\n        }\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteDocValuesGen so that a 2nd\n        // attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          try {\n            dir.deleteFile(fileName);\n          } catch (Throwable t) {\n            // Ignore so we throw only the first exc\n          }\n        }\n      }\n    }\n    \n    info.advanceFieldInfosGen();\n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // create a new map, keeping only the gens that are in use\n    Map<Long,Set<String>> genUpdatesFiles = info.getUpdatesFiles();\n    Map<Long,Set<String>> newGenUpdatesFiles = new HashMap<>();\n    final long fieldInfosGen = info.getFieldInfosGen();\n    for (FieldInfo fi : fieldInfos) {\n      long dvGen = fi.getDocValuesGen();\n      if (dvGen != -1 && !newGenUpdatesFiles.containsKey(dvGen)) {\n        if (dvGen == fieldInfosGen) {\n          newGenUpdatesFiles.put(fieldInfosGen, trackingDir.getCreatedFiles());\n        } else {\n          newGenUpdatesFiles.put(dvGen, genUpdatesFiles.get(dvGen));\n        }\n      }\n    }\n    \n    info.setGenUpdatesFiles(newGenUpdatesFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8a341947b8c97354c225eb5460c7f4b2cf454c0a","date":1398888860,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","sourceNew":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          builder.addOrUpdate(f, NumericDocValuesField.TYPE);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          builder.addOrUpdate(f, BinaryDocValuesField.TYPE);\n        }\n        \n        fieldInfos = builder.finish();\n        final long nextFieldInfosGen = info.getNextFieldInfosGen();\n        final String segmentSuffix = Long.toString(nextFieldInfosGen, Character.MAX_RADIX);\n        final long estUpdatesSize = dvUpdates.ramBytesPerDoc() * info.info.getDocCount();\n        final IOContext updatesContext = new IOContext(new FlushInfo(info.info.getDocCount(), estUpdatesSize));\n        final SegmentWriteState state = new SegmentWriteState(null, trackingDir, info.info, fieldInfos, null, updatesContext, segmentSuffix);\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        final DocValuesConsumer fieldsConsumer = docValuesFormat.fieldsConsumer(state);\n        boolean fieldsConsumerSuccess = false;\n        try {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n          for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n            final String field = e.getKey();\n            final NumericDocValuesFieldUpdates fieldUpdates = e.getValue();\n            final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n            assert fieldInfo != null;\n\n            fieldInfo.setDocValuesGen(nextFieldInfosGen);\n            // write the numeric updates to a new gen'd docvalues file\n            fieldsConsumer.addNumericField(fieldInfo, new Iterable<Number>() {\n              final NumericDocValues currentValues = reader.getNumericDocValues(field);\n              final Bits docsWithField = reader.getDocsWithField(field);\n              final int maxDoc = reader.maxDoc();\n              final NumericDocValuesFieldUpdates.Iterator updatesIter = fieldUpdates.iterator();\n              @Override\n              public Iterator<Number> iterator() {\n                updatesIter.reset();\n                return new Iterator<Number>() {\n\n                  int curDoc = -1;\n                  int updateDoc = updatesIter.nextDoc();\n                  \n                  @Override\n                  public boolean hasNext() {\n                    return curDoc < maxDoc - 1;\n                  }\n\n                  @Override\n                  public Number next() {\n                    if (++curDoc >= maxDoc) {\n                      throw new NoSuchElementException(\"no more documents to return values for\");\n                    }\n                    if (curDoc == updateDoc) { // this document has an updated value\n                      Long value = updatesIter.value(); // either null (unset value) or updated value\n                      updateDoc = updatesIter.nextDoc(); // prepare for next round\n                      return value;\n                    } else {\n                      // no update for this document\n                      assert curDoc < updateDoc;\n                      if (currentValues != null && docsWithField.get(curDoc)) {\n                        // only read the current value if the document had a value before\n                        return currentValues.get(curDoc);\n                      } else {\n                        return null;\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void remove() {\n                    throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                  }\n                };\n              }\n            });\n          }\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n          final String field = e.getKey();\n          final BinaryDocValuesFieldUpdates dvFieldUpdates = e.getValue();\n          final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n          assert fieldInfo != null;\n\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" f=\" + dvFieldUpdates + \", updates=\" + dvFieldUpdates);\n\n          fieldInfo.setDocValuesGen(nextFieldInfosGen);\n          // write the numeric updates to a new gen'd docvalues file\n          fieldsConsumer.addBinaryField(fieldInfo, new Iterable<BytesRef>() {\n            final BinaryDocValues currentValues = reader.getBinaryDocValues(field);\n            final Bits docsWithField = reader.getDocsWithField(field);\n            final int maxDoc = reader.maxDoc();\n            final BinaryDocValuesFieldUpdates.Iterator updatesIter = dvFieldUpdates.iterator();\n            @Override\n            public Iterator<BytesRef> iterator() {\n              updatesIter.reset();\n              return new Iterator<BytesRef>() {\n\n                int curDoc = -1;\n                int updateDoc = updatesIter.nextDoc();\n                BytesRef scratch = new BytesRef();\n                \n                @Override\n                public boolean hasNext() {\n                  return curDoc < maxDoc - 1;\n                }\n\n                @Override\n                public BytesRef next() {\n                  if (++curDoc >= maxDoc) {\n                    throw new NoSuchElementException(\"no more documents to return values for\");\n                  }\n                  if (curDoc == updateDoc) { // this document has an updated value\n                    BytesRef value = updatesIter.value(); // either null (unset value) or updated value\n                    updateDoc = updatesIter.nextDoc(); // prepare for next round\n                    return value;\n                  } else {\n                    // no update for this document\n                    assert curDoc < updateDoc;\n                    if (currentValues != null && docsWithField.get(curDoc)) {\n                      // only read the current value if the document had a value before\n                      currentValues.get(curDoc, scratch);\n                      return scratch;\n                    } else {\n                      return null;\n                    }\n                  }\n                }\n\n                @Override\n                public void remove() {\n                  throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                }\n              };\n            }\n          });\n        }\n        \n        // we write approximately that many bytes (based on Lucene46DVF):\n        // HEADER + FOOTER: 40\n        // 90 bytes per-field (over estimating long name and attributes map)\n        final long estInfosSize = 40 + 90 * fieldInfos.size();\n        final IOContext infosContext = new IOContext(new FlushInfo(info.info.getDocCount(), estInfosSize));\n        codec.fieldInfosFormat().getFieldInfosWriter().write(trackingDir, info.info.name, segmentSuffix, fieldInfos, infosContext);\n        fieldsConsumerSuccess = true;\n        } finally {\n          if (fieldsConsumerSuccess) {\n            fieldsConsumer.close();\n          } else {\n            IOUtils.closeWhileHandlingException(fieldsConsumer);\n          }\n        }\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen so that a 2nd\n        // attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          try {\n            dir.deleteFile(fileName);\n          } catch (Throwable t) {\n            // Ignore so we throw only the first exc\n          }\n        }\n      }\n    }\n    \n    info.advanceFieldInfosGen();\n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // create a new map, keeping only the gens that are in use\n    Map<Long,Set<String>> genUpdatesFiles = info.getUpdatesFiles();\n    Map<Long,Set<String>> newGenUpdatesFiles = new HashMap<>();\n    final long fieldInfosGen = info.getFieldInfosGen();\n    for (FieldInfo fi : fieldInfos) {\n      long dvGen = fi.getDocValuesGen();\n      if (dvGen != -1 && !newGenUpdatesFiles.containsKey(dvGen)) {\n        if (dvGen == fieldInfosGen) {\n          newGenUpdatesFiles.put(fieldInfosGen, trackingDir.getCreatedFiles());\n        } else {\n          newGenUpdatesFiles.put(dvGen, genUpdatesFiles.get(dvGen));\n        }\n      }\n    }\n    \n    info.setGenUpdatesFiles(newGenUpdatesFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          builder.addOrUpdate(f, NumericDocValuesField.TYPE);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          builder.addOrUpdate(f, BinaryDocValuesField.TYPE);\n        }\n        \n        fieldInfos = builder.finish();\n        final long nextFieldInfosGen = info.getNextFieldInfosGen();\n        final String segmentSuffix = Long.toString(nextFieldInfosGen, Character.MAX_RADIX);\n        final SegmentWriteState state = new SegmentWriteState(null, trackingDir, info.info, fieldInfos, null, IOContext.DEFAULT, segmentSuffix);\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        final DocValuesConsumer fieldsConsumer = docValuesFormat.fieldsConsumer(state);\n        boolean fieldsConsumerSuccess = false;\n        try {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n          for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n            final String field = e.getKey();\n            final NumericDocValuesFieldUpdates fieldUpdates = e.getValue();\n            final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n            assert fieldInfo != null;\n\n            fieldInfo.setDocValuesGen(nextFieldInfosGen);\n            // write the numeric updates to a new gen'd docvalues file\n            fieldsConsumer.addNumericField(fieldInfo, new Iterable<Number>() {\n              final NumericDocValues currentValues = reader.getNumericDocValues(field);\n              final Bits docsWithField = reader.getDocsWithField(field);\n              final int maxDoc = reader.maxDoc();\n              final NumericDocValuesFieldUpdates.Iterator updatesIter = fieldUpdates.iterator();\n              @Override\n              public Iterator<Number> iterator() {\n                updatesIter.reset();\n                return new Iterator<Number>() {\n\n                  int curDoc = -1;\n                  int updateDoc = updatesIter.nextDoc();\n                  \n                  @Override\n                  public boolean hasNext() {\n                    return curDoc < maxDoc - 1;\n                  }\n\n                  @Override\n                  public Number next() {\n                    if (++curDoc >= maxDoc) {\n                      throw new NoSuchElementException(\"no more documents to return values for\");\n                    }\n                    if (curDoc == updateDoc) { // this document has an updated value\n                      Long value = updatesIter.value(); // either null (unset value) or updated value\n                      updateDoc = updatesIter.nextDoc(); // prepare for next round\n                      return value;\n                    } else {\n                      // no update for this document\n                      assert curDoc < updateDoc;\n                      if (currentValues != null && docsWithField.get(curDoc)) {\n                        // only read the current value if the document had a value before\n                        return currentValues.get(curDoc);\n                      } else {\n                        return null;\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void remove() {\n                    throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                  }\n                };\n              }\n            });\n          }\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n          final String field = e.getKey();\n          final BinaryDocValuesFieldUpdates dvFieldUpdates = e.getValue();\n          final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n          assert fieldInfo != null;\n\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" f=\" + dvFieldUpdates + \", updates=\" + dvFieldUpdates);\n\n          fieldInfo.setDocValuesGen(nextFieldInfosGen);\n          // write the numeric updates to a new gen'd docvalues file\n          fieldsConsumer.addBinaryField(fieldInfo, new Iterable<BytesRef>() {\n            final BinaryDocValues currentValues = reader.getBinaryDocValues(field);\n            final Bits docsWithField = reader.getDocsWithField(field);\n            final int maxDoc = reader.maxDoc();\n            final BinaryDocValuesFieldUpdates.Iterator updatesIter = dvFieldUpdates.iterator();\n            @Override\n            public Iterator<BytesRef> iterator() {\n              updatesIter.reset();\n              return new Iterator<BytesRef>() {\n\n                int curDoc = -1;\n                int updateDoc = updatesIter.nextDoc();\n                BytesRef scratch = new BytesRef();\n                \n                @Override\n                public boolean hasNext() {\n                  return curDoc < maxDoc - 1;\n                }\n\n                @Override\n                public BytesRef next() {\n                  if (++curDoc >= maxDoc) {\n                    throw new NoSuchElementException(\"no more documents to return values for\");\n                  }\n                  if (curDoc == updateDoc) { // this document has an updated value\n                    BytesRef value = updatesIter.value(); // either null (unset value) or updated value\n                    updateDoc = updatesIter.nextDoc(); // prepare for next round\n                    return value;\n                  } else {\n                    // no update for this document\n                    assert curDoc < updateDoc;\n                    if (currentValues != null && docsWithField.get(curDoc)) {\n                      // only read the current value if the document had a value before\n                      currentValues.get(curDoc, scratch);\n                      return scratch;\n                    } else {\n                      return null;\n                    }\n                  }\n                }\n\n                @Override\n                public void remove() {\n                  throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                }\n              };\n            }\n          });\n        }\n\n          codec.fieldInfosFormat().getFieldInfosWriter().write(trackingDir, info.info.name, segmentSuffix, fieldInfos, IOContext.DEFAULT);\n          fieldsConsumerSuccess = true;\n        } finally {\n          if (fieldsConsumerSuccess) {\n            fieldsConsumer.close();\n          } else {\n            IOUtils.closeWhileHandlingException(fieldsConsumer);\n          }\n        }\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen so that a 2nd\n        // attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          try {\n            dir.deleteFile(fileName);\n          } catch (Throwable t) {\n            // Ignore so we throw only the first exc\n          }\n        }\n      }\n    }\n    \n    info.advanceFieldInfosGen();\n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // create a new map, keeping only the gens that are in use\n    Map<Long,Set<String>> genUpdatesFiles = info.getUpdatesFiles();\n    Map<Long,Set<String>> newGenUpdatesFiles = new HashMap<>();\n    final long fieldInfosGen = info.getFieldInfosGen();\n    for (FieldInfo fi : fieldInfos) {\n      long dvGen = fi.getDocValuesGen();\n      if (dvGen != -1 && !newGenUpdatesFiles.containsKey(dvGen)) {\n        if (dvGen == fieldInfosGen) {\n          newGenUpdatesFiles.put(fieldInfosGen, trackingDir.getCreatedFiles());\n        } else {\n          newGenUpdatesFiles.put(dvGen, genUpdatesFiles.get(dvGen));\n        }\n      }\n    }\n    \n    info.setGenUpdatesFiles(newGenUpdatesFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["0567bdc5c86c94ced64201187cfcef2417d76dda"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0567bdc5c86c94ced64201187cfcef2417d76dda","date":1400678298,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","sourceNew":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          builder.addOrUpdate(f, NumericDocValuesField.TYPE);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          builder.addOrUpdate(f, BinaryDocValuesField.TYPE);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          try {\n            dir.deleteFile(fileName);\n          } catch (Throwable t) {\n            // Ignore so we throw only the first exc\n          }\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          builder.addOrUpdate(f, NumericDocValuesField.TYPE);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          builder.addOrUpdate(f, BinaryDocValuesField.TYPE);\n        }\n        \n        fieldInfos = builder.finish();\n        final long nextFieldInfosGen = info.getNextFieldInfosGen();\n        final String segmentSuffix = Long.toString(nextFieldInfosGen, Character.MAX_RADIX);\n        final long estUpdatesSize = dvUpdates.ramBytesPerDoc() * info.info.getDocCount();\n        final IOContext updatesContext = new IOContext(new FlushInfo(info.info.getDocCount(), estUpdatesSize));\n        final SegmentWriteState state = new SegmentWriteState(null, trackingDir, info.info, fieldInfos, null, updatesContext, segmentSuffix);\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        final DocValuesConsumer fieldsConsumer = docValuesFormat.fieldsConsumer(state);\n        boolean fieldsConsumerSuccess = false;\n        try {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n          for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n            final String field = e.getKey();\n            final NumericDocValuesFieldUpdates fieldUpdates = e.getValue();\n            final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n            assert fieldInfo != null;\n\n            fieldInfo.setDocValuesGen(nextFieldInfosGen);\n            // write the numeric updates to a new gen'd docvalues file\n            fieldsConsumer.addNumericField(fieldInfo, new Iterable<Number>() {\n              final NumericDocValues currentValues = reader.getNumericDocValues(field);\n              final Bits docsWithField = reader.getDocsWithField(field);\n              final int maxDoc = reader.maxDoc();\n              final NumericDocValuesFieldUpdates.Iterator updatesIter = fieldUpdates.iterator();\n              @Override\n              public Iterator<Number> iterator() {\n                updatesIter.reset();\n                return new Iterator<Number>() {\n\n                  int curDoc = -1;\n                  int updateDoc = updatesIter.nextDoc();\n                  \n                  @Override\n                  public boolean hasNext() {\n                    return curDoc < maxDoc - 1;\n                  }\n\n                  @Override\n                  public Number next() {\n                    if (++curDoc >= maxDoc) {\n                      throw new NoSuchElementException(\"no more documents to return values for\");\n                    }\n                    if (curDoc == updateDoc) { // this document has an updated value\n                      Long value = updatesIter.value(); // either null (unset value) or updated value\n                      updateDoc = updatesIter.nextDoc(); // prepare for next round\n                      return value;\n                    } else {\n                      // no update for this document\n                      assert curDoc < updateDoc;\n                      if (currentValues != null && docsWithField.get(curDoc)) {\n                        // only read the current value if the document had a value before\n                        return currentValues.get(curDoc);\n                      } else {\n                        return null;\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void remove() {\n                    throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                  }\n                };\n              }\n            });\n          }\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n          final String field = e.getKey();\n          final BinaryDocValuesFieldUpdates dvFieldUpdates = e.getValue();\n          final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n          assert fieldInfo != null;\n\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" f=\" + dvFieldUpdates + \", updates=\" + dvFieldUpdates);\n\n          fieldInfo.setDocValuesGen(nextFieldInfosGen);\n          // write the numeric updates to a new gen'd docvalues file\n          fieldsConsumer.addBinaryField(fieldInfo, new Iterable<BytesRef>() {\n            final BinaryDocValues currentValues = reader.getBinaryDocValues(field);\n            final Bits docsWithField = reader.getDocsWithField(field);\n            final int maxDoc = reader.maxDoc();\n            final BinaryDocValuesFieldUpdates.Iterator updatesIter = dvFieldUpdates.iterator();\n            @Override\n            public Iterator<BytesRef> iterator() {\n              updatesIter.reset();\n              return new Iterator<BytesRef>() {\n\n                int curDoc = -1;\n                int updateDoc = updatesIter.nextDoc();\n                BytesRef scratch = new BytesRef();\n                \n                @Override\n                public boolean hasNext() {\n                  return curDoc < maxDoc - 1;\n                }\n\n                @Override\n                public BytesRef next() {\n                  if (++curDoc >= maxDoc) {\n                    throw new NoSuchElementException(\"no more documents to return values for\");\n                  }\n                  if (curDoc == updateDoc) { // this document has an updated value\n                    BytesRef value = updatesIter.value(); // either null (unset value) or updated value\n                    updateDoc = updatesIter.nextDoc(); // prepare for next round\n                    return value;\n                  } else {\n                    // no update for this document\n                    assert curDoc < updateDoc;\n                    if (currentValues != null && docsWithField.get(curDoc)) {\n                      // only read the current value if the document had a value before\n                      currentValues.get(curDoc, scratch);\n                      return scratch;\n                    } else {\n                      return null;\n                    }\n                  }\n                }\n\n                @Override\n                public void remove() {\n                  throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                }\n              };\n            }\n          });\n        }\n        \n        // we write approximately that many bytes (based on Lucene46DVF):\n        // HEADER + FOOTER: 40\n        // 90 bytes per-field (over estimating long name and attributes map)\n        final long estInfosSize = 40 + 90 * fieldInfos.size();\n        final IOContext infosContext = new IOContext(new FlushInfo(info.info.getDocCount(), estInfosSize));\n        codec.fieldInfosFormat().getFieldInfosWriter().write(trackingDir, info.info.name, segmentSuffix, fieldInfos, infosContext);\n        fieldsConsumerSuccess = true;\n        } finally {\n          if (fieldsConsumerSuccess) {\n            fieldsConsumer.close();\n          } else {\n            IOUtils.closeWhileHandlingException(fieldsConsumer);\n          }\n        }\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen so that a 2nd\n        // attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          try {\n            dir.deleteFile(fileName);\n          } catch (Throwable t) {\n            // Ignore so we throw only the first exc\n          }\n        }\n      }\n    }\n    \n    info.advanceFieldInfosGen();\n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // create a new map, keeping only the gens that are in use\n    Map<Long,Set<String>> genUpdatesFiles = info.getUpdatesFiles();\n    Map<Long,Set<String>> newGenUpdatesFiles = new HashMap<>();\n    final long fieldInfosGen = info.getFieldInfosGen();\n    for (FieldInfo fi : fieldInfos) {\n      long dvGen = fi.getDocValuesGen();\n      if (dvGen != -1 && !newGenUpdatesFiles.containsKey(dvGen)) {\n        if (dvGen == fieldInfosGen) {\n          newGenUpdatesFiles.put(fieldInfosGen, trackingDir.getCreatedFiles());\n        } else {\n          newGenUpdatesFiles.put(dvGen, genUpdatesFiles.get(dvGen));\n        }\n      }\n    }\n    \n    info.setGenUpdatesFiles(newGenUpdatesFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","bugFix":["c1d0b4a65bc57bd59cef05619306aa8ee7431a0f","634f330c54fd3f9f491d52036dc3f40b4f4d8934","1bae040fd1d5e03e0d8d695a9c25cf4f402e7ffe","06805da26538ed636bd89b10c2699cc3834032ae","8a341947b8c97354c225eb5460c7f4b2cf454c0a"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a656b32c3aa151037a8c52e9b134acc3cbf482bc","date":1400688195,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","sourceNew":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          builder.addOrUpdate(f, NumericDocValuesField.TYPE);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          builder.addOrUpdate(f, BinaryDocValuesField.TYPE);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          try {\n            dir.deleteFile(fileName);\n          } catch (Throwable t) {\n            // Ignore so we throw only the first exc\n          }\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          builder.addOrUpdate(f, NumericDocValuesField.TYPE);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          builder.addOrUpdate(f, BinaryDocValuesField.TYPE);\n        }\n        \n        fieldInfos = builder.finish();\n        final long nextFieldInfosGen = info.getNextFieldInfosGen();\n        final String segmentSuffix = Long.toString(nextFieldInfosGen, Character.MAX_RADIX);\n        final long estUpdatesSize = dvUpdates.ramBytesPerDoc() * info.info.getDocCount();\n        final IOContext updatesContext = new IOContext(new FlushInfo(info.info.getDocCount(), estUpdatesSize));\n        final SegmentWriteState state = new SegmentWriteState(null, trackingDir, info.info, fieldInfos, null, updatesContext, segmentSuffix);\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        final DocValuesConsumer fieldsConsumer = docValuesFormat.fieldsConsumer(state);\n        boolean fieldsConsumerSuccess = false;\n        try {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n          for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n            final String field = e.getKey();\n            final NumericDocValuesFieldUpdates fieldUpdates = e.getValue();\n            final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n            assert fieldInfo != null;\n\n            fieldInfo.setDocValuesGen(nextFieldInfosGen);\n            // write the numeric updates to a new gen'd docvalues file\n            fieldsConsumer.addNumericField(fieldInfo, new Iterable<Number>() {\n              final NumericDocValues currentValues = reader.getNumericDocValues(field);\n              final Bits docsWithField = reader.getDocsWithField(field);\n              final int maxDoc = reader.maxDoc();\n              final NumericDocValuesFieldUpdates.Iterator updatesIter = fieldUpdates.iterator();\n              @Override\n              public Iterator<Number> iterator() {\n                updatesIter.reset();\n                return new Iterator<Number>() {\n\n                  int curDoc = -1;\n                  int updateDoc = updatesIter.nextDoc();\n                  \n                  @Override\n                  public boolean hasNext() {\n                    return curDoc < maxDoc - 1;\n                  }\n\n                  @Override\n                  public Number next() {\n                    if (++curDoc >= maxDoc) {\n                      throw new NoSuchElementException(\"no more documents to return values for\");\n                    }\n                    if (curDoc == updateDoc) { // this document has an updated value\n                      Long value = updatesIter.value(); // either null (unset value) or updated value\n                      updateDoc = updatesIter.nextDoc(); // prepare for next round\n                      return value;\n                    } else {\n                      // no update for this document\n                      assert curDoc < updateDoc;\n                      if (currentValues != null && docsWithField.get(curDoc)) {\n                        // only read the current value if the document had a value before\n                        return currentValues.get(curDoc);\n                      } else {\n                        return null;\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void remove() {\n                    throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                  }\n                };\n              }\n            });\n          }\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n          final String field = e.getKey();\n          final BinaryDocValuesFieldUpdates dvFieldUpdates = e.getValue();\n          final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n          assert fieldInfo != null;\n\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" f=\" + dvFieldUpdates + \", updates=\" + dvFieldUpdates);\n\n          fieldInfo.setDocValuesGen(nextFieldInfosGen);\n          // write the numeric updates to a new gen'd docvalues file\n          fieldsConsumer.addBinaryField(fieldInfo, new Iterable<BytesRef>() {\n            final BinaryDocValues currentValues = reader.getBinaryDocValues(field);\n            final Bits docsWithField = reader.getDocsWithField(field);\n            final int maxDoc = reader.maxDoc();\n            final BinaryDocValuesFieldUpdates.Iterator updatesIter = dvFieldUpdates.iterator();\n            @Override\n            public Iterator<BytesRef> iterator() {\n              updatesIter.reset();\n              return new Iterator<BytesRef>() {\n\n                int curDoc = -1;\n                int updateDoc = updatesIter.nextDoc();\n                BytesRef scratch = new BytesRef();\n                \n                @Override\n                public boolean hasNext() {\n                  return curDoc < maxDoc - 1;\n                }\n\n                @Override\n                public BytesRef next() {\n                  if (++curDoc >= maxDoc) {\n                    throw new NoSuchElementException(\"no more documents to return values for\");\n                  }\n                  if (curDoc == updateDoc) { // this document has an updated value\n                    BytesRef value = updatesIter.value(); // either null (unset value) or updated value\n                    updateDoc = updatesIter.nextDoc(); // prepare for next round\n                    return value;\n                  } else {\n                    // no update for this document\n                    assert curDoc < updateDoc;\n                    if (currentValues != null && docsWithField.get(curDoc)) {\n                      // only read the current value if the document had a value before\n                      currentValues.get(curDoc, scratch);\n                      return scratch;\n                    } else {\n                      return null;\n                    }\n                  }\n                }\n\n                @Override\n                public void remove() {\n                  throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                }\n              };\n            }\n          });\n        }\n        \n        // we write approximately that many bytes (based on Lucene46DVF):\n        // HEADER + FOOTER: 40\n        // 90 bytes per-field (over estimating long name and attributes map)\n        final long estInfosSize = 40 + 90 * fieldInfos.size();\n        final IOContext infosContext = new IOContext(new FlushInfo(info.info.getDocCount(), estInfosSize));\n        codec.fieldInfosFormat().getFieldInfosWriter().write(trackingDir, info.info.name, segmentSuffix, fieldInfos, infosContext);\n        fieldsConsumerSuccess = true;\n        } finally {\n          if (fieldsConsumerSuccess) {\n            fieldsConsumer.close();\n          } else {\n            IOUtils.closeWhileHandlingException(fieldsConsumer);\n          }\n        }\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen so that a 2nd\n        // attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          try {\n            dir.deleteFile(fileName);\n          } catch (Throwable t) {\n            // Ignore so we throw only the first exc\n          }\n        }\n      }\n    }\n    \n    info.advanceFieldInfosGen();\n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // create a new map, keeping only the gens that are in use\n    Map<Long,Set<String>> genUpdatesFiles = info.getUpdatesFiles();\n    Map<Long,Set<String>> newGenUpdatesFiles = new HashMap<>();\n    final long fieldInfosGen = info.getFieldInfosGen();\n    for (FieldInfo fi : fieldInfos) {\n      long dvGen = fi.getDocValuesGen();\n      if (dvGen != -1 && !newGenUpdatesFiles.containsKey(dvGen)) {\n        if (dvGen == fieldInfosGen) {\n          newGenUpdatesFiles.put(fieldInfosGen, trackingDir.getCreatedFiles());\n        } else {\n          newGenUpdatesFiles.put(dvGen, genUpdatesFiles.get(dvGen));\n        }\n      }\n    }\n    \n    info.setGenUpdatesFiles(newGenUpdatesFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7605579001505896d48b07160075a5c8b8e128e","date":1400758727,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","sourceNew":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          builder.addOrUpdate(f, NumericDocValuesField.TYPE);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          builder.addOrUpdate(f, BinaryDocValuesField.TYPE);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          try {\n            dir.deleteFile(fileName);\n          } catch (Throwable t) {\n            // Ignore so we throw only the first exc\n          }\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          builder.addOrUpdate(f, NumericDocValuesField.TYPE);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          builder.addOrUpdate(f, BinaryDocValuesField.TYPE);\n        }\n        \n        fieldInfos = builder.finish();\n        final long nextFieldInfosGen = info.getNextFieldInfosGen();\n        final String segmentSuffix = Long.toString(nextFieldInfosGen, Character.MAX_RADIX);\n        final long estUpdatesSize = dvUpdates.ramBytesPerDoc() * info.info.getDocCount();\n        final IOContext updatesContext = new IOContext(new FlushInfo(info.info.getDocCount(), estUpdatesSize));\n        final SegmentWriteState state = new SegmentWriteState(null, trackingDir, info.info, fieldInfos, null, updatesContext, segmentSuffix);\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        final DocValuesConsumer fieldsConsumer = docValuesFormat.fieldsConsumer(state);\n        boolean fieldsConsumerSuccess = false;\n        try {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n          for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n            final String field = e.getKey();\n            final NumericDocValuesFieldUpdates fieldUpdates = e.getValue();\n            final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n            assert fieldInfo != null;\n\n            fieldInfo.setDocValuesGen(nextFieldInfosGen);\n            // write the numeric updates to a new gen'd docvalues file\n            fieldsConsumer.addNumericField(fieldInfo, new Iterable<Number>() {\n              final NumericDocValues currentValues = reader.getNumericDocValues(field);\n              final Bits docsWithField = reader.getDocsWithField(field);\n              final int maxDoc = reader.maxDoc();\n              final NumericDocValuesFieldUpdates.Iterator updatesIter = fieldUpdates.iterator();\n              @Override\n              public Iterator<Number> iterator() {\n                updatesIter.reset();\n                return new Iterator<Number>() {\n\n                  int curDoc = -1;\n                  int updateDoc = updatesIter.nextDoc();\n                  \n                  @Override\n                  public boolean hasNext() {\n                    return curDoc < maxDoc - 1;\n                  }\n\n                  @Override\n                  public Number next() {\n                    if (++curDoc >= maxDoc) {\n                      throw new NoSuchElementException(\"no more documents to return values for\");\n                    }\n                    if (curDoc == updateDoc) { // this document has an updated value\n                      Long value = updatesIter.value(); // either null (unset value) or updated value\n                      updateDoc = updatesIter.nextDoc(); // prepare for next round\n                      return value;\n                    } else {\n                      // no update for this document\n                      assert curDoc < updateDoc;\n                      if (currentValues != null && docsWithField.get(curDoc)) {\n                        // only read the current value if the document had a value before\n                        return currentValues.get(curDoc);\n                      } else {\n                        return null;\n                      }\n                    }\n                  }\n\n                  @Override\n                  public void remove() {\n                    throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                  }\n                };\n              }\n            });\n          }\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n          final String field = e.getKey();\n          final BinaryDocValuesFieldUpdates dvFieldUpdates = e.getValue();\n          final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n          assert fieldInfo != null;\n\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" f=\" + dvFieldUpdates + \", updates=\" + dvFieldUpdates);\n\n          fieldInfo.setDocValuesGen(nextFieldInfosGen);\n          // write the numeric updates to a new gen'd docvalues file\n          fieldsConsumer.addBinaryField(fieldInfo, new Iterable<BytesRef>() {\n            final BinaryDocValues currentValues = reader.getBinaryDocValues(field);\n            final Bits docsWithField = reader.getDocsWithField(field);\n            final int maxDoc = reader.maxDoc();\n            final BinaryDocValuesFieldUpdates.Iterator updatesIter = dvFieldUpdates.iterator();\n            @Override\n            public Iterator<BytesRef> iterator() {\n              updatesIter.reset();\n              return new Iterator<BytesRef>() {\n\n                int curDoc = -1;\n                int updateDoc = updatesIter.nextDoc();\n                BytesRef scratch = new BytesRef();\n                \n                @Override\n                public boolean hasNext() {\n                  return curDoc < maxDoc - 1;\n                }\n\n                @Override\n                public BytesRef next() {\n                  if (++curDoc >= maxDoc) {\n                    throw new NoSuchElementException(\"no more documents to return values for\");\n                  }\n                  if (curDoc == updateDoc) { // this document has an updated value\n                    BytesRef value = updatesIter.value(); // either null (unset value) or updated value\n                    updateDoc = updatesIter.nextDoc(); // prepare for next round\n                    return value;\n                  } else {\n                    // no update for this document\n                    assert curDoc < updateDoc;\n                    if (currentValues != null && docsWithField.get(curDoc)) {\n                      // only read the current value if the document had a value before\n                      currentValues.get(curDoc, scratch);\n                      return scratch;\n                    } else {\n                      return null;\n                    }\n                  }\n                }\n\n                @Override\n                public void remove() {\n                  throw new UnsupportedOperationException(\"this iterator does not support removing elements\");\n                }\n              };\n            }\n          });\n        }\n        \n        // we write approximately that many bytes (based on Lucene46DVF):\n        // HEADER + FOOTER: 40\n        // 90 bytes per-field (over estimating long name and attributes map)\n        final long estInfosSize = 40 + 90 * fieldInfos.size();\n        final IOContext infosContext = new IOContext(new FlushInfo(info.info.getDocCount(), estInfosSize));\n        codec.fieldInfosFormat().getFieldInfosWriter().write(trackingDir, info.info.name, segmentSuffix, fieldInfos, infosContext);\n        fieldsConsumerSuccess = true;\n        } finally {\n          if (fieldsConsumerSuccess) {\n            fieldsConsumer.close();\n          } else {\n            IOUtils.closeWhileHandlingException(fieldsConsumer);\n          }\n        }\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen so that a 2nd\n        // attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          try {\n            dir.deleteFile(fileName);\n          } catch (Throwable t) {\n            // Ignore so we throw only the first exc\n          }\n        }\n      }\n    }\n    \n    info.advanceFieldInfosGen();\n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // create a new map, keeping only the gens that are in use\n    Map<Long,Set<String>> genUpdatesFiles = info.getUpdatesFiles();\n    Map<Long,Set<String>> newGenUpdatesFiles = new HashMap<>();\n    final long fieldInfosGen = info.getFieldInfosGen();\n    for (FieldInfo fi : fieldInfos) {\n      long dvGen = fi.getDocValuesGen();\n      if (dvGen != -1 && !newGenUpdatesFiles.containsKey(dvGen)) {\n        if (dvGen == fieldInfosGen) {\n          newGenUpdatesFiles.put(fieldInfosGen, trackingDir.getCreatedFiles());\n        } else {\n          newGenUpdatesFiles.put(dvGen, genUpdatesFiles.get(dvGen));\n        }\n      }\n    }\n    \n    info.setGenUpdatesFiles(newGenUpdatesFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"98d2deb8c96c79ebef084a1f8e5a1a6c08608f13","date":1409346855,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","sourceNew":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          builder.addOrUpdate(f, NumericDocValuesField.TYPE);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          builder.addOrUpdate(f, BinaryDocValuesField.TYPE);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          builder.addOrUpdate(f, NumericDocValuesField.TYPE);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          builder.addOrUpdate(f, BinaryDocValuesField.TYPE);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          try {\n            dir.deleteFile(fileName);\n          } catch (Throwable t) {\n            // Ignore so we throw only the first exc\n          }\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eac6ccb51c439bec7f67cb0e299d3cb77b62b87e","date":1415435053,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","sourceNew":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.NUMERIC);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.BINARY);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          builder.addOrUpdate(f, NumericDocValuesField.TYPE);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          builder.addOrUpdate(f, BinaryDocValuesField.TYPE);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"79700663e164dece87bed4adfd3e28bab6cb1385","date":1425241849,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","sourceNew":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.NUMERIC);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.BINARY);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.NUMERIC);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.BINARY);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"299a2348fa24151d150182211b6208a38e5e3450","date":1425304608,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","sourceNew":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.NUMERIC);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.BINARY);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.NUMERIC);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.BINARY);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0267c69e2456a3477a1ad785723f2135da3117e","date":1425317087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","sourceNew":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.NUMERIC);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.BINARY);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.NUMERIC);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.BINARY);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b06445ae1731e049327712db0454e5643ca9b7fe","date":1425329139,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","sourceNew":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.NUMERIC);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.BINARY);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.NUMERIC);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.BINARY);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","sourceNew":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.NUMERIC);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.BINARY);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          if (fi.attributes() != null) {\n            for (Entry<String,String> e : fi.attributes().entrySet()) {\n              clone.putAttribute(e.getKey(), e.getValue());\n            }\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.NUMERIC);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.BINARY);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"31741cf1390044e38a2ec3127cf302ba841bfd75","date":1491292636,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","sourceNew":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, writer.segmentInfos.getIndexCreatedVersionMajor(), IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.NUMERIC);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.BINARY);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.NUMERIC);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.BINARY);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"92212fd254551a0b1156aafc3a1a6ed1a43932ad","date":1491296431,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","sourceNew":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, writer.segmentInfos.getIndexCreatedVersionMajor(), IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.NUMERIC);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.BINARY);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.NUMERIC);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.BINARY);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f4363cd33f6eff7fb4753574a441e2d18c1022a4","date":1498067235,"type":5,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,long,InfoStream).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","sourceNew":"  public synchronized boolean writeFieldUpdates(Directory dir, long maxDelGen, InfoStream infoStream) throws IOException {\n\n    long startTimeNS = System.nanoTime();\n    \n    assert Thread.holdsLock(writer);\n\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n\n    boolean any = false;\n    int count = 0;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, (a, b) -> Long.compare(a.delGen, b.delGen));\n      count += updates.size();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, writer.segmentInfos.getIndexCreatedVersionMajor(), IOContext.READONCE);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleNumericDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n        handleBinaryDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - info.getDelCount() - pendingDeleteCount);\n      boolean success2 = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        success2 = true;\n      } finally {\n        if (success2 == false) {\n          newReader.decRef();\n        }\n      }\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    // wrote new files, should checkpoint()\n    writer.checkpointNoSIS();\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n\n    return true;\n  }\n\n","sourceOld":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, writer.segmentInfos.getIndexCreatedVersionMajor(), IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.NUMERIC);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.BINARY);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["11b5a40d323ce34cc4159e1b8a44aeea352e0222","11b5a40d323ce34cc4159e1b8a44aeea352e0222","11b5a40d323ce34cc4159e1b8a44aeea352e0222","11b5a40d323ce34cc4159e1b8a44aeea352e0222"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":5,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,long,InfoStream).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","sourceNew":"  public synchronized boolean writeFieldUpdates(Directory dir, long maxDelGen, InfoStream infoStream) throws IOException {\n\n    long startTimeNS = System.nanoTime();\n    \n    assert Thread.holdsLock(writer);\n\n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n\n    boolean any = false;\n    int count = 0;\n    for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n      // Sort by increasing delGen:\n      Collections.sort(updates, (a, b) -> Long.compare(a.delGen, b.delGen));\n      count += updates.size();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen <= maxDelGen && update.any()) {\n          any = true;\n          break;\n        }\n      }\n    }\n\n    if (any == false) {\n      // no updates\n      return false;\n    }\n\n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader;\n      if (this.reader == null) {\n        reader = new SegmentReader(info, writer.segmentInfos.getIndexCreatedVersionMajor(), IOContext.READONCE);\n      } else {\n        reader = this.reader;\n      }\n      \n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n\n        // create new fields with the right DV type\n        for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n          DocValuesFieldUpdates update = updates.get(0);\n          FieldInfo fieldInfo = builder.getOrAdd(update.field);\n          fieldInfo.setDocValuesType(update.type);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n        handleNumericDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n        handleBinaryDVUpdates(fieldInfos, trackingDir, docValuesFormat, reader, newDVFiles, maxDelGen, infoStream);\n\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (success == false) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n\n    // Prune the now-written DV updates:\n    long bytesFreed = 0;\n    Iterator<Map.Entry<String,List<DocValuesFieldUpdates>>> it = pendingDVUpdates.entrySet().iterator();\n    while (it.hasNext()) {\n      Map.Entry<String,List<DocValuesFieldUpdates>> ent = it.next();\n      int upto = 0;\n      List<DocValuesFieldUpdates> updates = ent.getValue();\n      for (DocValuesFieldUpdates update : updates) {\n        if (update.delGen > maxDelGen) {\n          // not yet applied\n          updates.set(upto, update);\n          upto++;\n        } else {\n          bytesFreed += update.ramBytesUsed();\n        }\n      }\n      if (upto == 0) {\n        it.remove();\n      } else {\n        updates.subList(upto, updates.size()).clear();\n      }\n    }\n\n    long bytes = ramBytesUsed.addAndGet(-bytesFreed);\n    assert bytes >= 0;\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - info.getDelCount() - pendingDeleteCount);\n      boolean success2 = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        success2 = true;\n      } finally {\n        if (success2 == false) {\n          newReader.decRef();\n        }\n      }\n    }\n\n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert newDVFiles.isEmpty() == false;\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (newDVFiles.containsKey(e.getKey()) == false) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n\n    // wrote new files, should checkpoint()\n    writer.checkpointNoSIS();\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write field updates for seg=%s; took %.3fs; new files: %s\",\n                                             info, (System.nanoTime() - startTimeNS)/1000000000.0, newDVFiles));\n    }\n\n    return true;\n  }\n\n","sourceOld":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, writer.segmentInfos.getIndexCreatedVersionMajor(), IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.NUMERIC);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.BINARY);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":4,"author":"Karl Wright","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates#writeFieldUpdates(Directory,DocValuesFieldUpdates.Container).mjava","sourceNew":null,"sourceOld":"  // Writes field updates (new _X_N updates files) to the directory\n  public synchronized void writeFieldUpdates(Directory dir, DocValuesFieldUpdates.Container dvUpdates) throws IOException {\n    assert Thread.holdsLock(writer);\n    //System.out.println(\"rld.writeFieldUpdates: seg=\" + info + \" numericFieldUpdates=\" + numericFieldUpdates);\n    \n    assert dvUpdates.any();\n    \n    // Do this so we can delete any created files on\n    // exception; this saves all codecs from having to do\n    // it:\n    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);\n    \n    final Map<Integer,Set<String>> newDVFiles = new HashMap<>();\n    Set<String> fieldInfosFiles = null;\n    FieldInfos fieldInfos = null;\n    boolean success = false;\n    try {\n      final Codec codec = info.info.getCodec();\n\n      // reader could be null e.g. for a just merged segment (from\n      // IndexWriter.commitMergedDeletes).\n      final SegmentReader reader = this.reader == null ? new SegmentReader(info, writer.segmentInfos.getIndexCreatedVersionMajor(), IOContext.READONCE) : this.reader;\n      try {\n        // clone FieldInfos so that we can update their dvGen separately from\n        // the reader's infos and write them to a new fieldInfos_gen file\n        FieldInfos.Builder builder = new FieldInfos.Builder(writer.globalFieldNumberMap);\n        // cannot use builder.add(reader.getFieldInfos()) because it does not\n        // clone FI.attributes as well FI.dvGen\n        for (FieldInfo fi : reader.getFieldInfos()) {\n          FieldInfo clone = builder.add(fi);\n          // copy the stuff FieldInfos.Builder doesn't copy\n          for (Entry<String,String> e : fi.attributes().entrySet()) {\n            clone.putAttribute(e.getKey(), e.getValue());\n          }\n          clone.setDocValuesGen(fi.getDocValuesGen());\n        }\n        // create new fields or update existing ones to have NumericDV type\n        for (String f : dvUpdates.numericDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.NUMERIC);\n        }\n        // create new fields or update existing ones to have BinaryDV type\n        for (String f : dvUpdates.binaryDVUpdates.keySet()) {\n          FieldInfo fieldInfo = builder.getOrAdd(f);\n          fieldInfo.setDocValuesType(DocValuesType.BINARY);\n        }\n        \n        fieldInfos = builder.finish();\n        final DocValuesFormat docValuesFormat = codec.docValuesFormat();\n        \n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeFieldUpdates: applying numeric updates; seg=\" + info + \" updates=\" + numericFieldUpdates);\n        handleNumericDVUpdates(fieldInfos, dvUpdates.numericDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n        \n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: applying binary updates; seg=\" + info + \" updates=\" + dvUpdates.binaryDVUpdates);\n        handleBinaryDVUpdates(fieldInfos, dvUpdates.binaryDVUpdates, trackingDir, docValuesFormat, reader, newDVFiles);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] RAU.writeFieldUpdates: write fieldInfos; seg=\" + info);\n        fieldInfosFiles = writeFieldInfosGen(fieldInfos, trackingDir, docValuesFormat, codec.fieldInfosFormat());\n      } finally {\n        if (reader != this.reader) {\n//          System.out.println(\"[\" + Thread.currentThread().getName() + \"] RLD.writeLiveDocs: closeReader \" + reader);\n          reader.close();\n        }\n      }\n    \n      success = true;\n    } finally {\n      if (!success) {\n        // Advance only the nextWriteFieldInfosGen and nextWriteDocValuesGen, so\n        // that a 2nd attempt to write will write to a new file\n        info.advanceNextWriteFieldInfosGen();\n        info.advanceNextWriteDocValuesGen();\n        \n        // Delete any partially created file(s):\n        for (String fileName : trackingDir.getCreatedFiles()) {\n          IOUtils.deleteFilesIgnoringExceptions(dir, fileName);\n        }\n      }\n    }\n    \n    // copy all the updates to mergingUpdates, so they can later be applied to the merged segment\n    if (isMerging) {\n      for (Entry<String,NumericDocValuesFieldUpdates> e : dvUpdates.numericDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n      for (Entry<String,BinaryDocValuesFieldUpdates> e : dvUpdates.binaryDVUpdates.entrySet()) {\n        DocValuesFieldUpdates updates = mergingDVUpdates.get(e.getKey());\n        if (updates == null) {\n          mergingDVUpdates.put(e.getKey(), e.getValue());\n        } else {\n          updates.merge(e.getValue());\n        }\n      }\n    }\n    \n    // writing field updates succeeded\n    assert fieldInfosFiles != null;\n    info.setFieldInfosFiles(fieldInfosFiles);\n    \n    // update the doc-values updates files. the files map each field to its set\n    // of files, hence we copy from the existing map all fields w/ updates that\n    // were not updated in this session, and add new mappings for fields that\n    // were updated now.\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n      if (!newDVFiles.containsKey(e.getKey())) {\n        newDVFiles.put(e.getKey(), e.getValue());\n      }\n    }\n    info.setDocValuesUpdatesFiles(newDVFiles);\n    \n    // wrote new files, should checkpoint()\n    writer.checkpoint();\n\n    // if there is a reader open, reopen it to reflect the updates\n    if (reader != null) {\n      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - info.getDelCount() - pendingDeleteCount);\n      boolean reopened = false;\n      try {\n        reader.decRef();\n        reader = newReader;\n        reopened = true;\n      } finally {\n        if (!reopened) {\n          newReader.decRef();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c1d0b4a65bc57bd59cef05619306aa8ee7431a0f":["06805da26538ed636bd89b10c2699cc3834032ae"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":["06805da26538ed636bd89b10c2699cc3834032ae","c1d0b4a65bc57bd59cef05619306aa8ee7431a0f"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["31741cf1390044e38a2ec3127cf302ba841bfd75"],"b0267c69e2456a3477a1ad785723f2135da3117e":["79700663e164dece87bed4adfd3e28bab6cb1385"],"0567bdc5c86c94ced64201187cfcef2417d76dda":["8a341947b8c97354c225eb5460c7f4b2cf454c0a"],"98d2deb8c96c79ebef084a1f8e5a1a6c08608f13":["0567bdc5c86c94ced64201187cfcef2417d76dda"],"06805da26538ed636bd89b10c2699cc3834032ae":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"b7605579001505896d48b07160075a5c8b8e128e":["8a341947b8c97354c225eb5460c7f4b2cf454c0a","0567bdc5c86c94ced64201187cfcef2417d76dda"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["92212fd254551a0b1156aafc3a1a6ed1a43932ad","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"28288370235ed02234a64753cdbf0c6ec096304a":["31741cf1390044e38a2ec3127cf302ba841bfd75","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"31741cf1390044e38a2ec3127cf302ba841bfd75":["b0267c69e2456a3477a1ad785723f2135da3117e"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["eac6ccb51c439bec7f67cb0e299d3cb77b62b87e","b0267c69e2456a3477a1ad785723f2135da3117e"],"8a341947b8c97354c225eb5460c7f4b2cf454c0a":["c1d0b4a65bc57bd59cef05619306aa8ee7431a0f"],"b06445ae1731e049327712db0454e5643ca9b7fe":["299a2348fa24151d150182211b6208a38e5e3450","b0267c69e2456a3477a1ad785723f2135da3117e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"79700663e164dece87bed4adfd3e28bab6cb1385":["eac6ccb51c439bec7f67cb0e299d3cb77b62b87e"],"a656b32c3aa151037a8c52e9b134acc3cbf482bc":["8a341947b8c97354c225eb5460c7f4b2cf454c0a","0567bdc5c86c94ced64201187cfcef2417d76dda"],"eac6ccb51c439bec7f67cb0e299d3cb77b62b87e":["98d2deb8c96c79ebef084a1f8e5a1a6c08608f13"],"92212fd254551a0b1156aafc3a1a6ed1a43932ad":["b0267c69e2456a3477a1ad785723f2135da3117e"],"299a2348fa24151d150182211b6208a38e5e3450":["eac6ccb51c439bec7f67cb0e299d3cb77b62b87e","79700663e164dece87bed4adfd3e28bab6cb1385"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["28288370235ed02234a64753cdbf0c6ec096304a"]},"commit2Childs":{"c1d0b4a65bc57bd59cef05619306aa8ee7431a0f":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","8a341947b8c97354c225eb5460c7f4b2cf454c0a"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":[],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"b0267c69e2456a3477a1ad785723f2135da3117e":["31741cf1390044e38a2ec3127cf302ba841bfd75","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe","92212fd254551a0b1156aafc3a1a6ed1a43932ad"],"0567bdc5c86c94ced64201187cfcef2417d76dda":["98d2deb8c96c79ebef084a1f8e5a1a6c08608f13","b7605579001505896d48b07160075a5c8b8e128e","a656b32c3aa151037a8c52e9b134acc3cbf482bc"],"06805da26538ed636bd89b10c2699cc3834032ae":["c1d0b4a65bc57bd59cef05619306aa8ee7431a0f","c0cd85fde84cb318b4dc97710dcf15e2959a1bbe"],"98d2deb8c96c79ebef084a1f8e5a1a6c08608f13":["eac6ccb51c439bec7f67cb0e299d3cb77b62b87e"],"b7605579001505896d48b07160075a5c8b8e128e":[],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":[],"28288370235ed02234a64753cdbf0c6ec096304a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"31741cf1390044e38a2ec3127cf302ba841bfd75":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","28288370235ed02234a64753cdbf0c6ec096304a"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"8a341947b8c97354c225eb5460c7f4b2cf454c0a":["0567bdc5c86c94ced64201187cfcef2417d76dda","b7605579001505896d48b07160075a5c8b8e128e","a656b32c3aa151037a8c52e9b134acc3cbf482bc"],"b06445ae1731e049327712db0454e5643ca9b7fe":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["06805da26538ed636bd89b10c2699cc3834032ae"],"79700663e164dece87bed4adfd3e28bab6cb1385":["b0267c69e2456a3477a1ad785723f2135da3117e","299a2348fa24151d150182211b6208a38e5e3450"],"a656b32c3aa151037a8c52e9b134acc3cbf482bc":[],"92212fd254551a0b1156aafc3a1a6ed1a43932ad":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1"],"eac6ccb51c439bec7f67cb0e299d3cb77b62b87e":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","79700663e164dece87bed4adfd3e28bab6cb1385","299a2348fa24151d150182211b6208a38e5e3450"],"299a2348fa24151d150182211b6208a38e5e3450":["b06445ae1731e049327712db0454e5643ca9b7fe"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","b7605579001505896d48b07160075a5c8b8e128e","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe","a656b32c3aa151037a8c52e9b134acc3cbf482bc","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}