{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0791b41f65aecff2e75db0c1ebf95d745a5ab1b1","date":1338332414,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":["38a62612cfa4e104080d89d7751a8f1a258ac335"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d19974432be9aed28ee7dca73bdf01d139e763a9","date":1342822166,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":["1f653cfcf159baeaafe5d01682a911e95bba4012","1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c"],"bugIntro":["11c6df42fb3eba174c3ca0d9a5194eaecd893b77"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","date":1343059585,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aba371508186796cc6151d8223a5b4e16d02e26e","date":1343474871,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7b103c0ed2ba7edf422d1ccb5489815dc6acb84","date":1345973500,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"05a14b2611ead08655a2b2bdc61632eb31316e57","date":1346366621,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"088a7ef694fd43d5d9a4d200c4005865f773d1e7","date":1371136274,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES).setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cd4e13d997cf4fb810398a20a299c2c5a9f6b796","date":1395594336,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES).setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    assertTrue(!slowFileExists(dir, \"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES).setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.shutdown();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES).setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.shutdown();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    assertTrue(!slowFileExists(dir, \"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));\n    writer.shutdown();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES).setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    assertTrue(!slowFileExists(dir, \"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7e2fb55c0777755badd3b46d8140f3d4301febed","date":1398881584,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.shutdown();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergePolicy(NoMergePolicy.INSTANCE).setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.shutdown();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    assertTrue(!slowFileExists(dir, \"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));\n    writer.shutdown();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.shutdown();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES).setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.shutdown();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    assertTrue(!slowFileExists(dir, \"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));\n    writer.shutdown();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.shutdown();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.shutdown();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    assertTrue(!slowFileExists(dir, \"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.shutdown();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.shutdown();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergePolicy(NoMergePolicy.INSTANCE).setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.shutdown();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    assertTrue(!slowFileExists(dir, \"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));\n    writer.shutdown();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    assertTrue(!slowFileExists(dir, \"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.shutdown();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.shutdown();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    assertTrue(!slowFileExists(dir, \"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.shutdown();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"98d2deb8c96c79ebef084a1f8e5a1a6c08608f13","date":1409346855,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n      // ensure we actually delete files\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    assertTrue(!slowFileExists(dir, \"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    assertTrue(!slowFileExists(dir, \"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b11b097f011a298f1a54676482032c2b261e26f3","date":1411698138,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n      // ensure we actually delete files\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = \".liv\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    assertTrue(!slowFileExists(dir, \"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n      // ensure we actually delete files\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    assertTrue(!slowFileExists(dir, \"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5f6bd27530a2846413fe2d00030493c0e2d3a072","date":1411811855,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n      // ensure we actually delete files\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = \".liv\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    assertTrue(!slowFileExists(dir, \"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n      // ensure we actually delete files\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    assertTrue(!slowFileExists(dir, \"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":["b6242d3e8721ca39385f5aaf3c6b660f1d3c3992"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"41307b73b6c5ab4779490d54afb6393c80ba5a3b","date":1412433761,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n      // ensure we actually delete files\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n    \n    // read in index to try to not depend on codec-specific filenames so much\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    SegmentInfo si0 = sis.info(0).info;\n    SegmentInfo si1 = sis.info(1).info;\n    SegmentInfo si3 = sis.info(3).info;\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = \".liv\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    String cfsFiles0[] = si0.getCodec().compoundFormat().files(si0);\n    \n    // Create a bogus segment file:\n    copyFile(dir, cfsFiles0[0], \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, cfsFiles0[0], \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    \n    String cfsFiles3[] = si3.getCodec().compoundFormat().files(si3);\n    for (String f : cfsFiles3) {\n      assertTrue(!slowFileExists(dir, f));\n    }\n    \n    String cfsFiles1[] = si1.getCodec().compoundFormat().files(si1);\n    copyFile(dir, cfsFiles1[0], \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n      // ensure we actually delete files\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = \".liv\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    assertTrue(!slowFileExists(dir, \"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9bb9a29a5e71a90295f175df8919802993142c9a","date":1412517673,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n      // ensure we actually delete files\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n    \n    // read in index to try to not depend on codec-specific filenames so much\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    SegmentInfo si0 = sis.info(0).info;\n    SegmentInfo si1 = sis.info(1).info;\n    SegmentInfo si3 = sis.info(3).info;\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = \".liv\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    String cfsFiles0[] = si0.getCodec().compoundFormat().files(si0);\n    \n    // Create a bogus segment file:\n    copyFile(dir, cfsFiles0[0], \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, cfsFiles0[0], \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    \n    String cfsFiles3[] = si3.getCodec().compoundFormat().files(si3);\n    for (String f : cfsFiles3) {\n      assertTrue(!slowFileExists(dir, f));\n    }\n    \n    String cfsFiles1[] = si1.getCodec().compoundFormat().files(si1);\n    copyFile(dir, cfsFiles1[0], \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n      // ensure we actually delete files\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = \".liv\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    assertTrue(!slowFileExists(dir, \"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3384e6013a93e4d11b7d75388693f8d0388602bf","date":1413951663,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n      // ensure we actually delete files\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n    \n    // read in index to try to not depend on codec-specific filenames so much\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    SegmentInfo si0 = sis.info(0).info;\n    SegmentInfo si1 = sis.info(1).info;\n    SegmentInfo si3 = sis.info(3).info;\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = \".liv\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    String cfsFiles0[] = si0.getCodec().compoundFormat().files(si0);\n    \n    // Create a bogus segment file:\n    copyFile(dir, cfsFiles0[0], \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, cfsFiles0[0], \"_0.fnm\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    \n    String cfsFiles3[] = si3.getCodec().compoundFormat().files(si3);\n    for (String f : cfsFiles3) {\n      assertTrue(!slowFileExists(dir, f));\n    }\n    \n    String cfsFiles1[] = si1.getCodec().compoundFormat().files(si1);\n    copyFile(dir, cfsFiles1[0], \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n      // ensure we actually delete files\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n    \n    // read in index to try to not depend on codec-specific filenames so much\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    SegmentInfo si0 = sis.info(0).info;\n    SegmentInfo si1 = sis.info(1).info;\n    SegmentInfo si3 = sis.info(3).info;\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = \".liv\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    String cfsFiles0[] = si0.getCodec().compoundFormat().files(si0);\n    \n    // Create a bogus segment file:\n    copyFile(dir, cfsFiles0[0], \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, cfsFiles0[0], \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    \n    String cfsFiles3[] = si3.getCodec().compoundFormat().files(si3);\n    for (String f : cfsFiles3) {\n      assertTrue(!slowFileExists(dir, f));\n    }\n    \n    String cfsFiles1[] = si1.getCodec().compoundFormat().files(si1);\n    copyFile(dir, cfsFiles1[0], \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"db68c63cbfaa8698b9c4475f75ed2b9c9696d238","date":1414118621,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n      // ensure we actually delete files\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n    \n    // read in index to try to not depend on codec-specific filenames so much\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    SegmentInfo si0 = sis.info(0).info;\n    SegmentInfo si1 = sis.info(1).info;\n    SegmentInfo si3 = sis.info(3).info;\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = \".liv\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    String cfsFiles0[] = si0.getCodec().compoundFormat().files(si0);\n    \n    // Create a bogus segment file:\n    copyFile(dir, cfsFiles0[0], \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, cfsFiles0[0], \"_0.fnm\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    \n    String cfsFiles3[] = si3.getCodec().compoundFormat().files(si3);\n    for (String f : cfsFiles3) {\n      assertTrue(!slowFileExists(dir, f));\n    }\n    \n    String cfsFiles1[] = si1.getCodec().compoundFormat().files(si1);\n    copyFile(dir, cfsFiles1[0], \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n      // ensure we actually delete files\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n    \n    // read in index to try to not depend on codec-specific filenames so much\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    SegmentInfo si0 = sis.info(0).info;\n    SegmentInfo si1 = sis.info(1).info;\n    SegmentInfo si3 = sis.info(3).info;\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = \".liv\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    String cfsFiles0[] = si0.getCodec().compoundFormat().files(si0);\n    \n    // Create a bogus segment file:\n    copyFile(dir, cfsFiles0[0], \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, cfsFiles0[0], \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    \n    String cfsFiles3[] = si3.getCodec().compoundFormat().files(si3);\n    for (String f : cfsFiles3) {\n      assertTrue(!slowFileExists(dir, f));\n    }\n    \n    String cfsFiles1[] = si1.getCodec().compoundFormat().files(si1);\n    copyFile(dir, cfsFiles1[0], \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3c5705cb93fb3daa46c676cad08b916dd57bf1be","date":1422473298,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n      // ensure we actually delete files\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n    \n    // read in index to try to not depend on codec-specific filenames so much\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    SegmentInfo si0 = sis.info(0).info;\n    SegmentInfo si1 = sis.info(1).info;\n    SegmentInfo si3 = sis.info(3).info;\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = \".liv\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    String cfsFiles0[] = si0.getCodec() instanceof SimpleTextCodec ? new String[] { \"_0.scf\" } : new String[] { \"_0.cfs\", \"_0.cfe\" };\n    \n    // Create a bogus segment file:\n    copyFile(dir, cfsFiles0[0], \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, cfsFiles0[0], \"_0.fnm\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    \n    String cfsFiles3[] = si3.getCodec() instanceof SimpleTextCodec ? new String[] { \"_3.scf\" } : new String[] { \"_3.cfs\", \"_3.cfe\" };\n    for (String f : cfsFiles3) {\n      assertTrue(!slowFileExists(dir, f));\n    }\n    \n    String cfsFiles1[] = si1.getCodec() instanceof SimpleTextCodec ? new String[] { \"_1.scf\" } : new String[] { \"_1.cfs\", \"_1.cfe\" };\n    copyFile(dir, cfsFiles1[0], \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n      // ensure we actually delete files\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n    \n    // read in index to try to not depend on codec-specific filenames so much\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    SegmentInfo si0 = sis.info(0).info;\n    SegmentInfo si1 = sis.info(1).info;\n    SegmentInfo si3 = sis.info(3).info;\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = \".liv\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    String cfsFiles0[] = si0.getCodec().compoundFormat().files(si0);\n    \n    // Create a bogus segment file:\n    copyFile(dir, cfsFiles0[0], \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, cfsFiles0[0], \"_0.fnm\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    \n    String cfsFiles3[] = si3.getCodec().compoundFormat().files(si3);\n    for (String f : cfsFiles3) {\n      assertTrue(!slowFileExists(dir, f));\n    }\n    \n    String cfsFiles1[] = si1.getCodec().compoundFormat().files(si1);\n    copyFile(dir, cfsFiles1[0], \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6bfe104fc023fadc9e709f8d17403d2cc61133fe","date":1454446396,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n    \n    // read in index to try to not depend on codec-specific filenames so much\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    SegmentInfo si0 = sis.info(0).info;\n    SegmentInfo si1 = sis.info(1).info;\n    SegmentInfo si3 = sis.info(3).info;\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = \".liv\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    String cfsFiles0[] = si0.getCodec() instanceof SimpleTextCodec ? new String[] { \"_0.scf\" } : new String[] { \"_0.cfs\", \"_0.cfe\" };\n    \n    // Create a bogus segment file:\n    copyFile(dir, cfsFiles0[0], \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, cfsFiles0[0], \"_0.fnm\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    \n    String cfsFiles3[] = si3.getCodec() instanceof SimpleTextCodec ? new String[] { \"_3.scf\" } : new String[] { \"_3.cfs\", \"_3.cfe\" };\n    for (String f : cfsFiles3) {\n      assertTrue(!slowFileExists(dir, f));\n    }\n    \n    String cfsFiles1[] = si1.getCodec() instanceof SimpleTextCodec ? new String[] { \"_1.scf\" } : new String[] { \"_1.cfs\", \"_1.cfe\" };\n    copyFile(dir, cfsFiles1[0], \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n      // ensure we actually delete files\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n    \n    // read in index to try to not depend on codec-specific filenames so much\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    SegmentInfo si0 = sis.info(0).info;\n    SegmentInfo si1 = sis.info(1).info;\n    SegmentInfo si3 = sis.info(3).info;\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = \".liv\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    String cfsFiles0[] = si0.getCodec() instanceof SimpleTextCodec ? new String[] { \"_0.scf\" } : new String[] { \"_0.cfs\", \"_0.cfe\" };\n    \n    // Create a bogus segment file:\n    copyFile(dir, cfsFiles0[0], \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, cfsFiles0[0], \"_0.fnm\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    \n    String cfsFiles3[] = si3.getCodec() instanceof SimpleTextCodec ? new String[] { \"_3.scf\" } : new String[] { \"_3.cfs\", \"_3.cfe\" };\n    for (String f : cfsFiles3) {\n      assertTrue(!slowFileExists(dir, f));\n    }\n    \n    String cfsFiles1[] = si1.getCodec() instanceof SimpleTextCodec ? new String[] { \"_1.scf\" } : new String[] { \"_1.cfs\", \"_1.cfe\" };\n    copyFile(dir, cfsFiles1[0], \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b470f36a9372c97283360b1304eacbde22df6c0d","date":1454765175,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n    \n    // read in index to try to not depend on codec-specific filenames so much\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    SegmentInfo si0 = sis.info(0).info;\n    SegmentInfo si1 = sis.info(1).info;\n    SegmentInfo si3 = sis.info(3).info;\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = \".liv\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    String cfsFiles0[] = si0.getCodec() instanceof SimpleTextCodec ? new String[] { \"_0.scf\" } : new String[] { \"_0.cfs\", \"_0.cfe\" };\n    \n    // Create a bogus segment file:\n    copyFile(dir, cfsFiles0[0], \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, cfsFiles0[0], \"_0.fnm\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    \n    String cfsFiles3[] = si3.getCodec() instanceof SimpleTextCodec ? new String[] { \"_3.scf\" } : new String[] { \"_3.cfs\", \"_3.cfe\" };\n    for (String f : cfsFiles3) {\n      assertTrue(!slowFileExists(dir, f));\n    }\n    \n    String cfsFiles1[] = si1.getCodec() instanceof SimpleTextCodec ? new String[] { \"_1.scf\" } : new String[] { \"_1.cfs\", \"_1.cfe\" };\n    copyFile(dir, cfsFiles1[0], \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n      // ensure we actually delete files\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n    \n    // read in index to try to not depend on codec-specific filenames so much\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    SegmentInfo si0 = sis.info(0).info;\n    SegmentInfo si1 = sis.info(1).info;\n    SegmentInfo si3 = sis.info(3).info;\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = \".liv\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    String cfsFiles0[] = si0.getCodec() instanceof SimpleTextCodec ? new String[] { \"_0.scf\" } : new String[] { \"_0.cfs\", \"_0.cfe\" };\n    \n    // Create a bogus segment file:\n    copyFile(dir, cfsFiles0[0], \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, cfsFiles0[0], \"_0.fnm\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    \n    String cfsFiles3[] = si3.getCodec() instanceof SimpleTextCodec ? new String[] { \"_3.scf\" } : new String[] { \"_3.cfs\", \"_3.cfe\" };\n    for (String f : cfsFiles3) {\n      assertTrue(!slowFileExists(dir, f));\n    }\n    \n    String cfsFiles1[] = si1.getCodec() instanceof SimpleTextCodec ? new String[] { \"_1.scf\" } : new String[] { \"_1.cfs\", \"_1.cfe\" };\n    copyFile(dir, cfsFiles1[0], \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1e6acbaae7af722f17204ceccf0f7db5753eccf3","date":1454775255,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n    \n    // read in index to try to not depend on codec-specific filenames so much\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    SegmentInfo si0 = sis.info(0).info;\n    SegmentInfo si1 = sis.info(1).info;\n    SegmentInfo si3 = sis.info(3).info;\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = \".liv\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    String cfsFiles0[] = si0.getCodec() instanceof SimpleTextCodec ? new String[] { \"_0.scf\" } : new String[] { \"_0.cfs\", \"_0.cfe\" };\n    \n    // Create a bogus segment file:\n    copyFile(dir, cfsFiles0[0], \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, cfsFiles0[0], \"_0.fnm\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    \n    String cfsFiles3[] = si3.getCodec() instanceof SimpleTextCodec ? new String[] { \"_3.scf\" } : new String[] { \"_3.cfs\", \"_3.cfe\" };\n    for (String f : cfsFiles3) {\n      assertTrue(!slowFileExists(dir, f));\n    }\n    \n    String cfsFiles1[] = si1.getCodec() instanceof SimpleTextCodec ? new String[] { \"_1.scf\" } : new String[] { \"_1.cfs\", \"_1.cfe\" };\n    copyFile(dir, cfsFiles1[0], \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n      // ensure we actually delete files\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n    \n    // read in index to try to not depend on codec-specific filenames so much\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    SegmentInfo si0 = sis.info(0).info;\n    SegmentInfo si1 = sis.info(1).info;\n    SegmentInfo si3 = sis.info(3).info;\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = \".liv\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    String cfsFiles0[] = si0.getCodec() instanceof SimpleTextCodec ? new String[] { \"_0.scf\" } : new String[] { \"_0.cfs\", \"_0.cfe\" };\n    \n    // Create a bogus segment file:\n    copyFile(dir, cfsFiles0[0], \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, cfsFiles0[0], \"_0.fnm\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    \n    String cfsFiles3[] = si3.getCodec() instanceof SimpleTextCodec ? new String[] { \"_3.scf\" } : new String[] { \"_3.cfs\", \"_3.cfe\" };\n    for (String f : cfsFiles3) {\n      assertTrue(!slowFileExists(dir, f));\n    }\n    \n    String cfsFiles1[] = si1.getCodec() instanceof SimpleTextCodec ? new String[] { \"_1.scf\" } : new String[] { \"_1.cfs\", \"_1.cfe\" };\n    copyFile(dir, cfsFiles1[0], \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a207d19eac354d649c3f0e2cce070017c78125e","date":1454776470,"type":3,"author":"Erick Erickson","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n    \n    // read in index to try to not depend on codec-specific filenames so much\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    SegmentInfo si0 = sis.info(0).info;\n    SegmentInfo si1 = sis.info(1).info;\n    SegmentInfo si3 = sis.info(3).info;\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = \".liv\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    String cfsFiles0[] = si0.getCodec() instanceof SimpleTextCodec ? new String[] { \"_0.scf\" } : new String[] { \"_0.cfs\", \"_0.cfe\" };\n    \n    // Create a bogus segment file:\n    copyFile(dir, cfsFiles0[0], \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, cfsFiles0[0], \"_0.fnm\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    \n    String cfsFiles3[] = si3.getCodec() instanceof SimpleTextCodec ? new String[] { \"_3.scf\" } : new String[] { \"_3.cfs\", \"_3.cfe\" };\n    for (String f : cfsFiles3) {\n      assertTrue(!slowFileExists(dir, f));\n    }\n    \n    String cfsFiles1[] = si1.getCodec() instanceof SimpleTextCodec ? new String[] { \"_1.scf\" } : new String[] { \"_1.cfs\", \"_1.cfe\" };\n    copyFile(dir, cfsFiles1[0], \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n      // ensure we actually delete files\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n    \n    // read in index to try to not depend on codec-specific filenames so much\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    SegmentInfo si0 = sis.info(0).info;\n    SegmentInfo si1 = sis.info(1).info;\n    SegmentInfo si3 = sis.info(3).info;\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = \".liv\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    String cfsFiles0[] = si0.getCodec() instanceof SimpleTextCodec ? new String[] { \"_0.scf\" } : new String[] { \"_0.cfs\", \"_0.cfe\" };\n    \n    // Create a bogus segment file:\n    copyFile(dir, cfsFiles0[0], \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, cfsFiles0[0], \"_0.fnm\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    \n    String cfsFiles3[] = si3.getCodec() instanceof SimpleTextCodec ? new String[] { \"_3.scf\" } : new String[] { \"_3.cfs\", \"_3.cfe\" };\n    for (String f : cfsFiles3) {\n      assertTrue(!slowFileExists(dir, f));\n    }\n    \n    String cfsFiles1[] = si1.getCodec() instanceof SimpleTextCodec ? new String[] { \"_1.scf\" } : new String[] { \"_1.cfs\", \"_1.cfe\" };\n    copyFile(dir, cfsFiles1[0], \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"11c6df42fb3eba174c3ca0d9a5194eaecd893b77","date":1465931757,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n    \n    // read in index to try to not depend on codec-specific filenames so much\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    SegmentInfo si0 = sis.info(0).info;\n    SegmentInfo si1 = sis.info(1).info;\n    SegmentInfo si3 = sis.info(3).info;\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = \".liv\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    String cfsFiles0[] = si0.getCodec() instanceof SimpleTextCodec ? new String[] { \"_0.scf\" } : new String[] { \"_0.cfs\", \"_0.cfe\" };\n    \n    // Create a bogus segment file:\n    copyFile(dir, cfsFiles0[0], \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, cfsFiles0[0], \"_0.fnm\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    \n    String cfsFiles3[] = si3.getCodec() instanceof SimpleTextCodec ? new String[] { \"_3.scf\" } : new String[] { \"_3.cfs\", \"_3.cfe\" };\n    for (String f : cfsFiles3) {\n      assertTrue(!slowFileExists(dir, f));\n    }\n    \n    String cfsFiles1[] = si1.getCodec() instanceof SimpleTextCodec ? new String[] { \"_1.scf\" } : new String[] { \"_1.cfs\", \"_1.cfe\" };\n    copyFile(dir, cfsFiles1[0], \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n    \n    // read in index to try to not depend on codec-specific filenames so much\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    SegmentInfo si0 = sis.info(0).info;\n    SegmentInfo si1 = sis.info(1).info;\n    SegmentInfo si3 = sis.info(3).info;\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = \".liv\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    String cfsFiles0[] = si0.getCodec() instanceof SimpleTextCodec ? new String[] { \"_0.scf\" } : new String[] { \"_0.cfs\", \"_0.cfe\" };\n    \n    // Create a bogus segment file:\n    copyFile(dir, cfsFiles0[0], \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, cfsFiles0[0], \"_0.fnm\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    \n    String cfsFiles3[] = si3.getCodec() instanceof SimpleTextCodec ? new String[] { \"_3.scf\" } : new String[] { \"_3.cfs\", \"_3.cfe\" };\n    for (String f : cfsFiles3) {\n      assertTrue(!slowFileExists(dir, f));\n    }\n    \n    String cfsFiles1[] = si1.getCodec() instanceof SimpleTextCodec ? new String[] { \"_1.scf\" } : new String[] { \"_1.cfs\", \"_1.cfe\" };\n    copyFile(dir, cfsFiles1[0], \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":["d19974432be9aed28ee7dca73bdf01d139e763a9"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n    \n    // read in index to try to not depend on codec-specific filenames so much\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    SegmentInfo si0 = sis.info(0).info;\n    SegmentInfo si1 = sis.info(1).info;\n    SegmentInfo si3 = sis.info(3).info;\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = \".liv\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    String cfsFiles0[] = si0.getCodec() instanceof SimpleTextCodec ? new String[] { \"_0.scf\" } : new String[] { \"_0.cfs\", \"_0.cfe\" };\n    \n    // Create a bogus segment file:\n    copyFile(dir, cfsFiles0[0], \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, cfsFiles0[0], \"_0.fnm\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    \n    String cfsFiles3[] = si3.getCodec() instanceof SimpleTextCodec ? new String[] { \"_3.scf\" } : new String[] { \"_3.cfs\", \"_3.cfe\" };\n    for (String f : cfsFiles3) {\n      assertTrue(!slowFileExists(dir, f));\n    }\n    \n    String cfsFiles1[] = si1.getCodec() instanceof SimpleTextCodec ? new String[] { \"_1.scf\" } : new String[] { \"_1.cfs\", \"_1.cfe\" };\n    copyFile(dir, cfsFiles1[0], \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setPreventDoubleWrite(false);\n    }\n\n    MergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    \n    // This test expects all of its segments to be in CFS\n    mergePolicy.setNoCFSRatio(1.0);\n    mergePolicy.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMaxBufferedDocs(10)\n            .setMergePolicy(mergePolicy).setUseCompoundFile(true)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    writer.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n    writer.getConfig().setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n            .setMergePolicy(NoMergePolicy.INSTANCE)\n            .setUseCompoundFile(true)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n    \n    // read in index to try to not depend on codec-specific filenames so much\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    SegmentInfo si0 = sis.info(0).info;\n    SegmentInfo si1 = sis.info(1).info;\n    SegmentInfo si3 = sis.info(3).info;\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = \".liv\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    String cfsFiles0[] = si0.getCodec() instanceof SimpleTextCodec ? new String[] { \"_0.scf\" } : new String[] { \"_0.cfs\", \"_0.cfe\" };\n    \n    // Create a bogus segment file:\n    copyFile(dir, cfsFiles0[0], \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, cfsFiles0[0], \"_0.fnm\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(slowFileExists(dir, \"_3.fdt\") || slowFileExists(dir, \"_3.fld\"));\n    \n    String cfsFiles3[] = si3.getCodec() instanceof SimpleTextCodec ? new String[] { \"_3.scf\" } : new String[] { \"_3.cfs\", \"_3.cfe\" };\n    for (String f : cfsFiles3) {\n      assertTrue(!slowFileExists(dir, f));\n    }\n    \n    String cfsFiles1[] = si1.getCodec() instanceof SimpleTextCodec ? new String[] { \"_1.scf\" } : new String[] { \"_1.cfs\", \"_1.cfe\" };\n    copyFile(dir, cfsFiles1[0], \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                    .setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7e2fb55c0777755badd3b46d8140f3d4301febed":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"41307b73b6c5ab4779490d54afb6393c80ba5a3b":["b11b097f011a298f1a54676482032c2b261e26f3"],"3c5705cb93fb3daa46c676cad08b916dd57bf1be":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5a207d19eac354d649c3f0e2cce070017c78125e":["3c5705cb93fb3daa46c676cad08b916dd57bf1be","b470f36a9372c97283360b1304eacbde22df6c0d"],"5f6bd27530a2846413fe2d00030493c0e2d3a072":["98d2deb8c96c79ebef084a1f8e5a1a6c08608f13","b11b097f011a298f1a54676482032c2b261e26f3"],"11c6df42fb3eba174c3ca0d9a5194eaecd893b77":["5a207d19eac354d649c3f0e2cce070017c78125e"],"98d2deb8c96c79ebef084a1f8e5a1a6c08608f13":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"b11b097f011a298f1a54676482032c2b261e26f3":["98d2deb8c96c79ebef084a1f8e5a1a6c08608f13"],"aba371508186796cc6151d8223a5b4e16d02e26e":["0791b41f65aecff2e75db0c1ebf95d745a5ab1b1","d19974432be9aed28ee7dca73bdf01d139e763a9"],"9bb9a29a5e71a90295f175df8919802993142c9a":["5f6bd27530a2846413fe2d00030493c0e2d3a072","41307b73b6c5ab4779490d54afb6393c80ba5a3b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"6bfe104fc023fadc9e709f8d17403d2cc61133fe":["3c5705cb93fb3daa46c676cad08b916dd57bf1be"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["cd4e13d997cf4fb810398a20a299c2c5a9f6b796"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":["0791b41f65aecff2e75db0c1ebf95d745a5ab1b1","d19974432be9aed28ee7dca73bdf01d139e763a9"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["7e2fb55c0777755badd3b46d8140f3d4301febed"],"cd4e13d997cf4fb810398a20a299c2c5a9f6b796":["088a7ef694fd43d5d9a4d200c4005865f773d1e7"],"1e6acbaae7af722f17204ceccf0f7db5753eccf3":["3c5705cb93fb3daa46c676cad08b916dd57bf1be","b470f36a9372c97283360b1304eacbde22df6c0d"],"0791b41f65aecff2e75db0c1ebf95d745a5ab1b1":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"3384e6013a93e4d11b7d75388693f8d0388602bf":["41307b73b6c5ab4779490d54afb6393c80ba5a3b"],"05a14b2611ead08655a2b2bdc61632eb31316e57":["aba371508186796cc6151d8223a5b4e16d02e26e","e7b103c0ed2ba7edf422d1ccb5489815dc6acb84"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["5a207d19eac354d649c3f0e2cce070017c78125e","11c6df42fb3eba174c3ca0d9a5194eaecd893b77"],"088a7ef694fd43d5d9a4d200c4005865f773d1e7":["e7b103c0ed2ba7edf422d1ccb5489815dc6acb84"],"e7b103c0ed2ba7edf422d1ccb5489815dc6acb84":["d19974432be9aed28ee7dca73bdf01d139e763a9"],"d19974432be9aed28ee7dca73bdf01d139e763a9":["0791b41f65aecff2e75db0c1ebf95d745a5ab1b1"],"b470f36a9372c97283360b1304eacbde22df6c0d":["3c5705cb93fb3daa46c676cad08b916dd57bf1be","6bfe104fc023fadc9e709f8d17403d2cc61133fe"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"db68c63cbfaa8698b9c4475f75ed2b9c9696d238":["9bb9a29a5e71a90295f175df8919802993142c9a","3384e6013a93e4d11b7d75388693f8d0388602bf"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["11c6df42fb3eba174c3ca0d9a5194eaecd893b77"]},"commit2Childs":{"7e2fb55c0777755badd3b46d8140f3d4301febed":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"41307b73b6c5ab4779490d54afb6393c80ba5a3b":["9bb9a29a5e71a90295f175df8919802993142c9a","3384e6013a93e4d11b7d75388693f8d0388602bf"],"3c5705cb93fb3daa46c676cad08b916dd57bf1be":["5a207d19eac354d649c3f0e2cce070017c78125e","6bfe104fc023fadc9e709f8d17403d2cc61133fe","1e6acbaae7af722f17204ceccf0f7db5753eccf3","b470f36a9372c97283360b1304eacbde22df6c0d"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"5a207d19eac354d649c3f0e2cce070017c78125e":["11c6df42fb3eba174c3ca0d9a5194eaecd893b77","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"5f6bd27530a2846413fe2d00030493c0e2d3a072":["9bb9a29a5e71a90295f175df8919802993142c9a"],"98d2deb8c96c79ebef084a1f8e5a1a6c08608f13":["5f6bd27530a2846413fe2d00030493c0e2d3a072","b11b097f011a298f1a54676482032c2b261e26f3"],"11c6df42fb3eba174c3ca0d9a5194eaecd893b77":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"b11b097f011a298f1a54676482032c2b261e26f3":["41307b73b6c5ab4779490d54afb6393c80ba5a3b","5f6bd27530a2846413fe2d00030493c0e2d3a072"],"aba371508186796cc6151d8223a5b4e16d02e26e":["05a14b2611ead08655a2b2bdc61632eb31316e57"],"9bb9a29a5e71a90295f175df8919802993142c9a":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"6bfe104fc023fadc9e709f8d17403d2cc61133fe":["b470f36a9372c97283360b1304eacbde22df6c0d"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["7e2fb55c0777755badd3b46d8140f3d4301febed"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":[],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"cd4e13d997cf4fb810398a20a299c2c5a9f6b796":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"1e6acbaae7af722f17204ceccf0f7db5753eccf3":[],"0791b41f65aecff2e75db0c1ebf95d745a5ab1b1":["aba371508186796cc6151d8223a5b4e16d02e26e","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","d19974432be9aed28ee7dca73bdf01d139e763a9"],"3384e6013a93e4d11b7d75388693f8d0388602bf":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"05a14b2611ead08655a2b2bdc61632eb31316e57":[],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"088a7ef694fd43d5d9a4d200c4005865f773d1e7":["cd4e13d997cf4fb810398a20a299c2c5a9f6b796"],"e7b103c0ed2ba7edf422d1ccb5489815dc6acb84":["05a14b2611ead08655a2b2bdc61632eb31316e57","088a7ef694fd43d5d9a4d200c4005865f773d1e7"],"d19974432be9aed28ee7dca73bdf01d139e763a9":["aba371508186796cc6151d8223a5b4e16d02e26e","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","e7b103c0ed2ba7edf422d1ccb5489815dc6acb84"],"b470f36a9372c97283360b1304eacbde22df6c0d":["5a207d19eac354d649c3f0e2cce070017c78125e","1e6acbaae7af722f17204ceccf0f7db5753eccf3"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["98d2deb8c96c79ebef084a1f8e5a1a6c08608f13"],"db68c63cbfaa8698b9c4475f75ed2b9c9696d238":["3c5705cb93fb3daa46c676cad08b916dd57bf1be"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["0791b41f65aecff2e75db0c1ebf95d745a5ab1b1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","1e6acbaae7af722f17204ceccf0f7db5753eccf3","05a14b2611ead08655a2b2bdc61632eb31316e57","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}