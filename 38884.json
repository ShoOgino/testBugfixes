{"path":"solr/core/src/java/org/apache/solr/handler/component/PhrasesIdentificationComponent.Phrase#computeFieldScore(Phrase,String,int,int).mjava","commits":[{"id":"0d1411e62d30c460b09c6f3643df82daa10a27cc","date":1536256256,"type":0,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/PhrasesIdentificationComponent.Phrase#computeFieldScore(Phrase,String,int,int).mjava","pathOld":"/dev/null","sourceNew":"    /** \n     * Uses the previously popuated stats to compute a score for the specified field.\n     *\n     * <p>\n     * The current implementation returns scores in the range of <code>[0,1]</code>, but this \n     * may change in future implementations.  The only current garuntees are:\n     * </p>\n     * \n     * <ul>\n     * <li>0 (or less) means this is garunteed to not be a phrase</li>\n     * <li>larger numbers are higher confidence</li>\n     * </li>\n     * \n     * @see #populateStats\n     * @see #populateScores\n     * @see #getFieldScore(String)\n     * @return a score value\n     */\n    private static double computeFieldScore(final Phrase input,\n                                            final String field,\n                                            final int maxIndexedPositionLength,\n                                            final int maxQueryPositionLength) {\n      final long num_indexed_sub_phrases = input.getLargestIndexedSubPhrases().size();\n      assert 0 <= num_indexed_sub_phrases; // should be impossible\n\n      if (input.getIndividualIndexedTerms().size() < input.getPositionLength()) {\n        // there are \"gaps\" in our input, where individual words have not been indexed (stop words, \n        // or multivalue position gap) which means we are not a viable candidate for being a valid Phrase.\n        return -1.0D;\n      }\n      \n      final long phrase_conj_count = input.getConjunctionDocCount(field);\n      // if there isn't a single document containing all the terms in our\n      // phrase, then it is 100% not a phrase\n      if (phrase_conj_count <= 0) {\n        return -1.0D;\n      }\n      \n      // single words automatically score 0.0 (unless they already scored less for not existing\n      if (input.getPositionLength() <= 1) {\n        return 0.0D;\n      }\n      \n      double field_score = 0.0D;\n      long max_sub_conj_count = phrase_conj_count;\n      \n      // At the moment, the contribution of each \"words\" sub-Phrase to the field score to the input\n      // Phrase is independent of any context of \"input\".  Depending on if/how sub-phrase scoring\n      // changes, we might consider computing the scores of all the indexed phrases first, and\n      // aching the portions of their values that are re-used when computing the scores of\n      // longer phrases?\n      //\n      // This would make the overall scoring of all phrases a lot more complicated,\n      // but could save CPU cycles? \n      // (particularly when maxIndexedPositionLength <<< maxQueryPositionLength ???)\n      //\n      // My gut says that knowing the conj_count(input) \"context\" should help us score the \n      // sub-phrases better, but i can't yet put my finger on why/how.  maybe by comparing\n      // the conj_count(input) to the max(conj_count(parent of words)) ?\n      \n      // for each of the longest indexed phrases, aka indexed sub-sequence of \"words\", we have...\n      for (Phrase words : input.getLargestIndexedSubPhrases()) {\n        // we're going to compute scores in range of [-1:1] to indicate the likelihood that our\n        // \"words\" should be used as a \"phrase\", based on a bayesian document categorization model,\n        // where the \"words as a phrase\" (aka: phrase) is our candidate category.\n        //\n        //  P(words|phrase) * P(phrase) - P(words|not phrase) * P(not phrase)\n        //\n        // Where...\n        //  P(words|phrase)     =  phrase_ttf / min(word_ttf)\n        //  P(phrase)           =~ phrase_docFreq / conj_count(words in phrase)      *SEE NOTE BELOW*\n        //  P(words|not phrase) =  phrase_ttf / max(word_ttf) \n        //  P(not a phrase)     =  1 - P(phrase)\n        //\n        //       ... BUT! ...\n        //\n        // NOTE: we're going to reduce our \"P(phrase) by the max \"P(phrase)\" of all the (indexed)\n        // candidate phrases we are a sub-phrase of, to try to offset the inherent bias in favor \n        // of small indexed phrases -- because anytime the super-phrase exists, the sub-phrase exists\n\n        \n        // IDEA: consider replacing this entire baysian model with LLR (or rootLLR)...\n        //  http://mahout.apache.org/docs/0.13.0/api/docs/mahout-math/org/apache/mahout/math/stats/LogLikelihood.html\n        // ...where we compute LLR over each of the TTF of the pairs of adjacent sub-phrases of each \n        // indexed phrase and take the min|max|avg of the LLR scores.\n        //\n        // ie: for indexed shingle \"quick brown fox\" compute LLR(ttf(\"quick\"), ttf(\"brown fox\")) &\n        // LLR(ttf(\"quick brown\"), ttf(\"fox\")) using ttf(\"quick brown fox\") as the co-occurance\n        // count, and sumTTF-ttf(\"quick\")-ttf(\"brown\")-ttf(\"fox\") as the \"something else\"\n        //\n        // (we could actually compute LLR stats over TTF and DF and combine them)\n        //\n        // NOTE: Going the LLR/rootLLR route would require building a full \"tree\" of every (indexed)\n        // sub-phrase of every other phrase (or at least: all siblings of diff sizes that add up to\n        // an existing phrase).  As well as require us to give up on a predictible \"range\" of\n        // legal values for scores (IIUC from the LLR docs)\n        \n        final long phrase_ttf = words.getTTF(field);\n        final long phrase_df = words.getDocFreq(field);\n        final long words_conj_count = words.getConjunctionDocCount(field);\n        max_sub_conj_count = Math.max(words_conj_count, max_sub_conj_count);\n        \n        final double max_wrapper_phrase_probability = \n          words.getIndexedSuperPhrases().stream()\n          .mapToDouble(p -> p.getConjunctionDocCount(field) <= 0 ?\n                       // special case check -- we already know *our* conj count > 0,\n                       // but we need a similar check for wrapper phrases: if <= 0, their probability is 0\n                       0.0D : ((double)p.getDocFreq(field) / p.getConjunctionDocCount(field))).max().orElse(0.0D);\n        \n        final LongSummaryStatistics words_ttfs = \n          words.getIndividualIndexedTerms().stream()\n          .collect(Collectors.summarizingLong(t -> t.getTTF(field)));\n        \n        final double words_phrase_prob = (phrase_ttf / (double)words_ttfs.getMin());\n        final double words_not_phrase_prob = (phrase_ttf / (double)words_ttfs.getMax());\n        \n        final double phrase_prob = (phrase_conj_count / (double)words_conj_count);\n        \n          \n        final double phrase_score = words_phrase_prob * (phrase_prob - max_wrapper_phrase_probability);\n        final double not_phrase_score =  words_not_phrase_prob * (1 - (phrase_prob - max_wrapper_phrase_probability));\n        final double words_score = phrase_score - not_phrase_score;\n        \n        field_score += words_score;\n      }\n\n      // NOTE: the \"scaling\" factors below can \"increase\" negative scores (by reducing the unsigned value)\n      // when they should ideally be penalizing the scores further, but since we currently don't care\n      // about any score lower then 0, it's not worth worrying about.\n      \n      // Average the accumulated score over the number of actual indexed sub-phrases that contributed\n      //\n      // NOTE: since we subsequently want to multiply the score by a fraction with num_indexed_sub_phrases\n      // in the numerator, we can skip this...\n      // SEE BELOW // field_score /= (double) num_indexed_sub_phrases;\n      \n      // If we leave field_score as is, then a phrase longer then the maxIndexedPositionLength\n      // will never score higher then the highest scoring sub-phrase it has (because we've averaged them)\n      // so we scale the scores against the longest possible phrase length we're considering\n      //\n      // NOTE: We don't use num_indexed_sub_phrases in the numerator since we skipped it when\n      // averating above...\n      field_score *= ( 1.0D // SEE ABOVE // * ( (double)num_indexed_sub_phrases )\n                       / (1 + maxQueryPositionLength - maxIndexedPositionLength) );\n      \n      // scale the field_score based on the ratio of the conjunction docCount for the whole phrase\n      // realtive to the largest conjunction docCount of it's (largest indexed) sub phrases, to penalize\n      // the scores of very long phrases that exist very rarely relative to the how often their\n      // sub phrases exist in the index\n      field_score *= ( ((double) phrase_conj_count) / max_sub_conj_count);\n\n      return field_score;\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0d1411e62d30c460b09c6f3643df82daa10a27cc":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0d1411e62d30c460b09c6f3643df82daa10a27cc"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0d1411e62d30c460b09c6f3643df82daa10a27cc"],"0d1411e62d30c460b09c6f3643df82daa10a27cc":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}