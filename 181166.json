{"path":"sandbox/contributions/similarity/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map,String).mjava","commits":[{"id":"0d57805a939f8923b86ca8202ccef1f7cf548098","date":1105995946,"type":0,"author":"David Spencer","isMerge":false,"pathNew":"sandbox/contributions/similarity/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map,String).mjava","pathOld":"/dev/null","sourceNew":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\torg.apache.lucene.analysis.Token token;\n\t\t\tint tokenCount=0;\n\t\t\twhile ((token = ts.next()) != null) { // for every token\n\t\t\t\tString word = token.termText();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = (Int) termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aa8dcd4adc562c6e8da67c42872ef9194f8598fa","date":1108145465,"type":5,"author":"Erik Hatcher","isMerge":false,"pathNew":"contrib/similarity/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map,String).mjava","pathOld":"sandbox/contributions/similarity/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map,String).mjava","sourceNew":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\torg.apache.lucene.analysis.Token token;\n\t\t\tint tokenCount=0;\n\t\t\twhile ((token = ts.next()) != null) { // for every token\n\t\t\t\tString word = token.termText();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = (Int) termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","sourceOld":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\torg.apache.lucene.analysis.Token token;\n\t\t\tint tokenCount=0;\n\t\t\twhile ((token = ts.next()) != null) { // for every token\n\t\t\t\tString word = token.termText();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = (Int) termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0d57805a939f8923b86ca8202ccef1f7cf548098":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"aa8dcd4adc562c6e8da67c42872ef9194f8598fa":["0d57805a939f8923b86ca8202ccef1f7cf548098"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["aa8dcd4adc562c6e8da67c42872ef9194f8598fa"]},"commit2Childs":{"0d57805a939f8923b86ca8202ccef1f7cf548098":["aa8dcd4adc562c6e8da67c42872ef9194f8598fa"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0d57805a939f8923b86ca8202ccef1f7cf548098"],"aa8dcd4adc562c6e8da67c42872ef9194f8598fa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}