{"path":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40SegmentTermsEnumFrame#loadBlock().mjava","commits":[{"id":"0628077afea69a2955260949478afabab8e500d8","date":1413915332,"type":0,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40SegmentTermsEnumFrame#loadBlock().mjava","pathOld":"/dev/null","sourceNew":"  /* Does initial decode of next block of terms; this\n     doesn't actually decode the docFreq, totalTermFreq,\n     postings details (frq/prx offset, etc.) metadata;\n     it just loads them as byte[] blobs which are then      \n     decoded on-demand if the metadata is ever requested\n     for any term in this block.  This enables terms-only\n     intensive consumes (eg certain MTQs, respelling) to\n     not pay the price of decoding metadata they won't\n     use. */\n  void loadBlock() throws IOException {\n\n    // Clone the IndexInput lazily, so that consumers\n    // that just pull a TermsEnum to\n    // seekExact(TermState) don't pay this cost:\n    ste.initIndexInput();\n\n    if (nextEnt != -1) {\n      // Already loaded\n      return;\n    }\n    //System.out.println(\"blc=\" + blockLoadCount);\n\n    ste.in.seek(fp);\n    int code = ste.in.readVInt();\n    entCount = code >>> 1;\n    assert entCount > 0;\n    isLastInFloor = (code & 1) != 0;\n\n    assert arc == null || (isLastInFloor || isFloor): \"fp=\" + fp + \" arc=\" + arc + \" isFloor=\" + isFloor + \" isLastInFloor=\" + isLastInFloor;\n\n    // TODO: if suffixes were stored in random-access\n    // array structure, then we could do binary search\n    // instead of linear scan to find target term; eg\n    // we could have simple array of offsets\n\n    // term suffixes:\n    code = ste.in.readVInt();\n    isLeafBlock = (code & 1) != 0;\n    int numBytes = code >>> 1;\n    if (suffixBytes.length < numBytes) {\n      suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(suffixBytes, 0, numBytes);\n    suffixesReader.reset(suffixBytes, 0, numBytes);\n\n    /*if (DEBUG) {\n      if (arc == null) {\n      System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      } else {\n      System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      }\n      }*/\n\n    // stats\n    numBytes = ste.in.readVInt();\n    if (statBytes.length < numBytes) {\n      statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(statBytes, 0, numBytes);\n    statsReader.reset(statBytes, 0, numBytes);\n    metaDataUpto = 0;\n\n    state.termBlockOrd = 0;\n    nextEnt = 0;\n    lastSubFP = -1;\n\n    // TODO: we could skip this if !hasTerms; but\n    // that's rare so won't help much\n    // metadata\n    numBytes = ste.in.readVInt();\n    if (bytes == null) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n      bytesReader = new ByteArrayDataInput();\n    } else if (bytes.length < numBytes) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(bytes, 0, numBytes);\n    bytesReader.reset(bytes, 0, numBytes);\n\n\n    // Sub-blocks of a single floor block are always\n    // written one after another -- tail recurse:\n    fpEnd = ste.in.getFilePointer();\n    // if (DEBUG) {\n    //   System.out.println(\"      fpEnd=\" + fpEnd);\n    // }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"db68c63cbfaa8698b9c4475f75ed2b9c9696d238","date":1414118621,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40SegmentTermsEnumFrame#loadBlock().mjava","pathOld":"/dev/null","sourceNew":"  /* Does initial decode of next block of terms; this\n     doesn't actually decode the docFreq, totalTermFreq,\n     postings details (frq/prx offset, etc.) metadata;\n     it just loads them as byte[] blobs which are then      \n     decoded on-demand if the metadata is ever requested\n     for any term in this block.  This enables terms-only\n     intensive consumes (eg certain MTQs, respelling) to\n     not pay the price of decoding metadata they won't\n     use. */\n  void loadBlock() throws IOException {\n\n    // Clone the IndexInput lazily, so that consumers\n    // that just pull a TermsEnum to\n    // seekExact(TermState) don't pay this cost:\n    ste.initIndexInput();\n\n    if (nextEnt != -1) {\n      // Already loaded\n      return;\n    }\n    //System.out.println(\"blc=\" + blockLoadCount);\n\n    ste.in.seek(fp);\n    int code = ste.in.readVInt();\n    entCount = code >>> 1;\n    assert entCount > 0;\n    isLastInFloor = (code & 1) != 0;\n\n    assert arc == null || (isLastInFloor || isFloor): \"fp=\" + fp + \" arc=\" + arc + \" isFloor=\" + isFloor + \" isLastInFloor=\" + isLastInFloor;\n\n    // TODO: if suffixes were stored in random-access\n    // array structure, then we could do binary search\n    // instead of linear scan to find target term; eg\n    // we could have simple array of offsets\n\n    // term suffixes:\n    code = ste.in.readVInt();\n    isLeafBlock = (code & 1) != 0;\n    int numBytes = code >>> 1;\n    if (suffixBytes.length < numBytes) {\n      suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(suffixBytes, 0, numBytes);\n    suffixesReader.reset(suffixBytes, 0, numBytes);\n\n    /*if (DEBUG) {\n      if (arc == null) {\n      System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      } else {\n      System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      }\n      }*/\n\n    // stats\n    numBytes = ste.in.readVInt();\n    if (statBytes.length < numBytes) {\n      statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(statBytes, 0, numBytes);\n    statsReader.reset(statBytes, 0, numBytes);\n    metaDataUpto = 0;\n\n    state.termBlockOrd = 0;\n    nextEnt = 0;\n    lastSubFP = -1;\n\n    // TODO: we could skip this if !hasTerms; but\n    // that's rare so won't help much\n    // metadata\n    numBytes = ste.in.readVInt();\n    if (bytes == null) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n      bytesReader = new ByteArrayDataInput();\n    } else if (bytes.length < numBytes) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(bytes, 0, numBytes);\n    bytesReader.reset(bytes, 0, numBytes);\n\n\n    // Sub-blocks of a single floor block are always\n    // written one after another -- tail recurse:\n    fpEnd = ste.in.getFilePointer();\n    // if (DEBUG) {\n    //   System.out.println(\"      fpEnd=\" + fpEnd);\n    // }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"71387d8cb6923eb831b17a8b734608ba2e21c653","date":1414126093,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40SegmentTermsEnumFrame#loadBlock().mjava","sourceNew":null,"sourceOld":"  /* Does initial decode of next block of terms; this\n     doesn't actually decode the docFreq, totalTermFreq,\n     postings details (frq/prx offset, etc.) metadata;\n     it just loads them as byte[] blobs which are then      \n     decoded on-demand if the metadata is ever requested\n     for any term in this block.  This enables terms-only\n     intensive consumes (eg certain MTQs, respelling) to\n     not pay the price of decoding metadata they won't\n     use. */\n  void loadBlock() throws IOException {\n\n    // Clone the IndexInput lazily, so that consumers\n    // that just pull a TermsEnum to\n    // seekExact(TermState) don't pay this cost:\n    ste.initIndexInput();\n\n    if (nextEnt != -1) {\n      // Already loaded\n      return;\n    }\n    //System.out.println(\"blc=\" + blockLoadCount);\n\n    ste.in.seek(fp);\n    int code = ste.in.readVInt();\n    entCount = code >>> 1;\n    assert entCount > 0;\n    isLastInFloor = (code & 1) != 0;\n\n    assert arc == null || (isLastInFloor || isFloor): \"fp=\" + fp + \" arc=\" + arc + \" isFloor=\" + isFloor + \" isLastInFloor=\" + isLastInFloor;\n\n    // TODO: if suffixes were stored in random-access\n    // array structure, then we could do binary search\n    // instead of linear scan to find target term; eg\n    // we could have simple array of offsets\n\n    // term suffixes:\n    code = ste.in.readVInt();\n    isLeafBlock = (code & 1) != 0;\n    int numBytes = code >>> 1;\n    if (suffixBytes.length < numBytes) {\n      suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(suffixBytes, 0, numBytes);\n    suffixesReader.reset(suffixBytes, 0, numBytes);\n\n    /*if (DEBUG) {\n      if (arc == null) {\n      System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      } else {\n      System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      }\n      }*/\n\n    // stats\n    numBytes = ste.in.readVInt();\n    if (statBytes.length < numBytes) {\n      statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(statBytes, 0, numBytes);\n    statsReader.reset(statBytes, 0, numBytes);\n    metaDataUpto = 0;\n\n    state.termBlockOrd = 0;\n    nextEnt = 0;\n    lastSubFP = -1;\n\n    // TODO: we could skip this if !hasTerms; but\n    // that's rare so won't help much\n    // metadata\n    numBytes = ste.in.readVInt();\n    if (bytes == null) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n      bytesReader = new ByteArrayDataInput();\n    } else if (bytes.length < numBytes) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(bytes, 0, numBytes);\n    bytesReader.reset(bytes, 0, numBytes);\n\n\n    // Sub-blocks of a single floor block are always\n    // written one after another -- tail recurse:\n    fpEnd = ste.in.getFilePointer();\n    // if (DEBUG) {\n    //   System.out.println(\"      fpEnd=\" + fpEnd);\n    // }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"db68c63cbfaa8698b9c4475f75ed2b9c9696d238":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","0628077afea69a2955260949478afabab8e500d8"],"0628077afea69a2955260949478afabab8e500d8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"71387d8cb6923eb831b17a8b734608ba2e21c653":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["71387d8cb6923eb831b17a8b734608ba2e21c653"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238","0628077afea69a2955260949478afabab8e500d8"],"db68c63cbfaa8698b9c4475f75ed2b9c9696d238":["71387d8cb6923eb831b17a8b734608ba2e21c653"],"0628077afea69a2955260949478afabab8e500d8":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"71387d8cb6923eb831b17a8b734608ba2e21c653":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}