{"path":"modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","commits":[{"id":"ecc11368dc265bfdad90214f8bf5da99016ab1e2","date":1294144090,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<Fieldable> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Fieldable field : fields) {\n      if (!field.isTokenized() || field instanceof NumericField) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      while(stream.incrementToken())\n        tokenCount++;\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<Fieldable> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Fieldable field : fields) {\n      if (!field.isTokenized() || field instanceof NumericField) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      while(stream.incrementToken())\n        tokenCount++;\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70ad682703b8585f5d0a637efec044d57ec05efb","date":1294259117,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<Fieldable> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Fieldable field : fields) {\n      if (!field.isTokenized() || field instanceof NumericField) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      while(stream.incrementToken())\n        tokenCount++;\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":1,"author":"Michael Busch","isMerge":true,"pathNew":"modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<Fieldable> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Fieldable field : fields) {\n      if (!field.isTokenized() || field instanceof NumericField) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      while(stream.incrementToken())\n        tokenCount++;\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<Fieldable> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Fieldable field : fields) {\n      if (!field.isTokenized() || field instanceof NumericField) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      while(stream.incrementToken())\n        tokenCount++;\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f26801ad07ad4b76e8e02df34a14102477f2edf1","date":1301611460,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<Fieldable> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Fieldable field : fields) {\n      if (!field.isTokenized() || field instanceof NumericField) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<Fieldable> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Fieldable field : fields) {\n      if (!field.isTokenized() || field instanceof NumericField) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      while(stream.incrementToken())\n        tokenCount++;\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"45669a651c970812a680841b97a77cce06af559f","date":1301922222,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<Fieldable> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Fieldable field : fields) {\n      if (!field.isTokenized() || field instanceof NumericField) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<Fieldable> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Fieldable field : fields) {\n      if (!field.isTokenized() || field instanceof NumericField) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      while(stream.incrementToken())\n        tokenCount++;\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<Fieldable> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Fieldable field : fields) {\n      if (!field.isTokenized() || field instanceof NumericField) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<Fieldable> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Fieldable field : fields) {\n      if (!field.isTokenized() || field instanceof NumericField) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      while(stream.incrementToken())\n        tokenCount++;\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<IndexableField> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final IndexableField field : fields) {\n      if (!field.tokenized() || field instanceof NumericField) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<Fieldable> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Fieldable field : fields) {\n      if (!field.isTokenized() || field instanceof NumericField) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6eb141f80638abdb6ffaa5149877f36ea39b6ad5","date":1315714072,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<IndexableField> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final IndexableField field : fields) {\n      if (!field.fieldType().tokenized() || field instanceof NumericField) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<IndexableField> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final IndexableField field : fields) {\n      if (!field.tokenized() || field instanceof NumericField) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8b3bdb938a073ccc28d7ed813f6e8c4cb58e04c5","date":1316747797,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<IndexableField> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final IndexableField field : fields) {\n      if (!field.fieldType().tokenized() || field instanceof NumericField) continue;\n      \n      final TokenStream stream = field.tokenStream(analyzer);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<IndexableField> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final IndexableField field : fields) {\n      if (!field.fieldType().tokenized() || field instanceof NumericField) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a78a90fc9701e511308346ea29f4f5e548bb39fe","date":1329489995,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<IndexableField> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final IndexableField field : fields) {\n      if (!field.fieldType().tokenized() ||\n          field instanceof IntField ||\n          field instanceof LongField ||\n          field instanceof FloatField ||\n          field instanceof DoubleField) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<IndexableField> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final IndexableField field : fields) {\n      if (!field.fieldType().tokenized() || field instanceof NumericField) continue;\n      \n      final TokenStream stream = field.tokenStream(analyzer);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":["6eb141f80638abdb6ffaa5149877f36ea39b6ad5"],"bugIntro":["067bb525d2e4993889147c508e2ccb5158f409b1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<IndexableField> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final IndexableField field : fields) {\n      if (!field.fieldType().tokenized() ||\n          field instanceof IntField ||\n          field instanceof LongField ||\n          field instanceof FloatField ||\n          field instanceof DoubleField) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<IndexableField> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final IndexableField field : fields) {\n      if (!field.fieldType().tokenized() ||\n          field instanceof IntField ||\n          field instanceof LongField ||\n          field instanceof FloatField ||\n          field instanceof DoubleField) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"f26801ad07ad4b76e8e02df34a14102477f2edf1":["ecc11368dc265bfdad90214f8bf5da99016ab1e2"],"70ad682703b8585f5d0a637efec044d57ec05efb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","ecc11368dc265bfdad90214f8bf5da99016ab1e2"],"6eb141f80638abdb6ffaa5149877f36ea39b6ad5":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["70ad682703b8585f5d0a637efec044d57ec05efb","f26801ad07ad4b76e8e02df34a14102477f2edf1"],"a78a90fc9701e511308346ea29f4f5e548bb39fe":["8b3bdb938a073ccc28d7ed813f6e8c4cb58e04c5"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"ecc11368dc265bfdad90214f8bf5da99016ab1e2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["f26801ad07ad4b76e8e02df34a14102477f2edf1"],"8b3bdb938a073ccc28d7ed813f6e8c4cb58e04c5":["6eb141f80638abdb6ffaa5149877f36ea39b6ad5"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","ecc11368dc265bfdad90214f8bf5da99016ab1e2"],"45669a651c970812a680841b97a77cce06af559f":["868da859b43505d9d2a023bfeae6dd0c795f5295","f26801ad07ad4b76e8e02df34a14102477f2edf1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b89678825b68eccaf09e6ab71675fc0b0af1e099"]},"commit2Childs":{"f26801ad07ad4b76e8e02df34a14102477f2edf1":["135621f3a0670a9394eb563224a3b76cc4dddc0f","1509f151d7692d84fae414b2b799ac06ba60fcb4","45669a651c970812a680841b97a77cce06af559f"],"70ad682703b8585f5d0a637efec044d57ec05efb":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"6eb141f80638abdb6ffaa5149877f36ea39b6ad5":["8b3bdb938a073ccc28d7ed813f6e8c4cb58e04c5"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"a78a90fc9701e511308346ea29f4f5e548bb39fe":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"ecc11368dc265bfdad90214f8bf5da99016ab1e2":["f26801ad07ad4b76e8e02df34a14102477f2edf1","70ad682703b8585f5d0a637efec044d57ec05efb","868da859b43505d9d2a023bfeae6dd0c795f5295"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["70ad682703b8585f5d0a637efec044d57ec05efb","ecc11368dc265bfdad90214f8bf5da99016ab1e2","868da859b43505d9d2a023bfeae6dd0c795f5295"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["6eb141f80638abdb6ffaa5149877f36ea39b6ad5"],"8b3bdb938a073ccc28d7ed813f6e8c4cb58e04c5":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["45669a651c970812a680841b97a77cce06af559f"],"45669a651c970812a680841b97a77cce06af559f":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["135621f3a0670a9394eb563224a3b76cc4dddc0f","45669a651c970812a680841b97a77cce06af559f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}