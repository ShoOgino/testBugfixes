{"path":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/FlattenGraphFilter#incrementToken().mjava","commits":[{"id":"24a98f5fdd23e04f85819dbc63b47a12f7c44311","date":1482439157,"type":0,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/FlattenGraphFilter#incrementToken().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public boolean incrementToken() throws IOException {\n    //System.out.println(\"\\nF.increment inputFrom=\" + inputFrom + \" outputFrom=\" + outputFrom);\n\n    while (true) {\n      if (releaseBufferedToken()) {\n        //retOutputFrom += posIncAtt.getPositionIncrement();\n        //System.out.println(\"    return buffered: \" + termAtt + \" \" + retOutputFrom + \"-\" + (retOutputFrom + posLenAtt.getPositionLength()));\n        //printStates();\n        return true;\n      } else if (done) {\n        //System.out.println(\"    done, return false\");\n        return false;\n      }\n\n      if (input.incrementToken()) {\n        // Input node this token leaves from:\n        inputFrom += posIncAtt.getPositionIncrement();\n\n        int startOffset = offsetAtt.startOffset();\n        int endOffset = offsetAtt.endOffset();\n\n        // Input node this token goes to:\n        int inputTo = inputFrom + posLenAtt.getPositionLength();\n        //System.out.println(\"  input.inc \" + termAtt + \": \" + inputFrom + \"-\" + inputTo);\n\n        InputNode src = inputNodes.get(inputFrom);\n        if (src.node == -1) {\n          // This means the \"from\" node of this token was never seen as a \"to\" node,\n          // which should only happen if we just crossed a hole.  This is a challenging\n          // case for us because we normally rely on the full dependencies expressed\n          // by the arcs to assign outgoing node IDs.  It would be better if tokens\n          // were never dropped but instead just marked deleted with a new\n          // TermDeletedAttribute (boolean valued) ... but until that future, we have\n          // a hack here to forcefully jump the output node ID:\n          assert src.outputNode == -1;\n          src.node = inputFrom;\n\n          src.outputNode = outputNodes.getMaxPos() + 1;\n          //System.out.println(\"    hole: force to outputNode=\" + src.outputNode);\n          OutputNode outSrc = outputNodes.get(src.outputNode);\n\n          // Not assigned yet:\n          assert outSrc.node == -1;\n          outSrc.node = src.outputNode;\n          outSrc.inputNodes.add(inputFrom);\n          outSrc.startOffset = startOffset;\n        } else {\n          OutputNode outSrc = outputNodes.get(src.outputNode);\n          if (outSrc.startOffset == -1 || startOffset > outSrc.startOffset) {\n            // \"shrink wrap\" the offsets so the original tokens (with most\n            // restrictive offsets) win:\n            outSrc.startOffset = Math.max(startOffset, outSrc.startOffset);\n          }\n        }\n\n        // Buffer this token:\n        src.tokens.add(captureState());\n        src.maxToNode = Math.max(src.maxToNode, inputTo);\n        maxLookaheadUsed = Math.max(maxLookaheadUsed, inputNodes.getBufferSize());\n\n        InputNode dest = inputNodes.get(inputTo);\n        if (dest.node == -1) {\n          // Common case: first time a token is arriving to this input position:\n          dest.node = inputTo;\n        }\n\n        // Always number output nodes sequentially:\n        int outputEndNode = src.outputNode + 1;\n\n        if (outputEndNode > dest.outputNode) {\n          if (dest.outputNode != -1) {\n            boolean removed = outputNodes.get(dest.outputNode).inputNodes.remove(Integer.valueOf(inputTo));\n            assert removed;\n          }\n          //System.out.println(\"    increase output node: \" + dest.outputNode + \" vs \" + outputEndNode);\n          outputNodes.get(outputEndNode).inputNodes.add(inputTo);\n          dest.outputNode = outputEndNode;\n\n          // Since all we ever do is merge incoming nodes together, and then renumber\n          // the merged nodes sequentially, we should only ever assign smaller node\n          // numbers:\n          assert outputEndNode <= inputTo: \"outputEndNode=\" + outputEndNode + \" vs inputTo=\" + inputTo;\n        }\n\n        OutputNode outDest = outputNodes.get(dest.outputNode);\n        // \"shrink wrap\" the offsets so the original tokens (with most\n        // restrictive offsets) win:\n        if (outDest.endOffset == -1 || endOffset < outDest.endOffset) {\n          outDest.endOffset = endOffset;\n        }\n\n      } else {\n        //System.out.println(\"  got false from input\");\n        input.end();\n        finalPosInc = posIncAtt.getPositionIncrement();\n        finalOffset = offsetAtt.endOffset();\n        done = true;\n        // Don't return false here: we need to force release any buffered tokens now\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f03e4bed5023ec3ef93a771b8888cae991cf448d","date":1483469262,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/FlattenGraphFilter#incrementToken().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public boolean incrementToken() throws IOException {\n    //System.out.println(\"\\nF.increment inputFrom=\" + inputFrom + \" outputFrom=\" + outputFrom);\n\n    while (true) {\n      if (releaseBufferedToken()) {\n        //retOutputFrom += posIncAtt.getPositionIncrement();\n        //System.out.println(\"    return buffered: \" + termAtt + \" \" + retOutputFrom + \"-\" + (retOutputFrom + posLenAtt.getPositionLength()));\n        //printStates();\n        return true;\n      } else if (done) {\n        //System.out.println(\"    done, return false\");\n        return false;\n      }\n\n      if (input.incrementToken()) {\n        // Input node this token leaves from:\n        inputFrom += posIncAtt.getPositionIncrement();\n\n        int startOffset = offsetAtt.startOffset();\n        int endOffset = offsetAtt.endOffset();\n\n        // Input node this token goes to:\n        int inputTo = inputFrom + posLenAtt.getPositionLength();\n        //System.out.println(\"  input.inc \" + termAtt + \": \" + inputFrom + \"-\" + inputTo);\n\n        InputNode src = inputNodes.get(inputFrom);\n        if (src.node == -1) {\n          // This means the \"from\" node of this token was never seen as a \"to\" node,\n          // which should only happen if we just crossed a hole.  This is a challenging\n          // case for us because we normally rely on the full dependencies expressed\n          // by the arcs to assign outgoing node IDs.  It would be better if tokens\n          // were never dropped but instead just marked deleted with a new\n          // TermDeletedAttribute (boolean valued) ... but until that future, we have\n          // a hack here to forcefully jump the output node ID:\n          assert src.outputNode == -1;\n          src.node = inputFrom;\n\n          src.outputNode = outputNodes.getMaxPos() + 1;\n          //System.out.println(\"    hole: force to outputNode=\" + src.outputNode);\n          OutputNode outSrc = outputNodes.get(src.outputNode);\n\n          // Not assigned yet:\n          assert outSrc.node == -1;\n          outSrc.node = src.outputNode;\n          outSrc.inputNodes.add(inputFrom);\n          outSrc.startOffset = startOffset;\n        } else {\n          OutputNode outSrc = outputNodes.get(src.outputNode);\n          if (outSrc.startOffset == -1 || startOffset > outSrc.startOffset) {\n            // \"shrink wrap\" the offsets so the original tokens (with most\n            // restrictive offsets) win:\n            outSrc.startOffset = Math.max(startOffset, outSrc.startOffset);\n          }\n        }\n\n        // Buffer this token:\n        src.tokens.add(captureState());\n        src.maxToNode = Math.max(src.maxToNode, inputTo);\n        maxLookaheadUsed = Math.max(maxLookaheadUsed, inputNodes.getBufferSize());\n\n        InputNode dest = inputNodes.get(inputTo);\n        if (dest.node == -1) {\n          // Common case: first time a token is arriving to this input position:\n          dest.node = inputTo;\n        }\n\n        // Always number output nodes sequentially:\n        int outputEndNode = src.outputNode + 1;\n\n        if (outputEndNode > dest.outputNode) {\n          if (dest.outputNode != -1) {\n            boolean removed = outputNodes.get(dest.outputNode).inputNodes.remove(Integer.valueOf(inputTo));\n            assert removed;\n          }\n          //System.out.println(\"    increase output node: \" + dest.outputNode + \" vs \" + outputEndNode);\n          outputNodes.get(outputEndNode).inputNodes.add(inputTo);\n          dest.outputNode = outputEndNode;\n\n          // Since all we ever do is merge incoming nodes together, and then renumber\n          // the merged nodes sequentially, we should only ever assign smaller node\n          // numbers:\n          assert outputEndNode <= inputTo: \"outputEndNode=\" + outputEndNode + \" vs inputTo=\" + inputTo;\n        }\n\n        OutputNode outDest = outputNodes.get(dest.outputNode);\n        // \"shrink wrap\" the offsets so the original tokens (with most\n        // restrictive offsets) win:\n        if (outDest.endOffset == -1 || endOffset < outDest.endOffset) {\n          outDest.endOffset = endOffset;\n        }\n\n      } else {\n        //System.out.println(\"  got false from input\");\n        input.end();\n        finalPosInc = posIncAtt.getPositionIncrement();\n        finalOffset = offsetAtt.endOffset();\n        done = true;\n        // Don't return false here: we need to force release any buffered tokens now\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"098528909bb70948871fd7ed865fafb87ed73964","date":1484667487,"type":5,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/core/FlattenGraphFilter#incrementToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/FlattenGraphFilter#incrementToken().mjava","sourceNew":"  @Override\n  public boolean incrementToken() throws IOException {\n    //System.out.println(\"\\nF.increment inputFrom=\" + inputFrom + \" outputFrom=\" + outputFrom);\n\n    while (true) {\n      if (releaseBufferedToken()) {\n        //retOutputFrom += posIncAtt.getPositionIncrement();\n        //System.out.println(\"    return buffered: \" + termAtt + \" \" + retOutputFrom + \"-\" + (retOutputFrom + posLenAtt.getPositionLength()));\n        //printStates();\n        return true;\n      } else if (done) {\n        //System.out.println(\"    done, return false\");\n        return false;\n      }\n\n      if (input.incrementToken()) {\n        // Input node this token leaves from:\n        inputFrom += posIncAtt.getPositionIncrement();\n\n        int startOffset = offsetAtt.startOffset();\n        int endOffset = offsetAtt.endOffset();\n\n        // Input node this token goes to:\n        int inputTo = inputFrom + posLenAtt.getPositionLength();\n        //System.out.println(\"  input.inc \" + termAtt + \": \" + inputFrom + \"-\" + inputTo);\n\n        InputNode src = inputNodes.get(inputFrom);\n        if (src.node == -1) {\n          // This means the \"from\" node of this token was never seen as a \"to\" node,\n          // which should only happen if we just crossed a hole.  This is a challenging\n          // case for us because we normally rely on the full dependencies expressed\n          // by the arcs to assign outgoing node IDs.  It would be better if tokens\n          // were never dropped but instead just marked deleted with a new\n          // TermDeletedAttribute (boolean valued) ... but until that future, we have\n          // a hack here to forcefully jump the output node ID:\n          assert src.outputNode == -1;\n          src.node = inputFrom;\n\n          src.outputNode = outputNodes.getMaxPos() + 1;\n          //System.out.println(\"    hole: force to outputNode=\" + src.outputNode);\n          OutputNode outSrc = outputNodes.get(src.outputNode);\n\n          // Not assigned yet:\n          assert outSrc.node == -1;\n          outSrc.node = src.outputNode;\n          outSrc.inputNodes.add(inputFrom);\n          outSrc.startOffset = startOffset;\n        } else {\n          OutputNode outSrc = outputNodes.get(src.outputNode);\n          if (outSrc.startOffset == -1 || startOffset > outSrc.startOffset) {\n            // \"shrink wrap\" the offsets so the original tokens (with most\n            // restrictive offsets) win:\n            outSrc.startOffset = Math.max(startOffset, outSrc.startOffset);\n          }\n        }\n\n        // Buffer this token:\n        src.tokens.add(captureState());\n        src.maxToNode = Math.max(src.maxToNode, inputTo);\n        maxLookaheadUsed = Math.max(maxLookaheadUsed, inputNodes.getBufferSize());\n\n        InputNode dest = inputNodes.get(inputTo);\n        if (dest.node == -1) {\n          // Common case: first time a token is arriving to this input position:\n          dest.node = inputTo;\n        }\n\n        // Always number output nodes sequentially:\n        int outputEndNode = src.outputNode + 1;\n\n        if (outputEndNode > dest.outputNode) {\n          if (dest.outputNode != -1) {\n            boolean removed = outputNodes.get(dest.outputNode).inputNodes.remove(Integer.valueOf(inputTo));\n            assert removed;\n          }\n          //System.out.println(\"    increase output node: \" + dest.outputNode + \" vs \" + outputEndNode);\n          outputNodes.get(outputEndNode).inputNodes.add(inputTo);\n          dest.outputNode = outputEndNode;\n\n          // Since all we ever do is merge incoming nodes together, and then renumber\n          // the merged nodes sequentially, we should only ever assign smaller node\n          // numbers:\n          assert outputEndNode <= inputTo: \"outputEndNode=\" + outputEndNode + \" vs inputTo=\" + inputTo;\n        }\n\n        OutputNode outDest = outputNodes.get(dest.outputNode);\n        // \"shrink wrap\" the offsets so the original tokens (with most\n        // restrictive offsets) win:\n        if (outDest.endOffset == -1 || endOffset < outDest.endOffset) {\n          outDest.endOffset = endOffset;\n        }\n\n      } else {\n        //System.out.println(\"  got false from input\");\n        input.end();\n        finalPosInc = posIncAtt.getPositionIncrement();\n        finalOffset = offsetAtt.endOffset();\n        done = true;\n        // Don't return false here: we need to force release any buffered tokens now\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public boolean incrementToken() throws IOException {\n    //System.out.println(\"\\nF.increment inputFrom=\" + inputFrom + \" outputFrom=\" + outputFrom);\n\n    while (true) {\n      if (releaseBufferedToken()) {\n        //retOutputFrom += posIncAtt.getPositionIncrement();\n        //System.out.println(\"    return buffered: \" + termAtt + \" \" + retOutputFrom + \"-\" + (retOutputFrom + posLenAtt.getPositionLength()));\n        //printStates();\n        return true;\n      } else if (done) {\n        //System.out.println(\"    done, return false\");\n        return false;\n      }\n\n      if (input.incrementToken()) {\n        // Input node this token leaves from:\n        inputFrom += posIncAtt.getPositionIncrement();\n\n        int startOffset = offsetAtt.startOffset();\n        int endOffset = offsetAtt.endOffset();\n\n        // Input node this token goes to:\n        int inputTo = inputFrom + posLenAtt.getPositionLength();\n        //System.out.println(\"  input.inc \" + termAtt + \": \" + inputFrom + \"-\" + inputTo);\n\n        InputNode src = inputNodes.get(inputFrom);\n        if (src.node == -1) {\n          // This means the \"from\" node of this token was never seen as a \"to\" node,\n          // which should only happen if we just crossed a hole.  This is a challenging\n          // case for us because we normally rely on the full dependencies expressed\n          // by the arcs to assign outgoing node IDs.  It would be better if tokens\n          // were never dropped but instead just marked deleted with a new\n          // TermDeletedAttribute (boolean valued) ... but until that future, we have\n          // a hack here to forcefully jump the output node ID:\n          assert src.outputNode == -1;\n          src.node = inputFrom;\n\n          src.outputNode = outputNodes.getMaxPos() + 1;\n          //System.out.println(\"    hole: force to outputNode=\" + src.outputNode);\n          OutputNode outSrc = outputNodes.get(src.outputNode);\n\n          // Not assigned yet:\n          assert outSrc.node == -1;\n          outSrc.node = src.outputNode;\n          outSrc.inputNodes.add(inputFrom);\n          outSrc.startOffset = startOffset;\n        } else {\n          OutputNode outSrc = outputNodes.get(src.outputNode);\n          if (outSrc.startOffset == -1 || startOffset > outSrc.startOffset) {\n            // \"shrink wrap\" the offsets so the original tokens (with most\n            // restrictive offsets) win:\n            outSrc.startOffset = Math.max(startOffset, outSrc.startOffset);\n          }\n        }\n\n        // Buffer this token:\n        src.tokens.add(captureState());\n        src.maxToNode = Math.max(src.maxToNode, inputTo);\n        maxLookaheadUsed = Math.max(maxLookaheadUsed, inputNodes.getBufferSize());\n\n        InputNode dest = inputNodes.get(inputTo);\n        if (dest.node == -1) {\n          // Common case: first time a token is arriving to this input position:\n          dest.node = inputTo;\n        }\n\n        // Always number output nodes sequentially:\n        int outputEndNode = src.outputNode + 1;\n\n        if (outputEndNode > dest.outputNode) {\n          if (dest.outputNode != -1) {\n            boolean removed = outputNodes.get(dest.outputNode).inputNodes.remove(Integer.valueOf(inputTo));\n            assert removed;\n          }\n          //System.out.println(\"    increase output node: \" + dest.outputNode + \" vs \" + outputEndNode);\n          outputNodes.get(outputEndNode).inputNodes.add(inputTo);\n          dest.outputNode = outputEndNode;\n\n          // Since all we ever do is merge incoming nodes together, and then renumber\n          // the merged nodes sequentially, we should only ever assign smaller node\n          // numbers:\n          assert outputEndNode <= inputTo: \"outputEndNode=\" + outputEndNode + \" vs inputTo=\" + inputTo;\n        }\n\n        OutputNode outDest = outputNodes.get(dest.outputNode);\n        // \"shrink wrap\" the offsets so the original tokens (with most\n        // restrictive offsets) win:\n        if (outDest.endOffset == -1 || endOffset < outDest.endOffset) {\n          outDest.endOffset = endOffset;\n        }\n\n      } else {\n        //System.out.println(\"  got false from input\");\n        input.end();\n        finalPosInc = posIncAtt.getPositionIncrement();\n        finalOffset = offsetAtt.endOffset();\n        done = true;\n        // Don't return false here: we need to force release any buffered tokens now\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"302d34f2c66e8d489ee13078305c330cbf67b226","date":1484754357,"type":5,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/core/FlattenGraphFilter#incrementToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/FlattenGraphFilter#incrementToken().mjava","sourceNew":"  @Override\n  public boolean incrementToken() throws IOException {\n    //System.out.println(\"\\nF.increment inputFrom=\" + inputFrom + \" outputFrom=\" + outputFrom);\n\n    while (true) {\n      if (releaseBufferedToken()) {\n        //retOutputFrom += posIncAtt.getPositionIncrement();\n        //System.out.println(\"    return buffered: \" + termAtt + \" \" + retOutputFrom + \"-\" + (retOutputFrom + posLenAtt.getPositionLength()));\n        //printStates();\n        return true;\n      } else if (done) {\n        //System.out.println(\"    done, return false\");\n        return false;\n      }\n\n      if (input.incrementToken()) {\n        // Input node this token leaves from:\n        inputFrom += posIncAtt.getPositionIncrement();\n\n        int startOffset = offsetAtt.startOffset();\n        int endOffset = offsetAtt.endOffset();\n\n        // Input node this token goes to:\n        int inputTo = inputFrom + posLenAtt.getPositionLength();\n        //System.out.println(\"  input.inc \" + termAtt + \": \" + inputFrom + \"-\" + inputTo);\n\n        InputNode src = inputNodes.get(inputFrom);\n        if (src.node == -1) {\n          // This means the \"from\" node of this token was never seen as a \"to\" node,\n          // which should only happen if we just crossed a hole.  This is a challenging\n          // case for us because we normally rely on the full dependencies expressed\n          // by the arcs to assign outgoing node IDs.  It would be better if tokens\n          // were never dropped but instead just marked deleted with a new\n          // TermDeletedAttribute (boolean valued) ... but until that future, we have\n          // a hack here to forcefully jump the output node ID:\n          assert src.outputNode == -1;\n          src.node = inputFrom;\n\n          src.outputNode = outputNodes.getMaxPos() + 1;\n          //System.out.println(\"    hole: force to outputNode=\" + src.outputNode);\n          OutputNode outSrc = outputNodes.get(src.outputNode);\n\n          // Not assigned yet:\n          assert outSrc.node == -1;\n          outSrc.node = src.outputNode;\n          outSrc.inputNodes.add(inputFrom);\n          outSrc.startOffset = startOffset;\n        } else {\n          OutputNode outSrc = outputNodes.get(src.outputNode);\n          if (outSrc.startOffset == -1 || startOffset > outSrc.startOffset) {\n            // \"shrink wrap\" the offsets so the original tokens (with most\n            // restrictive offsets) win:\n            outSrc.startOffset = Math.max(startOffset, outSrc.startOffset);\n          }\n        }\n\n        // Buffer this token:\n        src.tokens.add(captureState());\n        src.maxToNode = Math.max(src.maxToNode, inputTo);\n        maxLookaheadUsed = Math.max(maxLookaheadUsed, inputNodes.getBufferSize());\n\n        InputNode dest = inputNodes.get(inputTo);\n        if (dest.node == -1) {\n          // Common case: first time a token is arriving to this input position:\n          dest.node = inputTo;\n        }\n\n        // Always number output nodes sequentially:\n        int outputEndNode = src.outputNode + 1;\n\n        if (outputEndNode > dest.outputNode) {\n          if (dest.outputNode != -1) {\n            boolean removed = outputNodes.get(dest.outputNode).inputNodes.remove(Integer.valueOf(inputTo));\n            assert removed;\n          }\n          //System.out.println(\"    increase output node: \" + dest.outputNode + \" vs \" + outputEndNode);\n          outputNodes.get(outputEndNode).inputNodes.add(inputTo);\n          dest.outputNode = outputEndNode;\n\n          // Since all we ever do is merge incoming nodes together, and then renumber\n          // the merged nodes sequentially, we should only ever assign smaller node\n          // numbers:\n          assert outputEndNode <= inputTo: \"outputEndNode=\" + outputEndNode + \" vs inputTo=\" + inputTo;\n        }\n\n        OutputNode outDest = outputNodes.get(dest.outputNode);\n        // \"shrink wrap\" the offsets so the original tokens (with most\n        // restrictive offsets) win:\n        if (outDest.endOffset == -1 || endOffset < outDest.endOffset) {\n          outDest.endOffset = endOffset;\n        }\n\n      } else {\n        //System.out.println(\"  got false from input\");\n        input.end();\n        finalPosInc = posIncAtt.getPositionIncrement();\n        finalOffset = offsetAtt.endOffset();\n        done = true;\n        // Don't return false here: we need to force release any buffered tokens now\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public boolean incrementToken() throws IOException {\n    //System.out.println(\"\\nF.increment inputFrom=\" + inputFrom + \" outputFrom=\" + outputFrom);\n\n    while (true) {\n      if (releaseBufferedToken()) {\n        //retOutputFrom += posIncAtt.getPositionIncrement();\n        //System.out.println(\"    return buffered: \" + termAtt + \" \" + retOutputFrom + \"-\" + (retOutputFrom + posLenAtt.getPositionLength()));\n        //printStates();\n        return true;\n      } else if (done) {\n        //System.out.println(\"    done, return false\");\n        return false;\n      }\n\n      if (input.incrementToken()) {\n        // Input node this token leaves from:\n        inputFrom += posIncAtt.getPositionIncrement();\n\n        int startOffset = offsetAtt.startOffset();\n        int endOffset = offsetAtt.endOffset();\n\n        // Input node this token goes to:\n        int inputTo = inputFrom + posLenAtt.getPositionLength();\n        //System.out.println(\"  input.inc \" + termAtt + \": \" + inputFrom + \"-\" + inputTo);\n\n        InputNode src = inputNodes.get(inputFrom);\n        if (src.node == -1) {\n          // This means the \"from\" node of this token was never seen as a \"to\" node,\n          // which should only happen if we just crossed a hole.  This is a challenging\n          // case for us because we normally rely on the full dependencies expressed\n          // by the arcs to assign outgoing node IDs.  It would be better if tokens\n          // were never dropped but instead just marked deleted with a new\n          // TermDeletedAttribute (boolean valued) ... but until that future, we have\n          // a hack here to forcefully jump the output node ID:\n          assert src.outputNode == -1;\n          src.node = inputFrom;\n\n          src.outputNode = outputNodes.getMaxPos() + 1;\n          //System.out.println(\"    hole: force to outputNode=\" + src.outputNode);\n          OutputNode outSrc = outputNodes.get(src.outputNode);\n\n          // Not assigned yet:\n          assert outSrc.node == -1;\n          outSrc.node = src.outputNode;\n          outSrc.inputNodes.add(inputFrom);\n          outSrc.startOffset = startOffset;\n        } else {\n          OutputNode outSrc = outputNodes.get(src.outputNode);\n          if (outSrc.startOffset == -1 || startOffset > outSrc.startOffset) {\n            // \"shrink wrap\" the offsets so the original tokens (with most\n            // restrictive offsets) win:\n            outSrc.startOffset = Math.max(startOffset, outSrc.startOffset);\n          }\n        }\n\n        // Buffer this token:\n        src.tokens.add(captureState());\n        src.maxToNode = Math.max(src.maxToNode, inputTo);\n        maxLookaheadUsed = Math.max(maxLookaheadUsed, inputNodes.getBufferSize());\n\n        InputNode dest = inputNodes.get(inputTo);\n        if (dest.node == -1) {\n          // Common case: first time a token is arriving to this input position:\n          dest.node = inputTo;\n        }\n\n        // Always number output nodes sequentially:\n        int outputEndNode = src.outputNode + 1;\n\n        if (outputEndNode > dest.outputNode) {\n          if (dest.outputNode != -1) {\n            boolean removed = outputNodes.get(dest.outputNode).inputNodes.remove(Integer.valueOf(inputTo));\n            assert removed;\n          }\n          //System.out.println(\"    increase output node: \" + dest.outputNode + \" vs \" + outputEndNode);\n          outputNodes.get(outputEndNode).inputNodes.add(inputTo);\n          dest.outputNode = outputEndNode;\n\n          // Since all we ever do is merge incoming nodes together, and then renumber\n          // the merged nodes sequentially, we should only ever assign smaller node\n          // numbers:\n          assert outputEndNode <= inputTo: \"outputEndNode=\" + outputEndNode + \" vs inputTo=\" + inputTo;\n        }\n\n        OutputNode outDest = outputNodes.get(dest.outputNode);\n        // \"shrink wrap\" the offsets so the original tokens (with most\n        // restrictive offsets) win:\n        if (outDest.endOffset == -1 || endOffset < outDest.endOffset) {\n          outDest.endOffset = endOffset;\n        }\n\n      } else {\n        //System.out.println(\"  got false from input\");\n        input.end();\n        finalPosInc = posIncAtt.getPositionIncrement();\n        finalOffset = offsetAtt.endOffset();\n        done = true;\n        // Don't return false here: we need to force release any buffered tokens now\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"098528909bb70948871fd7ed865fafb87ed73964":["24a98f5fdd23e04f85819dbc63b47a12f7c44311"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"24a98f5fdd23e04f85819dbc63b47a12f7c44311":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["098528909bb70948871fd7ed865fafb87ed73964"],"f03e4bed5023ec3ef93a771b8888cae991cf448d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","24a98f5fdd23e04f85819dbc63b47a12f7c44311"],"302d34f2c66e8d489ee13078305c330cbf67b226":["f03e4bed5023ec3ef93a771b8888cae991cf448d","098528909bb70948871fd7ed865fafb87ed73964"]},"commit2Childs":{"098528909bb70948871fd7ed865fafb87ed73964":["cd5edd1f2b162a5cfa08efd17851a07373a96817","302d34f2c66e8d489ee13078305c330cbf67b226"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["24a98f5fdd23e04f85819dbc63b47a12f7c44311","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"24a98f5fdd23e04f85819dbc63b47a12f7c44311":["098528909bb70948871fd7ed865fafb87ed73964","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"f03e4bed5023ec3ef93a771b8888cae991cf448d":["302d34f2c66e8d489ee13078305c330cbf67b226"],"302d34f2c66e8d489ee13078305c330cbf67b226":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817","302d34f2c66e8d489ee13078305c330cbf67b226"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}