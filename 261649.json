{"path":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"955c32f886db6f6356c9fcdea6b1f1cb4effda24","date":1270581567,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":["8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"433777d1eaf9998136cd16515dc0e1eb26f5d535","date":1273839120,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"82470b9595905eea74268b47bd3e8a94306d28d9","date":1274294241,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fb10b6bcde550b87d8f10e5f010bd8f3021023b6","date":1274974592,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":["8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f470b537db3da4e8d0c39bc72fae5f9865a9ec3c","date":1275494784,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":["2a186ae8733084223c22044e935e4ef848a143d1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c0716ddfa41d3662d014c42086a700ad78fc5dcb","date":1275676935,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":["8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"08932c793647a36953d1816b1060121f48820d3f","date":1277386540,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9b832cbed6eb3d54a8bb9339296bdda8eeb53014","date":1279708040,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          // nocommit\n//          if (dss.contains(docWriter.getDocStoreSegment())) {\n//            if (infoStream != null)\n//              message(\"now flush at mergeMiddle\");\n//            doFlush(true, false);\n//          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"334c1175813aea771a71728cd2c4ee4754fd0603","date":1279710173,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          // nocommit\n//          if (dss.contains(docWriter.getDocStoreSegment())) {\n//            if (infoStream != null)\n//              message(\"now flush at mergeMiddle\");\n//            doFlush(true, false);\n//          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":["8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8fe956d65251358d755c56f14fe8380644790e47","date":1279711318,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          // nocommit\n//          if (dss.contains(docWriter.getDocStoreSegment())) {\n//            if (infoStream != null)\n//              message(\"now flush at mergeMiddle\");\n//            doFlush(true, false);\n//          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"833a7987bc1c94455fde83e3311f72bddedcfb93","date":1279951470,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      for(int i=0;i<numSegments;i++) {\n        merge.readersClone[i].openDocStores();\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          // nocommit\n//          if (dss.contains(docWriter.getDocStoreSegment())) {\n//            if (infoStream != null)\n//              message(\"now flush at mergeMiddle\");\n//            doFlush(true, false);\n//          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e277c90a96e81a581f45e4b6aaf384ddc50f79bc","date":1280401404,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n\n      if (infoStream != null) {\n        message(\"merge codec=\" + merger.getCodec().name);\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"619e84c4a25ac93018dce34af2bb41dafbeac829","date":1281217684,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n\n      if (infoStream != null) {\n        message(\"merge codec=\" + merger.getCodec().name);\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n\n      if (infoStream != null) {\n        message(\"merge codec=\" + merger.getCodec().name);\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1085ea837da8f1e96697e17cf73e1d08e7329261","date":1281469548,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n\n      if (infoStream != null) {\n        message(\"merge codec=\" + merger.getCodec().name);\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], false);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n\n      if (infoStream != null) {\n        message(\"merge codec=\" + merger.getCodec().name);\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":["c4ff8864209d2e972cb4393600c26082f9a6533d"],"bugIntro":["8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a493e6d0c3ad86bd55c0a1360d110142e948f2bd","date":1289406991,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], false);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n\n      if (infoStream != null) {\n        message(\"merge codec=\" + merger.getCodec().name);\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], false);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"85a883878c0af761245ab048babc63d099f835f3","date":1289553330,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], false);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n\n      if (infoStream != null) {\n        message(\"merge codec=\" + merger.getCodec().name);\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], false);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2a186ae8733084223c22044e935e4ef848a143d1","date":1289694819,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n      \n      // if the merged segment warmer was not installed when\n      // this merge was started, causing us to not force\n      // the docStores to close, we can't warm it now\n      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);\n\n      if (poolReaders && mergedSegmentWarmer != null && canWarm) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], false);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], false);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":["f470b537db3da4e8d0c39bc72fae5f9865a9ec3c"],"bugIntro":["8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c498d3f8d75170b121f5eda2c6210ac5beb5d411","date":1289726298,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n      \n      // if the merged segment warmer was not installed when\n      // this merge was started, causing us to not force\n      // the docStores to close, we can't warm it now\n      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);\n\n      if (poolReaders && mergedSegmentWarmer != null && canWarm) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], false);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], false);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5fff8d3ee91620a83b0a3ac1c9f85b384c7cf32a","date":1289919830,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n      \n      // if the merged segment warmer was not installed when\n      // this merge was started, causing us to not force\n      // the docStores to close, we can't warm it now\n      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);\n\n      if (poolReaders && mergedSegmentWarmer != null && canWarm) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], false);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        synchronized(this) {\n          deleter.deleteFile(compoundFileName);\n        }\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n      \n      // if the merged segment warmer was not installed when\n      // this merge was started, causing us to not force\n      // the docStores to close, we can't warm it now\n      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);\n\n      if (poolReaders && mergedSegmentWarmer != null && canWarm) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], false);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":["e82780afe6097066eb5befb86e9432f077667e3d"],"bugIntro":["8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9ab1f5591dc05f1f2b5407d809c9699f75554a32","date":1290008586,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n      \n      // if the merged segment warmer was not installed when\n      // this merge was started, causing us to not force\n      // the docStores to close, we can't warm it now\n      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);\n\n      if (poolReaders && mergedSegmentWarmer != null && canWarm) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], false);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        synchronized(this) {\n          deleter.deleteFile(compoundFileName);\n        }\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n      \n      // if the merged segment warmer was not installed when\n      // this merge was started, causing us to not force\n      // the docStores to close, we can't warm it now\n      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);\n\n      if (poolReaders && mergedSegmentWarmer != null && canWarm) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], false);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5","date":1290247889,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final String currentDocStoreSegment;\n    synchronized(this) {\n      currentDocStoreSegment = docWriter.getDocStoreSegment();\n    }\n\n    boolean currentDSSMerged = false;\n      \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1 && currentDocStoreSegment != null) {\n          currentDSSMerged |= currentDocStoreSegment.equals(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (currentDSSMerged) {\n            if (infoStream != null) {\n              message(\"now flush at mergeMiddle\");\n            }\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        merge.info.setDocStore(-1, null, false);\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      if (merge.useCompoundFile) {\n\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      // if the merged segment warmer was not installed when\n      // this merge was started, causing us to not force\n      // the docStores to close, we can't warm it now\n      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);\n\n      if (poolReaders && mergedSegmentWarmer != null && canWarm) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n      \n      // if the merged segment warmer was not installed when\n      // this merge was started, causing us to not force\n      // the docStores to close, we can't warm it now\n      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);\n\n      if (poolReaders && mergedSegmentWarmer != null && canWarm) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], false);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        synchronized(this) {\n          deleter.deleteFile(compoundFileName);\n        }\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":["1085ea837da8f1e96697e17cf73e1d08e7329261","b1405362241b561f5590ff4a87d5d6e173bcd9cf","c4ff8864209d2e972cb4393600c26082f9a6533d","2586f96f60332eb97ecd2934b0763791462568b2","334c1175813aea771a71728cd2c4ee4754fd0603","e82780afe6097066eb5befb86e9432f077667e3d","b6b0122d107a2f2a35007aca038d2a7fde039266","ef82ff03e4016c705811b2658e81471a645c0e49","2a186ae8733084223c22044e935e4ef848a143d1","fb10b6bcde550b87d8f10e5f010bd8f3021023b6","47737b13e8268ad290e1540fe7cd264a29355092","955c32f886db6f6356c9fcdea6b1f1cb4effda24","346d5897e4c4e77ed5dbd31f7730ff30973d5971","5fff8d3ee91620a83b0a3ac1c9f85b384c7cf32a","c0716ddfa41d3662d014c42086a700ad78fc5dcb"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4e8cc373c801e54cec75daf9f52792cb4b17f536","date":1291116159,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final String currentDocStoreSegment;\n    synchronized(this) {\n      currentDocStoreSegment = docWriter.getDocStoreSegment();\n    }\n\n    boolean currentDSSMerged = false;\n      \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1 && currentDocStoreSegment != null) {\n          currentDSSMerged |= currentDocStoreSegment.equals(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (currentDSSMerged) {\n            if (infoStream != null) {\n              message(\"now flush at mergeMiddle\");\n            }\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        merge.info.setDocStore(-1, null, false);\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      if (merge.useCompoundFile) {\n\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      // if the merged segment warmer was not installed when\n      // this merge was started, causing us to not force\n      // the docStores to close, we can't warm it now\n      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);\n\n      if (poolReaders && mergedSegmentWarmer != null && canWarm) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final String currentDocStoreSegment;\n    synchronized(this) {\n      currentDocStoreSegment = docWriter.getDocStoreSegment();\n    }\n\n    boolean currentDSSMerged = false;\n      \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1 && currentDocStoreSegment != null) {\n          currentDSSMerged |= currentDocStoreSegment.equals(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (currentDSSMerged) {\n            if (infoStream != null) {\n              message(\"now flush at mergeMiddle\");\n            }\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        merge.info.setDocStore(-1, null, false);\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      if (merge.useCompoundFile) {\n\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      // if the merged segment warmer was not installed when\n      // this merge was started, causing us to not force\n      // the docStores to close, we can't warm it now\n      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);\n\n      if (poolReaders && mergedSegmentWarmer != null && canWarm) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":["4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"38a62612cfa4e104080d89d7751a8f1a258ac335","date":1291442315,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final String currentDocStoreSegment;\n    synchronized(this) {\n      currentDocStoreSegment = docWriter.getDocStoreSegment();\n    }\n\n    boolean currentDSSMerged = false;\n      \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1 && currentDocStoreSegment != null) {\n          currentDSSMerged |= currentDocStoreSegment.equals(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (currentDSSMerged) {\n            if (infoStream != null) {\n              message(\"now flush at mergeMiddle\");\n            }\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        merge.info.setDocStore(-1, null, false);\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      // if the merged segment warmer was not installed when\n      // this merge was started, causing us to not force\n      // the docStores to close, we can't warm it now\n      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);\n\n      if (poolReaders && mergedSegmentWarmer != null && canWarm) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final String currentDocStoreSegment;\n    synchronized(this) {\n      currentDocStoreSegment = docWriter.getDocStoreSegment();\n    }\n\n    boolean currentDSSMerged = false;\n      \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1 && currentDocStoreSegment != null) {\n          currentDSSMerged |= currentDocStoreSegment.equals(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (currentDSSMerged) {\n            if (infoStream != null) {\n              message(\"now flush at mergeMiddle\");\n            }\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        merge.info.setDocStore(-1, null, false);\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      if (merge.useCompoundFile) {\n\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      // if the merged segment warmer was not installed when\n      // this merge was started, causing us to not force\n      // the docStores to close, we can't warm it now\n      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);\n\n      if (poolReaders && mergedSegmentWarmer != null && canWarm) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3bb13258feba31ab676502787ab2e1779f129b7a","date":1291596436,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final String currentDocStoreSegment;\n    synchronized(this) {\n      currentDocStoreSegment = docWriter.getDocStoreSegment();\n    }\n\n    boolean currentDSSMerged = false;\n      \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1 && currentDocStoreSegment != null) {\n          currentDSSMerged |= currentDocStoreSegment.equals(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (currentDSSMerged) {\n            if (infoStream != null) {\n              message(\"now flush at mergeMiddle\");\n            }\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        merge.info.setDocStore(-1, null, false);\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      if (merge.useCompoundFile) {\n\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      // if the merged segment warmer was not installed when\n      // this merge was started, causing us to not force\n      // the docStores to close, we can't warm it now\n      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);\n\n      if (poolReaders && mergedSegmentWarmer != null && canWarm) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        // If the doc store we are using has been closed and\n        // is in now compound format (but wasn't when we\n        // started), then we will switch to the compound\n        // format as well:\n        setMergeDocStoreIsCompoundFile(merge);\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n      \n      // if the merged segment warmer was not installed when\n      // this merge was started, causing us to not force\n      // the docStores to close, we can't warm it now\n      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);\n\n      if (poolReaders && mergedSegmentWarmer != null && canWarm) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], false);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        synchronized(this) {\n          deleter.deleteFile(compoundFileName);\n        }\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4a69e5860d014751cc9329dfeb441a6d8fd1ed8e","date":1291833341,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final String currentDocStoreSegment;\n    synchronized(this) {\n      currentDocStoreSegment = docWriter.getDocStoreSegment();\n    }\n\n    boolean currentDSSMerged = false;\n      \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1 && currentDocStoreSegment != null) {\n          currentDSSMerged |= currentDocStoreSegment.equals(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (currentDSSMerged) {\n            if (infoStream != null) {\n              message(\"now flush at mergeMiddle\");\n            }\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        merge.info.setDocStore(-1, null, false);\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      // if the merged segment warmer was not installed when\n      // this merge was started, causing us to not force\n      // the docStores to close, we can't warm it now\n      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);\n\n      if (poolReaders && mergedSegmentWarmer != null && canWarm) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final String currentDocStoreSegment;\n    synchronized(this) {\n      currentDocStoreSegment = docWriter.getDocStoreSegment();\n    }\n\n    boolean currentDSSMerged = false;\n      \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1 && currentDocStoreSegment != null) {\n          currentDSSMerged |= currentDocStoreSegment.equals(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (currentDSSMerged) {\n            if (infoStream != null) {\n              message(\"now flush at mergeMiddle\");\n            }\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        merge.info.setDocStore(-1, null, false);\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      if (merge.useCompoundFile) {\n\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      // if the merged segment warmer was not installed when\n      // this merge was started, causing us to not force\n      // the docStores to close, we can't warm it now\n      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);\n\n      if (poolReaders && mergedSegmentWarmer != null && canWarm) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"44fcbde6fb2ac44ee3b45e013e54a42911e689ff","date":1292065621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final String currentDocStoreSegment;\n    synchronized(this) {\n      currentDocStoreSegment = docWriter.getDocStoreSegment();\n    }\n\n    boolean currentDSSMerged = false;\n      \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1 && currentDocStoreSegment != null) {\n          currentDSSMerged |= currentDocStoreSegment.equals(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (currentDSSMerged) {\n            if (infoStream != null) {\n              message(\"now flush at mergeMiddle\");\n            }\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        merge.info.setDocStore(-1, null, false);\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      // if the merged segment warmer was not installed when\n      // this merge was started, causing us to not force\n      // the docStores to close, we can't warm it now\n      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);\n\n      if (poolReaders && mergedSegmentWarmer != null && canWarm) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, merger, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final String currentDocStoreSegment;\n    synchronized(this) {\n      currentDocStoreSegment = docWriter.getDocStoreSegment();\n    }\n\n    boolean currentDSSMerged = false;\n      \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1 && currentDocStoreSegment != null) {\n          currentDSSMerged |= currentDocStoreSegment.equals(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (currentDSSMerged) {\n            if (infoStream != null) {\n              message(\"now flush at mergeMiddle\");\n            }\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        merge.info.setDocStore(-1, null, false);\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      // if the merged segment warmer was not installed when\n      // this merge was started, causing us to not force\n      // the docStores to close, we can't warm it now\n      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);\n\n      if (poolReaders && mergedSegmentWarmer != null && canWarm) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7e1cbd7e289dc1243c7a59e1a83d078163a147fe","date":1292268032,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final String currentDocStoreSegment;\n    synchronized(this) {\n      currentDocStoreSegment = docWriter.getDocStoreSegment();\n    }\n\n    boolean currentDSSMerged = false;\n      \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1 && currentDocStoreSegment != null) {\n          currentDSSMerged |= currentDocStoreSegment.equals(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (currentDSSMerged) {\n            if (infoStream != null) {\n              message(\"now flush at mergeMiddle\");\n            }\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        merge.info.setDocStore(-1, null, false);\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n      merge.info.setHasVectors(merger.hasVectors() || merge.hasVectors);\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      // if the merged segment warmer was not installed when\n      // this merge was started, causing us to not force\n      // the docStores to close, we can't warm it now\n      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);\n\n      if (poolReaders && mergedSegmentWarmer != null && canWarm) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, merger, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final String currentDocStoreSegment;\n    synchronized(this) {\n      currentDocStoreSegment = docWriter.getDocStoreSegment();\n    }\n\n    boolean currentDSSMerged = false;\n      \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1 && currentDocStoreSegment != null) {\n          currentDSSMerged |= currentDocStoreSegment.equals(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (currentDSSMerged) {\n            if (infoStream != null) {\n              message(\"now flush at mergeMiddle\");\n            }\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        merge.info.setDocStore(-1, null, false);\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      // if the merged segment warmer was not installed when\n      // this merge was started, causing us to not force\n      // the docStores to close, we can't warm it now\n      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);\n\n      if (poolReaders && mergedSegmentWarmer != null && canWarm) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, merger, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e06c9d5fba0a2f937941d199d64ccb32aac502d1","date":1292411167,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final String currentDocStoreSegment;\n    synchronized(this) {\n      currentDocStoreSegment = docWriter.getDocStoreSegment();\n    }\n\n    boolean currentDSSMerged = false;\n      \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1 && currentDocStoreSegment != null) {\n          currentDSSMerged |= currentDocStoreSegment.equals(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (currentDSSMerged) {\n            if (infoStream != null) {\n              message(\"now flush at mergeMiddle\");\n            }\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        merge.info.setDocStore(-1, null, false);\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      // if the merged segment warmer was not installed when\n      // this merge was started, causing us to not force\n      // the docStores to close, we can't warm it now\n      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);\n\n      if (poolReaders && mergedSegmentWarmer != null && canWarm) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, merger, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final String currentDocStoreSegment;\n    synchronized(this) {\n      currentDocStoreSegment = docWriter.getDocStoreSegment();\n    }\n\n    boolean currentDSSMerged = false;\n      \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1 && currentDocStoreSegment != null) {\n          currentDSSMerged |= currentDocStoreSegment.equals(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (currentDSSMerged) {\n            if (infoStream != null) {\n              message(\"now flush at mergeMiddle\");\n            }\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        merge.info.setDocStore(-1, null, false);\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n      merge.info.setHasVectors(merger.hasVectors() || merge.hasVectors);\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      // if the merged segment warmer was not installed when\n      // this merge was started, causing us to not force\n      // the docStores to close, we can't warm it now\n      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);\n\n      if (poolReaders && mergedSegmentWarmer != null && canWarm) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, merger, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5","date":1292695408,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final String currentDocStoreSegment;\n    synchronized(this) {\n      currentDocStoreSegment = docWriter.getDocStoreSegment();\n    }\n\n    boolean currentDSSMerged = false;\n      \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1 && currentDocStoreSegment != null) {\n          currentDSSMerged |= currentDocStoreSegment.equals(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (currentDSSMerged) {\n            if (infoStream != null) {\n              message(\"now flush at mergeMiddle\");\n            }\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        merge.info.setDocStore(-1, null, false);\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      // if the merged segment warmer was not installed when\n      // this merge was started, causing us to not force\n      // the docStores to close, we can't warm it now\n      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);\n\n      if (poolReaders && mergedSegmentWarmer != null && canWarm) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, merger, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"94cb8b3ec0439dfd8e179637ee4191cd9c6227e5","date":1292711882,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + numSegments);\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f82bdb39e96d0f03d4e6482f4c835775856ccdef","date":1292757839,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + numSegments);\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + numSegments);\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"355542ffa5812b7a09147c6a46a0948b36c5966f","date":1292760986,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + numSegments);\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + numSegments);\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + numSegments);\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final String currentDocStoreSegment;\n    synchronized(this) {\n      currentDocStoreSegment = docWriter.getDocStoreSegment();\n    }\n\n    boolean currentDSSMerged = false;\n      \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1 && currentDocStoreSegment != null) {\n          currentDSSMerged |= currentDocStoreSegment.equals(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n\n          // If 1) we must now merge doc stores, and 2) at\n          // least one of the segments we are merging uses\n          // the doc store we are now writing to, we must at\n          // this point force this doc store closed (by\n          // calling flush).  If we didn't do this then the\n          // readers will attempt to open an IndexInput\n          // on files that have still-open IndexOutputs\n          // against them:\n          if (currentDSSMerged) {\n            if (infoStream != null) {\n              message(\"now flush at mergeMiddle\");\n            }\n            doFlush(true, false);\n            updatePendingMerges(1, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        merge.info.setDocStore(-1, null, false);\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      // if the merged segment warmer was not installed when\n      // this merge was started, causing us to not force\n      // the docStores to close, we can't warm it now\n      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);\n\n      if (poolReaders && mergedSegmentWarmer != null && canWarm) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + numSegments);\n      }\n\n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      for(int i=0;i<numSegments;i++) {\n        merge.readersClone[i].openDocStores();\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.hasProx());\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      synchronized(this) {\n        assert merge.mergeFiles == null;\n        merge.mergeFiles = merge.info.files();\n        deleter.incRef(merge.mergeFiles);\n      }\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the\n              // only reference\n              assert merge.readersClone[i].getRefCount() == 0: \"refCount should be 0 but is \" + merge.readersClone[i].getRefCount();\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName, merge.info);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe","date":1294227869,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + numSegments);\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + numSegments);\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"70ad682703b8585f5d0a637efec044d57ec05efb","date":1294259117,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + numSegments);\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + numSegments);\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + numSegments);\n      }\n\n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + numSegments);\n      }\n\n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"87d122575733b906e11f496c1d6b4d1327f5308d","date":1295812808,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + numSegments);\n      }\n      anyNonBulkMerges |= merger.getMatchedSubReaderCount() != numSegments;\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + numSegments);\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bb9b72f7c3d7827c64dd4ec580ded81778da361d","date":1295897920,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + numSegments);\n      }\n      anyNonBulkMerges |= merger.getMatchedSubReaderCount() != numSegments;\n\n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + numSegments);\n      }\n\n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + numSegments);\n      }\n      anyNonBulkMerges |= merger.getMatchedSubReaderCount() != numSegments;\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + numSegments);\n      }\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3","date":1297940445,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (reader.numDocs() > 0) {\n          merger.add(clone);\n        }\n        totDocCount += clone.numDocs();\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getMatchedSubReaderCount() != merge.readers.size();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + numSegments);\n      }\n      anyNonBulkMerges |= merger.getMatchedSubReaderCount() != numSegments;\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f1bdbf92da222965b46c0a942c3857ba56e5c638","date":1298297608,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (reader.numDocs() > 0) {\n          merger.add(clone);\n        }\n        totDocCount += clone.numDocs();\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getMatchedSubReaderCount() != merge.readers.size();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + numSegments);\n      }\n      anyNonBulkMerges |= merger.getMatchedSubReaderCount() != numSegments;\n      \n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0c7a8f7304b75b1528814c5820fa23a96816c27","date":1298314239,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.info.clearFilesCache();\n\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (reader.numDocs() > 0) {\n          merger.add(clone);\n        }\n        totDocCount += clone.numDocs();\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getMatchedSubReaderCount() != merge.readers.size();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.clearFilesCache();\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (reader.numDocs() > 0) {\n          merger.add(clone);\n        }\n        totDocCount += clone.numDocs();\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getMatchedSubReaderCount() != merge.readers.size();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.info.clearFilesCache();\n\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (reader.numDocs() > 0) {\n          merger.add(clone);\n        }\n        totDocCount += clone.numDocs();\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getMatchedSubReaderCount() != merge.readers.size();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.clearFilesCache();\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, true,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -config.getReaderTermsIndexDivisor());\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + numSegments);\n      }\n      anyNonBulkMerges |= merger.getMatchedSubReaderCount() != numSegments;\n\n      assert mergedDocCount == totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4b557198058275e3f7a063a47d60c1f94426bc5e","date":1298804526,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.info.clearFilesCache();\n\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (reader.numDocs() > 0) {\n          merger.add(clone);\n        }\n        totDocCount += clone.numDocs();\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getMatchedSubReaderCount() != merge.readers.size();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.clearFilesCache();\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.info.clearFilesCache();\n\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (reader.numDocs() > 0) {\n          merger.add(clone);\n        }\n        totDocCount += clone.numDocs();\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getMatchedSubReaderCount() != merge.readers.size();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.clearFilesCache();\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f4b7c426b6e92fa66946ff52f0508da82ee4de33","date":1298804814,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.info.clearFilesCache();\n\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (reader.numDocs() > 0) {\n          merger.add(clone);\n        }\n        totDocCount += clone.numDocs();\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getMatchedSubReaderCount() != merge.readers.size();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.clearFilesCache();\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.info.clearFilesCache();\n\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (reader.numDocs() > 0) {\n          merger.add(clone);\n        }\n        totDocCount += clone.numDocs();\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getMatchedSubReaderCount() != merge.readers.size();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.clearFilesCache();\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"14ec33385f6fbb6ce172882d14605790418a5d31","date":1298910796,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (reader.numDocs() > 0) {\n          merger.add(clone);\n        }\n        totDocCount += clone.numDocs();\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getMatchedSubReaderCount() != merge.readers.size();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.info.clearFilesCache();\n\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (reader.numDocs() > 0) {\n          merger.add(clone);\n        }\n        totDocCount += clone.numDocs();\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getMatchedSubReaderCount() != merge.readers.size();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.clearFilesCache();\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"df59c069ce7c8a1965072ad853d74ba87cfdf8a4","date":1298914713,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getMatchedSubReaderCount() != merge.readers.size();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (reader.numDocs() > 0) {\n          merger.add(clone);\n        }\n        totDocCount += clone.numDocs();\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getMatchedSubReaderCount() != merge.readers.size();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"91824676e57e2d1c945918662b835813385cd291","date":1299972146,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getMatchedSubReaderCount() != merge.readers.size();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"00b21520fafb9860ce0318d7be5ea84619c185ad","date":1300444600,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1224a4027481acce15495b03bce9b48b93b42722","date":1300792329,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n      // Record if we have merged vectors\n      merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d619839baa8ce5503e496b94a9e42ad6f079293f","date":1301309428,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n      // Record if we have merged vectors\n      merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (reader.numDocs() > 0) {\n          merger.add(clone);\n        }\n        totDocCount += clone.numDocs();\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getMatchedSubReaderCount() != merge.readers.size();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0ef0193974807e4bddf5432a6b0287fe4d6c9df","date":1301476645,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n      // Record if we have merged vectors\n      merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.info.clearFilesCache();\n\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (reader.numDocs() > 0) {\n          merger.add(clone);\n        }\n        totDocCount += clone.numDocs();\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getMatchedSubReaderCount() != merge.readers.size();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.clearFilesCache();\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"01e5948db9a07144112d2f08f28ca2e3cd880348","date":1301759232,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.estimatedMergeBytes = 0;\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        final int readerMaxDoc = reader.maxDoc();\n        if (readerMaxDoc > 0) {\n          final int delCount = reader.numDeletedDocs();\n          final double delRatio = ((double) delCount)/readerMaxDoc;\n          merge.estimatedMergeBytes += info.sizeInBytes(true) * (1.0 - delRatio);\n        }\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n      // Record if we have merged vectors\n      merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n      // Record if we have merged vectors\n      merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":["c50bf8a3310d2aec44c01b0818c308b2e0ac6b33"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"45669a651c970812a680841b97a77cce06af559f","date":1301922222,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.estimatedMergeBytes = 0;\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        final int readerMaxDoc = reader.maxDoc();\n        if (readerMaxDoc > 0) {\n          final int delCount = reader.numDeletedDocs();\n          final double delRatio = ((double) delCount)/readerMaxDoc;\n          merge.estimatedMergeBytes += info.sizeInBytes(true) * (1.0 - delRatio);\n        }\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n      // Record if we have merged vectors\n      merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n      // Record if we have merged vectors\n      merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.estimatedMergeBytes = 0;\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        final int readerMaxDoc = reader.maxDoc();\n        if (readerMaxDoc > 0) {\n          final int delCount = reader.numDeletedDocs();\n          final double delRatio = ((double) delCount)/readerMaxDoc;\n          merge.estimatedMergeBytes += info.sizeInBytes(true) * (1.0 - delRatio);\n        }\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n      // Record if we have merged vectors\n      merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.estimatedMergeBytes = 0;\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        final int readerMaxDoc = reader.maxDoc();\n        if (readerMaxDoc > 0) {\n          final int delCount = reader.numDeletedDocs();\n          final double delRatio = ((double) delCount)/readerMaxDoc;\n          merge.estimatedMergeBytes += info.sizeInBytes(true) * (1.0 - delRatio);\n        }\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n      // Record if we have merged vectors\n      merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.estimatedMergeBytes = 0;\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        final int readerMaxDoc = reader.maxDoc();\n        if (readerMaxDoc > 0) {\n          final int delCount = reader.numDeletedDocs();\n          final double delRatio = ((double) delCount)/readerMaxDoc;\n          merge.estimatedMergeBytes += info.sizeInBytes(true) * (1.0 - delRatio);\n        }\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n      // Record if we have merged vectors\n      merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n      // Record if we have merged vectors\n      merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c50bf8a3310d2aec44c01b0818c308b2e0ac6b33","date":1304363189,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n      // Record if we have merged vectors\n      merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.estimatedMergeBytes = 0;\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        final int readerMaxDoc = reader.maxDoc();\n        if (readerMaxDoc > 0) {\n          final int delCount = reader.numDeletedDocs();\n          final double delRatio = ((double) delCount)/readerMaxDoc;\n          merge.estimatedMergeBytes += info.sizeInBytes(true) * (1.0 - delRatio);\n        }\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n      // Record if we have merged vectors\n      merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":["01e5948db9a07144112d2f08f28ca2e3cd880348"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5270fb4f55a1b77663dda53cb8090c083f0a23b3","date":1305050821,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n      // Record if we have merged vectors\n      merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n      // Record if we have merged vectors\n      merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d3fe2fc74577855eadfb5eae3153c2fffdaaf791","date":1305237079,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n      // Record if we have merged vectors\n      merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c700f8d0842d3e52bb2bdfbfdc046a137e836edb","date":1305285499,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    merge.estimatedMergeBytes = 0;\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        final int readerMaxDoc = reader.maxDoc();\n        if (readerMaxDoc > 0) {\n          final int delCount = reader.numDeletedDocs();\n          final double delRatio = ((double) delCount)/readerMaxDoc;\n          merge.estimatedMergeBytes += info.sizeInBytes(true) * (1.0 - delRatio);\n        }\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n      // Record if we have merged vectors\n      merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             codecs, payloadProcessorProvider,\n                                             merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merger.fieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.info(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n      // Record if we have merged vectors\n      merge.info.setHasVectors(merger.fieldInfos().hasVectors());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n      \n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      merge.info.setHasProx(merger.fieldInfos().hasProx());\n\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"639c36565ce03aed5b0fce7c9e4448e53a1f7efd","date":1308580104,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    context,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      // nocommit  should we use another flag \"isMergedSegment\" or a \"READ\" context here?\n      \n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, context, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0aab6e810b4b0d3743d6a048be0602801f4b3920","date":1308671625,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              \n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2553b00f699380c64959ccb27991289aae87be2e","date":1309290151,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              \n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b6f9be74ca7baaef11857ad002cad40419979516","date":1309449808,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), context);\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    context,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      // nocommit  should we use another flag \"isMergedSegment\" or a \"READ\" context here?\n      \n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, context, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    context,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      // nocommit  should we use another flag \"isMergedSegment\" or a \"READ\" context here?\n      \n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, context, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c162b4b9ef005383c38e0912c16267584dc40de5","date":1310021443,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), context);\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    context,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, context, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), context);\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    context,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      // nocommit  should we use another flag \"isMergedSegment\" or a \"READ\" context here?\n      \n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, context, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), context);\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    context,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              \n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, context, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), context);\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    context,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, context, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ddc4c914be86e34b54f70023f45a60fa7f04e929","date":1310115160,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), context);\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    context,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              \n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, context, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              \n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5d004d0e0b3f65bb40da76d476d659d7888270e8","date":1310158940,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), context);\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    context,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              \n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, context, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos());\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    MERGE_READ_BUFFER_SIZE,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              \n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0061262413ecc163d6eebba1b5c43ab91a0c2dc5","date":1311195279,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), context);\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<BitVector>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, context);\n\n        // Carefully pull the most recent live docs:\n        final BitVector liveDocs = getLiveDocsClone(info, reader);\n\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n\n        if (liveDocs == null || liveDocs.count() > 0) {\n          merger.add(reader, liveDocs);\n          totDocCount += liveDocs == null ? reader.maxDoc() : liveDocs.count();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              \n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final boolean loadDocStores;\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        loadDocStores = true;\n      } else {\n        loadDocStores = false;\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, new IOContext(IOContext.Context.READ));\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader, IOContext.Context.READ)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), context);\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerClones = new ArrayList<SegmentReader>();\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, true,\n                                                    context,\n                                                    -config.getReaderTermsIndexDivisor());\n        merge.readers.add(reader);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        final SegmentReader clone = (SegmentReader) reader.clone(true);\n        merge.readerClones.add(clone);\n\n        if (clone.numDocs() > 0) {\n          merger.add(clone);\n          totDocCount += clone.numDocs();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              \n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      final int termsIndexDivisor;\n      final boolean loadDocStores;\n\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        termsIndexDivisor = config.getReaderTermsIndexDivisor();\n        loadDocStores = true;\n      } else {\n        termsIndexDivisor = -1;\n        loadDocStores = false;\n      }\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, context, termsIndexDivisor);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":["c95a819869502635864dac0a788f874787e3395b"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a1b4dec651d0a89767d1233eef7353f88a42bea0","date":1314786996,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), context);\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<BitVector>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, context);\n\n        // Carefully pull the most recent live docs:\n        final BitVector liveDocs = getLiveDocsClone(info, reader);\n\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n\n        if (liveDocs == null || liveDocs.count() > 0) {\n          merger.add(reader, liveDocs);\n          totDocCount += liveDocs == null ? reader.maxDoc() : liveDocs.count();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final boolean loadDocStores;\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        loadDocStores = true;\n      } else {\n        loadDocStores = false;\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, new IOContext(IOContext.Context.READ));\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader, IOContext.Context.READ)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), context);\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<BitVector>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, context);\n\n        // Carefully pull the most recent live docs:\n        final BitVector liveDocs = getLiveDocsClone(info, reader);\n\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n\n        if (liveDocs == null || liveDocs.count() > 0) {\n          merger.add(reader, liveDocs);\n          totDocCount += liveDocs == null ? reader.maxDoc() : liveDocs.count();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              \n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final boolean loadDocStores;\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        loadDocStores = true;\n      } else {\n        loadDocStores = false;\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, new IOContext(IOContext.Context.READ));\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader, IOContext.Context.READ)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<BitVector>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, context);\n\n        // Carefully pull the most recent live docs:\n        final BitVector liveDocs = getLiveDocsClone(info, reader);\n\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n\n        if (liveDocs == null || liveDocs.count() > 0) {\n          merger.add(reader, liveDocs);\n          totDocCount += liveDocs == null ? reader.maxDoc() : liveDocs.count();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n\n      if (infoStream != null) {\n        message(\"merge codecs=\" + merger.getCodec());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final boolean loadDocStores;\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        loadDocStores = true;\n      } else {\n        loadDocStores = false;\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, new IOContext(IOContext.Context.READ));\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader, IOContext.Context.READ)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), context);\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<BitVector>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, context);\n\n        // Carefully pull the most recent live docs:\n        final BitVector liveDocs = getLiveDocsClone(info, reader);\n\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n\n        if (liveDocs == null || liveDocs.count() > 0) {\n          merger.add(reader, liveDocs);\n          totDocCount += liveDocs == null ? reader.maxDoc() : liveDocs.count();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setSegmentCodecs(merger.getSegmentCodecs());\n\n      if (infoStream != null) {\n        message(\"merge segmentCodecs=\" + merger.getSegmentCodecs());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final boolean loadDocStores;\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        loadDocStores = true;\n      } else {\n        loadDocStores = false;\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, new IOContext(IOContext.Context.READ));\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader, IOContext.Context.READ)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream != null) {\n      infoStream.message(\"IW\", \"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<BitVector>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, context);\n\n        // Carefully pull the most recent live docs:\n        final BitVector liveDocs = getLiveDocsClone(info, reader);\n\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n\n        if (liveDocs == null || liveDocs.count() > 0) {\n          merger.add(reader, liveDocs);\n          totDocCount += liveDocs == null ? reader.maxDoc() : liveDocs.count();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        infoStream.message(\"IW\", \"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream != null) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec);\n      }\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final boolean loadDocStores;\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        loadDocStores = true;\n      } else {\n        loadDocStores = false;\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, new IOContext(IOContext.Context.READ));\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader, IOContext.Context.READ)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(), mergedName, merge,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream != null) {\n      message(\"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<BitVector>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, context);\n\n        // Carefully pull the most recent live docs:\n        final BitVector liveDocs = getLiveDocsClone(info, reader);\n\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n\n        if (liveDocs == null || liveDocs.count() > 0) {\n          merger.add(reader, liveDocs);\n          totDocCount += liveDocs == null ? reader.maxDoc() : liveDocs.count();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge();\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(merger.getCodec());\n\n      if (infoStream != null) {\n        message(\"merge codecs=\" + merger.getCodec());\n        message(\"merge store matchedCount=\" + merger.getMatchedSubReaderCount() + \" vs \" + merge.readers.size());\n      }\n      anyNonBulkMerges |= merger.getAnyNonBulkMerges();\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            message(\"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              message(\"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              message(\"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final boolean loadDocStores;\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        loadDocStores = true;\n      } else {\n        loadDocStores = false;\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, new IOContext(IOContext.Context.READ));\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader, IOContext.Context.READ)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream != null) {\n      infoStream.message(\"IW\", \"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<BitVector>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, context);\n\n        // Carefully pull the most recent live docs:\n        final BitVector liveDocs = getLiveDocsClone(info, reader);\n\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n\n        if (liveDocs == null || liveDocs.count() > 0) {\n          merger.add(reader, liveDocs);\n          totDocCount += liveDocs == null ? reader.maxDoc() : liveDocs.count();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        infoStream.message(\"IW\", \"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream != null) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec);\n      }\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final boolean loadDocStores;\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        loadDocStores = true;\n      } else {\n        loadDocStores = false;\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, new IOContext(IOContext.Context.READ));\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader, IOContext.Context.READ)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream != null) {\n      infoStream.message(\"IW\", \"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<BitVector>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, context);\n\n        // Carefully pull the most recent live docs:\n        final BitVector liveDocs = getLiveDocsClone(info, reader);\n\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n\n        if (liveDocs == null || liveDocs.count() > 0) {\n          merger.add(reader, liveDocs);\n          totDocCount += liveDocs == null ? reader.maxDoc() : liveDocs.count();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        infoStream.message(\"IW\", \"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream != null) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec);\n      }\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          merger.createCompoundFile(compoundFileName, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes(true)/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final boolean loadDocStores;\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        loadDocStores = true;\n      } else {\n        loadDocStores = false;\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, new IOContext(IOContext.Context.READ));\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader, IOContext.Context.READ)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"58c6bbc222f074c844e736e6fb23647e3db9cfe3","date":1322743940,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<BitVector>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, context);\n\n        // Carefully pull the most recent live docs:\n        final BitVector liveDocs = getLiveDocsClone(info, reader);\n\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n\n        if (liveDocs == null || liveDocs.count() > 0) {\n          merger.add(reader, liveDocs);\n          totDocCount += liveDocs == null ? reader.maxDoc() : liveDocs.count();\n        }\n        segUpto++;\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec);\n      }\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            infoStream.message(\"IW\", \"abort merge after building CFS\");\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final boolean loadDocStores;\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        loadDocStores = true;\n      } else {\n        loadDocStores = false;\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, new IOContext(IOContext.Context.READ));\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader, IOContext.Context.READ)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream != null) {\n      infoStream.message(\"IW\", \"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<BitVector>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, context);\n\n        // Carefully pull the most recent live docs:\n        final BitVector liveDocs = getLiveDocsClone(info, reader);\n\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n\n        if (liveDocs == null || liveDocs.count() > 0) {\n          merger.add(reader, liveDocs);\n          totDocCount += liveDocs == null ? reader.maxDoc() : liveDocs.count();\n        }\n        segUpto++;\n      }\n\n      if (infoStream != null) {\n        infoStream.message(\"IW\", \"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream != null) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec);\n      }\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream != null) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream != null) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream != null) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final boolean loadDocStores;\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        loadDocStores = true;\n      } else {\n        loadDocStores = false;\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, new IOContext(IOContext.Context.READ));\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader, IOContext.Context.READ)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c5df35ab57c223ea11aec64b53bf611904f3dced","date":1323640545,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<BitVector>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, context);\n\n        // Carefully pull the most recent live docs:\n        final BitVector liveDocs = getLiveDocsClone(info, reader);\n\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n\n        if (liveDocs == null || liveDocs.count() > 0) {\n          merger.add(reader, liveDocs);\n          totDocCount += liveDocs == null ? reader.maxDoc() : liveDocs.count();\n        }\n        segUpto++;\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec);\n      }\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final boolean loadDocStores;\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        loadDocStores = true;\n      } else {\n        loadDocStores = false;\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, new IOContext(IOContext.Context.READ));\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader, IOContext.Context.READ)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<BitVector>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, context);\n\n        // Carefully pull the most recent live docs:\n        final BitVector liveDocs = getLiveDocsClone(info, reader);\n\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n\n        if (liveDocs == null || liveDocs.count() > 0) {\n          merger.add(reader, liveDocs);\n          totDocCount += liveDocs == null ? reader.maxDoc() : liveDocs.count();\n        }\n        segUpto++;\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec);\n      }\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            infoStream.message(\"IW\", \"abort merge after building CFS\");\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final boolean loadDocStores;\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        loadDocStores = true;\n      } else {\n        loadDocStores = false;\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, new IOContext(IOContext.Context.READ));\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader, IOContext.Context.READ)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","date":1323720782,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<BitVector>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, context);\n\n        // Carefully pull the most recent live docs:\n        final BitVector liveDocs = getLiveDocsClone(info, reader);\n\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n\n        if (liveDocs == null || liveDocs.count() > 0) {\n          merger.add(reader, liveDocs);\n          totDocCount += liveDocs == null ? reader.maxDoc() : liveDocs.count();\n        }\n        segUpto++;\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec);\n      }\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final boolean loadDocStores;\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        loadDocStores = true;\n      } else {\n        loadDocStores = false;\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, new IOContext(IOContext.Context.READ));\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader, IOContext.Context.READ)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<BitVector>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, context);\n\n        // Carefully pull the most recent live docs:\n        final BitVector liveDocs = getLiveDocsClone(info, reader);\n\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n\n        if (liveDocs == null || liveDocs.count() > 0) {\n          merger.add(reader, liveDocs);\n          totDocCount += liveDocs == null ? reader.maxDoc() : liveDocs.count();\n        }\n        segUpto++;\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec);\n      }\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            infoStream.message(\"IW\", \"abort merge after building CFS\");\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final boolean loadDocStores;\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        loadDocStores = true;\n      } else {\n        loadDocStores = false;\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, new IOContext(IOContext.Context.READ));\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader, IOContext.Context.READ)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9ce667c6d3400b22523701c549c0d35e26da8b46","date":1324405053,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<BitVector>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndLiveDocs rld = readerPool.get(info, true);\n        final SegmentReader reader = rld.getMergeReader(context);\n        assert reader != null;\n\n        // Carefully pull the most recent live docs:\n        final BitVector liveDocs;\n        synchronized(this) {\n          // Must sync to ensure BufferedDeletesStream\n          // cannot change liveDocs/pendingDeleteCount while\n          // we pull a copy:\n          liveDocs = rld.getReadOnlyLiveDocs();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.pendingDeleteCount != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.pendingDeleteCount);\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + info + \" no deletes\");\n            }\n          }\n        }\n\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n\n        if (liveDocs == null || liveDocs.count() > 0) {\n          merger.add(reader, liveDocs);\n          totDocCount += liveDocs == null ? reader.maxDoc() : liveDocs.count();\n        } else {\n          //System.out.println(\"  skip seg: fully deleted\");\n        }\n        segUpto++;\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec);\n      }\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndLiveDocs rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            readerPool.release(sr, false);\n          }\n        }\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      if (!commitMerge(merge)) {\n        // commitMerge will return false if this merge was aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + merge.segString(directory) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<BitVector>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final SegmentReader reader = readerPool.get(info, context);\n\n        // Carefully pull the most recent live docs:\n        final BitVector liveDocs = getLiveDocsClone(info, reader);\n\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n\n        if (liveDocs == null || liveDocs.count() > 0) {\n          merger.add(reader, liveDocs);\n          totDocCount += liveDocs == null ? reader.maxDoc() : liveDocs.count();\n        }\n        segUpto++;\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec);\n      }\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final boolean loadDocStores;\n      if (mergedSegmentWarmer != null) {\n        // Load terms index & doc stores so the segment\n        // warmer can run searches, load documents/term\n        // vectors\n        loadDocStores = true;\n      } else {\n        loadDocStores = false;\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, new IOContext(IOContext.Context.READ));\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n\n        if (!commitMerge(merge, mergedReader)) {\n          // commitMerge will return false if this merge was aborted\n          return 0;\n        }\n      } finally {\n        synchronized(this) {\n          if (readerPool.release(mergedReader, IOContext.Context.READ)) {\n            // Must checkpoint after releasing the\n            // mergedReader since it may have written a new\n            // deletes file:\n            checkpoint();\n          }\n        }\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":["c95a819869502635864dac0a788f874787e3395b","ae695f21c50b03702b5d0fa2543d5af844bb7cd3"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ef2642aa76c0d0714b26e5e7d5e3438a62db3cb7","date":1326996651,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<MutableBits>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndLiveDocs rld = readerPool.get(info, true);\n        final SegmentReader reader = rld.getMergeReader(context);\n        assert reader != null;\n\n        // Carefully pull the most recent live docs:\n        final MutableBits liveDocs;\n        synchronized(this) {\n          // Must sync to ensure BufferedDeletesStream\n          // cannot change liveDocs/pendingDeleteCount while\n          // we pull a copy:\n          liveDocs = rld.getReadOnlyLiveDocs();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.pendingDeleteCount != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.pendingDeleteCount);\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + info + \" no deletes\");\n            }\n          }\n        }\n\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n\n        if (liveDocs == null || liveDocs.count() > 0) {\n          merger.add(reader, liveDocs);\n          totDocCount += liveDocs == null ? reader.maxDoc() : liveDocs.count();\n        } else {\n          //System.out.println(\"  skip seg: fully deleted\");\n        }\n        segUpto++;\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec);\n      }\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndLiveDocs rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            readerPool.release(sr, false);\n          }\n        }\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      if (!commitMerge(merge)) {\n        // commitMerge will return false if this merge was aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<BitVector>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndLiveDocs rld = readerPool.get(info, true);\n        final SegmentReader reader = rld.getMergeReader(context);\n        assert reader != null;\n\n        // Carefully pull the most recent live docs:\n        final BitVector liveDocs;\n        synchronized(this) {\n          // Must sync to ensure BufferedDeletesStream\n          // cannot change liveDocs/pendingDeleteCount while\n          // we pull a copy:\n          liveDocs = rld.getReadOnlyLiveDocs();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.pendingDeleteCount != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.pendingDeleteCount);\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + info + \" no deletes\");\n            }\n          }\n        }\n\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n\n        if (liveDocs == null || liveDocs.count() > 0) {\n          merger.add(reader, liveDocs);\n          totDocCount += liveDocs == null ? reader.maxDoc() : liveDocs.count();\n        } else {\n          //System.out.println(\"  skip seg: fully deleted\");\n        }\n        segUpto++;\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec);\n      }\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndLiveDocs rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            readerPool.release(sr, false);\n          }\n        }\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      if (!commitMerge(merge)) {\n        // commitMerge will return false if this merge was aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8a16d06e7522604de20b2d758d9b9464bb30fe02","date":1327070101,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<Bits>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndLiveDocs rld = readerPool.get(info, true);\n        final SegmentReader reader = rld.getMergeReader(context);\n        assert reader != null;\n\n        // Carefully pull the most recent live docs:\n        final Bits liveDocs;\n        synchronized(this) {\n          // Must sync to ensure BufferedDeletesStream\n          // cannot change liveDocs/pendingDeleteCount while\n          // we pull a copy:\n          liveDocs = rld.getReadOnlyLiveDocs();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.pendingDeleteCount != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.pendingDeleteCount);\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + info + \" no deletes\");\n            }\n          }\n        }\n\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n\n        merger.add(reader, liveDocs);\n        segUpto++;\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + mergedDocCount);\n      }\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndLiveDocs rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            readerPool.release(sr, false);\n          }\n        }\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<MutableBits>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndLiveDocs rld = readerPool.get(info, true);\n        final SegmentReader reader = rld.getMergeReader(context);\n        assert reader != null;\n\n        // Carefully pull the most recent live docs:\n        final MutableBits liveDocs;\n        synchronized(this) {\n          // Must sync to ensure BufferedDeletesStream\n          // cannot change liveDocs/pendingDeleteCount while\n          // we pull a copy:\n          liveDocs = rld.getReadOnlyLiveDocs();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.pendingDeleteCount != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.pendingDeleteCount);\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + info + \" no deletes\");\n            }\n          }\n        }\n\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n\n        if (liveDocs == null || liveDocs.count() > 0) {\n          merger.add(reader, liveDocs);\n          totDocCount += liveDocs == null ? reader.maxDoc() : liveDocs.count();\n        } else {\n          //System.out.println(\"  skip seg: fully deleted\");\n        }\n        segUpto++;\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec);\n      }\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndLiveDocs rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            readerPool.release(sr, false);\n          }\n        }\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      if (!commitMerge(merge)) {\n        // commitMerge will return false if this merge was aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":["c95a819869502635864dac0a788f874787e3395b"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ccad4bab070f323ce610caa0040346d4a87213dc","date":1327747432,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<Bits>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndLiveDocs rld = readerPool.get(info, true);\n        final SegmentReader reader = rld.getMergeReader(context);\n        assert reader != null;\n\n        // Carefully pull the most recent live docs:\n        final Bits liveDocs;\n        synchronized(this) {\n          // Must sync to ensure BufferedDeletesStream\n          // cannot change liveDocs/pendingDeleteCount while\n          // we pull a copy:\n          liveDocs = rld.getReadOnlyLiveDocs();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.pendingDeleteCount != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.pendingDeleteCount);\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + info + \" no deletes\");\n            }\n          }\n        }\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n        merger.add(reader, liveDocs);\n        segUpto++;\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + mergedDocCount);\n      }\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndLiveDocs rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            readerPool.release(sr, false);\n          }\n        }\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<Bits>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndLiveDocs rld = readerPool.get(info, true);\n        final SegmentReader reader = rld.getMergeReader(context);\n        assert reader != null;\n\n        // Carefully pull the most recent live docs:\n        final Bits liveDocs;\n        synchronized(this) {\n          // Must sync to ensure BufferedDeletesStream\n          // cannot change liveDocs/pendingDeleteCount while\n          // we pull a copy:\n          liveDocs = rld.getReadOnlyLiveDocs();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.pendingDeleteCount != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.pendingDeleteCount);\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + info + \" no deletes\");\n            }\n          }\n        }\n\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n\n        merger.add(reader, liveDocs);\n        segUpto++;\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + mergedDocCount);\n      }\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndLiveDocs rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            readerPool.release(sr, false);\n          }\n        }\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"32feb7c2c571b402d2e231bd8e3b6add4af6d6eb","date":1327773585,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<Bits>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndLiveDocs rld = readerPool.get(info, true);\n        final SegmentReader reader = rld.getMergeReader(context);\n        assert reader != null;\n\n        // Carefully pull the most recent live docs:\n        final Bits liveDocs;\n        synchronized(this) {\n          // Must sync to ensure BufferedDeletesStream\n          // cannot change liveDocs/pendingDeleteCount while\n          // we pull a copy:\n          liveDocs = rld.getReadOnlyLiveDocs();\n\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.pendingDeleteCount != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.pendingDeleteCount);\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + info + \" no deletes\");\n            }\n          }\n        }\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n        final int delCount = rld.pendingDeleteCount + info.getDelCount();\n        assert delCount <= info.docCount;\n        if (delCount < info.docCount) {\n          merger.add(reader, liveDocs);\n        }\n        segUpto++;\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + mergedDocCount);\n      }\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndLiveDocs rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            readerPool.release(sr, false);\n          }\n        }\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      if (!commitMerge(merge)) {\n        // commitMerge will return false if this merge was aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<Bits>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndLiveDocs rld = readerPool.get(info, true);\n        final SegmentReader reader = rld.getMergeReader(context);\n        assert reader != null;\n\n        // Carefully pull the most recent live docs:\n        final Bits liveDocs;\n        synchronized(this) {\n          // Must sync to ensure BufferedDeletesStream\n          // cannot change liveDocs/pendingDeleteCount while\n          // we pull a copy:\n          liveDocs = rld.getReadOnlyLiveDocs();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.pendingDeleteCount != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.pendingDeleteCount);\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + info + \" no deletes\");\n            }\n          }\n        }\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n        merger.add(reader, liveDocs);\n        segUpto++;\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + mergedDocCount);\n      }\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndLiveDocs rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            readerPool.release(sr, false);\n          }\n        }\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":["ae695f21c50b03702b5d0fa2543d5af844bb7cd3"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31","date":1327836826,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<Bits>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndLiveDocs rld = readerPool.get(info, true);\n        final SegmentReader reader = rld.getMergeReader(context);\n        assert reader != null;\n\n        // Carefully pull the most recent live docs:\n        final Bits liveDocs;\n        synchronized(this) {\n          // Must sync to ensure BufferedDeletesStream\n          // cannot change liveDocs/pendingDeleteCount while\n          // we pull a copy:\n          liveDocs = rld.getReadOnlyLiveDocs();\n\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.pendingDeleteCount != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.pendingDeleteCount);\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + info + \" no deletes\");\n            }\n          }\n        }\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n        final int delCount = rld.pendingDeleteCount + info.getDelCount();\n        assert delCount <= info.docCount;\n        if (delCount < info.docCount) {\n          merger.add(reader, liveDocs);\n        }\n        segUpto++;\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + mergedDocCount);\n      }\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndLiveDocs rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            readerPool.release(sr, false);\n          }\n        }\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      if (!commitMerge(merge)) {\n        // commitMerge will return false if this merge was aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<BitVector>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndLiveDocs rld = readerPool.get(info, true);\n        final SegmentReader reader = rld.getMergeReader(context);\n        assert reader != null;\n\n        // Carefully pull the most recent live docs:\n        final BitVector liveDocs;\n        synchronized(this) {\n          // Must sync to ensure BufferedDeletesStream\n          // cannot change liveDocs/pendingDeleteCount while\n          // we pull a copy:\n          liveDocs = rld.getReadOnlyLiveDocs();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.pendingDeleteCount != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.pendingDeleteCount);\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + info + \" no deletes\");\n            }\n          }\n        }\n\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n\n        if (liveDocs == null || liveDocs.count() > 0) {\n          merger.add(reader, liveDocs);\n          totDocCount += liveDocs == null ? reader.maxDoc() : liveDocs.count();\n        } else {\n          //System.out.println(\"  skip seg: fully deleted\");\n        }\n        segUpto++;\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec);\n      }\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndLiveDocs rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            readerPool.release(sr, false);\n          }\n        }\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      if (!commitMerge(merge)) {\n        // commitMerge will return false if this merge was aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fd92b8bcc88e969302510acf77bd6970da3994c4","date":1327839530,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<Bits>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndLiveDocs rld = readerPool.get(info, true);\n        final SegmentReader reader = rld.getMergeReader(context);\n        assert reader != null;\n\n        // Carefully pull the most recent live docs:\n        final Bits liveDocs;\n        synchronized(this) {\n          // Must sync to ensure BufferedDeletesStream\n          // cannot change liveDocs/pendingDeleteCount while\n          // we pull a copy:\n          liveDocs = rld.getReadOnlyLiveDocs();\n\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.pendingDeleteCount != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.pendingDeleteCount);\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + info + \" no deletes\");\n            }\n          }\n        }\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n        final int delCount = rld.pendingDeleteCount + info.getDelCount();\n        assert delCount <= info.docCount;\n        if (delCount < info.docCount) {\n          merger.add(reader, liveDocs);\n        }\n        segUpto++;\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + mergedDocCount);\n      }\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndLiveDocs rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            readerPool.release(sr, false);\n          }\n        }\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      if (!commitMerge(merge)) {\n        // commitMerge will return false if this merge was aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<BitVector>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndLiveDocs rld = readerPool.get(info, true);\n        final SegmentReader reader = rld.getMergeReader(context);\n        assert reader != null;\n\n        // Carefully pull the most recent live docs:\n        final BitVector liveDocs;\n        synchronized(this) {\n          // Must sync to ensure BufferedDeletesStream\n          // cannot change liveDocs/pendingDeleteCount while\n          // we pull a copy:\n          liveDocs = rld.getReadOnlyLiveDocs();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.pendingDeleteCount != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.pendingDeleteCount);\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + info + \" no deletes\");\n            }\n          }\n        }\n\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n\n        if (liveDocs == null || liveDocs.count() > 0) {\n          merger.add(reader, liveDocs);\n          totDocCount += liveDocs == null ? reader.maxDoc() : liveDocs.count();\n        } else {\n          //System.out.println(\"  skip seg: fully deleted\");\n        }\n        segUpto++;\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge: total \" + totDocCount + \" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec);\n      }\n\n      assert mergedDocCount == totDocCount: \"mergedDocCount=\" + mergedDocCount + \" vs \" + totDocCount;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndLiveDocs rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            readerPool.release(sr, false);\n          }\n        }\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      if (!commitMerge(merge)) {\n        // commitMerge will return false if this merge was aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<Bits>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndLiveDocs rld = readerPool.get(info, true);\n        final SegmentReader reader = rld.getMergeReader(context);\n        assert reader != null;\n\n        // Carefully pull the most recent live docs:\n        final Bits liveDocs;\n        synchronized(this) {\n          // Must sync to ensure BufferedDeletesStream\n          // cannot change liveDocs/pendingDeleteCount while\n          // we pull a copy:\n          liveDocs = rld.getReadOnlyLiveDocs();\n\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.pendingDeleteCount != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.pendingDeleteCount);\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + info + \" no deletes\");\n            }\n          }\n        }\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n        final int delCount = rld.pendingDeleteCount + info.getDelCount();\n        assert delCount <= info.docCount;\n        if (delCount < info.docCount) {\n          merger.add(reader, liveDocs);\n        }\n        segUpto++;\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + mergedDocCount);\n      }\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndLiveDocs rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            readerPool.release(sr, false);\n          }\n        }\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      if (!commitMerge(merge)) {\n        // commitMerge will return false if this merge was aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge)\n    throws CorruptIndexException, IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n\n    int mergedDocCount = 0;\n\n    List<SegmentInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,\n                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments) + \" mergeVectors=\" + merge.info.getFieldInfos().hasVectors());\n    }\n\n    merge.readers = new ArrayList<SegmentReader>();\n    merge.readerLiveDocs = new ArrayList<Bits>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndLiveDocs rld = readerPool.get(info, true);\n        final SegmentReader reader = rld.getMergeReader(context);\n        assert reader != null;\n\n        // Carefully pull the most recent live docs:\n        final Bits liveDocs;\n        synchronized(this) {\n          // Must sync to ensure BufferedDeletesStream\n          // cannot change liveDocs/pendingDeleteCount while\n          // we pull a copy:\n          liveDocs = rld.getReadOnlyLiveDocs();\n\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.pendingDeleteCount != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.pendingDeleteCount);\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + info + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + info + \" no deletes\");\n            }\n          }\n        }\n        merge.readerLiveDocs.add(liveDocs);\n        merge.readers.add(reader);\n        final int delCount = rld.pendingDeleteCount + info.getDelCount();\n        assert delCount <= info.docCount;\n        if (delCount < info.docCount) {\n          merger.add(reader, liveDocs);\n        }\n        segUpto++;\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState = merger.merge();\n      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;\n\n      // Record which codec was used to write the segment\n      merge.info.setCodec(codec);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + mergedDocCount);\n      }\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);\n      }\n      \n      if (useCompoundFile) {\n        success = false;\n        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        try {\n          if (infoStream.isEnabled(\"IW\")) {\n            infoStream.message(\"IW\", \"create compound file \" + compoundFileName);\n          }\n          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(compoundFileName);\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(merge.info.files());\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(compoundFileName);\n            return 0;\n          }\n        }\n\n        merge.info.setUseCompoundFile(true);\n      }\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(\"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndLiveDocs rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            readerPool.release(sr, false);\n          }\n        }\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      if (!commitMerge(merge)) {\n        // commitMerge will return false if this merge was aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"c5df35ab57c223ea11aec64b53bf611904f3dced":["58c6bbc222f074c844e736e6fb23647e3db9cfe3"],"ef2642aa76c0d0714b26e5e7d5e3438a62db3cb7":["9ce667c6d3400b22523701c549c0d35e26da8b46"],"44fcbde6fb2ac44ee3b45e013e54a42911e689ff":["38a62612cfa4e104080d89d7751a8f1a258ac335"],"0061262413ecc163d6eebba1b5c43ab91a0c2dc5":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"d3fe2fc74577855eadfb5eae3153c2fffdaaf791":["5270fb4f55a1b77663dda53cb8090c083f0a23b3"],"f1bdbf92da222965b46c0a942c3857ba56e5c638":["29ef99d61cda9641b6250bf9567329a6e65f901d","e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31"],"5fff8d3ee91620a83b0a3ac1c9f85b384c7cf32a":["2a186ae8733084223c22044e935e4ef848a143d1"],"a493e6d0c3ad86bd55c0a1360d110142e948f2bd":["1085ea837da8f1e96697e17cf73e1d08e7329261"],"df59c069ce7c8a1965072ad853d74ba87cfdf8a4":["14ec33385f6fbb6ce172882d14605790418a5d31"],"f82bdb39e96d0f03d4e6482f4c835775856ccdef":["94cb8b3ec0439dfd8e179637ee4191cd9c6227e5"],"355542ffa5812b7a09147c6a46a0948b36c5966f":["f82bdb39e96d0f03d4e6482f4c835775856ccdef"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["833a7987bc1c94455fde83e3311f72bddedcfb93","355542ffa5812b7a09147c6a46a0948b36c5966f"],"01e5948db9a07144112d2f08f28ca2e3cd880348":["1224a4027481acce15495b03bce9b48b93b42722"],"14ec33385f6fbb6ce172882d14605790418a5d31":["f4b7c426b6e92fa66946ff52f0508da82ee4de33"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["70ad682703b8585f5d0a637efec044d57ec05efb","87d122575733b906e11f496c1d6b4d1327f5308d"],"f4b7c426b6e92fa66946ff52f0508da82ee4de33":["4b557198058275e3f7a063a47d60c1f94426bc5e"],"94cb8b3ec0439dfd8e179637ee4191cd9c6227e5":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"433777d1eaf9998136cd16515dc0e1eb26f5d535":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"32feb7c2c571b402d2e231bd8e3b6add4af6d6eb":["ccad4bab070f323ce610caa0040346d4a87213dc"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["135621f3a0670a9394eb563224a3b76cc4dddc0f","d3fe2fc74577855eadfb5eae3153c2fffdaaf791"],"9ce667c6d3400b22523701c549c0d35e26da8b46":["c5df35ab57c223ea11aec64b53bf611904f3dced"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["e06c9d5fba0a2f937941d199d64ccb32aac502d1"],"fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe":["355542ffa5812b7a09147c6a46a0948b36c5966f"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["bb9b72f7c3d7827c64dd4ec580ded81778da361d","b0c7a8f7304b75b1528814c5820fa23a96816c27"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"70ad682703b8585f5d0a637efec044d57ec05efb":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe"],"2a186ae8733084223c22044e935e4ef848a143d1":["a493e6d0c3ad86bd55c0a1360d110142e948f2bd"],"1085ea837da8f1e96697e17cf73e1d08e7329261":["619e84c4a25ac93018dce34af2bb41dafbeac829"],"38a62612cfa4e104080d89d7751a8f1a258ac335":["4e8cc373c801e54cec75daf9f52792cb4b17f536"],"9ab1f5591dc05f1f2b5407d809c9699f75554a32":["c498d3f8d75170b121f5eda2c6210ac5beb5d411","5fff8d3ee91620a83b0a3ac1c9f85b384c7cf32a"],"06584e6e98d592b34e1329b384182f368d2025e8":["7b91922b55d15444d554721b352861d028eb8278"],"82470b9595905eea74268b47bd3e8a94306d28d9":["433777d1eaf9998136cd16515dc0e1eb26f5d535"],"0aab6e810b4b0d3743d6a048be0602801f4b3920":["d3fe2fc74577855eadfb5eae3153c2fffdaaf791"],"2553b00f699380c64959ccb27991289aae87be2e":["a3776dccca01c11e7046323cfad46a3b4a471233","0aab6e810b4b0d3743d6a048be0602801f4b3920"],"00b21520fafb9860ce0318d7be5ea84619c185ad":["91824676e57e2d1c945918662b835813385cd291"],"9b832cbed6eb3d54a8bb9339296bdda8eeb53014":["08932c793647a36953d1816b1060121f48820d3f"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["c162b4b9ef005383c38e0912c16267584dc40de5","0aab6e810b4b0d3743d6a048be0602801f4b3920"],"e277c90a96e81a581f45e4b6aaf384ddc50f79bc":["334c1175813aea771a71728cd2c4ee4754fd0603"],"08932c793647a36953d1816b1060121f48820d3f":["c0716ddfa41d3662d014c42086a700ad78fc5dcb"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["f1bdbf92da222965b46c0a942c3857ba56e5c638","1224a4027481acce15495b03bce9b48b93b42722"],"a3776dccca01c11e7046323cfad46a3b4a471233":["1224a4027481acce15495b03bce9b48b93b42722","d3fe2fc74577855eadfb5eae3153c2fffdaaf791"],"91824676e57e2d1c945918662b835813385cd291":["df59c069ce7c8a1965072ad853d74ba87cfdf8a4"],"c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31":["9ce667c6d3400b22523701c549c0d35e26da8b46","32feb7c2c571b402d2e231bd8e3b6add4af6d6eb"],"619e84c4a25ac93018dce34af2bb41dafbeac829":["e277c90a96e81a581f45e4b6aaf384ddc50f79bc"],"f470b537db3da4e8d0c39bc72fae5f9865a9ec3c":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"58c6bbc222f074c844e736e6fb23647e3db9cfe3":["3cc749c053615f5871f3b95715fe292f34e70a53"],"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":["58c6bbc222f074c844e736e6fb23647e3db9cfe3","c5df35ab57c223ea11aec64b53bf611904f3dced"],"8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5":["5fff8d3ee91620a83b0a3ac1c9f85b384c7cf32a"],"fd92b8bcc88e969302510acf77bd6970da3994c4":["9ce667c6d3400b22523701c549c0d35e26da8b46","c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["d3fe2fc74577855eadfb5eae3153c2fffdaaf791"],"4a69e5860d014751cc9329dfeb441a6d8fd1ed8e":["3bb13258feba31ab676502787ab2e1779f129b7a"],"bb9b72f7c3d7827c64dd4ec580ded81778da361d":["868da859b43505d9d2a023bfeae6dd0c795f5295","87d122575733b906e11f496c1d6b4d1327f5308d"],"e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3":["87d122575733b906e11f496c1d6b4d1327f5308d"],"c162b4b9ef005383c38e0912c16267584dc40de5":["b6f9be74ca7baaef11857ad002cad40419979516"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["bde51b089eb7f86171eb3406e38a274743f9b7ac","1224a4027481acce15495b03bce9b48b93b42722"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b6f9be74ca7baaef11857ad002cad40419979516":["639c36565ce03aed5b0fce7c9e4448e53a1f7efd"],"a1b4dec651d0a89767d1233eef7353f88a42bea0":["0061262413ecc163d6eebba1b5c43ab91a0c2dc5"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":["2553b00f699380c64959ccb27991289aae87be2e","ddc4c914be86e34b54f70023f45a60fa7f04e929"],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["0aab6e810b4b0d3743d6a048be0602801f4b3920","d083e83f225b11e5fdd900e83d26ddb385b6955c"],"ccad4bab070f323ce610caa0040346d4a87213dc":["8a16d06e7522604de20b2d758d9b9464bb30fe02"],"7e1cbd7e289dc1243c7a59e1a83d078163a147fe":["44fcbde6fb2ac44ee3b45e013e54a42911e689ff"],"c50bf8a3310d2aec44c01b0818c308b2e0ac6b33":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"1224a4027481acce15495b03bce9b48b93b42722":["00b21520fafb9860ce0318d7be5ea84619c185ad"],"87d122575733b906e11f496c1d6b4d1327f5308d":["fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe"],"833a7987bc1c94455fde83e3311f72bddedcfb93":["8fe956d65251358d755c56f14fe8380644790e47"],"e06c9d5fba0a2f937941d199d64ccb32aac502d1":["7e1cbd7e289dc1243c7a59e1a83d078163a147fe"],"c498d3f8d75170b121f5eda2c6210ac5beb5d411":["85a883878c0af761245ab048babc63d099f835f3","2a186ae8733084223c22044e935e4ef848a143d1"],"85a883878c0af761245ab048babc63d099f835f3":["1085ea837da8f1e96697e17cf73e1d08e7329261","a493e6d0c3ad86bd55c0a1360d110142e948f2bd"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["d619839baa8ce5503e496b94a9e42ad6f079293f","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"3cc749c053615f5871f3b95715fe292f34e70a53":["06584e6e98d592b34e1329b384182f368d2025e8"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["01e5948db9a07144112d2f08f28ca2e3cd880348","45669a651c970812a680841b97a77cce06af559f"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["4a69e5860d014751cc9329dfeb441a6d8fd1ed8e","355542ffa5812b7a09147c6a46a0948b36c5966f"],"334c1175813aea771a71728cd2c4ee4754fd0603":["9b832cbed6eb3d54a8bb9339296bdda8eeb53014"],"5270fb4f55a1b77663dda53cb8090c083f0a23b3":["c50bf8a3310d2aec44c01b0818c308b2e0ac6b33"],"fb10b6bcde550b87d8f10e5f010bd8f3021023b6":["82470b9595905eea74268b47bd3e8a94306d28d9"],"c0716ddfa41d3662d014c42086a700ad78fc5dcb":["f470b537db3da4e8d0c39bc72fae5f9865a9ec3c"],"7b91922b55d15444d554721b352861d028eb8278":["a1b4dec651d0a89767d1233eef7353f88a42bea0"],"b0c7a8f7304b75b1528814c5820fa23a96816c27":["e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3"],"8fe956d65251358d755c56f14fe8380644790e47":["c0716ddfa41d3662d014c42086a700ad78fc5dcb"],"4b557198058275e3f7a063a47d60c1f94426bc5e":["b0c7a8f7304b75b1528814c5820fa23a96816c27"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe"],"45669a651c970812a680841b97a77cce06af559f":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df","01e5948db9a07144112d2f08f28ca2e3cd880348"],"3bb13258feba31ab676502787ab2e1779f129b7a":["9ab1f5591dc05f1f2b5407d809c9699f75554a32","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5"],"8a16d06e7522604de20b2d758d9b9464bb30fe02":["ef2642aa76c0d0714b26e5e7d5e3438a62db3cb7"]},"commit2Childs":{"c5df35ab57c223ea11aec64b53bf611904f3dced":["9ce667c6d3400b22523701c549c0d35e26da8b46","93ccd971aca7fb61b7f1b946e44714cfc80bfc7c"],"ef2642aa76c0d0714b26e5e7d5e3438a62db3cb7":["8a16d06e7522604de20b2d758d9b9464bb30fe02"],"44fcbde6fb2ac44ee3b45e013e54a42911e689ff":["7e1cbd7e289dc1243c7a59e1a83d078163a147fe"],"0061262413ecc163d6eebba1b5c43ab91a0c2dc5":["a1b4dec651d0a89767d1233eef7353f88a42bea0"],"d3fe2fc74577855eadfb5eae3153c2fffdaaf791":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb","0aab6e810b4b0d3743d6a048be0602801f4b3920","a3776dccca01c11e7046323cfad46a3b4a471233","639c36565ce03aed5b0fce7c9e4448e53a1f7efd"],"f1bdbf92da222965b46c0a942c3857ba56e5c638":["d619839baa8ce5503e496b94a9e42ad6f079293f"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5fff8d3ee91620a83b0a3ac1c9f85b384c7cf32a":["9ab1f5591dc05f1f2b5407d809c9699f75554a32","8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5"],"a493e6d0c3ad86bd55c0a1360d110142e948f2bd":["2a186ae8733084223c22044e935e4ef848a143d1","85a883878c0af761245ab048babc63d099f835f3"],"df59c069ce7c8a1965072ad853d74ba87cfdf8a4":["91824676e57e2d1c945918662b835813385cd291"],"f82bdb39e96d0f03d4e6482f4c835775856ccdef":["355542ffa5812b7a09147c6a46a0948b36c5966f"],"355542ffa5812b7a09147c6a46a0948b36c5966f":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe","ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["868da859b43505d9d2a023bfeae6dd0c795f5295"],"01e5948db9a07144112d2f08f28ca2e3cd880348":["b3e06be49006ecac364d39d12b9c9f74882f9b9f","45669a651c970812a680841b97a77cce06af559f"],"14ec33385f6fbb6ce172882d14605790418a5d31":["df59c069ce7c8a1965072ad853d74ba87cfdf8a4"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["f1bdbf92da222965b46c0a942c3857ba56e5c638"],"f4b7c426b6e92fa66946ff52f0508da82ee4de33":["14ec33385f6fbb6ce172882d14605790418a5d31"],"94cb8b3ec0439dfd8e179637ee4191cd9c6227e5":["f82bdb39e96d0f03d4e6482f4c835775856ccdef"],"433777d1eaf9998136cd16515dc0e1eb26f5d535":["82470b9595905eea74268b47bd3e8a94306d28d9"],"32feb7c2c571b402d2e231bd8e3b6add4af6d6eb":["c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":[],"9ce667c6d3400b22523701c549c0d35e26da8b46":["ef2642aa76c0d0714b26e5e7d5e3438a62db3cb7","c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31","fd92b8bcc88e969302510acf77bd6970da3994c4"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["94cb8b3ec0439dfd8e179637ee4191cd9c6227e5"],"fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe":["70ad682703b8585f5d0a637efec044d57ec05efb","87d122575733b906e11f496c1d6b4d1327f5308d","868da859b43505d9d2a023bfeae6dd0c795f5295"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"70ad682703b8585f5d0a637efec044d57ec05efb":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"2a186ae8733084223c22044e935e4ef848a143d1":["5fff8d3ee91620a83b0a3ac1c9f85b384c7cf32a","c498d3f8d75170b121f5eda2c6210ac5beb5d411"],"1085ea837da8f1e96697e17cf73e1d08e7329261":["a493e6d0c3ad86bd55c0a1360d110142e948f2bd","85a883878c0af761245ab048babc63d099f835f3"],"38a62612cfa4e104080d89d7751a8f1a258ac335":["44fcbde6fb2ac44ee3b45e013e54a42911e689ff"],"9ab1f5591dc05f1f2b5407d809c9699f75554a32":["3bb13258feba31ab676502787ab2e1779f129b7a"],"06584e6e98d592b34e1329b384182f368d2025e8":["3cc749c053615f5871f3b95715fe292f34e70a53"],"82470b9595905eea74268b47bd3e8a94306d28d9":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"0aab6e810b4b0d3743d6a048be0602801f4b3920":["2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c","ddc4c914be86e34b54f70023f45a60fa7f04e929"],"2553b00f699380c64959ccb27991289aae87be2e":["5d004d0e0b3f65bb40da76d476d659d7888270e8"],"00b21520fafb9860ce0318d7be5ea84619c185ad":["1224a4027481acce15495b03bce9b48b93b42722"],"9b832cbed6eb3d54a8bb9339296bdda8eeb53014":["334c1175813aea771a71728cd2c4ee4754fd0603"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"e277c90a96e81a581f45e4b6aaf384ddc50f79bc":["619e84c4a25ac93018dce34af2bb41dafbeac829"],"08932c793647a36953d1816b1060121f48820d3f":["9b832cbed6eb3d54a8bb9339296bdda8eeb53014"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a3776dccca01c11e7046323cfad46a3b4a471233":["2553b00f699380c64959ccb27991289aae87be2e"],"c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","fd92b8bcc88e969302510acf77bd6970da3994c4"],"91824676e57e2d1c945918662b835813385cd291":["00b21520fafb9860ce0318d7be5ea84619c185ad"],"619e84c4a25ac93018dce34af2bb41dafbeac829":["1085ea837da8f1e96697e17cf73e1d08e7329261"],"f470b537db3da4e8d0c39bc72fae5f9865a9ec3c":["c0716ddfa41d3662d014c42086a700ad78fc5dcb"],"58c6bbc222f074c844e736e6fb23647e3db9cfe3":["c5df35ab57c223ea11aec64b53bf611904f3dced","93ccd971aca7fb61b7f1b946e44714cfc80bfc7c"],"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":[],"8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5":["4e8cc373c801e54cec75daf9f52792cb4b17f536"],"fd92b8bcc88e969302510acf77bd6970da3994c4":[],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["433777d1eaf9998136cd16515dc0e1eb26f5d535"],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["b6f9be74ca7baaef11857ad002cad40419979516"],"4a69e5860d014751cc9329dfeb441a6d8fd1ed8e":["ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"bb9b72f7c3d7827c64dd4ec580ded81778da361d":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3":["f1bdbf92da222965b46c0a942c3857ba56e5c638","b0c7a8f7304b75b1528814c5820fa23a96816c27"],"c162b4b9ef005383c38e0912c16267584dc40de5":["d083e83f225b11e5fdd900e83d26ddb385b6955c"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["45669a651c970812a680841b97a77cce06af559f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"b6f9be74ca7baaef11857ad002cad40419979516":["c162b4b9ef005383c38e0912c16267584dc40de5"],"a1b4dec651d0a89767d1233eef7353f88a42bea0":["7b91922b55d15444d554721b352861d028eb8278"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":[],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["0061262413ecc163d6eebba1b5c43ab91a0c2dc5","5d004d0e0b3f65bb40da76d476d659d7888270e8"],"ccad4bab070f323ce610caa0040346d4a87213dc":["32feb7c2c571b402d2e231bd8e3b6add4af6d6eb"],"7e1cbd7e289dc1243c7a59e1a83d078163a147fe":["e06c9d5fba0a2f937941d199d64ccb32aac502d1"],"c50bf8a3310d2aec44c01b0818c308b2e0ac6b33":["5270fb4f55a1b77663dda53cb8090c083f0a23b3"],"833a7987bc1c94455fde83e3311f72bddedcfb93":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"1224a4027481acce15495b03bce9b48b93b42722":["01e5948db9a07144112d2f08f28ca2e3cd880348","d619839baa8ce5503e496b94a9e42ad6f079293f","a3776dccca01c11e7046323cfad46a3b4a471233","c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"87d122575733b906e11f496c1d6b4d1327f5308d":["29ef99d61cda9641b6250bf9567329a6e65f901d","bb9b72f7c3d7827c64dd4ec580ded81778da361d","e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3"],"e06c9d5fba0a2f937941d199d64ccb32aac502d1":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"c498d3f8d75170b121f5eda2c6210ac5beb5d411":["9ab1f5591dc05f1f2b5407d809c9699f75554a32"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"85a883878c0af761245ab048babc63d099f835f3":["c498d3f8d75170b121f5eda2c6210ac5beb5d411"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["70ad682703b8585f5d0a637efec044d57ec05efb"],"3cc749c053615f5871f3b95715fe292f34e70a53":["58c6bbc222f074c844e736e6fb23647e3db9cfe3"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["c50bf8a3310d2aec44c01b0818c308b2e0ac6b33","135621f3a0670a9394eb563224a3b76cc4dddc0f"],"334c1175813aea771a71728cd2c4ee4754fd0603":["e277c90a96e81a581f45e4b6aaf384ddc50f79bc"],"5270fb4f55a1b77663dda53cb8090c083f0a23b3":["d3fe2fc74577855eadfb5eae3153c2fffdaaf791"],"fb10b6bcde550b87d8f10e5f010bd8f3021023b6":["f470b537db3da4e8d0c39bc72fae5f9865a9ec3c"],"c0716ddfa41d3662d014c42086a700ad78fc5dcb":["08932c793647a36953d1816b1060121f48820d3f","8fe956d65251358d755c56f14fe8380644790e47"],"7b91922b55d15444d554721b352861d028eb8278":["06584e6e98d592b34e1329b384182f368d2025e8"],"b0c7a8f7304b75b1528814c5820fa23a96816c27":["bde51b089eb7f86171eb3406e38a274743f9b7ac","4b557198058275e3f7a063a47d60c1f94426bc5e"],"8fe956d65251358d755c56f14fe8380644790e47":["833a7987bc1c94455fde83e3311f72bddedcfb93"],"4b557198058275e3f7a063a47d60c1f94426bc5e":["f4b7c426b6e92fa66946ff52f0508da82ee4de33"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["bb9b72f7c3d7827c64dd4ec580ded81778da361d"],"45669a651c970812a680841b97a77cce06af559f":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"3bb13258feba31ab676502787ab2e1779f129b7a":["4a69e5860d014751cc9329dfeb441a6d8fd1ed8e"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["38a62612cfa4e104080d89d7751a8f1a258ac335","3bb13258feba31ab676502787ab2e1779f129b7a"],"8a16d06e7522604de20b2d758d9b9464bb30fe02":["ccad4bab070f323ce610caa0040346d4a87213dc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb","93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","fd92b8bcc88e969302510acf77bd6970da3994c4","5d004d0e0b3f65bb40da76d476d659d7888270e8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}