{"path":"solr/contrib/analytics/src/java/org/apache/solr/analytics/AnalyticsDriver#drive(AnalyticsRequestManager,SolrIndexSearcher,Filter,SolrQueryRequest).mjava","commits":[{"id":"c495edcca4d0bc51bf62d9be3527c87bf9b44ded","date":1498673617,"type":0,"author":"Dennis Gove","isMerge":false,"pathNew":"solr/contrib/analytics/src/java/org/apache/solr/analytics/AnalyticsDriver#drive(AnalyticsRequestManager,SolrIndexSearcher,Filter,SolrQueryRequest).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Drive the collection of reduction data. This includes overall data as well as faceted data.\n   * \n   * @param manager of the request to drive\n   * @param searcher the results of the query\n   * @param filter that represents the overall query\n   * @param queryRequest used for the search request\n   * @throws IOException if an error occurs while reading from Solr\n   */\n  public static void drive(AnalyticsRequestManager manager, SolrIndexSearcher searcher, Filter filter, SolrQueryRequest queryRequest) throws IOException {\n    StreamingInfo streamingInfo = manager.getStreamingFacetInfo();\n    Iterable<StreamingFacet> streamingFacets = streamingInfo.streamingFacets;\n    ReductionCollectionManager collectionManager = streamingInfo.streamingCollectionManager;\n    \n    Iterable<FacetValueQueryExecuter> facetExecuters = manager.getFacetExecuters(filter, queryRequest);\n    \n    // Streaming phase (Overall results & Value/Pivot Facets)\n    // Loop through all documents and collect reduction data for streaming facets and overall results\n    if (collectionManager.needsCollection()) {\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        LeafReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        if (dis == null) {\n          continue;\n        }\n        DocIdSetIterator disi = dis.iterator();\n        if (disi != null) {\n          collectionManager.doSetNextReader(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            collectionManager.collect(doc);\n            streamingFacets.forEach( facet -> facet.addFacetValueCollectionTargets() );\n            collectionManager.apply();\n            doc = disi.nextDoc();\n          }\n        }\n      }\n    }\n    \n    // Executing phase (Query/Range Facets)\n    // Send additional Solr Queries to compute facet values\n    for (FacetValueQueryExecuter executer : facetExecuters) {\n      executer.execute(searcher);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"solr/contrib/analytics/src/java/org/apache/solr/analytics/AnalyticsDriver#drive(AnalyticsRequestManager,SolrIndexSearcher,Filter,SolrQueryRequest).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Drive the collection of reduction data. This includes overall data as well as faceted data.\n   * \n   * @param manager of the request to drive\n   * @param searcher the results of the query\n   * @param filter that represents the overall query\n   * @param queryRequest used for the search request\n   * @throws IOException if an error occurs while reading from Solr\n   */\n  public static void drive(AnalyticsRequestManager manager, SolrIndexSearcher searcher, Filter filter, SolrQueryRequest queryRequest) throws IOException {\n    StreamingInfo streamingInfo = manager.getStreamingFacetInfo();\n    Iterable<StreamingFacet> streamingFacets = streamingInfo.streamingFacets;\n    ReductionCollectionManager collectionManager = streamingInfo.streamingCollectionManager;\n    \n    Iterable<FacetValueQueryExecuter> facetExecuters = manager.getFacetExecuters(filter, queryRequest);\n    \n    // Streaming phase (Overall results & Value/Pivot Facets)\n    // Loop through all documents and collect reduction data for streaming facets and overall results\n    if (collectionManager.needsCollection()) {\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        LeafReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        if (dis == null) {\n          continue;\n        }\n        DocIdSetIterator disi = dis.iterator();\n        if (disi != null) {\n          collectionManager.doSetNextReader(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            collectionManager.collect(doc);\n            streamingFacets.forEach( facet -> facet.addFacetValueCollectionTargets() );\n            collectionManager.apply();\n            doc = disi.nextDoc();\n          }\n        }\n      }\n    }\n    \n    // Executing phase (Query/Range Facets)\n    // Send additional Solr Queries to compute facet values\n    for (FacetValueQueryExecuter executer : facetExecuters) {\n      executer.execute(searcher);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30c8e5574b55d57947e989443dfde611646530ee","date":1499131153,"type":0,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/contrib/analytics/src/java/org/apache/solr/analytics/AnalyticsDriver#drive(AnalyticsRequestManager,SolrIndexSearcher,Filter,SolrQueryRequest).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Drive the collection of reduction data. This includes overall data as well as faceted data.\n   * \n   * @param manager of the request to drive\n   * @param searcher the results of the query\n   * @param filter that represents the overall query\n   * @param queryRequest used for the search request\n   * @throws IOException if an error occurs while reading from Solr\n   */\n  public static void drive(AnalyticsRequestManager manager, SolrIndexSearcher searcher, Filter filter, SolrQueryRequest queryRequest) throws IOException {\n    StreamingInfo streamingInfo = manager.getStreamingFacetInfo();\n    Iterable<StreamingFacet> streamingFacets = streamingInfo.streamingFacets;\n    ReductionCollectionManager collectionManager = streamingInfo.streamingCollectionManager;\n    \n    Iterable<FacetValueQueryExecuter> facetExecuters = manager.getFacetExecuters(filter, queryRequest);\n    \n    // Streaming phase (Overall results & Value/Pivot Facets)\n    // Loop through all documents and collect reduction data for streaming facets and overall results\n    if (collectionManager.needsCollection()) {\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        LeafReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        if (dis == null) {\n          continue;\n        }\n        DocIdSetIterator disi = dis.iterator();\n        if (disi != null) {\n          collectionManager.doSetNextReader(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            collectionManager.collect(doc);\n            streamingFacets.forEach( facet -> facet.addFacetValueCollectionTargets() );\n            collectionManager.apply();\n            doc = disi.nextDoc();\n          }\n        }\n      }\n    }\n    \n    // Executing phase (Query/Range Facets)\n    // Send additional Solr Queries to compute facet values\n    for (FacetValueQueryExecuter executer : facetExecuters) {\n      executer.execute(searcher);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1e4e64b7199d2f2a17be7f3926c7532553910dce","date":1564342581,"type":3,"author":"Jason Gerlowski","isMerge":false,"pathNew":"solr/contrib/analytics/src/java/org/apache/solr/analytics/AnalyticsDriver#drive(AnalyticsRequestManager,SolrIndexSearcher,Filter,SolrQueryRequest).mjava","pathOld":"solr/contrib/analytics/src/java/org/apache/solr/analytics/AnalyticsDriver#drive(AnalyticsRequestManager,SolrIndexSearcher,Filter,SolrQueryRequest).mjava","sourceNew":"  /**\n   * Drive the collection of reduction data. This includes overall data as well as faceted data.\n   *\n   * @param manager of the request to drive\n   * @param searcher the results of the query\n   * @param filter that represents the overall query\n   * @param queryRequest used for the search request\n   * @throws IOException if an error occurs while reading from Solr\n   */\n  public static void drive(AnalyticsRequestManager manager, SolrIndexSearcher searcher, Filter filter, SolrQueryRequest queryRequest) throws IOException {\n    StreamingInfo streamingInfo = manager.getStreamingFacetInfo();\n    Iterable<StreamingFacet> streamingFacets = streamingInfo.streamingFacets;\n    ReductionCollectionManager collectionManager = streamingInfo.streamingCollectionManager;\n\n    Iterable<FacetValueQueryExecuter> facetExecuters = manager.getFacetExecuters(filter, queryRequest);\n\n    // Streaming phase (Overall results & Value/Pivot Facets)\n    // Loop through all documents and collect reduction data for streaming facets and overall results\n    if (collectionManager.needsCollection()) {\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        LeafReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        if (dis == null) {\n          continue;\n        }\n        DocIdSetIterator disi = dis.iterator();\n        if (disi != null) {\n          collectionManager.doSetNextReader(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            collectionManager.collect(doc);\n            streamingFacets.forEach( facet -> facet.addFacetValueCollectionTargets() );\n            collectionManager.apply();\n            doc = disi.nextDoc();\n          }\n        }\n      }\n    }\n\n    // Executing phase (Query/Range Facets)\n    // Send additional Solr Queries to compute facet values\n    for (FacetValueQueryExecuter executer : facetExecuters) {\n      executer.execute(searcher);\n    }\n  }\n\n","sourceOld":"  /**\n   * Drive the collection of reduction data. This includes overall data as well as faceted data.\n   * \n   * @param manager of the request to drive\n   * @param searcher the results of the query\n   * @param filter that represents the overall query\n   * @param queryRequest used for the search request\n   * @throws IOException if an error occurs while reading from Solr\n   */\n  public static void drive(AnalyticsRequestManager manager, SolrIndexSearcher searcher, Filter filter, SolrQueryRequest queryRequest) throws IOException {\n    StreamingInfo streamingInfo = manager.getStreamingFacetInfo();\n    Iterable<StreamingFacet> streamingFacets = streamingInfo.streamingFacets;\n    ReductionCollectionManager collectionManager = streamingInfo.streamingCollectionManager;\n    \n    Iterable<FacetValueQueryExecuter> facetExecuters = manager.getFacetExecuters(filter, queryRequest);\n    \n    // Streaming phase (Overall results & Value/Pivot Facets)\n    // Loop through all documents and collect reduction data for streaming facets and overall results\n    if (collectionManager.needsCollection()) {\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        LeafReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        if (dis == null) {\n          continue;\n        }\n        DocIdSetIterator disi = dis.iterator();\n        if (disi != null) {\n          collectionManager.doSetNextReader(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            collectionManager.collect(doc);\n            streamingFacets.forEach( facet -> facet.addFacetValueCollectionTargets() );\n            collectionManager.apply();\n            doc = disi.nextDoc();\n          }\n        }\n      }\n    }\n    \n    // Executing phase (Query/Range Facets)\n    // Send additional Solr Queries to compute facet values\n    for (FacetValueQueryExecuter executer : facetExecuters) {\n      executer.execute(searcher);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f8061ddd97f3352007d927dae445884a6f3d857b","date":1564988276,"type":3,"author":"Atri Sharma","isMerge":true,"pathNew":"solr/contrib/analytics/src/java/org/apache/solr/analytics/AnalyticsDriver#drive(AnalyticsRequestManager,SolrIndexSearcher,Filter,SolrQueryRequest).mjava","pathOld":"solr/contrib/analytics/src/java/org/apache/solr/analytics/AnalyticsDriver#drive(AnalyticsRequestManager,SolrIndexSearcher,Filter,SolrQueryRequest).mjava","sourceNew":"  /**\n   * Drive the collection of reduction data. This includes overall data as well as faceted data.\n   *\n   * @param manager of the request to drive\n   * @param searcher the results of the query\n   * @param filter that represents the overall query\n   * @param queryRequest used for the search request\n   * @throws IOException if an error occurs while reading from Solr\n   */\n  public static void drive(AnalyticsRequestManager manager, SolrIndexSearcher searcher, Filter filter, SolrQueryRequest queryRequest) throws IOException {\n    StreamingInfo streamingInfo = manager.getStreamingFacetInfo();\n    Iterable<StreamingFacet> streamingFacets = streamingInfo.streamingFacets;\n    ReductionCollectionManager collectionManager = streamingInfo.streamingCollectionManager;\n\n    Iterable<FacetValueQueryExecuter> facetExecuters = manager.getFacetExecuters(filter, queryRequest);\n\n    // Streaming phase (Overall results & Value/Pivot Facets)\n    // Loop through all documents and collect reduction data for streaming facets and overall results\n    if (collectionManager.needsCollection()) {\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        LeafReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        if (dis == null) {\n          continue;\n        }\n        DocIdSetIterator disi = dis.iterator();\n        if (disi != null) {\n          collectionManager.doSetNextReader(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            collectionManager.collect(doc);\n            streamingFacets.forEach( facet -> facet.addFacetValueCollectionTargets() );\n            collectionManager.apply();\n            doc = disi.nextDoc();\n          }\n        }\n      }\n    }\n\n    // Executing phase (Query/Range Facets)\n    // Send additional Solr Queries to compute facet values\n    for (FacetValueQueryExecuter executer : facetExecuters) {\n      executer.execute(searcher);\n    }\n  }\n\n","sourceOld":"  /**\n   * Drive the collection of reduction data. This includes overall data as well as faceted data.\n   * \n   * @param manager of the request to drive\n   * @param searcher the results of the query\n   * @param filter that represents the overall query\n   * @param queryRequest used for the search request\n   * @throws IOException if an error occurs while reading from Solr\n   */\n  public static void drive(AnalyticsRequestManager manager, SolrIndexSearcher searcher, Filter filter, SolrQueryRequest queryRequest) throws IOException {\n    StreamingInfo streamingInfo = manager.getStreamingFacetInfo();\n    Iterable<StreamingFacet> streamingFacets = streamingInfo.streamingFacets;\n    ReductionCollectionManager collectionManager = streamingInfo.streamingCollectionManager;\n    \n    Iterable<FacetValueQueryExecuter> facetExecuters = manager.getFacetExecuters(filter, queryRequest);\n    \n    // Streaming phase (Overall results & Value/Pivot Facets)\n    // Loop through all documents and collect reduction data for streaming facets and overall results\n    if (collectionManager.needsCollection()) {\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        LeafReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        if (dis == null) {\n          continue;\n        }\n        DocIdSetIterator disi = dis.iterator();\n        if (disi != null) {\n          collectionManager.doSetNextReader(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            collectionManager.collect(doc);\n            streamingFacets.forEach( facet -> facet.addFacetValueCollectionTargets() );\n            collectionManager.apply();\n            doc = disi.nextDoc();\n          }\n        }\n      }\n    }\n    \n    // Executing phase (Query/Range Facets)\n    // Send additional Solr Queries to compute facet values\n    for (FacetValueQueryExecuter executer : facetExecuters) {\n      executer.execute(searcher);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"1e4e64b7199d2f2a17be7f3926c7532553910dce":["28288370235ed02234a64753cdbf0c6ec096304a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"30c8e5574b55d57947e989443dfde611646530ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","28288370235ed02234a64753cdbf0c6ec096304a"],"c495edcca4d0bc51bf62d9be3527c87bf9b44ded":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"28288370235ed02234a64753cdbf0c6ec096304a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c495edcca4d0bc51bf62d9be3527c87bf9b44ded"],"f8061ddd97f3352007d927dae445884a6f3d857b":["28288370235ed02234a64753cdbf0c6ec096304a","1e4e64b7199d2f2a17be7f3926c7532553910dce"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["1e4e64b7199d2f2a17be7f3926c7532553910dce"]},"commit2Childs":{"1e4e64b7199d2f2a17be7f3926c7532553910dce":["f8061ddd97f3352007d927dae445884a6f3d857b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["30c8e5574b55d57947e989443dfde611646530ee","c495edcca4d0bc51bf62d9be3527c87bf9b44ded","28288370235ed02234a64753cdbf0c6ec096304a"],"30c8e5574b55d57947e989443dfde611646530ee":[],"c495edcca4d0bc51bf62d9be3527c87bf9b44ded":["28288370235ed02234a64753cdbf0c6ec096304a"],"28288370235ed02234a64753cdbf0c6ec096304a":["1e4e64b7199d2f2a17be7f3926c7532553910dce","30c8e5574b55d57947e989443dfde611646530ee","f8061ddd97f3352007d927dae445884a6f3d857b"],"f8061ddd97f3352007d927dae445884a6f3d857b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["30c8e5574b55d57947e989443dfde611646530ee","f8061ddd97f3352007d927dae445884a6f3d857b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}