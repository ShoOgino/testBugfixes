{"path":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCache#test().mjava","commits":[{"id":"a076c3c721f685b7559308fdc2cd72d91bba67e5","date":1464168992,"type":1,"author":"Mike McCandless","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCache#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache#test().mjava","sourceNew":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        s = null;\n      } else {\n        s = termsIndex.lookupOrd(ord).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\", null);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (!bits.get(i)) {\n        s = null;\n      } else {\n        s = terms.get(i).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        s = null;\n      } else {\n        s = termsIndex.lookupOrd(ord).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\", null);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (!bits.get(i)) {\n        s = null;\n      } else {\n        s = terms.get(i).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0e121d43b5a10f2df530f406f935102656e9c4e8","date":1464198131,"type":1,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCache#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache#test().mjava","sourceNew":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        s = null;\n      } else {\n        s = termsIndex.lookupOrd(ord).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\", null);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (!bits.get(i)) {\n        s = null;\n      } else {\n        s = terms.get(i).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        s = null;\n      } else {\n        s = termsIndex.lookupOrd(ord).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\", null);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (!bits.get(i)) {\n        s = null;\n      } else {\n        s = terms.get(i).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83870855d82aba6819217abeff5a40779dbb28b4","date":1464291012,"type":1,"author":"Mike McCandless","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCache#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache#test().mjava","sourceNew":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        s = null;\n      } else {\n        s = termsIndex.lookupOrd(ord).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\", null);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (!bits.get(i)) {\n        s = null;\n      } else {\n        s = terms.get(i).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        s = null;\n      } else {\n        s = termsIndex.lookupOrd(ord).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\", null);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (!bits.get(i)) {\n        s = null;\n      } else {\n        s = terms.get(i).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6652c74b2358a0b13223817a6a793bf1c9d0749d","date":1474465301,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCache#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCache#test().mjava","sourceNew":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(i, doubles.nextDoc());\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.longValue());\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(i, longs.nextDoc());\n      assertEquals(Long.MAX_VALUE - i, longs.longValue());\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(i, ints.nextDoc());\n      assertEquals(Integer.MAX_VALUE - i, ints.longValue());\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(i, floats.nextDoc());\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.longValue());\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (i > termsIndex.docID()) {\n        termsIndex.advance(i);\n      }\n      if (i == termsIndex.docID()) {\n        s = termsIndex.binaryValue().utf8ToString();\n      } else {\n        s = null;\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      if (terms.docID() < i) {\n        terms.nextDoc();\n      }\n      if (terms.docID() == i) {\n        assertEquals(unicodeStrings[i], terms.binaryValue().utf8ToString());\n      } else {\n        assertNull(unicodeStrings[i]);\n      }\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\");\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        if (i > termOrds.docID()) {\n          assertEquals(i, termOrds.nextDoc());\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      if (i == termOrds.docID()) {\n        assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n      }\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        s = null;\n      } else {\n        s = termsIndex.lookupOrd(ord).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\", null);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (!bits.get(i)) {\n        s = null;\n      } else {\n        s = terms.get(i).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCache#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCache#test().mjava","sourceNew":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(i, doubles.nextDoc());\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.longValue());\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(i, longs.nextDoc());\n      assertEquals(Long.MAX_VALUE - i, longs.longValue());\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(i, ints.nextDoc());\n      assertEquals(Integer.MAX_VALUE - i, ints.longValue());\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(i, floats.nextDoc());\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.longValue());\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (i > termsIndex.docID()) {\n        termsIndex.advance(i);\n      }\n      if (i == termsIndex.docID()) {\n        s = termsIndex.binaryValue().utf8ToString();\n      } else {\n        s = null;\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      if (terms.docID() < i) {\n        terms.nextDoc();\n      }\n      if (terms.docID() == i) {\n        assertEquals(unicodeStrings[i], terms.binaryValue().utf8ToString());\n      } else {\n        assertNull(unicodeStrings[i]);\n      }\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\");\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        if (i > termOrds.docID()) {\n          assertEquals(i, termOrds.nextDoc());\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      if (i == termOrds.docID()) {\n        assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n      }\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        s = null;\n      } else {\n        s = termsIndex.lookupOrd(ord).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\", null);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (!bits.get(i)) {\n        s = null;\n      } else {\n        s = terms.get(i).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCache#test().mjava","pathOld":"/dev/null","sourceNew":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(i, doubles.nextDoc());\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.longValue());\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(i, longs.nextDoc());\n      assertEquals(Long.MAX_VALUE - i, longs.longValue());\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(i, ints.nextDoc());\n      assertEquals(Integer.MAX_VALUE - i, ints.longValue());\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(i, floats.nextDoc());\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.longValue());\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (i > termsIndex.docID()) {\n        termsIndex.advance(i);\n      }\n      if (i == termsIndex.docID()) {\n        s = termsIndex.binaryValue().utf8ToString();\n      } else {\n        s = null;\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      if (terms.docID() < i) {\n        terms.nextDoc();\n      }\n      if (terms.docID() == i) {\n        assertEquals(unicodeStrings[i], terms.binaryValue().utf8ToString());\n      } else {\n        assertNull(unicodeStrings[i]);\n      }\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\");\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        if (i > termOrds.docID()) {\n          assertEquals(i, termOrds.nextDoc());\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      if (i == termOrds.docID()) {\n        assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n      }\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d211216c83f01894810543d1c107160a9ae3650b","date":1488289605,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCache#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCache#test().mjava","sourceNew":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(i, doubles.nextDoc());\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.longValue());\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(i, longs.nextDoc());\n      assertEquals(Long.MAX_VALUE - i, longs.longValue());\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(i, ints.nextDoc());\n      assertEquals(Integer.MAX_VALUE - i, ints.longValue());\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(i, floats.nextDoc());\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.longValue());\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (i > termsIndex.docID()) {\n        termsIndex.advance(i);\n      }\n      if (i == termsIndex.docID()) {\n        s = termsIndex.binaryValue().utf8ToString();\n      } else {\n        s = null;\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      if (terms.docID() < i) {\n        terms.nextDoc();\n      }\n      if (terms.docID() == i) {\n        assertEquals(unicodeStrings[i], terms.binaryValue().utf8ToString());\n      } else {\n        assertNull(unicodeStrings[i]);\n      }\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\");\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        if (i > termOrds.docID()) {\n          assertEquals(i, termOrds.nextDoc());\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      if (i == termOrds.docID()) {\n        assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n      }\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheHelper().getKey());\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(i, doubles.nextDoc());\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.longValue());\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(i, longs.nextDoc());\n      assertEquals(Long.MAX_VALUE - i, longs.longValue());\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(i, ints.nextDoc());\n      assertEquals(Integer.MAX_VALUE - i, ints.longValue());\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(i, floats.nextDoc());\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.longValue());\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (i > termsIndex.docID()) {\n        termsIndex.advance(i);\n      }\n      if (i == termsIndex.docID()) {\n        s = termsIndex.binaryValue().utf8ToString();\n      } else {\n        s = null;\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      if (terms.docID() < i) {\n        terms.nextDoc();\n      }\n      if (terms.docID() == i) {\n        assertEquals(unicodeStrings[i], terms.binaryValue().utf8ToString());\n      } else {\n        assertNull(unicodeStrings[i]);\n      }\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\");\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        if (i > termOrds.docID()) {\n          assertEquals(i, termOrds.nextDoc());\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      if (i == termOrds.docID()) {\n        assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n      }\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a076c3c721f685b7559308fdc2cd72d91bba67e5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"0e121d43b5a10f2df530f406f935102656e9c4e8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","a076c3c721f685b7559308fdc2cd72d91bba67e5"],"d211216c83f01894810543d1c107160a9ae3650b":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["0e121d43b5a10f2df530f406f935102656e9c4e8","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["0e121d43b5a10f2df530f406f935102656e9c4e8"],"83870855d82aba6819217abeff5a40779dbb28b4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","0e121d43b5a10f2df530f406f935102656e9c4e8"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d211216c83f01894810543d1c107160a9ae3650b"]},"commit2Childs":{"a076c3c721f685b7559308fdc2cd72d91bba67e5":["0e121d43b5a10f2df530f406f935102656e9c4e8"],"0e121d43b5a10f2df530f406f935102656e9c4e8":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","6652c74b2358a0b13223817a6a793bf1c9d0749d","83870855d82aba6819217abeff5a40779dbb28b4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a076c3c721f685b7559308fdc2cd72d91bba67e5","0e121d43b5a10f2df530f406f935102656e9c4e8","83870855d82aba6819217abeff5a40779dbb28b4","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"d211216c83f01894810543d1c107160a9ae3650b":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["d211216c83f01894810543d1c107160a9ae3650b","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"83870855d82aba6819217abeff5a40779dbb28b4":[],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["83870855d82aba6819217abeff5a40779dbb28b4","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}