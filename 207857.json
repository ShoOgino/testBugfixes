{"path":"solr/core/src/java/org/apache/solr/cloud/Overseer.CloudStateUpdater#removeCore(CloudState,ZkNodeProps).mjava","commits":[{"id":"a5093a9e893633cc091cf2f729d7863671c2b715","date":1339132888,"type":1,"author":"Sami Siren","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.CloudStateUpdater#removeCore(CloudState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.CloudStateUpdater#removeCore(CloudState,String,String).mjava","sourceNew":"      /*\n       * Remove core from cloudstate\n       */\n      private CloudState removeCore(final CloudState cloudState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.get(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.get(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.get(ZkStateReader.COLLECTION_PROP);\n\n        final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();\n        for(String collectionName: cloudState.getCollections()) {\n          if(collection.equals(collectionName)) {\n            Map<String, Slice> slices = cloudState.getSlices(collection);\n            LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n            for(Slice slice: slices.values()) {\n              if(slice.getShards().containsKey(coreNodeName)) {\n                LinkedHashMap<String, ZkNodeProps> newShards = new LinkedHashMap<String, ZkNodeProps>();\n                newShards.putAll(slice.getShards());\n                newShards.remove(coreNodeName);\n                Slice newSlice = new Slice(slice.getName(), newShards);\n                newSlices.put(slice.getName(), newSlice);\n              } else {\n                newSlices.put(slice.getName(), slice);\n              }\n            }\n            newStates.put(collectionName, newSlices);\n          } else {\n            newStates.put(collectionName, cloudState.getSlices(collectionName));\n          }\n        }\n        CloudState newState = new CloudState(cloudState.getLiveNodes(), newStates);\n        return newState;\n     }\n\n","sourceOld":"      /*\n       * Remove core from cloudstate\n       */\n      private CloudState removeCore(final CloudState cloudState, final String collection, final String coreNodeName) {\n        final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();\n        for(String collectionName: cloudState.getCollections()) {\n          if(collection.equals(collectionName)) {\n            Map<String, Slice> slices = cloudState.getSlices(collection);\n            LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n            for(Slice slice: slices.values()) {\n              if(slice.getShards().containsKey(coreNodeName)) {\n                LinkedHashMap<String, ZkNodeProps> newShards = new LinkedHashMap<String, ZkNodeProps>();\n                newShards.putAll(slice.getShards());\n                newShards.remove(coreNodeName);\n                Slice newSlice = new Slice(slice.getName(), newShards);\n                newSlices.put(slice.getName(), newSlice);\n              } else {\n                newSlices.put(slice.getName(), slice);\n              }\n            }\n            newStates.put(collectionName, newSlices);\n          } else {\n            newStates.put(collectionName, cloudState.getSlices(collectionName));\n          }\n        }\n        CloudState newState = new CloudState(cloudState.getLiveNodes(), newStates);\n        return newState;\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"33d0ed80b7b47e34ad3ff033a77544563aba3085","date":1341244632,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.CloudStateUpdater#removeCore(CloudState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.CloudStateUpdater#removeCore(CloudState,ZkNodeProps).mjava","sourceNew":"      /*\n       * Remove core from cloudstate\n       */\n      private CloudState removeCore(final CloudState cloudState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.get(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.get(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.get(ZkStateReader.COLLECTION_PROP);\n\n        final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();\n        for(String collectionName: cloudState.getCollections()) {\n          if(collection.equals(collectionName)) {\n            Map<String, Slice> slices = cloudState.getSlices(collection);\n            LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n            for(Slice slice: slices.values()) {\n              if(slice.getShards().containsKey(coreNodeName)) {\n                LinkedHashMap<String, ZkNodeProps> newShards = new LinkedHashMap<String, ZkNodeProps>();\n                newShards.putAll(slice.getShards());\n                newShards.remove(coreNodeName);\n                \n                Slice newSlice = new Slice(slice.getName(), newShards);\n                newSlices.put(slice.getName(), newSlice);\n\n              } else {\n                newSlices.put(slice.getName(), slice);\n              }\n            }\n            int cnt = 0;\n            for (Slice slice : newSlices.values()) {\n              cnt+=slice.getShards().size();\n            }\n            // TODO: if no nodes are left after this unload\n            // remove from zk - do we have a race where Overseer\n            // see's registered nodes and publishes though?\n            if (cnt > 0) {\n              newStates.put(collectionName, newSlices);\n            } else {\n              // TODO: it might be better logically to have this in ZkController\n              // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n              // ZkController out of the Overseer.\n              try {\n                zkClient.clean(\"/collections/\" + collectionName);\n              } catch (InterruptedException e) {\n                SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collectionName, e);\n                Thread.currentThread().interrupt();\n              } catch (KeeperException e) {\n                SolrException.log(log, \"Problem cleaning up collection in zk:\" + collectionName, e);\n              }\n            }\n          } else {\n            newStates.put(collectionName, cloudState.getSlices(collectionName));\n          }\n        }\n        CloudState newState = new CloudState(cloudState.getLiveNodes(), newStates);\n        return newState;\n     }\n\n","sourceOld":"      /*\n       * Remove core from cloudstate\n       */\n      private CloudState removeCore(final CloudState cloudState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.get(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.get(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.get(ZkStateReader.COLLECTION_PROP);\n\n        final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();\n        for(String collectionName: cloudState.getCollections()) {\n          if(collection.equals(collectionName)) {\n            Map<String, Slice> slices = cloudState.getSlices(collection);\n            LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n            for(Slice slice: slices.values()) {\n              if(slice.getShards().containsKey(coreNodeName)) {\n                LinkedHashMap<String, ZkNodeProps> newShards = new LinkedHashMap<String, ZkNodeProps>();\n                newShards.putAll(slice.getShards());\n                newShards.remove(coreNodeName);\n                Slice newSlice = new Slice(slice.getName(), newShards);\n                newSlices.put(slice.getName(), newSlice);\n              } else {\n                newSlices.put(slice.getName(), slice);\n              }\n            }\n            newStates.put(collectionName, newSlices);\n          } else {\n            newStates.put(collectionName, cloudState.getSlices(collectionName));\n          }\n        }\n        CloudState newState = new CloudState(cloudState.getLiveNodes(), newStates);\n        return newState;\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fe33227f6805edab2036cbb80645cc4e2d1fa424","date":1342713534,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.CloudStateUpdater#removeCore(CloudState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.CloudStateUpdater#removeCore(CloudState,ZkNodeProps).mjava","sourceNew":"      /*\n       * Remove core from cloudstate\n       */\n      private CloudState removeCore(final CloudState cloudState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.get(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.get(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.get(ZkStateReader.COLLECTION_PROP);\n\n        final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();\n        for(String collectionName: cloudState.getCollections()) {\n          if(collection.equals(collectionName)) {\n            Map<String, Slice> slices = cloudState.getSlices(collection);\n            LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n            for(Slice slice: slices.values()) {\n              if(slice.getShards().containsKey(coreNodeName)) {\n                LinkedHashMap<String, ZkNodeProps> newShards = new LinkedHashMap<String, ZkNodeProps>();\n                newShards.putAll(slice.getShards());\n                newShards.remove(coreNodeName);\n                \n                Slice newSlice = new Slice(slice.getName(), newShards);\n                newSlices.put(slice.getName(), newSlice);\n\n              } else {\n                newSlices.put(slice.getName(), slice);\n              }\n            }\n            int cnt = 0;\n            for (Slice slice : newSlices.values()) {\n              cnt+=slice.getShards().size();\n            }\n            // TODO: if no nodes are left after this unload\n            // remove from zk - do we have a race where Overseer\n            // see's registered nodes and publishes though?\n            if (cnt > 0) {\n              newStates.put(collectionName, newSlices);\n            } else {\n              // TODO: it might be better logically to have this in ZkController\n              // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n              // ZkController out of the Overseer.\n              try {\n                zkClient.clean(\"/collections/\" + collectionName);\n              } catch (InterruptedException e) {\n                SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collectionName, e);\n                Thread.currentThread().interrupt();\n              } catch (KeeperException e) {\n                SolrException.log(log, \"Problem cleaning up collection in zk:\" + collectionName, e);\n              }\n            }\n          } else {\n            newStates.put(collectionName, cloudState.getSlices(collectionName));\n          }\n        }\n        CloudState newState = new CloudState(cloudState.getLiveNodes(), newStates);\n        return newState;\n     }\n\n","sourceOld":"      /*\n       * Remove core from cloudstate\n       */\n      private CloudState removeCore(final CloudState cloudState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.get(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.get(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.get(ZkStateReader.COLLECTION_PROP);\n\n        final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();\n        for(String collectionName: cloudState.getCollections()) {\n          if(collection.equals(collectionName)) {\n            Map<String, Slice> slices = cloudState.getSlices(collection);\n            LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n            for(Slice slice: slices.values()) {\n              if(slice.getShards().containsKey(coreNodeName)) {\n                LinkedHashMap<String, ZkNodeProps> newShards = new LinkedHashMap<String, ZkNodeProps>();\n                newShards.putAll(slice.getShards());\n                newShards.remove(coreNodeName);\n                Slice newSlice = new Slice(slice.getName(), newShards);\n                newSlices.put(slice.getName(), newSlice);\n              } else {\n                newSlices.put(slice.getName(), slice);\n              }\n            }\n            newStates.put(collectionName, newSlices);\n          } else {\n            newStates.put(collectionName, cloudState.getSlices(collectionName));\n          }\n        }\n        CloudState newState = new CloudState(cloudState.getLiveNodes(), newStates);\n        return newState;\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3f767f8c99eaedb984df754fe61f21c5de260f94","date":1344105153,"type":5,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.CloudStateUpdater#removeCore(CloudState,ZkNodeProps).mjava","sourceNew":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.get(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.get(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.get(ZkStateReader.COLLECTION_PROP);\n\n        final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();\n        for(String collectionName: clusterState.getCollections()) {\n          if(collection.equals(collectionName)) {\n            Map<String, Slice> slices = clusterState.getSlices(collection);\n            LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n            for(Slice slice: slices.values()) {\n              if(slice.getShards().containsKey(coreNodeName)) {\n                LinkedHashMap<String, ZkNodeProps> newShards = new LinkedHashMap<String, ZkNodeProps>();\n                newShards.putAll(slice.getShards());\n                newShards.remove(coreNodeName);\n                \n                Slice newSlice = new Slice(slice.getName(), newShards);\n                newSlices.put(slice.getName(), newSlice);\n\n              } else {\n                newSlices.put(slice.getName(), slice);\n              }\n            }\n            int cnt = 0;\n            for (Slice slice : newSlices.values()) {\n              cnt+=slice.getShards().size();\n            }\n            // TODO: if no nodes are left after this unload\n            // remove from zk - do we have a race where Overseer\n            // see's registered nodes and publishes though?\n            if (cnt > 0) {\n              newStates.put(collectionName, newSlices);\n            } else {\n              // TODO: it might be better logically to have this in ZkController\n              // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n              // ZkController out of the Overseer.\n              try {\n                zkClient.clean(\"/collections/\" + collectionName);\n              } catch (InterruptedException e) {\n                SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collectionName, e);\n                Thread.currentThread().interrupt();\n              } catch (KeeperException e) {\n                SolrException.log(log, \"Problem cleaning up collection in zk:\" + collectionName, e);\n              }\n            }\n          } else {\n            newStates.put(collectionName, clusterState.getSlices(collectionName));\n          }\n        }\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newStates);\n        return newState;\n     }\n\n","sourceOld":"      /*\n       * Remove core from cloudstate\n       */\n      private CloudState removeCore(final CloudState cloudState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.get(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.get(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.get(ZkStateReader.COLLECTION_PROP);\n\n        final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();\n        for(String collectionName: cloudState.getCollections()) {\n          if(collection.equals(collectionName)) {\n            Map<String, Slice> slices = cloudState.getSlices(collection);\n            LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n            for(Slice slice: slices.values()) {\n              if(slice.getShards().containsKey(coreNodeName)) {\n                LinkedHashMap<String, ZkNodeProps> newShards = new LinkedHashMap<String, ZkNodeProps>();\n                newShards.putAll(slice.getShards());\n                newShards.remove(coreNodeName);\n                \n                Slice newSlice = new Slice(slice.getName(), newShards);\n                newSlices.put(slice.getName(), newSlice);\n\n              } else {\n                newSlices.put(slice.getName(), slice);\n              }\n            }\n            int cnt = 0;\n            for (Slice slice : newSlices.values()) {\n              cnt+=slice.getShards().size();\n            }\n            // TODO: if no nodes are left after this unload\n            // remove from zk - do we have a race where Overseer\n            // see's registered nodes and publishes though?\n            if (cnt > 0) {\n              newStates.put(collectionName, newSlices);\n            } else {\n              // TODO: it might be better logically to have this in ZkController\n              // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n              // ZkController out of the Overseer.\n              try {\n                zkClient.clean(\"/collections/\" + collectionName);\n              } catch (InterruptedException e) {\n                SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collectionName, e);\n                Thread.currentThread().interrupt();\n              } catch (KeeperException e) {\n                SolrException.log(log, \"Problem cleaning up collection in zk:\" + collectionName, e);\n              }\n            }\n          } else {\n            newStates.put(collectionName, cloudState.getSlices(collectionName));\n          }\n        }\n        CloudState newState = new CloudState(cloudState.getLiveNodes(), newStates);\n        return newState;\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8fd5be977c105554c6a7b68afcdbc511439723ab","date":1344115570,"type":5,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/Overseer.ClusterStateUpdater#removeCore(ClusterState,ZkNodeProps).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.CloudStateUpdater#removeCore(CloudState,ZkNodeProps).mjava","sourceNew":"      /*\n       * Remove core from cloudstate\n       */\n      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.get(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.get(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.get(ZkStateReader.COLLECTION_PROP);\n\n        final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();\n        for(String collectionName: clusterState.getCollections()) {\n          if(collection.equals(collectionName)) {\n            Map<String, Slice> slices = clusterState.getSlices(collection);\n            LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n            for(Slice slice: slices.values()) {\n              if(slice.getShards().containsKey(coreNodeName)) {\n                LinkedHashMap<String, ZkNodeProps> newShards = new LinkedHashMap<String, ZkNodeProps>();\n                newShards.putAll(slice.getShards());\n                newShards.remove(coreNodeName);\n                \n                Slice newSlice = new Slice(slice.getName(), newShards);\n                newSlices.put(slice.getName(), newSlice);\n\n              } else {\n                newSlices.put(slice.getName(), slice);\n              }\n            }\n            int cnt = 0;\n            for (Slice slice : newSlices.values()) {\n              cnt+=slice.getShards().size();\n            }\n            // TODO: if no nodes are left after this unload\n            // remove from zk - do we have a race where Overseer\n            // see's registered nodes and publishes though?\n            if (cnt > 0) {\n              newStates.put(collectionName, newSlices);\n            } else {\n              // TODO: it might be better logically to have this in ZkController\n              // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n              // ZkController out of the Overseer.\n              try {\n                zkClient.clean(\"/collections/\" + collectionName);\n              } catch (InterruptedException e) {\n                SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collectionName, e);\n                Thread.currentThread().interrupt();\n              } catch (KeeperException e) {\n                SolrException.log(log, \"Problem cleaning up collection in zk:\" + collectionName, e);\n              }\n            }\n          } else {\n            newStates.put(collectionName, clusterState.getSlices(collectionName));\n          }\n        }\n        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newStates);\n        return newState;\n     }\n\n","sourceOld":"      /*\n       * Remove core from cloudstate\n       */\n      private CloudState removeCore(final CloudState cloudState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.get(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.get(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.get(ZkStateReader.COLLECTION_PROP);\n\n        final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();\n        for(String collectionName: cloudState.getCollections()) {\n          if(collection.equals(collectionName)) {\n            Map<String, Slice> slices = cloudState.getSlices(collection);\n            LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n            for(Slice slice: slices.values()) {\n              if(slice.getShards().containsKey(coreNodeName)) {\n                LinkedHashMap<String, ZkNodeProps> newShards = new LinkedHashMap<String, ZkNodeProps>();\n                newShards.putAll(slice.getShards());\n                newShards.remove(coreNodeName);\n                \n                Slice newSlice = new Slice(slice.getName(), newShards);\n                newSlices.put(slice.getName(), newSlice);\n\n              } else {\n                newSlices.put(slice.getName(), slice);\n              }\n            }\n            int cnt = 0;\n            for (Slice slice : newSlices.values()) {\n              cnt+=slice.getShards().size();\n            }\n            // TODO: if no nodes are left after this unload\n            // remove from zk - do we have a race where Overseer\n            // see's registered nodes and publishes though?\n            if (cnt > 0) {\n              newStates.put(collectionName, newSlices);\n            } else {\n              // TODO: it might be better logically to have this in ZkController\n              // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n              // ZkController out of the Overseer.\n              try {\n                zkClient.clean(\"/collections/\" + collectionName);\n              } catch (InterruptedException e) {\n                SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collectionName, e);\n                Thread.currentThread().interrupt();\n              } catch (KeeperException e) {\n                SolrException.log(log, \"Problem cleaning up collection in zk:\" + collectionName, e);\n              }\n            }\n          } else {\n            newStates.put(collectionName, cloudState.getSlices(collectionName));\n          }\n        }\n        CloudState newState = new CloudState(cloudState.getLiveNodes(), newStates);\n        return newState;\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":4,"author":"Uwe Schindler","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/cloud/Overseer.CloudStateUpdater#removeCore(CloudState,ZkNodeProps).mjava","sourceNew":null,"sourceOld":"      /*\n       * Remove core from cloudstate\n       */\n      private CloudState removeCore(final CloudState cloudState, ZkNodeProps message) {\n        \n        final String coreNodeName = message.get(ZkStateReader.NODE_NAME_PROP) + \"_\" + message.get(ZkStateReader.CORE_NAME_PROP);\n        final String collection = message.get(ZkStateReader.COLLECTION_PROP);\n\n        final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();\n        for(String collectionName: cloudState.getCollections()) {\n          if(collection.equals(collectionName)) {\n            Map<String, Slice> slices = cloudState.getSlices(collection);\n            LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();\n            for(Slice slice: slices.values()) {\n              if(slice.getShards().containsKey(coreNodeName)) {\n                LinkedHashMap<String, ZkNodeProps> newShards = new LinkedHashMap<String, ZkNodeProps>();\n                newShards.putAll(slice.getShards());\n                newShards.remove(coreNodeName);\n                \n                Slice newSlice = new Slice(slice.getName(), newShards);\n                newSlices.put(slice.getName(), newSlice);\n\n              } else {\n                newSlices.put(slice.getName(), slice);\n              }\n            }\n            int cnt = 0;\n            for (Slice slice : newSlices.values()) {\n              cnt+=slice.getShards().size();\n            }\n            // TODO: if no nodes are left after this unload\n            // remove from zk - do we have a race where Overseer\n            // see's registered nodes and publishes though?\n            if (cnt > 0) {\n              newStates.put(collectionName, newSlices);\n            } else {\n              // TODO: it might be better logically to have this in ZkController\n              // but for tests (it's easier) it seems better for the moment to leave CoreContainer and/or\n              // ZkController out of the Overseer.\n              try {\n                zkClient.clean(\"/collections/\" + collectionName);\n              } catch (InterruptedException e) {\n                SolrException.log(log, \"Cleaning up collection in zk was interrupted:\" + collectionName, e);\n                Thread.currentThread().interrupt();\n              } catch (KeeperException e) {\n                SolrException.log(log, \"Problem cleaning up collection in zk:\" + collectionName, e);\n              }\n            }\n          } else {\n            newStates.put(collectionName, cloudState.getSlices(collectionName));\n          }\n        }\n        CloudState newState = new CloudState(cloudState.getLiveNodes(), newStates);\n        return newState;\n     }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3f767f8c99eaedb984df754fe61f21c5de260f94":["33d0ed80b7b47e34ad3ff033a77544563aba3085"],"33d0ed80b7b47e34ad3ff033a77544563aba3085":["a5093a9e893633cc091cf2f729d7863671c2b715"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["a5093a9e893633cc091cf2f729d7863671c2b715","33d0ed80b7b47e34ad3ff033a77544563aba3085"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a5093a9e893633cc091cf2f729d7863671c2b715":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["33d0ed80b7b47e34ad3ff033a77544563aba3085","3f767f8c99eaedb984df754fe61f21c5de260f94"],"8fd5be977c105554c6a7b68afcdbc511439723ab":["fe33227f6805edab2036cbb80645cc4e2d1fa424","3f767f8c99eaedb984df754fe61f21c5de260f94"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3f767f8c99eaedb984df754fe61f21c5de260f94"]},"commit2Childs":{"3f767f8c99eaedb984df754fe61f21c5de260f94":["d6f074e73200c07d54f242d3880a8da5a35ff97b","8fd5be977c105554c6a7b68afcdbc511439723ab","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"33d0ed80b7b47e34ad3ff033a77544563aba3085":["3f767f8c99eaedb984df754fe61f21c5de260f94","fe33227f6805edab2036cbb80645cc4e2d1fa424","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["8fd5be977c105554c6a7b68afcdbc511439723ab"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a5093a9e893633cc091cf2f729d7863671c2b715"],"a5093a9e893633cc091cf2f729d7863671c2b715":["33d0ed80b7b47e34ad3ff033a77544563aba3085","fe33227f6805edab2036cbb80645cc4e2d1fa424"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":[],"8fd5be977c105554c6a7b68afcdbc511439723ab":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d6f074e73200c07d54f242d3880a8da5a35ff97b","8fd5be977c105554c6a7b68afcdbc511439723ab","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}