{"path":"lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms64#beforeClass().mjava","commits":[{"id":"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c","date":1399816179,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms64#beforeClass().mjava","pathOld":"/dev/null","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LongField\n      field8 = new LongField(\"field8\", 0L, storedLong8),\n      field6 = new LongField(\"field6\", 0L, storedLong6),\n      field4 = new LongField(\"field4\", 0L, storedLong4),\n      field2 = new LongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LONG);\n    map.put(\"field4\", Type.LONG);\n    map.put(\"field6\", Type.LONG);\n    map.put(\"field8\", Type.LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.shutdown();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["8c146731a64debc22c115bbf11ee1a060aa7ea02"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"93dd449115a9247533e44bab47e8429e5dccbc6d","date":1400258396,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms64#beforeClass().mjava","pathOld":"/dev/null","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LongField\n      field8 = new LongField(\"field8\", 0L, storedLong8),\n      field6 = new LongField(\"field6\", 0L, storedLong6),\n      field4 = new LongField(\"field4\", 0L, storedLong4),\n      field2 = new LongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LONG);\n    map.put(\"field4\", Type.LONG);\n    map.put(\"field6\", Type.LONG);\n    map.put(\"field8\", Type.LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.shutdown();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"56572ec06f1407c066d6b7399413178b33176cd8","date":1400495675,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms64#beforeClass().mjava","pathOld":"/dev/null","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LongField\n      field8 = new LongField(\"field8\", 0L, storedLong8),\n      field6 = new LongField(\"field6\", 0L, storedLong6),\n      field4 = new LongField(\"field4\", 0L, storedLong4),\n      field2 = new LongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LONG);\n    map.put(\"field4\", Type.LONG);\n    map.put(\"field6\", Type.LONG);\n    map.put(\"field8\", Type.LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.shutdown();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms64#beforeClass().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms64#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LongField\n      field8 = new LongField(\"field8\", 0L, storedLong8),\n      field6 = new LongField(\"field6\", 0L, storedLong6),\n      field4 = new LongField(\"field4\", 0L, storedLong4),\n      field2 = new LongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LONG);\n    map.put(\"field4\", Type.LONG);\n    map.put(\"field6\", Type.LONG);\n    map.put(\"field8\", Type.LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.shutdown();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LongField\n      field8 = new LongField(\"field8\", 0L, storedLong8),\n      field6 = new LongField(\"field6\", 0L, storedLong6),\n      field4 = new LongField(\"field4\", 0L, storedLong4),\n      field2 = new LongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LONG);\n    map.put(\"field4\", Type.LONG);\n    map.put(\"field6\", Type.LONG);\n    map.put(\"field8\", Type.LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.shutdown();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms64#beforeClass().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms64#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LongField\n      field8 = new LongField(\"field8\", 0L, storedLong8),\n      field6 = new LongField(\"field6\", 0L, storedLong6),\n      field4 = new LongField(\"field4\", 0L, storedLong4),\n      field2 = new LongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LONG);\n    map.put(\"field4\", Type.LONG);\n    map.put(\"field6\", Type.LONG);\n    map.put(\"field8\", Type.LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LongField\n      field8 = new LongField(\"field8\", 0L, storedLong8),\n      field6 = new LongField(\"field6\", 0L, storedLong6),\n      field4 = new LongField(\"field4\", 0L, storedLong4),\n      field2 = new LongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LONG);\n    map.put(\"field4\", Type.LONG);\n    map.put(\"field6\", Type.LONG);\n    map.put(\"field8\", Type.LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.shutdown();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"770342641f7b505eaa8dccdc666158bff2419109","date":1449868421,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms64#beforeClass().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms64#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LegacyLongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LegacyLongField\n      field8 = new LegacyLongField(\"field8\", 0L, storedLong8),\n      field6 = new LegacyLongField(\"field6\", 0L, storedLong6),\n      field4 = new LegacyLongField(\"field4\", 0L, storedLong4),\n      field2 = new LegacyLongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LONG);\n    map.put(\"field4\", Type.LONG);\n    map.put(\"field6\", Type.LONG);\n    map.put(\"field8\", Type.LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LongField\n      field8 = new LongField(\"field8\", 0L, storedLong8),\n      field6 = new LongField(\"field6\", 0L, storedLong6),\n      field4 = new LongField(\"field4\", 0L, storedLong4),\n      field2 = new LongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LONG);\n    map.put(\"field4\", Type.LONG);\n    map.put(\"field6\", Type.LONG);\n    map.put(\"field8\", Type.LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8c146731a64debc22c115bbf11ee1a060aa7ea02","date":1457616596,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms64#beforeClass().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms64#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LegacyLongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LegacyLongField\n      field8 = new LegacyLongField(\"field8\", 0L, storedLong8),\n      field6 = new LegacyLongField(\"field6\", 0L, storedLong6),\n      field4 = new LegacyLongField(\"field4\", 0L, storedLong4),\n      field2 = new LegacyLongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LEGACY_LONG);\n    map.put(\"field4\", Type.LEGACY_LONG);\n    map.put(\"field6\", Type.LEGACY_LONG);\n    map.put(\"field8\", Type.LEGACY_LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LegacyLongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LegacyLongField\n      field8 = new LegacyLongField(\"field8\", 0L, storedLong8),\n      field6 = new LegacyLongField(\"field6\", 0L, storedLong6),\n      field4 = new LegacyLongField(\"field4\", 0L, storedLong4),\n      field2 = new LegacyLongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LONG);\n    map.put(\"field4\", Type.LONG);\n    map.put(\"field6\", Type.LONG);\n    map.put(\"field8\", Type.LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":["b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a076c3c721f685b7559308fdc2cd72d91bba67e5","date":1464168992,"type":5,"author":"Mike McCandless","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestNumericTerms64#beforeClass().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms64#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LegacyLongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LegacyLongField\n      field8 = new LegacyLongField(\"field8\", 0L, storedLong8),\n      field6 = new LegacyLongField(\"field6\", 0L, storedLong6),\n      field4 = new LegacyLongField(\"field4\", 0L, storedLong4),\n      field2 = new LegacyLongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LEGACY_LONG);\n    map.put(\"field4\", Type.LEGACY_LONG);\n    map.put(\"field6\", Type.LEGACY_LONG);\n    map.put(\"field8\", Type.LEGACY_LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LegacyLongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LegacyLongField\n      field8 = new LegacyLongField(\"field8\", 0L, storedLong8),\n      field6 = new LegacyLongField(\"field6\", 0L, storedLong6),\n      field4 = new LegacyLongField(\"field4\", 0L, storedLong4),\n      field2 = new LegacyLongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LEGACY_LONG);\n    map.put(\"field4\", Type.LEGACY_LONG);\n    map.put(\"field6\", Type.LEGACY_LONG);\n    map.put(\"field8\", Type.LEGACY_LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0e121d43b5a10f2df530f406f935102656e9c4e8","date":1464198131,"type":5,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestNumericTerms64#beforeClass().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms64#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LegacyLongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LegacyLongField\n      field8 = new LegacyLongField(\"field8\", 0L, storedLong8),\n      field6 = new LegacyLongField(\"field6\", 0L, storedLong6),\n      field4 = new LegacyLongField(\"field4\", 0L, storedLong4),\n      field2 = new LegacyLongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LEGACY_LONG);\n    map.put(\"field4\", Type.LEGACY_LONG);\n    map.put(\"field6\", Type.LEGACY_LONG);\n    map.put(\"field8\", Type.LEGACY_LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LegacyLongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LegacyLongField\n      field8 = new LegacyLongField(\"field8\", 0L, storedLong8),\n      field6 = new LegacyLongField(\"field6\", 0L, storedLong6),\n      field4 = new LegacyLongField(\"field4\", 0L, storedLong4),\n      field2 = new LegacyLongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LEGACY_LONG);\n    map.put(\"field4\", Type.LEGACY_LONG);\n    map.put(\"field6\", Type.LEGACY_LONG);\n    map.put(\"field8\", Type.LEGACY_LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83870855d82aba6819217abeff5a40779dbb28b4","date":1464291012,"type":5,"author":"Mike McCandless","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestNumericTerms64#beforeClass().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms64#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LegacyLongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LegacyLongField\n      field8 = new LegacyLongField(\"field8\", 0L, storedLong8),\n      field6 = new LegacyLongField(\"field6\", 0L, storedLong6),\n      field4 = new LegacyLongField(\"field4\", 0L, storedLong4),\n      field2 = new LegacyLongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LEGACY_LONG);\n    map.put(\"field4\", Type.LEGACY_LONG);\n    map.put(\"field6\", Type.LEGACY_LONG);\n    map.put(\"field8\", Type.LEGACY_LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LegacyLongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LegacyLongField\n      field8 = new LegacyLongField(\"field8\", 0L, storedLong8),\n      field6 = new LegacyLongField(\"field6\", 0L, storedLong6),\n      field4 = new LegacyLongField(\"field4\", 0L, storedLong4),\n      field2 = new LegacyLongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LEGACY_LONG);\n    map.put(\"field4\", Type.LEGACY_LONG);\n    map.put(\"field6\", Type.LEGACY_LONG);\n    map.put(\"field8\", Type.LEGACY_LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms64#beforeClass().mjava","sourceNew":null,"sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LegacyLongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LegacyLongField\n      field8 = new LegacyLongField(\"field8\", 0L, storedLong8),\n      field6 = new LegacyLongField(\"field6\", 0L, storedLong6),\n      field4 = new LegacyLongField(\"field4\", 0L, storedLong4),\n      field2 = new LegacyLongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LEGACY_LONG);\n    map.put(\"field4\", Type.LEGACY_LONG);\n    map.put(\"field6\", Type.LEGACY_LONG);\n    map.put(\"field8\", Type.LEGACY_LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a076c3c721f685b7559308fdc2cd72d91bba67e5":["8c146731a64debc22c115bbf11ee1a060aa7ea02"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"0e121d43b5a10f2df530f406f935102656e9c4e8":["8c146731a64debc22c115bbf11ee1a060aa7ea02","a076c3c721f685b7559308fdc2cd72d91bba67e5"],"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"83870855d82aba6819217abeff5a40779dbb28b4":["8c146731a64debc22c115bbf11ee1a060aa7ea02","0e121d43b5a10f2df530f406f935102656e9c4e8"],"56572ec06f1407c066d6b7399413178b33176cd8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","93dd449115a9247533e44bab47e8429e5dccbc6d"],"770342641f7b505eaa8dccdc666158bff2419109":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["8c146731a64debc22c115bbf11ee1a060aa7ea02","0e121d43b5a10f2df530f406f935102656e9c4e8"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"8c146731a64debc22c115bbf11ee1a060aa7ea02":["770342641f7b505eaa8dccdc666158bff2419109"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0e121d43b5a10f2df530f406f935102656e9c4e8"]},"commit2Childs":{"a076c3c721f685b7559308fdc2cd72d91bba67e5":["0e121d43b5a10f2df530f406f935102656e9c4e8"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"0e121d43b5a10f2df530f406f935102656e9c4e8":["83870855d82aba6819217abeff5a40779dbb28b4","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"83870855d82aba6819217abeff5a40779dbb28b4":[],"56572ec06f1407c066d6b7399413178b33176cd8":[],"770342641f7b505eaa8dccdc666158bff2419109":["8c146731a64debc22c115bbf11ee1a060aa7ea02"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"93dd449115a9247533e44bab47e8429e5dccbc6d":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","56572ec06f1407c066d6b7399413178b33176cd8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c","56572ec06f1407c066d6b7399413178b33176cd8","93dd449115a9247533e44bab47e8429e5dccbc6d"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["770342641f7b505eaa8dccdc666158bff2419109"],"8c146731a64debc22c115bbf11ee1a060aa7ea02":["a076c3c721f685b7559308fdc2cd72d91bba67e5","0e121d43b5a10f2df530f406f935102656e9c4e8","83870855d82aba6819217abeff5a40779dbb28b4","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["83870855d82aba6819217abeff5a40779dbb28b4","56572ec06f1407c066d6b7399413178b33176cd8","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}