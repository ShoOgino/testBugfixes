{"path":"src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","commits":[{"id":"5350389bf83287111f7760b9e3db3af8e3648474","date":1216372812,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","pathOld":"/dev/null","sourceNew":"  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = (Fieldable) docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false);\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null)\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n\n    return consumer.finishDocument();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","99351c613f288821fa2b1fa505fe5cbab9ab0600","e16e7f861173cd33e238ebd1a7b7df19ba2dd6e7"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"902ba79f4590a41c663c447756d2e5041cbbdda9","date":1217956662,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","pathOld":"src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = (Fieldable) docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTf());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTf());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null)\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n\n    return consumer.finishDocument();\n  }\n\n","sourceOld":"  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = (Fieldable) docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false);\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false);\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null)\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n\n    return consumer.finishDocument();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"066b6ff5a08e35c3b6880e7c3ddda79526acdab1","date":1237569961,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","pathOld":"src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = (Fieldable) docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null)\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n\n    return consumer.finishDocument();\n  }\n\n","sourceOld":"  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = (Fieldable) docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTf());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTf());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null)\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n\n    return consumer.finishDocument();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e0e944cfdb2c6126eecac4d5dcc6b9fcf3d389eb","date":1239218276,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","pathOld":"src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = (Fieldable) docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTf());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTf());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null)\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n\n    return consumer.finishDocument();\n  }\n\n","sourceOld":"  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = (Fieldable) docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null)\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n\n    return consumer.finishDocument();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"99351c613f288821fa2b1fa505fe5cbab9ab0600","date":1247144008,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","pathOld":"src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = (Fieldable) docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTf());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTf());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null)\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","sourceOld":"  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = (Fieldable) docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTf());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTf());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null)\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n\n    return consumer.finishDocument();\n  }\n\n","bugFix":["5350389bf83287111f7760b9e3db3af8e3648474"],"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f44f07aa147c21735d6c43bafa4fde560d0362e1","date":1255024926,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","pathOld":"src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = (Fieldable) docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null)\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","sourceOld":"  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = (Fieldable) docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTf());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTf());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null)\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f011f01db72fa6f556a9a0843944ecee2de4aaa8","date":1255806907,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","pathOld":"src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = (Fieldable) docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null)\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","sourceOld":"  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = (Fieldable) docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null)\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4e1ce9be74263e9659aad8a6ee1f213193710b71","date":1256298843,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","pathOld":"src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null)\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","sourceOld":"  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = (Fieldable) docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null)\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8a9e385641d717e641408d8fbbc62be8fc766357","date":1256746606,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","pathOld":"src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null)\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","sourceOld":"  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null)\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e16e7f861173cd33e238ebd1a7b7df19ba2dd6e7","date":1260962136,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","pathOld":"src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null)\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":["5350389bf83287111f7760b9e3db3af8e3648474"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","pathOld":"src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"f44f07aa147c21735d6c43bafa4fde560d0362e1":["99351c613f288821fa2b1fa505fe5cbab9ab0600"],"4e1ce9be74263e9659aad8a6ee1f213193710b71":["f011f01db72fa6f556a9a0843944ecee2de4aaa8"],"8a9e385641d717e641408d8fbbc62be8fc766357":["4e1ce9be74263e9659aad8a6ee1f213193710b71"],"99351c613f288821fa2b1fa505fe5cbab9ab0600":["e0e944cfdb2c6126eecac4d5dcc6b9fcf3d389eb"],"f011f01db72fa6f556a9a0843944ecee2de4aaa8":["f44f07aa147c21735d6c43bafa4fde560d0362e1"],"902ba79f4590a41c663c447756d2e5041cbbdda9":["5350389bf83287111f7760b9e3db3af8e3648474"],"e0e944cfdb2c6126eecac4d5dcc6b9fcf3d389eb":["066b6ff5a08e35c3b6880e7c3ddda79526acdab1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"e16e7f861173cd33e238ebd1a7b7df19ba2dd6e7":["8a9e385641d717e641408d8fbbc62be8fc766357"],"066b6ff5a08e35c3b6880e7c3ddda79526acdab1":["902ba79f4590a41c663c447756d2e5041cbbdda9"],"5350389bf83287111f7760b9e3db3af8e3648474":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["e16e7f861173cd33e238ebd1a7b7df19ba2dd6e7"]},"commit2Childs":{"f44f07aa147c21735d6c43bafa4fde560d0362e1":["f011f01db72fa6f556a9a0843944ecee2de4aaa8"],"4e1ce9be74263e9659aad8a6ee1f213193710b71":["8a9e385641d717e641408d8fbbc62be8fc766357"],"99351c613f288821fa2b1fa505fe5cbab9ab0600":["f44f07aa147c21735d6c43bafa4fde560d0362e1"],"8a9e385641d717e641408d8fbbc62be8fc766357":["e16e7f861173cd33e238ebd1a7b7df19ba2dd6e7"],"f011f01db72fa6f556a9a0843944ecee2de4aaa8":["4e1ce9be74263e9659aad8a6ee1f213193710b71"],"902ba79f4590a41c663c447756d2e5041cbbdda9":["066b6ff5a08e35c3b6880e7c3ddda79526acdab1"],"e0e944cfdb2c6126eecac4d5dcc6b9fcf3d389eb":["99351c613f288821fa2b1fa505fe5cbab9ab0600"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["5350389bf83287111f7760b9e3db3af8e3648474"],"e16e7f861173cd33e238ebd1a7b7df19ba2dd6e7":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"066b6ff5a08e35c3b6880e7c3ddda79526acdab1":["e0e944cfdb2c6126eecac4d5dcc6b9fcf3d389eb"],"5350389bf83287111f7760b9e3db3af8e3648474":["902ba79f4590a41c663c447756d2e5041cbbdda9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}