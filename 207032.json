{"path":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","commits":[{"id":"2c007e7c4cf8c55bc2a5884e315123afaaeec87f","date":1327520966,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    // we cannot do delete by query\n    // as it's not supported for recovery\n    //del(\"*:*\");\n    \n    List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n    int threadCount = 1;\n    int i = 0;\n    for (i = 0; i < threadCount; i++) {\n      StopableIndexingThread indexThread = new StopableIndexingThread(i * 50000, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n        clients, i * 50000, true);\n    threads.add(ftIndexThread);\n    ftIndexThread.start();\n    \n    chaosMonkey.startTheMonkey(true, 1500);\n    try {\n      Thread.sleep(atLeast(6000));\n    } finally {\n      chaosMonkey.stopTheMonkey();\n    }\n    \n    for (StopableIndexingThread indexThread : threads) {\n      indexThread.safeStop();\n    }\n    \n    // wait for stop...\n    for (StopableIndexingThread indexThread : threads) {\n      indexThread.join();\n    }\n    \n    \n    // fails will happen...\n//    for (StopableIndexingThread indexThread : threads) {\n//      assertEquals(0, indexThread.getFails());\n//    }\n    \n    // try and wait for any replications and what not to finish...\n    \n    Thread.sleep(2000);\n    \n    // wait until there are no recoveries...\n    waitForThingsToLevelOut();\n    \n    // make sure we again have leaders for each shard\n    for (int j = 1; j < sliceCount; j++) {\n      zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n    }\n\n    commit();\n    \n    // TODO: assert we didnt kill everyone\n    \n    zkStateReader.updateCloudState(true);\n    assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n    \n    checkShardConsistency(false, false);\n    \n    // ensure we have added more than 0 docs\n    long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n\n    assertTrue(cloudClientDocs > 0);\n    \n    if (VERBOSE) System.out.println(\"control docs:\" + controlClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound() + \"\\n\\n\");\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","date":1327523564,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    // we cannot do delete by query\n    // as it's not supported for recovery\n    //del(\"*:*\");\n    \n    List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n    int threadCount = 1;\n    int i = 0;\n    for (i = 0; i < threadCount; i++) {\n      StopableIndexingThread indexThread = new StopableIndexingThread(i * 50000, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n        clients, i * 50000, true);\n    threads.add(ftIndexThread);\n    ftIndexThread.start();\n    \n    chaosMonkey.startTheMonkey(true, 1500);\n    try {\n      Thread.sleep(atLeast(6000));\n    } finally {\n      chaosMonkey.stopTheMonkey();\n    }\n    \n    for (StopableIndexingThread indexThread : threads) {\n      indexThread.safeStop();\n    }\n    \n    // wait for stop...\n    for (StopableIndexingThread indexThread : threads) {\n      indexThread.join();\n    }\n    \n    \n    // fails will happen...\n//    for (StopableIndexingThread indexThread : threads) {\n//      assertEquals(0, indexThread.getFails());\n//    }\n    \n    // try and wait for any replications and what not to finish...\n    \n    Thread.sleep(2000);\n    \n    // wait until there are no recoveries...\n    waitForThingsToLevelOut();\n    \n    // make sure we again have leaders for each shard\n    for (int j = 1; j < sliceCount; j++) {\n      zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n    }\n\n    commit();\n    \n    // TODO: assert we didnt kill everyone\n    \n    zkStateReader.updateCloudState(true);\n    assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n    \n    checkShardConsistency(false, false);\n    \n    // ensure we have added more than 0 docs\n    long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n\n    assertTrue(cloudClientDocs > 0);\n    \n    if (VERBOSE) System.out.println(\"control docs:\" + controlClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound() + \"\\n\\n\");\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0d22ac6a4146774c1bc8400160fc0b6150294e92","date":1327528604,"type":0,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    // we cannot do delete by query\n    // as it's not supported for recovery\n    //del(\"*:*\");\n    \n    List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n    int threadCount = 1;\n    int i = 0;\n    for (i = 0; i < threadCount; i++) {\n      StopableIndexingThread indexThread = new StopableIndexingThread(i * 50000, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n        clients, i * 50000, true);\n    threads.add(ftIndexThread);\n    ftIndexThread.start();\n    \n    chaosMonkey.startTheMonkey(true, 1500);\n    try {\n      Thread.sleep(atLeast(6000));\n    } finally {\n      chaosMonkey.stopTheMonkey();\n    }\n    \n    for (StopableIndexingThread indexThread : threads) {\n      indexThread.safeStop();\n    }\n    \n    // wait for stop...\n    for (StopableIndexingThread indexThread : threads) {\n      indexThread.join();\n    }\n    \n    \n    // fails will happen...\n//    for (StopableIndexingThread indexThread : threads) {\n//      assertEquals(0, indexThread.getFails());\n//    }\n    \n    // try and wait for any replications and what not to finish...\n    \n    Thread.sleep(2000);\n    \n    // wait until there are no recoveries...\n    waitForThingsToLevelOut();\n    \n    // make sure we again have leaders for each shard\n    for (int j = 1; j < sliceCount; j++) {\n      zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n    }\n\n    commit();\n    \n    // TODO: assert we didnt kill everyone\n    \n    zkStateReader.updateCloudState(true);\n    assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n    \n    checkShardConsistency(false, false);\n    \n    // ensure we have added more than 0 docs\n    long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n\n    assertTrue(cloudClientDocs > 0);\n    \n    if (VERBOSE) System.out.println(\"control docs:\" + controlClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound() + \"\\n\\n\");\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f6d5f9478e1045c449ecb420dd322ad9c64837b6","date":1327544551,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      try {\n        Thread.sleep(atLeast(6000));\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut();\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateCloudState(true);\n      assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    // we cannot do delete by query\n    // as it's not supported for recovery\n    //del(\"*:*\");\n    \n    List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n    int threadCount = 1;\n    int i = 0;\n    for (i = 0; i < threadCount; i++) {\n      StopableIndexingThread indexThread = new StopableIndexingThread(i * 50000, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n        clients, i * 50000, true);\n    threads.add(ftIndexThread);\n    ftIndexThread.start();\n    \n    chaosMonkey.startTheMonkey(true, 1500);\n    try {\n      Thread.sleep(atLeast(6000));\n    } finally {\n      chaosMonkey.stopTheMonkey();\n    }\n    \n    for (StopableIndexingThread indexThread : threads) {\n      indexThread.safeStop();\n    }\n    \n    // wait for stop...\n    for (StopableIndexingThread indexThread : threads) {\n      indexThread.join();\n    }\n    \n    \n    // fails will happen...\n//    for (StopableIndexingThread indexThread : threads) {\n//      assertEquals(0, indexThread.getFails());\n//    }\n    \n    // try and wait for any replications and what not to finish...\n    \n    Thread.sleep(2000);\n    \n    // wait until there are no recoveries...\n    waitForThingsToLevelOut();\n    \n    // make sure we again have leaders for each shard\n    for (int j = 1; j < sliceCount; j++) {\n      zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n    }\n\n    commit();\n    \n    // TODO: assert we didnt kill everyone\n    \n    zkStateReader.updateCloudState(true);\n    assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n    \n    checkShardConsistency(false, false);\n    \n    // ensure we have added more than 0 docs\n    long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n\n    assertTrue(cloudClientDocs > 0);\n    \n    if (VERBOSE) System.out.println(\"control docs:\" + controlClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound() + \"\\n\\n\");\n  }\n\n","bugFix":null,"bugIntro":["933fa8f09adfcd1a858cd0fc7912e21ee993b7fc","072f211dfa8387028bb978d128c35bf9a450bbbf","a6378064655e76cd7b908b1cab4ce425b384b508","d8469c0c0b45f7e3da74918efc8ede2ad2efe2b1","67e5fb3e1a3e5763eb7aabb2ee71d10e8617e699"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"78a55f24d9b493c2a1cecf79f1d78279062b545b","date":1327688152,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      try {\n        Thread.sleep(atLeast(6000));\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut();\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateCloudState(true);\n      assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    // we cannot do delete by query\n    // as it's not supported for recovery\n    //del(\"*:*\");\n    \n    List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n    int threadCount = 1;\n    int i = 0;\n    for (i = 0; i < threadCount; i++) {\n      StopableIndexingThread indexThread = new StopableIndexingThread(i * 50000, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n        clients, i * 50000, true);\n    threads.add(ftIndexThread);\n    ftIndexThread.start();\n    \n    chaosMonkey.startTheMonkey(true, 1500);\n    try {\n      Thread.sleep(atLeast(6000));\n    } finally {\n      chaosMonkey.stopTheMonkey();\n    }\n    \n    for (StopableIndexingThread indexThread : threads) {\n      indexThread.safeStop();\n    }\n    \n    // wait for stop...\n    for (StopableIndexingThread indexThread : threads) {\n      indexThread.join();\n    }\n    \n    \n    // fails will happen...\n//    for (StopableIndexingThread indexThread : threads) {\n//      assertEquals(0, indexThread.getFails());\n//    }\n    \n    // try and wait for any replications and what not to finish...\n    \n    Thread.sleep(2000);\n    \n    // wait until there are no recoveries...\n    waitForThingsToLevelOut();\n    \n    // make sure we again have leaders for each shard\n    for (int j = 1; j < sliceCount; j++) {\n      zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n    }\n\n    commit();\n    \n    // TODO: assert we didnt kill everyone\n    \n    zkStateReader.updateCloudState(true);\n    assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n    \n    checkShardConsistency(false, false);\n    \n    // ensure we have added more than 0 docs\n    long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n\n    assertTrue(cloudClientDocs > 0);\n    \n    if (VERBOSE) System.out.println(\"control docs:\" + controlClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound() + \"\\n\\n\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fd92b8bcc88e969302510acf77bd6970da3994c4","date":1327839530,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      try {\n        Thread.sleep(atLeast(6000));\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut();\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateCloudState(true);\n      assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    // we cannot do delete by query\n    // as it's not supported for recovery\n    //del(\"*:*\");\n    \n    List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n    int threadCount = 1;\n    int i = 0;\n    for (i = 0; i < threadCount; i++) {\n      StopableIndexingThread indexThread = new StopableIndexingThread(i * 50000, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n        clients, i * 50000, true);\n    threads.add(ftIndexThread);\n    ftIndexThread.start();\n    \n    chaosMonkey.startTheMonkey(true, 1500);\n    try {\n      Thread.sleep(atLeast(6000));\n    } finally {\n      chaosMonkey.stopTheMonkey();\n    }\n    \n    for (StopableIndexingThread indexThread : threads) {\n      indexThread.safeStop();\n    }\n    \n    // wait for stop...\n    for (StopableIndexingThread indexThread : threads) {\n      indexThread.join();\n    }\n    \n    \n    // fails will happen...\n//    for (StopableIndexingThread indexThread : threads) {\n//      assertEquals(0, indexThread.getFails());\n//    }\n    \n    // try and wait for any replications and what not to finish...\n    \n    Thread.sleep(2000);\n    \n    // wait until there are no recoveries...\n    waitForThingsToLevelOut();\n    \n    // make sure we again have leaders for each shard\n    for (int j = 1; j < sliceCount; j++) {\n      zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n    }\n\n    commit();\n    \n    // TODO: assert we didnt kill everyone\n    \n    zkStateReader.updateCloudState(true);\n    assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n    \n    checkShardConsistency(false, false);\n    \n    // ensure we have added more than 0 docs\n    long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n\n    assertTrue(cloudClientDocs > 0);\n    \n    if (VERBOSE) System.out.println(\"control docs:\" + controlClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound() + \"\\n\\n\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"33bdd4aa796e0067ba181b149f1580a94d0ff5e0","date":1342362884,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      try {\n        Thread.sleep(180000);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut();\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateCloudState(true);\n      assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      try {\n        Thread.sleep(atLeast(6000));\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut();\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateCloudState(true);\n      assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["933fa8f09adfcd1a858cd0fc7912e21ee993b7fc"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fe33227f6805edab2036cbb80645cc4e2d1fa424","date":1342713534,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      try {\n        Thread.sleep(180000);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut();\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateCloudState(true);\n      assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      try {\n        Thread.sleep(atLeast(6000));\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut();\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateCloudState(true);\n      assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"933fa8f09adfcd1a858cd0fc7912e21ee993b7fc","date":1342989037,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Math.round((runLength / 1000.0f / 5.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateCloudState(true);\n      assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      try {\n        Thread.sleep(180000);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut();\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateCloudState(true);\n      assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":["f6d5f9478e1045c449ecb420dd322ad9c64837b6","33bdd4aa796e0067ba181b149f1580a94d0ff5e0"],"bugIntro":["a6378064655e76cd7b908b1cab4ce425b384b508"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","date":1343059585,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Math.round((runLength / 1000.0f / 5.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateCloudState(true);\n      assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      try {\n        Thread.sleep(180000);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut();\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateCloudState(true);\n      assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"acb07c6d6e233ed02bfb1837f0e4e78d3615e50c","date":1343166596,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Math.round((runLength / 1000.0f / 5.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateCloudState(true);\n      assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Math.round((runLength / 1000.0f / 5.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateCloudState(true);\n      assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fe9077a1ed75bdcdcc7dfff6525f73941c8cf30b","date":1343203827,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Math.round((runLength / 1000.0f / 5.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateCloudState(true);\n      assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Math.round((runLength / 1000.0f / 5.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateCloudState(true);\n      assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aba371508186796cc6151d8223a5b4e16d02e26e","date":1343474871,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Math.round((runLength / 1000.0f / 5.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateCloudState(true);\n      assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      try {\n        Thread.sleep(atLeast(6000));\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut();\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateCloudState(true);\n      assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Math.round((runLength / 1000.0f / 5.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateCloudState(true);\n      assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableIndexingThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Math.round((runLength / 1000.0f / 5.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateCloudState(true);\n      assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3f767f8c99eaedb984df754fe61f21c5de260f94","date":1344105153,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Math.round((runLength / 1000.0f / 5.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Math.round((runLength / 1000.0f / 5.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateCloudState(true);\n      assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8fd5be977c105554c6a7b68afcdbc511439723ab","date":1344115570,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Math.round((runLength / 1000.0f / 5.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Math.round((runLength / 1000.0f / 5.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateCloudState(true);\n      assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Math.round((runLength / 1000.0f / 5.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Math.round((runLength / 1000.0f / 5.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateCloudState(true);\n      assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"66c64e8cfded6a585100e6430238faaf416f3fea","date":1344964603,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this randomly - if we don't do it, compare against control below\n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      // we dont't current check vs control because the full throttle thread can\n      // have request fails\n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Math.round((runLength / 1000.0f / 5.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["a6378064655e76cd7b908b1cab4ce425b384b508"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","date":1345029782,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this randomly - if we don't do it, compare against control below\n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      // we dont't current check vs control because the full throttle thread can\n      // have request fails\n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Math.round((runLength / 1000.0f / 5.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b05c56a41b733e02a189c48895922b5bd8c7f3d1","date":1345033322,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this randomly - if we don't do it, compare against control below\n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      // we dont't current check vs control because the full throttle thread can\n      // have request fails\n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Math.round((runLength / 1000.0f / 5.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6013b4c7388f1627659c8f96c44abd10a294d3a6","date":1346343796,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this randomly - if we don't do it, compare against control below\n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we dont't current check vs control because the full throttle thread can\n      // have request fails\n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this randomly - if we don't do it, compare against control below\n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      // we dont't current check vs control because the full throttle thread can\n      // have request fails\n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"05a14b2611ead08655a2b2bdc61632eb31316e57","date":1346366621,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this randomly - if we don't do it, compare against control below\n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we dont't current check vs control because the full throttle thread can\n      // have request fails\n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      // del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this randomly - if we don't do it, compare against control below\n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      // we dont't current check vs control because the full throttle thread can\n      // have request fails\n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7157fdda82ae1a1cb77b012ef0eb72249ee2f3d6","date":1346692465,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this randomly - if we don't do it, compare against control below\n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we dont't current check vs control because the full throttle thread can\n      // have request fails\n      checkShardConsistency(false, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this randomly - if we don't do it, compare against control below\n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we dont't current check vs control because the full throttle thread can\n      // have request fails\n      checkShardConsistency(false, true);\n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d8469c0c0b45f7e3da74918efc8ede2ad2efe2b1","date":1346817835,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this randomly - if we don't do it, compare against control below\n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but not cloud client...\n       for (StopableThread indexThread : threads) {\n         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n           assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n         }\n       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we dont't current check vs control because the full throttle thread can\n      // have request fails\n      checkShardConsistency(false, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this randomly - if we don't do it, compare against control below\n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // fails will happen...\n      // for (StopableIndexingThread indexThread : threads) {\n      // assertEquals(0, indexThread.getFails());\n      // }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we dont't current check vs control because the full throttle thread can\n      // have request fails\n      checkShardConsistency(false, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":["f6d5f9478e1045c449ecb420dd322ad9c64837b6"],"bugIntro":["a6378064655e76cd7b908b1cab4ce425b384b508"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a6378064655e76cd7b908b1cab4ce425b384b508","date":1347656715,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, i * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      //int runLength = atLeast(BASE_RUN_LENGTH);\n      int runLength = BASE_RUN_LENGTH;\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but not cloud client...\n       for (StopableThread indexThread : threads) {\n         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n           //assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n         }\n       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this randomly - if we don't do it, compare against control below\n      FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n          clients, i * 50000, true);\n      threads.add(ftIndexThread);\n      ftIndexThread.start();\n      \n      chaosMonkey.startTheMonkey(true, 1500);\n      int runLength = atLeast(BASE_RUN_LENGTH);\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but not cloud client...\n       for (StopableThread indexThread : threads) {\n         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n           assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n         }\n       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we dont't current check vs control because the full throttle thread can\n      // have request fails\n      checkShardConsistency(false, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":["f6d5f9478e1045c449ecb420dd322ad9c64837b6","66c64e8cfded6a585100e6430238faaf416f3fea","933fa8f09adfcd1a858cd0fc7912e21ee993b7fc","d8469c0c0b45f7e3da74918efc8ede2ad2efe2b1"],"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d9405f486872f1e416304dfe389741f4ee2f8a4d","date":1351276739,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, i * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      //int runLength = atLeast(BASE_RUN_LENGTH);\n      int runLength = BASE_RUN_LENGTH;\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but not cloud client...\n       for (StopableThread indexThread : threads) {\n         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n           //assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n         }\n       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, i * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      //int runLength = atLeast(BASE_RUN_LENGTH);\n      int runLength = BASE_RUN_LENGTH;\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but not cloud client...\n       for (StopableThread indexThread : threads) {\n         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n           //assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n         }\n       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f2126b84bd093fa3d921582a109a0ee578c28126","date":1351522501,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, i * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      //int runLength = atLeast(BASE_RUN_LENGTH);\n      int runLength = BASE_RUN_LENGTH;\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but not cloud client...\n       for (StopableThread indexThread : threads) {\n         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n           //assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n         }\n       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      \n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, i * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      //int runLength = atLeast(BASE_RUN_LENGTH);\n      int runLength = BASE_RUN_LENGTH;\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but not cloud client...\n       for (StopableThread indexThread : threads) {\n         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n           //assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n         }\n       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"56a558aa5aadd60ae850d1ab090098bc63bdfaf9","date":1355245333,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, i * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      //int runLength = atLeast(BASE_RUN_LENGTH);\n      int runLength = BASE_RUN_LENGTH;\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but not cloud client...\n       for (StopableThread indexThread : threads) {\n         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n           //assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n         }\n       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, i * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      //int runLength = atLeast(BASE_RUN_LENGTH);\n      int runLength = BASE_RUN_LENGTH;\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but not cloud client...\n       for (StopableThread indexThread : threads) {\n         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n           //assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n         }\n       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"80abb8f3c6cd9755d3203b2bc85d67c10aaf80ac","date":1357086196,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, i * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      //int runLength = atLeast(BASE_RUN_LENGTH);\n      int runLength = BASE_RUN_LENGTH;\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but not cloud client...\n       for (StopableThread indexThread : threads) {\n         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n           assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n         }\n       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, i * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      //int runLength = atLeast(BASE_RUN_LENGTH);\n      int runLength = BASE_RUN_LENGTH;\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but not cloud client...\n       for (StopableThread indexThread : threads) {\n         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n           //assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n         }\n       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c5e0a45dbd62a2d2a07b516f66f0b750f70bba98","date":1357329573,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, i * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      //int runLength = atLeast(BASE_RUN_LENGTH);\n      int runLength = BASE_RUN_LENGTH;\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but cloud client should not easily fail\n       // but it's allowed to fail and sometimes does, so commented out for now\n//       for (StopableThread indexThread : threads) {\n//         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n//           assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n//         }\n//       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, i * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      //int runLength = atLeast(BASE_RUN_LENGTH);\n      int runLength = BASE_RUN_LENGTH;\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but not cloud client...\n       for (StopableThread indexThread : threads) {\n         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n           assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n         }\n       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, i * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      //int runLength = atLeast(BASE_RUN_LENGTH);\n      int runLength = BASE_RUN_LENGTH;\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but cloud client should not easily fail\n       // but it's allowed to fail and sometimes does, so commented out for now\n//       for (StopableThread indexThread : threads) {\n//         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n//           assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n//         }\n//       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, i * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      //int runLength = atLeast(BASE_RUN_LENGTH);\n      int runLength = BASE_RUN_LENGTH;\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but not cloud client...\n       for (StopableThread indexThread : threads) {\n         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n           //assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n         }\n       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"108e7dc629c849ba42c0ccc2049f20213a776652","date":1360168324,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            (i+1) * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, (i+1) * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      //int runLength = atLeast(BASE_RUN_LENGTH);\n      int runLength = BASE_RUN_LENGTH;\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but cloud client should not easily fail\n       // but it's allowed to fail and sometimes does, so commented out for now\n//       for (StopableThread indexThread : threads) {\n//         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n//           assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n//         }\n//       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            i * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, i * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      //int runLength = atLeast(BASE_RUN_LENGTH);\n      int runLength = BASE_RUN_LENGTH;\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but cloud client should not easily fail\n       // but it's allowed to fail and sometimes does, so commented out for now\n//       for (StopableThread indexThread : threads) {\n//         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n//           assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n//         }\n//       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["67e5fb3e1a3e5763eb7aabb2ee71d10e8617e699"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2c4c76058cf64f5f3447673694ddec6e6c1eb675","date":1361060577,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            (i+1) * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, (i+1) * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n\n      long runLength;\n      if (RUN_LENGTH != -1) {\n        runLength = RUN_LENGTH;\n      } else {\n        int[] runTimes = new int[] {5000,6000,10000,15000,15000,30000,30000,45000,90000,120000};\n        runLength = runTimes[random().nextInt(runTimes.length - 1)];\n      }\n      \n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but cloud client should not easily fail\n       // but it's allowed to fail and sometimes does, so commented out for now\n//       for (StopableThread indexThread : threads) {\n//         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n//           assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n//         }\n//       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            (i+1) * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, (i+1) * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      //int runLength = atLeast(BASE_RUN_LENGTH);\n      int runLength = BASE_RUN_LENGTH;\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but cloud client should not easily fail\n       // but it's allowed to fail and sometimes does, so commented out for now\n//       for (StopableThread indexThread : threads) {\n//         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n//           assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n//         }\n//       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"95303ff3749680c743b9425f9cf99e6e4065e8a8","date":1361061922,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            (i+1) * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, (i+1) * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n\n      long runLength;\n      if (RUN_LENGTH != -1) {\n        runLength = RUN_LENGTH;\n      } else {\n        int[] runTimes = new int[] {5000,6000,10000,15000,15000,30000,30000,45000,90000,120000};\n        runLength = runTimes[random().nextInt(runTimes.length - 1)];\n      }\n      \n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but cloud client should not easily fail\n       // but it's allowed to fail and sometimes does, so commented out for now\n//       for (StopableThread indexThread : threads) {\n//         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n//           assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n//         }\n//       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            (i+1) * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, (i+1) * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      //int runLength = atLeast(BASE_RUN_LENGTH);\n      int runLength = BASE_RUN_LENGTH;\n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but cloud client should not easily fail\n       // but it's allowed to fail and sometimes does, so commented out for now\n//       for (StopableThread indexThread : threads) {\n//         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n//           assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n//         }\n//       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"270612d8e1a51cded91704d7af12f8979de0f584","date":1381502089,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            (i+1) * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, (i+1) * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n\n      long runLength;\n      if (RUN_LENGTH != -1) {\n        runLength = RUN_LENGTH;\n      } else {\n        int[] runTimes = new int[] {5000,6000,10000,15000,15000,30000,30000,45000,90000,120000};\n        runLength = runTimes[random().nextInt(runTimes.length - 1)];\n      }\n      \n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but cloud client should not easily fail\n       // but it's allowed to fail and sometimes does, so commented out for now\n//       for (StopableThread indexThread : threads) {\n//         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n//           assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n//         }\n//       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            (i+1) * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, (i+1) * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n\n      long runLength;\n      if (RUN_LENGTH != -1) {\n        runLength = RUN_LENGTH;\n      } else {\n        int[] runTimes = new int[] {5000,6000,10000,15000,15000,30000,30000,45000,90000,120000};\n        runLength = runTimes[random().nextInt(runTimes.length - 1)];\n      }\n      \n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but cloud client should not easily fail\n       // but it's allowed to fail and sometimes does, so commented out for now\n//       for (StopableThread indexThread : threads) {\n//         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n//           assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n//         }\n//       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2fc602b7d81247110d1ca76613bc560c1710e90c","date":1384981151,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            (i+1) * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, (i+1) * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n\n      long runLength;\n      if (RUN_LENGTH != -1) {\n        runLength = RUN_LENGTH;\n      } else {\n        int[] runTimes = new int[] {5000,6000,10000,15000,15000,30000,30000,45000,90000,120000};\n        runLength = runTimes[random().nextInt(runTimes.length - 1)];\n      }\n      \n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but cloud client should not easily fail\n       for (StopableThread indexThread : threads) {\n         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n           assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n         }\n       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            (i+1) * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, (i+1) * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n\n      long runLength;\n      if (RUN_LENGTH != -1) {\n        runLength = RUN_LENGTH;\n      } else {\n        int[] runTimes = new int[] {5000,6000,10000,15000,15000,30000,30000,45000,90000,120000};\n        runLength = runTimes[random().nextInt(runTimes.length - 1)];\n      }\n      \n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but cloud client should not easily fail\n       // but it's allowed to fail and sometimes does, so commented out for now\n//       for (StopableThread indexThread : threads) {\n//         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n//           assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n//         }\n//       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"767a42b1ba7578b44587b42170ad81003c6b479a","date":1385341911,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            (i+1) * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, (i+1) * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n\n      long runLength;\n      if (RUN_LENGTH != -1) {\n        runLength = RUN_LENGTH;\n      } else {\n        int[] runTimes = new int[] {5000,6000,10000,15000,25000,30000,30000,45000,90000,120000};\n        runLength = runTimes[random().nextInt(runTimes.length - 1)];\n      }\n      \n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but cloud client should not easily fail\n       for (StopableThread indexThread : threads) {\n         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n           assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n         }\n       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            (i+1) * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, (i+1) * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n\n      long runLength;\n      if (RUN_LENGTH != -1) {\n        runLength = RUN_LENGTH;\n      } else {\n        int[] runTimes = new int[] {5000,6000,10000,15000,15000,30000,30000,45000,90000,120000};\n        runLength = runTimes[random().nextInt(runTimes.length - 1)];\n      }\n      \n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but cloud client should not easily fail\n       for (StopableThread indexThread : threads) {\n         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n           assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n         }\n       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"68fe0b7fa3b6a69495244529754954d2ae5cf0f7","date":1385414051,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            (i+1) * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, (i+1) * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, true, 10000);\n\n      long runLength;\n      if (RUN_LENGTH != -1) {\n        runLength = RUN_LENGTH;\n      } else {\n        int[] runTimes = new int[] {5000,6000,10000,15000,25000,30000,30000,45000,90000,120000};\n        runLength = runTimes[random().nextInt(runTimes.length - 1)];\n      }\n      \n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertEquals(\"There were expected update fails\", 0, ((StopableIndexingThread) indexThread).getFails());\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            (i+1) * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, (i+1) * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n\n      long runLength;\n      if (RUN_LENGTH != -1) {\n        runLength = RUN_LENGTH;\n      } else {\n        int[] runTimes = new int[] {5000,6000,10000,15000,25000,30000,30000,45000,90000,120000};\n        runLength = runTimes[random().nextInt(runTimes.length - 1)];\n      }\n      \n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but cloud client should not easily fail\n       for (StopableThread indexThread : threads) {\n         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n           assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n         }\n       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7ecc0a3ce34111c2ebb18dc006e7f2f72d4162ca","date":1385777952,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            (i+1) * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, (i+1) * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n\n      long runLength;\n      if (RUN_LENGTH != -1) {\n        runLength = RUN_LENGTH;\n      } else {\n        int[] runTimes = new int[] {5000,6000,10000,15000,25000,30000,30000,45000,90000,120000};\n        runLength = runTimes[random().nextInt(runTimes.length - 1)];\n      }\n      \n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertEquals(\"There were expected update fails\", 0, ((StopableIndexingThread) indexThread).getFails());\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            (i+1) * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, (i+1) * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, true, 10000);\n\n      long runLength;\n      if (RUN_LENGTH != -1) {\n        runLength = RUN_LENGTH;\n      } else {\n        int[] runTimes = new int[] {5000,6000,10000,15000,25000,30000,30000,45000,90000,120000};\n        runLength = runTimes[random().nextInt(runTimes.length - 1)];\n      }\n      \n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertEquals(\"There were expected update fails\", 0, ((StopableIndexingThread) indexThread).getFails());\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b405eaada2e2ce4394ee3f8e36b9f0446a7be8ac","date":1385874571,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        // ensure the id start is high enough that threads will not overlap doc ids\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            (i+1) * 25000000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, (i+1) * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n\n      long runLength;\n      if (RUN_LENGTH != -1) {\n        runLength = RUN_LENGTH;\n      } else {\n        int[] runTimes = new int[] {5000,6000,10000,15000,25000,30000,30000,45000,90000,120000};\n        runLength = runTimes[random().nextInt(runTimes.length - 1)];\n      }\n      \n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertEquals(\"There were expected update fails\", 0, ((StopableIndexingThread) indexThread).getFails());\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            (i+1) * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, (i+1) * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n\n      long runLength;\n      if (RUN_LENGTH != -1) {\n        runLength = RUN_LENGTH;\n      } else {\n        int[] runTimes = new int[] {5000,6000,10000,15000,25000,30000,30000,45000,90000,120000};\n        runLength = runTimes[random().nextInt(runTimes.length - 1)];\n      }\n      \n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertEquals(\"There were expected update fails\", 0, ((StopableIndexingThread) indexThread).getFails());\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["67e5fb3e1a3e5763eb7aabb2ee71d10e8617e699"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"67e5fb3e1a3e5763eb7aabb2ee71d10e8617e699","date":1385913128,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(Integer.toString(i), true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n\n      long runLength;\n      if (RUN_LENGTH != -1) {\n        runLength = RUN_LENGTH;\n      } else {\n        int[] runTimes = new int[] {5000,6000,10000,15000,25000,30000,30000,45000,90000,120000};\n        runLength = runTimes[random().nextInt(runTimes.length - 1)];\n      }\n      \n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertEquals(\"There were expected update fails\", 0, ((StopableIndexingThread) indexThread).getFails());\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        // ensure the id start is high enough that threads will not overlap doc ids\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            (i+1) * 25000000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, (i+1) * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n\n      long runLength;\n      if (RUN_LENGTH != -1) {\n        runLength = RUN_LENGTH;\n      } else {\n        int[] runTimes = new int[] {5000,6000,10000,15000,25000,30000,30000,45000,90000,120000};\n        runLength = runTimes[random().nextInt(runTimes.length - 1)];\n      }\n      \n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertEquals(\"There were expected update fails\", 0, ((StopableIndexingThread) indexThread).getFails());\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":["b405eaada2e2ce4394ee3f8e36b9f0446a7be8ac","108e7dc629c849ba42c0ccc2049f20213a776652","f6d5f9478e1045c449ecb420dd322ad9c64837b6"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"74f45af4339b0daf7a95c820ab88c1aea74fbce0","date":1387475327,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(Integer.toString(i), true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n\n      long runLength;\n      if (RUN_LENGTH != -1) {\n        runLength = RUN_LENGTH;\n      } else {\n        int[] runTimes = new int[] {5000,6000,10000,15000,25000,30000,30000,45000,90000,120000};\n        runLength = runTimes[random().nextInt(runTimes.length - 1)];\n      }\n      \n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertEquals(\"There were expected update fails\", 0, ((StopableIndexingThread) indexThread).getFails());\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(\n            (i+1) * 50000, true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, (i+1) * 50000, true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n\n      long runLength;\n      if (RUN_LENGTH != -1) {\n        runLength = RUN_LENGTH;\n      } else {\n        int[] runTimes = new int[] {5000,6000,10000,15000,15000,30000,30000,45000,90000,120000};\n        runLength = runTimes[random().nextInt(runTimes.length - 1)];\n      }\n      \n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n       // we expect full throttle fails, but cloud client should not easily fail\n       // but it's allowed to fail and sometimes does, so commented out for now\n//       for (StopableThread indexThread : threads) {\n//         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n//           assertEquals(0, ((StopableIndexingThread) indexThread).getFails());\n//         }\n//       }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a1fdd9a08f8add7f8120c504fc73a542881f14db","date":1387820064,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(Integer.toString(i), true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n              30000, 45000, 90000, 120000};\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertEquals(\"There were expected update fails\", 0, ((StopableIndexingThread) indexThread).getFails());\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(Integer.toString(i), true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n\n      long runLength;\n      if (RUN_LENGTH != -1) {\n        runLength = RUN_LENGTH;\n      } else {\n        int[] runTimes = new int[] {5000,6000,10000,15000,25000,30000,30000,45000,90000,120000};\n        runLength = runTimes[random().nextInt(runTimes.length - 1)];\n      }\n      \n      try {\n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertEquals(\"There were expected update fails\", 0, ((StopableIndexingThread) indexThread).getFails());\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5320ee25aca451160d03b91e3af6b2f7530739e1","date":1390234467,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(Integer.toString(i), true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n              30000, 45000, 90000, 120000};\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFails() > 1);\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(Integer.toString(i), true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n              30000, 45000, 90000, 120000};\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertEquals(\"There were expected update fails\", 0, ((StopableIndexingThread) indexThread).getFails());\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["31d9b46cad4c17a1079c050a11068965b0221231"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6fa34c4c1d3393b382b251d33ef7990653e1cf5c","date":1390240458,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(Integer.toString(i), true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {145000, 240000, 300000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFails() > 1);\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(Integer.toString(i), true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n              30000, 45000, 90000, 120000};\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFails() > 1);\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"31d9b46cad4c17a1079c050a11068965b0221231","date":1392593363,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(Integer.toString(i), true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {145000, 240000, 300000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFails() > 10);\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(Integer.toString(i), true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {145000, 240000, 300000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFails() > 1);\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":["5320ee25aca451160d03b91e3af6b2f7530739e1"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"02c6a0e240c698414e7728a55f07361be84852d8","date":1392675457,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(Integer.toString(i), true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {145000, 240000, 300000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > 10);\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(Integer.toString(i), true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: only do this sometimes so that we can sometimes compare against control\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {145000, 240000, 300000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFails() > 10);\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"79603c1950d70bfd4e0780464629ad22bafc51e3","date":1393776231,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(Integer.toString(i), true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {145000, 240000, 300000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > FAIL_TOLERANCE);\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(Integer.toString(i), true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {145000, 240000, 300000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > 10);\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"19389fe47925b510b2811e2b385a75f7ad19dcca","date":1393903127,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {145000, 240000, 300000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > FAIL_TOLERANCE);\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(Integer.toString(i), true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {145000, 240000, 300000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > FAIL_TOLERANCE);\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"96ea64d994d340044e0d57aeb6a5871539d10ca5","date":1394225445,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {145000, 240000, 300000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > FAIL_TOLERANCE);\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(Integer.toString(i), true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {145000, 240000, 300000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > FAIL_TOLERANCE);\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {145000, 240000, 300000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > FAIL_TOLERANCE);\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<StopableThread>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {145000, 240000, 300000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > FAIL_TOLERANCE);\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9f4f50472ddb25195815e6b7c484fd656a228ac1","date":1395255069,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > FAIL_TOLERANCE);\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<>();\n      int threadCount = 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {145000, 240000, 300000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > FAIL_TOLERANCE);\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"35b47293d52729d80d78492b2aeda6e45d5a42f1","date":1397788632,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<>();\n      List<StopableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > FAIL_TOLERANCE);\n        }\n      }\n      \n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\",numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"072f211dfa8387028bb978d128c35bf9a450bbbf","date":1406041363,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<>();\n      List<StopableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"QTime\", SKIPVAL);\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<>();\n      List<StopableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":["f6d5f9478e1045c449ecb420dd322ad9c64837b6"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bafca15d8e408346a67f4282ad1143b88023893b","date":1420034748,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<>();\n      List<StopableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrClient client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<>();\n      List<StopableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrServer client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"abb23fcc2461782ab204e61213240feb77d355aa","date":1422029612,"type":5,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<>();\n      List<StopableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrClient client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<>();\n      List<StopableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrClient client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"9f4f50472ddb25195815e6b7c484fd656a228ac1":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":["79603c1950d70bfd4e0780464629ad22bafc51e3","19389fe47925b510b2811e2b385a75f7ad19dcca"],"b05c56a41b733e02a189c48895922b5bd8c7f3d1":["8fd5be977c105554c6a7b68afcdbc511439723ab","66c64e8cfded6a585100e6430238faaf416f3fea"],"02c6a0e240c698414e7728a55f07361be84852d8":["31d9b46cad4c17a1079c050a11068965b0221231"],"d9405f486872f1e416304dfe389741f4ee2f8a4d":["a6378064655e76cd7b908b1cab4ce425b384b508"],"d8469c0c0b45f7e3da74918efc8ede2ad2efe2b1":["7157fdda82ae1a1cb77b012ef0eb72249ee2f3d6"],"35b47293d52729d80d78492b2aeda6e45d5a42f1":["9f4f50472ddb25195815e6b7c484fd656a228ac1"],"933fa8f09adfcd1a858cd0fc7912e21ee993b7fc":["33bdd4aa796e0067ba181b149f1580a94d0ff5e0"],"7ecc0a3ce34111c2ebb18dc006e7f2f72d4162ca":["68fe0b7fa3b6a69495244529754954d2ae5cf0f7"],"acb07c6d6e233ed02bfb1837f0e4e78d3615e50c":["933fa8f09adfcd1a858cd0fc7912e21ee993b7fc"],"aba371508186796cc6151d8223a5b4e16d02e26e":["f6d5f9478e1045c449ecb420dd322ad9c64837b6","acb07c6d6e233ed02bfb1837f0e4e78d3615e50c"],"95303ff3749680c743b9425f9cf99e6e4065e8a8":["108e7dc629c849ba42c0ccc2049f20213a776652","2c4c76058cf64f5f3447673694ddec6e6c1eb675"],"767a42b1ba7578b44587b42170ad81003c6b479a":["2fc602b7d81247110d1ca76613bc560c1710e90c"],"f2126b84bd093fa3d921582a109a0ee578c28126":["a6378064655e76cd7b908b1cab4ce425b384b508","d9405f486872f1e416304dfe389741f4ee2f8a4d"],"66c64e8cfded6a585100e6430238faaf416f3fea":["3f767f8c99eaedb984df754fe61f21c5de260f94"],"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","2c007e7c4cf8c55bc2a5884e315123afaaeec87f"],"05a14b2611ead08655a2b2bdc61632eb31316e57":["c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","6013b4c7388f1627659c8f96c44abd10a294d3a6"],"80abb8f3c6cd9755d3203b2bc85d67c10aaf80ac":["56a558aa5aadd60ae850d1ab090098bc63bdfaf9"],"c5e0a45dbd62a2d2a07b516f66f0b750f70bba98":["80abb8f3c6cd9755d3203b2bc85d67c10aaf80ac"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["f6d5f9478e1045c449ecb420dd322ad9c64837b6","33bdd4aa796e0067ba181b149f1580a94d0ff5e0"],"2c4c76058cf64f5f3447673694ddec6e6c1eb675":["108e7dc629c849ba42c0ccc2049f20213a776652"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["aba371508186796cc6151d8223a5b4e16d02e26e","3f767f8c99eaedb984df754fe61f21c5de260f94"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":["270612d8e1a51cded91704d7af12f8979de0f584","67e5fb3e1a3e5763eb7aabb2ee71d10e8617e699"],"68fe0b7fa3b6a69495244529754954d2ae5cf0f7":["767a42b1ba7578b44587b42170ad81003c6b479a"],"31d9b46cad4c17a1079c050a11068965b0221231":["6fa34c4c1d3393b382b251d33ef7990653e1cf5c"],"270612d8e1a51cded91704d7af12f8979de0f584":["2c4c76058cf64f5f3447673694ddec6e6c1eb675"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["d9405f486872f1e416304dfe389741f4ee2f8a4d","c5e0a45dbd62a2d2a07b516f66f0b750f70bba98"],"072f211dfa8387028bb978d128c35bf9a450bbbf":["35b47293d52729d80d78492b2aeda6e45d5a42f1"],"abb23fcc2461782ab204e61213240feb77d355aa":["bafca15d8e408346a67f4282ad1143b88023893b"],"a1fdd9a08f8add7f8120c504fc73a542881f14db":["67e5fb3e1a3e5763eb7aabb2ee71d10e8617e699"],"fe9077a1ed75bdcdcc7dfff6525f73941c8cf30b":["933fa8f09adfcd1a858cd0fc7912e21ee993b7fc","acb07c6d6e233ed02bfb1837f0e4e78d3615e50c"],"fd92b8bcc88e969302510acf77bd6970da3994c4":["0d22ac6a4146774c1bc8400160fc0b6150294e92","f6d5f9478e1045c449ecb420dd322ad9c64837b6"],"bafca15d8e408346a67f4282ad1143b88023893b":["072f211dfa8387028bb978d128c35bf9a450bbbf"],"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198":["d6f074e73200c07d54f242d3880a8da5a35ff97b","66c64e8cfded6a585100e6430238faaf416f3fea"],"67e5fb3e1a3e5763eb7aabb2ee71d10e8617e699":["b405eaada2e2ce4394ee3f8e36b9f0446a7be8ac"],"108e7dc629c849ba42c0ccc2049f20213a776652":["c5e0a45dbd62a2d2a07b516f66f0b750f70bba98"],"6013b4c7388f1627659c8f96c44abd10a294d3a6":["66c64e8cfded6a585100e6430238faaf416f3fea"],"19389fe47925b510b2811e2b385a75f7ad19dcca":["79603c1950d70bfd4e0780464629ad22bafc51e3"],"0d22ac6a4146774c1bc8400160fc0b6150294e92":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","2c007e7c4cf8c55bc2a5884e315123afaaeec87f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":["fe33227f6805edab2036cbb80645cc4e2d1fa424","933fa8f09adfcd1a858cd0fc7912e21ee993b7fc"],"6fa34c4c1d3393b382b251d33ef7990653e1cf5c":["5320ee25aca451160d03b91e3af6b2f7530739e1"],"56a558aa5aadd60ae850d1ab090098bc63bdfaf9":["d9405f486872f1e416304dfe389741f4ee2f8a4d"],"f6d5f9478e1045c449ecb420dd322ad9c64837b6":["2c007e7c4cf8c55bc2a5884e315123afaaeec87f"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["19389fe47925b510b2811e2b385a75f7ad19dcca"],"78a55f24d9b493c2a1cecf79f1d78279062b545b":["a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","f6d5f9478e1045c449ecb420dd322ad9c64837b6"],"b405eaada2e2ce4394ee3f8e36b9f0446a7be8ac":["7ecc0a3ce34111c2ebb18dc006e7f2f72d4162ca"],"7157fdda82ae1a1cb77b012ef0eb72249ee2f3d6":["6013b4c7388f1627659c8f96c44abd10a294d3a6"],"2fc602b7d81247110d1ca76613bc560c1710e90c":["270612d8e1a51cded91704d7af12f8979de0f584"],"33bdd4aa796e0067ba181b149f1580a94d0ff5e0":["f6d5f9478e1045c449ecb420dd322ad9c64837b6"],"8fd5be977c105554c6a7b68afcdbc511439723ab":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","3f767f8c99eaedb984df754fe61f21c5de260f94"],"5320ee25aca451160d03b91e3af6b2f7530739e1":["a1fdd9a08f8add7f8120c504fc73a542881f14db"],"3f767f8c99eaedb984df754fe61f21c5de260f94":["acb07c6d6e233ed02bfb1837f0e4e78d3615e50c"],"a6378064655e76cd7b908b1cab4ce425b384b508":["d8469c0c0b45f7e3da74918efc8ede2ad2efe2b1"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","acb07c6d6e233ed02bfb1837f0e4e78d3615e50c"],"79603c1950d70bfd4e0780464629ad22bafc51e3":["02c6a0e240c698414e7728a55f07361be84852d8"],"2c007e7c4cf8c55bc2a5884e315123afaaeec87f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["abb23fcc2461782ab204e61213240feb77d355aa"]},"commit2Childs":{"9f4f50472ddb25195815e6b7c484fd656a228ac1":["35b47293d52729d80d78492b2aeda6e45d5a42f1"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":[],"b05c56a41b733e02a189c48895922b5bd8c7f3d1":[],"02c6a0e240c698414e7728a55f07361be84852d8":["79603c1950d70bfd4e0780464629ad22bafc51e3"],"d9405f486872f1e416304dfe389741f4ee2f8a4d":["f2126b84bd093fa3d921582a109a0ee578c28126","d3fcb70cf561547c7bb1506e0cf32ca7b1287064","56a558aa5aadd60ae850d1ab090098bc63bdfaf9"],"d8469c0c0b45f7e3da74918efc8ede2ad2efe2b1":["a6378064655e76cd7b908b1cab4ce425b384b508"],"35b47293d52729d80d78492b2aeda6e45d5a42f1":["072f211dfa8387028bb978d128c35bf9a450bbbf"],"933fa8f09adfcd1a858cd0fc7912e21ee993b7fc":["acb07c6d6e233ed02bfb1837f0e4e78d3615e50c","fe9077a1ed75bdcdcc7dfff6525f73941c8cf30b","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7"],"7ecc0a3ce34111c2ebb18dc006e7f2f72d4162ca":["b405eaada2e2ce4394ee3f8e36b9f0446a7be8ac"],"acb07c6d6e233ed02bfb1837f0e4e78d3615e50c":["aba371508186796cc6151d8223a5b4e16d02e26e","fe9077a1ed75bdcdcc7dfff6525f73941c8cf30b","3f767f8c99eaedb984df754fe61f21c5de260f94","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f"],"aba371508186796cc6151d8223a5b4e16d02e26e":["d6f074e73200c07d54f242d3880a8da5a35ff97b"],"95303ff3749680c743b9425f9cf99e6e4065e8a8":[],"767a42b1ba7578b44587b42170ad81003c6b479a":["68fe0b7fa3b6a69495244529754954d2ae5cf0f7"],"f2126b84bd093fa3d921582a109a0ee578c28126":[],"66c64e8cfded6a585100e6430238faaf416f3fea":["b05c56a41b733e02a189c48895922b5bd8c7f3d1","c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","6013b4c7388f1627659c8f96c44abd10a294d3a6"],"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d":["78a55f24d9b493c2a1cecf79f1d78279062b545b"],"05a14b2611ead08655a2b2bdc61632eb31316e57":[],"80abb8f3c6cd9755d3203b2bc85d67c10aaf80ac":["c5e0a45dbd62a2d2a07b516f66f0b750f70bba98"],"c5e0a45dbd62a2d2a07b516f66f0b750f70bba98":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","108e7dc629c849ba42c0ccc2049f20213a776652"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["4b51f65902cc2d20ddeb7a5b949aaddf990f31a7"],"2c4c76058cf64f5f3447673694ddec6e6c1eb675":["95303ff3749680c743b9425f9cf99e6e4065e8a8","270612d8e1a51cded91704d7af12f8979de0f584"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":[],"68fe0b7fa3b6a69495244529754954d2ae5cf0f7":["7ecc0a3ce34111c2ebb18dc006e7f2f72d4162ca"],"31d9b46cad4c17a1079c050a11068965b0221231":["02c6a0e240c698414e7728a55f07361be84852d8"],"270612d8e1a51cded91704d7af12f8979de0f584":["74f45af4339b0daf7a95c820ab88c1aea74fbce0","2fc602b7d81247110d1ca76613bc560c1710e90c"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":[],"072f211dfa8387028bb978d128c35bf9a450bbbf":["bafca15d8e408346a67f4282ad1143b88023893b"],"abb23fcc2461782ab204e61213240feb77d355aa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a1fdd9a08f8add7f8120c504fc73a542881f14db":["5320ee25aca451160d03b91e3af6b2f7530739e1"],"fe9077a1ed75bdcdcc7dfff6525f73941c8cf30b":[],"fd92b8bcc88e969302510acf77bd6970da3994c4":[],"bafca15d8e408346a67f4282ad1143b88023893b":["abb23fcc2461782ab204e61213240feb77d355aa"],"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198":["05a14b2611ead08655a2b2bdc61632eb31316e57"],"67e5fb3e1a3e5763eb7aabb2ee71d10e8617e699":["74f45af4339b0daf7a95c820ab88c1aea74fbce0","a1fdd9a08f8add7f8120c504fc73a542881f14db"],"108e7dc629c849ba42c0ccc2049f20213a776652":["95303ff3749680c743b9425f9cf99e6e4065e8a8","2c4c76058cf64f5f3447673694ddec6e6c1eb675"],"6013b4c7388f1627659c8f96c44abd10a294d3a6":["05a14b2611ead08655a2b2bdc61632eb31316e57","7157fdda82ae1a1cb77b012ef0eb72249ee2f3d6"],"19389fe47925b510b2811e2b385a75f7ad19dcca":["96ea64d994d340044e0d57aeb6a5871539d10ca5","634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"0d22ac6a4146774c1bc8400160fc0b6150294e92":["fd92b8bcc88e969302510acf77bd6970da3994c4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","0d22ac6a4146774c1bc8400160fc0b6150294e92","2c007e7c4cf8c55bc2a5884e315123afaaeec87f"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f"],"6fa34c4c1d3393b382b251d33ef7990653e1cf5c":["31d9b46cad4c17a1079c050a11068965b0221231"],"56a558aa5aadd60ae850d1ab090098bc63bdfaf9":["80abb8f3c6cd9755d3203b2bc85d67c10aaf80ac"],"f6d5f9478e1045c449ecb420dd322ad9c64837b6":["aba371508186796cc6151d8223a5b4e16d02e26e","fe33227f6805edab2036cbb80645cc4e2d1fa424","fd92b8bcc88e969302510acf77bd6970da3994c4","78a55f24d9b493c2a1cecf79f1d78279062b545b","33bdd4aa796e0067ba181b149f1580a94d0ff5e0"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["9f4f50472ddb25195815e6b7c484fd656a228ac1"],"78a55f24d9b493c2a1cecf79f1d78279062b545b":[],"7157fdda82ae1a1cb77b012ef0eb72249ee2f3d6":["d8469c0c0b45f7e3da74918efc8ede2ad2efe2b1"],"b405eaada2e2ce4394ee3f8e36b9f0446a7be8ac":["67e5fb3e1a3e5763eb7aabb2ee71d10e8617e699"],"2fc602b7d81247110d1ca76613bc560c1710e90c":["767a42b1ba7578b44587b42170ad81003c6b479a"],"8fd5be977c105554c6a7b68afcdbc511439723ab":["b05c56a41b733e02a189c48895922b5bd8c7f3d1"],"33bdd4aa796e0067ba181b149f1580a94d0ff5e0":["933fa8f09adfcd1a858cd0fc7912e21ee993b7fc","fe33227f6805edab2036cbb80645cc4e2d1fa424"],"5320ee25aca451160d03b91e3af6b2f7530739e1":["6fa34c4c1d3393b382b251d33ef7990653e1cf5c"],"3f767f8c99eaedb984df754fe61f21c5de260f94":["66c64e8cfded6a585100e6430238faaf416f3fea","d6f074e73200c07d54f242d3880a8da5a35ff97b","8fd5be977c105554c6a7b68afcdbc511439723ab"],"a6378064655e76cd7b908b1cab4ce425b384b508":["d9405f486872f1e416304dfe389741f4ee2f8a4d","f2126b84bd093fa3d921582a109a0ee578c28126"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["8fd5be977c105554c6a7b68afcdbc511439723ab"],"79603c1950d70bfd4e0780464629ad22bafc51e3":["96ea64d994d340044e0d57aeb6a5871539d10ca5","19389fe47925b510b2811e2b385a75f7ad19dcca"],"2c007e7c4cf8c55bc2a5884e315123afaaeec87f":["a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","0d22ac6a4146774c1bc8400160fc0b6150294e92","f6d5f9478e1045c449ecb420dd322ad9c64837b6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["96ea64d994d340044e0d57aeb6a5871539d10ca5","b05c56a41b733e02a189c48895922b5bd8c7f3d1","95303ff3749680c743b9425f9cf99e6e4065e8a8","f2126b84bd093fa3d921582a109a0ee578c28126","05a14b2611ead08655a2b2bdc61632eb31316e57","74f45af4339b0daf7a95c820ab88c1aea74fbce0","d3fcb70cf561547c7bb1506e0cf32ca7b1287064","fe9077a1ed75bdcdcc7dfff6525f73941c8cf30b","fd92b8bcc88e969302510acf77bd6970da3994c4","78a55f24d9b493c2a1cecf79f1d78279062b545b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}