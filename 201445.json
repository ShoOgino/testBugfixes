{"path":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/PhraseHelper#getTermToSpans(SpanQuery,LeafReaderContext,int,Map[BytesRef,Spans]).mjava","commits":[{"id":"1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2","date":1475611903,"type":0,"author":"David Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/PhraseHelper#getTermToSpans(SpanQuery,LeafReaderContext,int,Map[BytesRef,Spans]).mjava","pathOld":"/dev/null","sourceNew":"  // code extracted & refactored from WSTE.extractWeightedSpanTerms()\n  private void getTermToSpans(SpanQuery spanQuery, LeafReaderContext readerContext,\n                              int doc, Map<BytesRef, Spans> result)\n      throws IOException {\n    // note: in WSTE there was some field specific looping that seemed pointless so that isn't here.\n    final IndexSearcher searcher = new IndexSearcher(readerContext);\n    searcher.setQueryCache(null);\n    if (willRewrite) {\n      spanQuery = (SpanQuery) searcher.rewrite(spanQuery); // searcher.rewrite loops till done\n    }\n\n    // Get the underlying query terms\n\n    TreeSet<Term> termSet = new TreeSet<>(); // sorted so we can loop over results in order shortly...\n    searcher.createWeight(spanQuery, false, 1.0f).extractTerms(termSet);//needsScores==false\n\n    // Get Spans by running the query against the reader\n    // TODO it might make sense to re-use/cache the Spans instance, to advance forward between docs\n    SpanWeight spanWeight = (SpanWeight) searcher.createNormalizedWeight(spanQuery, false);\n    Spans spans = spanWeight.getSpans(readerContext, SpanWeight.Postings.POSITIONS);\n    if (spans == null) {\n      return;\n    }\n    TwoPhaseIterator twoPhaseIterator = spans.asTwoPhaseIterator();\n    if (twoPhaseIterator != null) {\n      if (twoPhaseIterator.approximation().advance(doc) != doc || !twoPhaseIterator.matches()) {\n        return;\n      }\n    } else if (spans.advance(doc) != doc) { // preposition, and return doing nothing if find none\n      return;\n    }\n\n    // Consume the Spans into a cache.  This instance is used as a source for multiple cloned copies.\n    // It's important we do this and not re-use the same original Spans instance since these will be iterated\n    // independently later on; sometimes in ways that prevents sharing the original Spans.\n    CachedSpans cachedSpansSource = new CachedSpans(spans); // consumes spans for this doc only and caches\n    spans = null;// we don't use it below\n\n    // Map terms to a Spans instance (aggregate if necessary)\n    for (final Term queryTerm : termSet) {\n      // note: we expect that at least one query term will pass these filters. This is because the collected\n      //   spanQuery list were already filtered by these conditions.\n      if (fieldName != null && fieldName.equals(queryTerm.field()) == false) {\n        continue;\n      }\n      if (positionInsensitiveTerms.contains(queryTerm)) {\n        continue;\n      }\n      // copy-constructor refers to same data (shallow) but has iteration state from the beginning\n      CachedSpans cachedSpans = new CachedSpans(cachedSpansSource);\n      // Add the span to whatever span may or may not exist\n      Spans existingSpans = result.get(queryTerm.bytes());\n      if (existingSpans != null) {\n        if (existingSpans instanceof MultiSpans) {\n          ((MultiSpans) existingSpans).addSpans(cachedSpans);\n        } else { // upgrade to MultiSpans\n          MultiSpans multiSpans = new MultiSpans();\n          multiSpans.addSpans(existingSpans);\n          multiSpans.addSpans(cachedSpans);\n          result.put(queryTerm.bytes(), multiSpans);\n        }\n      } else {\n        result.put(queryTerm.bytes(), cachedSpans);\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/PhraseHelper#getTermToSpans(SpanQuery,LeafReaderContext,int,Map[BytesRef,Spans]).mjava","pathOld":"/dev/null","sourceNew":"  // code extracted & refactored from WSTE.extractWeightedSpanTerms()\n  private void getTermToSpans(SpanQuery spanQuery, LeafReaderContext readerContext,\n                              int doc, Map<BytesRef, Spans> result)\n      throws IOException {\n    // note: in WSTE there was some field specific looping that seemed pointless so that isn't here.\n    final IndexSearcher searcher = new IndexSearcher(readerContext);\n    searcher.setQueryCache(null);\n    if (willRewrite) {\n      spanQuery = (SpanQuery) searcher.rewrite(spanQuery); // searcher.rewrite loops till done\n    }\n\n    // Get the underlying query terms\n\n    TreeSet<Term> termSet = new TreeSet<>(); // sorted so we can loop over results in order shortly...\n    searcher.createWeight(spanQuery, false, 1.0f).extractTerms(termSet);//needsScores==false\n\n    // Get Spans by running the query against the reader\n    // TODO it might make sense to re-use/cache the Spans instance, to advance forward between docs\n    SpanWeight spanWeight = (SpanWeight) searcher.createNormalizedWeight(spanQuery, false);\n    Spans spans = spanWeight.getSpans(readerContext, SpanWeight.Postings.POSITIONS);\n    if (spans == null) {\n      return;\n    }\n    TwoPhaseIterator twoPhaseIterator = spans.asTwoPhaseIterator();\n    if (twoPhaseIterator != null) {\n      if (twoPhaseIterator.approximation().advance(doc) != doc || !twoPhaseIterator.matches()) {\n        return;\n      }\n    } else if (spans.advance(doc) != doc) { // preposition, and return doing nothing if find none\n      return;\n    }\n\n    // Consume the Spans into a cache.  This instance is used as a source for multiple cloned copies.\n    // It's important we do this and not re-use the same original Spans instance since these will be iterated\n    // independently later on; sometimes in ways that prevents sharing the original Spans.\n    CachedSpans cachedSpansSource = new CachedSpans(spans); // consumes spans for this doc only and caches\n    spans = null;// we don't use it below\n\n    // Map terms to a Spans instance (aggregate if necessary)\n    for (final Term queryTerm : termSet) {\n      // note: we expect that at least one query term will pass these filters. This is because the collected\n      //   spanQuery list were already filtered by these conditions.\n      if (fieldName != null && fieldName.equals(queryTerm.field()) == false) {\n        continue;\n      }\n      if (positionInsensitiveTerms.contains(queryTerm)) {\n        continue;\n      }\n      // copy-constructor refers to same data (shallow) but has iteration state from the beginning\n      CachedSpans cachedSpans = new CachedSpans(cachedSpansSource);\n      // Add the span to whatever span may or may not exist\n      Spans existingSpans = result.get(queryTerm.bytes());\n      if (existingSpans != null) {\n        if (existingSpans instanceof MultiSpans) {\n          ((MultiSpans) existingSpans).addSpans(cachedSpans);\n        } else { // upgrade to MultiSpans\n          MultiSpans multiSpans = new MultiSpans();\n          multiSpans.addSpans(existingSpans);\n          multiSpans.addSpans(cachedSpans);\n          result.put(queryTerm.bytes(), multiSpans);\n        }\n      } else {\n        result.put(queryTerm.bytes(), cachedSpans);\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5675b3bced0b155f0ff8001ce2e1e502be7c92f6","date":1480972317,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/PhraseHelper#getTermToSpans(SpanQuery,LeafReaderContext,int,Map[BytesRef,Spans]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/PhraseHelper#getTermToSpans(SpanQuery,LeafReaderContext,int,Map[BytesRef,Spans]).mjava","sourceNew":"  // code extracted & refactored from WSTE.extractWeightedSpanTerms()\n  private void getTermToSpans(SpanQuery spanQuery, LeafReaderContext readerContext,\n                              int doc, Map<BytesRef, Spans> result)\n      throws IOException {\n    // note: in WSTE there was some field specific looping that seemed pointless so that isn't here.\n    final IndexSearcher searcher = new IndexSearcher(readerContext.reader());\n    searcher.setQueryCache(null);\n    if (willRewrite) {\n      spanQuery = (SpanQuery) searcher.rewrite(spanQuery); // searcher.rewrite loops till done\n    }\n\n    // Get the underlying query terms\n    TreeSet<Term> termSet = new FieldFilteringTermSet(); // sorted so we can loop over results in order shortly...\n    searcher.createWeight(spanQuery, false, 1.0f).extractTerms(termSet);//needsScores==false\n\n    // Get Spans by running the query against the reader\n    // TODO it might make sense to re-use/cache the Spans instance, to advance forward between docs\n    SpanWeight spanWeight = (SpanWeight) searcher.createNormalizedWeight(spanQuery, false);\n    Spans spans = spanWeight.getSpans(readerContext, SpanWeight.Postings.POSITIONS);\n    if (spans == null) {\n      return;\n    }\n    TwoPhaseIterator twoPhaseIterator = spans.asTwoPhaseIterator();\n    if (twoPhaseIterator != null) {\n      if (twoPhaseIterator.approximation().advance(doc) != doc || !twoPhaseIterator.matches()) {\n        return;\n      }\n    } else if (spans.advance(doc) != doc) { // preposition, and return doing nothing if find none\n      return;\n    }\n\n    // Consume the Spans into a cache.  This instance is used as a source for multiple cloned copies.\n    // It's important we do this and not re-use the same original Spans instance since these will be iterated\n    // independently later on; sometimes in ways that prevents sharing the original Spans.\n    CachedSpans cachedSpansSource = new CachedSpans(spans); // consumes spans for this doc only and caches\n    spans = null;// we don't use it below\n\n    // Map terms to a Spans instance (aggregate if necessary)\n    for (final Term queryTerm : termSet) {\n      // note: we expect that at least one query term will pass these filters. This is because the collected\n      //   spanQuery list were already filtered by these conditions.\n      if (positionInsensitiveTerms.contains(queryTerm)) {\n        continue;\n      }\n      // copy-constructor refers to same data (shallow) but has iteration state from the beginning\n      CachedSpans cachedSpans = new CachedSpans(cachedSpansSource);\n      // Add the span to whatever span may or may not exist\n      Spans existingSpans = result.get(queryTerm.bytes());\n      if (existingSpans != null) {\n        if (existingSpans instanceof MultiSpans) {\n          ((MultiSpans) existingSpans).addSpans(cachedSpans);\n        } else { // upgrade to MultiSpans\n          MultiSpans multiSpans = new MultiSpans();\n          multiSpans.addSpans(existingSpans);\n          multiSpans.addSpans(cachedSpans);\n          result.put(queryTerm.bytes(), multiSpans);\n        }\n      } else {\n        result.put(queryTerm.bytes(), cachedSpans);\n      }\n    }\n  }\n\n","sourceOld":"  // code extracted & refactored from WSTE.extractWeightedSpanTerms()\n  private void getTermToSpans(SpanQuery spanQuery, LeafReaderContext readerContext,\n                              int doc, Map<BytesRef, Spans> result)\n      throws IOException {\n    // note: in WSTE there was some field specific looping that seemed pointless so that isn't here.\n    final IndexSearcher searcher = new IndexSearcher(readerContext);\n    searcher.setQueryCache(null);\n    if (willRewrite) {\n      spanQuery = (SpanQuery) searcher.rewrite(spanQuery); // searcher.rewrite loops till done\n    }\n\n    // Get the underlying query terms\n\n    TreeSet<Term> termSet = new TreeSet<>(); // sorted so we can loop over results in order shortly...\n    searcher.createWeight(spanQuery, false, 1.0f).extractTerms(termSet);//needsScores==false\n\n    // Get Spans by running the query against the reader\n    // TODO it might make sense to re-use/cache the Spans instance, to advance forward between docs\n    SpanWeight spanWeight = (SpanWeight) searcher.createNormalizedWeight(spanQuery, false);\n    Spans spans = spanWeight.getSpans(readerContext, SpanWeight.Postings.POSITIONS);\n    if (spans == null) {\n      return;\n    }\n    TwoPhaseIterator twoPhaseIterator = spans.asTwoPhaseIterator();\n    if (twoPhaseIterator != null) {\n      if (twoPhaseIterator.approximation().advance(doc) != doc || !twoPhaseIterator.matches()) {\n        return;\n      }\n    } else if (spans.advance(doc) != doc) { // preposition, and return doing nothing if find none\n      return;\n    }\n\n    // Consume the Spans into a cache.  This instance is used as a source for multiple cloned copies.\n    // It's important we do this and not re-use the same original Spans instance since these will be iterated\n    // independently later on; sometimes in ways that prevents sharing the original Spans.\n    CachedSpans cachedSpansSource = new CachedSpans(spans); // consumes spans for this doc only and caches\n    spans = null;// we don't use it below\n\n    // Map terms to a Spans instance (aggregate if necessary)\n    for (final Term queryTerm : termSet) {\n      // note: we expect that at least one query term will pass these filters. This is because the collected\n      //   spanQuery list were already filtered by these conditions.\n      if (fieldName != null && fieldName.equals(queryTerm.field()) == false) {\n        continue;\n      }\n      if (positionInsensitiveTerms.contains(queryTerm)) {\n        continue;\n      }\n      // copy-constructor refers to same data (shallow) but has iteration state from the beginning\n      CachedSpans cachedSpans = new CachedSpans(cachedSpansSource);\n      // Add the span to whatever span may or may not exist\n      Spans existingSpans = result.get(queryTerm.bytes());\n      if (existingSpans != null) {\n        if (existingSpans instanceof MultiSpans) {\n          ((MultiSpans) existingSpans).addSpans(cachedSpans);\n        } else { // upgrade to MultiSpans\n          MultiSpans multiSpans = new MultiSpans();\n          multiSpans.addSpans(existingSpans);\n          multiSpans.addSpans(cachedSpans);\n          result.put(queryTerm.bytes(), multiSpans);\n        }\n      } else {\n        result.put(queryTerm.bytes(), cachedSpans);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ee8992dea79b51ba77a3fd1a5c0f94a5f0b8808d","date":1481116359,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/PhraseHelper#getTermToSpans(SpanQuery,LeafReaderContext,int,Map[BytesRef,Spans]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/PhraseHelper#getTermToSpans(SpanQuery,LeafReaderContext,int,Map[BytesRef,Spans]).mjava","sourceNew":"  // code extracted & refactored from WSTE.extractWeightedSpanTerms()\n  private void getTermToSpans(SpanQuery spanQuery, LeafReaderContext readerContext,\n                              int doc, Map<BytesRef, Spans> result)\n      throws IOException {\n    // note: in WSTE there was some field specific looping that seemed pointless so that isn't here.\n    final IndexSearcher searcher = new IndexSearcher(readerContext.reader());\n    searcher.setQueryCache(null);\n    if (willRewrite) {\n      spanQuery = (SpanQuery) searcher.rewrite(spanQuery); // searcher.rewrite loops till done\n    }\n\n    // Get the underlying query terms\n    TreeSet<Term> termSet = new FieldFilteringTermSet(); // sorted so we can loop over results in order shortly...\n    searcher.createWeight(spanQuery, false, 1.0f).extractTerms(termSet);//needsScores==false\n\n    // Get Spans by running the query against the reader\n    // TODO it might make sense to re-use/cache the Spans instance, to advance forward between docs\n    SpanWeight spanWeight = (SpanWeight) searcher.createNormalizedWeight(spanQuery, false);\n    Spans spans = spanWeight.getSpans(readerContext, SpanWeight.Postings.POSITIONS);\n    if (spans == null) {\n      return;\n    }\n    TwoPhaseIterator twoPhaseIterator = spans.asTwoPhaseIterator();\n    if (twoPhaseIterator != null) {\n      if (twoPhaseIterator.approximation().advance(doc) != doc || !twoPhaseIterator.matches()) {\n        return;\n      }\n    } else if (spans.advance(doc) != doc) { // preposition, and return doing nothing if find none\n      return;\n    }\n\n    // Consume the Spans into a cache.  This instance is used as a source for multiple cloned copies.\n    // It's important we do this and not re-use the same original Spans instance since these will be iterated\n    // independently later on; sometimes in ways that prevents sharing the original Spans.\n    CachedSpans cachedSpansSource = new CachedSpans(spans); // consumes spans for this doc only and caches\n    spans = null;// we don't use it below\n\n    // Map terms to a Spans instance (aggregate if necessary)\n    for (final Term queryTerm : termSet) {\n      // note: we expect that at least one query term will pass these filters. This is because the collected\n      //   spanQuery list were already filtered by these conditions.\n      if (positionInsensitiveTerms.contains(queryTerm)) {\n        continue;\n      }\n      // copy-constructor refers to same data (shallow) but has iteration state from the beginning\n      CachedSpans cachedSpans = new CachedSpans(cachedSpansSource);\n      // Add the span to whatever span may or may not exist\n      Spans existingSpans = result.get(queryTerm.bytes());\n      if (existingSpans != null) {\n        if (existingSpans instanceof MultiSpans) {\n          ((MultiSpans) existingSpans).addSpans(cachedSpans);\n        } else { // upgrade to MultiSpans\n          MultiSpans multiSpans = new MultiSpans();\n          multiSpans.addSpans(existingSpans);\n          multiSpans.addSpans(cachedSpans);\n          result.put(queryTerm.bytes(), multiSpans);\n        }\n      } else {\n        result.put(queryTerm.bytes(), cachedSpans);\n      }\n    }\n  }\n\n","sourceOld":"  // code extracted & refactored from WSTE.extractWeightedSpanTerms()\n  private void getTermToSpans(SpanQuery spanQuery, LeafReaderContext readerContext,\n                              int doc, Map<BytesRef, Spans> result)\n      throws IOException {\n    // note: in WSTE there was some field specific looping that seemed pointless so that isn't here.\n    final IndexSearcher searcher = new IndexSearcher(readerContext);\n    searcher.setQueryCache(null);\n    if (willRewrite) {\n      spanQuery = (SpanQuery) searcher.rewrite(spanQuery); // searcher.rewrite loops till done\n    }\n\n    // Get the underlying query terms\n\n    TreeSet<Term> termSet = new TreeSet<>(); // sorted so we can loop over results in order shortly...\n    searcher.createWeight(spanQuery, false, 1.0f).extractTerms(termSet);//needsScores==false\n\n    // Get Spans by running the query against the reader\n    // TODO it might make sense to re-use/cache the Spans instance, to advance forward between docs\n    SpanWeight spanWeight = (SpanWeight) searcher.createNormalizedWeight(spanQuery, false);\n    Spans spans = spanWeight.getSpans(readerContext, SpanWeight.Postings.POSITIONS);\n    if (spans == null) {\n      return;\n    }\n    TwoPhaseIterator twoPhaseIterator = spans.asTwoPhaseIterator();\n    if (twoPhaseIterator != null) {\n      if (twoPhaseIterator.approximation().advance(doc) != doc || !twoPhaseIterator.matches()) {\n        return;\n      }\n    } else if (spans.advance(doc) != doc) { // preposition, and return doing nothing if find none\n      return;\n    }\n\n    // Consume the Spans into a cache.  This instance is used as a source for multiple cloned copies.\n    // It's important we do this and not re-use the same original Spans instance since these will be iterated\n    // independently later on; sometimes in ways that prevents sharing the original Spans.\n    CachedSpans cachedSpansSource = new CachedSpans(spans); // consumes spans for this doc only and caches\n    spans = null;// we don't use it below\n\n    // Map terms to a Spans instance (aggregate if necessary)\n    for (final Term queryTerm : termSet) {\n      // note: we expect that at least one query term will pass these filters. This is because the collected\n      //   spanQuery list were already filtered by these conditions.\n      if (fieldName != null && fieldName.equals(queryTerm.field()) == false) {\n        continue;\n      }\n      if (positionInsensitiveTerms.contains(queryTerm)) {\n        continue;\n      }\n      // copy-constructor refers to same data (shallow) but has iteration state from the beginning\n      CachedSpans cachedSpans = new CachedSpans(cachedSpansSource);\n      // Add the span to whatever span may or may not exist\n      Spans existingSpans = result.get(queryTerm.bytes());\n      if (existingSpans != null) {\n        if (existingSpans instanceof MultiSpans) {\n          ((MultiSpans) existingSpans).addSpans(cachedSpans);\n        } else { // upgrade to MultiSpans\n          MultiSpans multiSpans = new MultiSpans();\n          multiSpans.addSpans(existingSpans);\n          multiSpans.addSpans(cachedSpans);\n          result.put(queryTerm.bytes(), multiSpans);\n        }\n      } else {\n        result.put(queryTerm.bytes(), cachedSpans);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9856095f7afb5a607bf5e65077615ed91273508c","date":1481837697,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/PhraseHelper#getTermToSpans(SpanQuery,LeafReaderContext,int,Map[BytesRef,Spans]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/PhraseHelper#getTermToSpans(SpanQuery,LeafReaderContext,int,Map[BytesRef,Spans]).mjava","sourceNew":"  // code extracted & refactored from WSTE.extractWeightedSpanTerms()\n  private void getTermToSpans(SpanQuery spanQuery, LeafReaderContext readerContext,\n                              int doc, Map<BytesRef, Spans> result)\n      throws IOException {\n    // note: in WSTE there was some field specific looping that seemed pointless so that isn't here.\n    final IndexSearcher searcher = new IndexSearcher(readerContext.reader());\n    searcher.setQueryCache(null);\n    if (willRewrite) {\n      spanQuery = (SpanQuery) searcher.rewrite(spanQuery); // searcher.rewrite loops till done\n    }\n\n    // Get the underlying query terms\n    TreeSet<Term> termSet = new FieldFilteringTermSet(); // sorted so we can loop over results in order shortly...\n    searcher.createWeight(spanQuery, false, 1.0f).extractTerms(termSet);//needsScores==false\n\n    // Get Spans by running the query against the reader\n    // TODO it might make sense to re-use/cache the Spans instance, to advance forward between docs\n    SpanWeight spanWeight = (SpanWeight) searcher.createNormalizedWeight(spanQuery, false);\n    Spans spans = spanWeight.getSpans(readerContext, SpanWeight.Postings.POSITIONS);\n    if (spans == null) {\n      return;\n    }\n    TwoPhaseIterator twoPhaseIterator = spans.asTwoPhaseIterator();\n    if (twoPhaseIterator != null) {\n      if (twoPhaseIterator.approximation().advance(doc) != doc || !twoPhaseIterator.matches()) {\n        return;\n      }\n    } else if (spans.advance(doc) != doc) { // preposition, and return doing nothing if find none\n      return;\n    }\n\n    // Consume the Spans into a cache.  This instance is used as a source for multiple cloned copies.\n    // It's important we do this and not re-use the same original Spans instance since these will be iterated\n    // independently later on; sometimes in ways that prevents sharing the original Spans.\n    CachedSpans cachedSpansSource = new CachedSpans(spans); // consumes spans for this doc only and caches\n    spans = null;// we don't use it below\n\n    // Map terms to a Spans instance (aggregate if necessary)\n    for (final Term queryTerm : termSet) {\n      // note: we expect that at least one query term will pass these filters. This is because the collected\n      //   spanQuery list were already filtered by these conditions.\n      if (positionInsensitiveTerms.contains(queryTerm)) {\n        continue;\n      }\n      // copy-constructor refers to same data (shallow) but has iteration state from the beginning\n      CachedSpans cachedSpans = new CachedSpans(cachedSpansSource);\n      // Add the span to whatever span may or may not exist\n      Spans existingSpans = result.get(queryTerm.bytes());\n      if (existingSpans != null) {\n        if (existingSpans instanceof MultiSpans) {\n          ((MultiSpans) existingSpans).addSpans(cachedSpans);\n        } else { // upgrade to MultiSpans\n          MultiSpans multiSpans = new MultiSpans();\n          multiSpans.addSpans(existingSpans);\n          multiSpans.addSpans(cachedSpans);\n          result.put(queryTerm.bytes(), multiSpans);\n        }\n      } else {\n        result.put(queryTerm.bytes(), cachedSpans);\n      }\n    }\n  }\n\n","sourceOld":"  // code extracted & refactored from WSTE.extractWeightedSpanTerms()\n  private void getTermToSpans(SpanQuery spanQuery, LeafReaderContext readerContext,\n                              int doc, Map<BytesRef, Spans> result)\n      throws IOException {\n    // note: in WSTE there was some field specific looping that seemed pointless so that isn't here.\n    final IndexSearcher searcher = new IndexSearcher(readerContext);\n    searcher.setQueryCache(null);\n    if (willRewrite) {\n      spanQuery = (SpanQuery) searcher.rewrite(spanQuery); // searcher.rewrite loops till done\n    }\n\n    // Get the underlying query terms\n\n    TreeSet<Term> termSet = new TreeSet<>(); // sorted so we can loop over results in order shortly...\n    searcher.createWeight(spanQuery, false, 1.0f).extractTerms(termSet);//needsScores==false\n\n    // Get Spans by running the query against the reader\n    // TODO it might make sense to re-use/cache the Spans instance, to advance forward between docs\n    SpanWeight spanWeight = (SpanWeight) searcher.createNormalizedWeight(spanQuery, false);\n    Spans spans = spanWeight.getSpans(readerContext, SpanWeight.Postings.POSITIONS);\n    if (spans == null) {\n      return;\n    }\n    TwoPhaseIterator twoPhaseIterator = spans.asTwoPhaseIterator();\n    if (twoPhaseIterator != null) {\n      if (twoPhaseIterator.approximation().advance(doc) != doc || !twoPhaseIterator.matches()) {\n        return;\n      }\n    } else if (spans.advance(doc) != doc) { // preposition, and return doing nothing if find none\n      return;\n    }\n\n    // Consume the Spans into a cache.  This instance is used as a source for multiple cloned copies.\n    // It's important we do this and not re-use the same original Spans instance since these will be iterated\n    // independently later on; sometimes in ways that prevents sharing the original Spans.\n    CachedSpans cachedSpansSource = new CachedSpans(spans); // consumes spans for this doc only and caches\n    spans = null;// we don't use it below\n\n    // Map terms to a Spans instance (aggregate if necessary)\n    for (final Term queryTerm : termSet) {\n      // note: we expect that at least one query term will pass these filters. This is because the collected\n      //   spanQuery list were already filtered by these conditions.\n      if (fieldName != null && fieldName.equals(queryTerm.field()) == false) {\n        continue;\n      }\n      if (positionInsensitiveTerms.contains(queryTerm)) {\n        continue;\n      }\n      // copy-constructor refers to same data (shallow) but has iteration state from the beginning\n      CachedSpans cachedSpans = new CachedSpans(cachedSpansSource);\n      // Add the span to whatever span may or may not exist\n      Spans existingSpans = result.get(queryTerm.bytes());\n      if (existingSpans != null) {\n        if (existingSpans instanceof MultiSpans) {\n          ((MultiSpans) existingSpans).addSpans(cachedSpans);\n        } else { // upgrade to MultiSpans\n          MultiSpans multiSpans = new MultiSpans();\n          multiSpans.addSpans(existingSpans);\n          multiSpans.addSpans(cachedSpans);\n          result.put(queryTerm.bytes(), multiSpans);\n        }\n      } else {\n        result.put(queryTerm.bytes(), cachedSpans);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9fc47cb7b4346802411bb432f501ed0673d7119e","date":1512640179,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/PhraseHelper#getTermToSpans(SpanQuery,LeafReaderContext,int,Map[BytesRef,Spans]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/PhraseHelper#getTermToSpans(SpanQuery,LeafReaderContext,int,Map[BytesRef,Spans]).mjava","sourceNew":"  // code extracted & refactored from WSTE.extractWeightedSpanTerms()\n  private void getTermToSpans(SpanQuery spanQuery, LeafReaderContext readerContext,\n                              int doc, Map<BytesRef, Spans> result)\n      throws IOException {\n    // note: in WSTE there was some field specific looping that seemed pointless so that isn't here.\n    final IndexSearcher searcher = new IndexSearcher(readerContext.reader());\n    searcher.setQueryCache(null);\n    if (willRewrite) {\n      spanQuery = (SpanQuery) searcher.rewrite(spanQuery); // searcher.rewrite loops till done\n    }\n\n    // Get the underlying query terms\n    TreeSet<Term> termSet = new FieldFilteringTermSet(); // sorted so we can loop over results in order shortly...\n    searcher.createWeight(spanQuery, ScoreMode.COMPLETE_NO_SCORES, 1.0f).extractTerms(termSet);//needsScores==false\n\n    // Get Spans by running the query against the reader\n    // TODO it might make sense to re-use/cache the Spans instance, to advance forward between docs\n    SpanWeight spanWeight = (SpanWeight) searcher.createNormalizedWeight(spanQuery, ScoreMode.COMPLETE_NO_SCORES);\n    Spans spans = spanWeight.getSpans(readerContext, SpanWeight.Postings.POSITIONS);\n    if (spans == null) {\n      return;\n    }\n    TwoPhaseIterator twoPhaseIterator = spans.asTwoPhaseIterator();\n    if (twoPhaseIterator != null) {\n      if (twoPhaseIterator.approximation().advance(doc) != doc || !twoPhaseIterator.matches()) {\n        return;\n      }\n    } else if (spans.advance(doc) != doc) { // preposition, and return doing nothing if find none\n      return;\n    }\n\n    // Consume the Spans into a cache.  This instance is used as a source for multiple cloned copies.\n    // It's important we do this and not re-use the same original Spans instance since these will be iterated\n    // independently later on; sometimes in ways that prevents sharing the original Spans.\n    CachedSpans cachedSpansSource = new CachedSpans(spans); // consumes spans for this doc only and caches\n    spans = null;// we don't use it below\n\n    // Map terms to a Spans instance (aggregate if necessary)\n    for (final Term queryTerm : termSet) {\n      // note: we expect that at least one query term will pass these filters. This is because the collected\n      //   spanQuery list were already filtered by these conditions.\n      if (positionInsensitiveTerms.contains(queryTerm)) {\n        continue;\n      }\n      // copy-constructor refers to same data (shallow) but has iteration state from the beginning\n      CachedSpans cachedSpans = new CachedSpans(cachedSpansSource);\n      // Add the span to whatever span may or may not exist\n      Spans existingSpans = result.get(queryTerm.bytes());\n      if (existingSpans != null) {\n        if (existingSpans instanceof MultiSpans) {\n          ((MultiSpans) existingSpans).addSpans(cachedSpans);\n        } else { // upgrade to MultiSpans\n          MultiSpans multiSpans = new MultiSpans();\n          multiSpans.addSpans(existingSpans);\n          multiSpans.addSpans(cachedSpans);\n          result.put(queryTerm.bytes(), multiSpans);\n        }\n      } else {\n        result.put(queryTerm.bytes(), cachedSpans);\n      }\n    }\n  }\n\n","sourceOld":"  // code extracted & refactored from WSTE.extractWeightedSpanTerms()\n  private void getTermToSpans(SpanQuery spanQuery, LeafReaderContext readerContext,\n                              int doc, Map<BytesRef, Spans> result)\n      throws IOException {\n    // note: in WSTE there was some field specific looping that seemed pointless so that isn't here.\n    final IndexSearcher searcher = new IndexSearcher(readerContext.reader());\n    searcher.setQueryCache(null);\n    if (willRewrite) {\n      spanQuery = (SpanQuery) searcher.rewrite(spanQuery); // searcher.rewrite loops till done\n    }\n\n    // Get the underlying query terms\n    TreeSet<Term> termSet = new FieldFilteringTermSet(); // sorted so we can loop over results in order shortly...\n    searcher.createWeight(spanQuery, false, 1.0f).extractTerms(termSet);//needsScores==false\n\n    // Get Spans by running the query against the reader\n    // TODO it might make sense to re-use/cache the Spans instance, to advance forward between docs\n    SpanWeight spanWeight = (SpanWeight) searcher.createNormalizedWeight(spanQuery, false);\n    Spans spans = spanWeight.getSpans(readerContext, SpanWeight.Postings.POSITIONS);\n    if (spans == null) {\n      return;\n    }\n    TwoPhaseIterator twoPhaseIterator = spans.asTwoPhaseIterator();\n    if (twoPhaseIterator != null) {\n      if (twoPhaseIterator.approximation().advance(doc) != doc || !twoPhaseIterator.matches()) {\n        return;\n      }\n    } else if (spans.advance(doc) != doc) { // preposition, and return doing nothing if find none\n      return;\n    }\n\n    // Consume the Spans into a cache.  This instance is used as a source for multiple cloned copies.\n    // It's important we do this and not re-use the same original Spans instance since these will be iterated\n    // independently later on; sometimes in ways that prevents sharing the original Spans.\n    CachedSpans cachedSpansSource = new CachedSpans(spans); // consumes spans for this doc only and caches\n    spans = null;// we don't use it below\n\n    // Map terms to a Spans instance (aggregate if necessary)\n    for (final Term queryTerm : termSet) {\n      // note: we expect that at least one query term will pass these filters. This is because the collected\n      //   spanQuery list were already filtered by these conditions.\n      if (positionInsensitiveTerms.contains(queryTerm)) {\n        continue;\n      }\n      // copy-constructor refers to same data (shallow) but has iteration state from the beginning\n      CachedSpans cachedSpans = new CachedSpans(cachedSpansSource);\n      // Add the span to whatever span may or may not exist\n      Spans existingSpans = result.get(queryTerm.bytes());\n      if (existingSpans != null) {\n        if (existingSpans instanceof MultiSpans) {\n          ((MultiSpans) existingSpans).addSpans(cachedSpans);\n        } else { // upgrade to MultiSpans\n          MultiSpans multiSpans = new MultiSpans();\n          multiSpans.addSpans(existingSpans);\n          multiSpans.addSpans(cachedSpans);\n          result.put(queryTerm.bytes(), multiSpans);\n        }\n      } else {\n        result.put(queryTerm.bytes(), cachedSpans);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"417142ff08fda9cf0b72d5133e63097a166c6458","date":1512729693,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/PhraseHelper#getTermToSpans(SpanQuery,LeafReaderContext,int,Map[BytesRef,Spans]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/PhraseHelper#getTermToSpans(SpanQuery,LeafReaderContext,int,Map[BytesRef,Spans]).mjava","sourceNew":"  // code extracted & refactored from WSTE.extractWeightedSpanTerms()\n  private void getTermToSpans(SpanQuery spanQuery, LeafReaderContext readerContext,\n                              int doc, Map<BytesRef, Spans> result)\n      throws IOException {\n    // note: in WSTE there was some field specific looping that seemed pointless so that isn't here.\n    final IndexSearcher searcher = new IndexSearcher(readerContext.reader());\n    searcher.setQueryCache(null);\n    if (willRewrite) {\n      spanQuery = (SpanQuery) searcher.rewrite(spanQuery); // searcher.rewrite loops till done\n    }\n\n    // Get the underlying query terms\n    TreeSet<Term> termSet = new FieldFilteringTermSet(); // sorted so we can loop over results in order shortly...\n    searcher.createWeight(spanQuery, ScoreMode.COMPLETE_NO_SCORES, 1.0f).extractTerms(termSet);//needsScores==false\n\n    // Get Spans by running the query against the reader\n    // TODO it might make sense to re-use/cache the Spans instance, to advance forward between docs\n    SpanWeight spanWeight = (SpanWeight) searcher.createNormalizedWeight(spanQuery, ScoreMode.COMPLETE_NO_SCORES);\n    Spans spans = spanWeight.getSpans(readerContext, SpanWeight.Postings.POSITIONS);\n    if (spans == null) {\n      return;\n    }\n    TwoPhaseIterator twoPhaseIterator = spans.asTwoPhaseIterator();\n    if (twoPhaseIterator != null) {\n      if (twoPhaseIterator.approximation().advance(doc) != doc || !twoPhaseIterator.matches()) {\n        return;\n      }\n    } else if (spans.advance(doc) != doc) { // preposition, and return doing nothing if find none\n      return;\n    }\n\n    // Consume the Spans into a cache.  This instance is used as a source for multiple cloned copies.\n    // It's important we do this and not re-use the same original Spans instance since these will be iterated\n    // independently later on; sometimes in ways that prevents sharing the original Spans.\n    CachedSpans cachedSpansSource = new CachedSpans(spans); // consumes spans for this doc only and caches\n    spans = null;// we don't use it below\n\n    // Map terms to a Spans instance (aggregate if necessary)\n    for (final Term queryTerm : termSet) {\n      // note: we expect that at least one query term will pass these filters. This is because the collected\n      //   spanQuery list were already filtered by these conditions.\n      if (positionInsensitiveTerms.contains(queryTerm)) {\n        continue;\n      }\n      // copy-constructor refers to same data (shallow) but has iteration state from the beginning\n      CachedSpans cachedSpans = new CachedSpans(cachedSpansSource);\n      // Add the span to whatever span may or may not exist\n      Spans existingSpans = result.get(queryTerm.bytes());\n      if (existingSpans != null) {\n        if (existingSpans instanceof MultiSpans) {\n          ((MultiSpans) existingSpans).addSpans(cachedSpans);\n        } else { // upgrade to MultiSpans\n          MultiSpans multiSpans = new MultiSpans();\n          multiSpans.addSpans(existingSpans);\n          multiSpans.addSpans(cachedSpans);\n          result.put(queryTerm.bytes(), multiSpans);\n        }\n      } else {\n        result.put(queryTerm.bytes(), cachedSpans);\n      }\n    }\n  }\n\n","sourceOld":"  // code extracted & refactored from WSTE.extractWeightedSpanTerms()\n  private void getTermToSpans(SpanQuery spanQuery, LeafReaderContext readerContext,\n                              int doc, Map<BytesRef, Spans> result)\n      throws IOException {\n    // note: in WSTE there was some field specific looping that seemed pointless so that isn't here.\n    final IndexSearcher searcher = new IndexSearcher(readerContext.reader());\n    searcher.setQueryCache(null);\n    if (willRewrite) {\n      spanQuery = (SpanQuery) searcher.rewrite(spanQuery); // searcher.rewrite loops till done\n    }\n\n    // Get the underlying query terms\n    TreeSet<Term> termSet = new FieldFilteringTermSet(); // sorted so we can loop over results in order shortly...\n    searcher.createWeight(spanQuery, false, 1.0f).extractTerms(termSet);//needsScores==false\n\n    // Get Spans by running the query against the reader\n    // TODO it might make sense to re-use/cache the Spans instance, to advance forward between docs\n    SpanWeight spanWeight = (SpanWeight) searcher.createNormalizedWeight(spanQuery, false);\n    Spans spans = spanWeight.getSpans(readerContext, SpanWeight.Postings.POSITIONS);\n    if (spans == null) {\n      return;\n    }\n    TwoPhaseIterator twoPhaseIterator = spans.asTwoPhaseIterator();\n    if (twoPhaseIterator != null) {\n      if (twoPhaseIterator.approximation().advance(doc) != doc || !twoPhaseIterator.matches()) {\n        return;\n      }\n    } else if (spans.advance(doc) != doc) { // preposition, and return doing nothing if find none\n      return;\n    }\n\n    // Consume the Spans into a cache.  This instance is used as a source for multiple cloned copies.\n    // It's important we do this and not re-use the same original Spans instance since these will be iterated\n    // independently later on; sometimes in ways that prevents sharing the original Spans.\n    CachedSpans cachedSpansSource = new CachedSpans(spans); // consumes spans for this doc only and caches\n    spans = null;// we don't use it below\n\n    // Map terms to a Spans instance (aggregate if necessary)\n    for (final Term queryTerm : termSet) {\n      // note: we expect that at least one query term will pass these filters. This is because the collected\n      //   spanQuery list were already filtered by these conditions.\n      if (positionInsensitiveTerms.contains(queryTerm)) {\n        continue;\n      }\n      // copy-constructor refers to same data (shallow) but has iteration state from the beginning\n      CachedSpans cachedSpans = new CachedSpans(cachedSpansSource);\n      // Add the span to whatever span may or may not exist\n      Spans existingSpans = result.get(queryTerm.bytes());\n      if (existingSpans != null) {\n        if (existingSpans instanceof MultiSpans) {\n          ((MultiSpans) existingSpans).addSpans(cachedSpans);\n        } else { // upgrade to MultiSpans\n          MultiSpans multiSpans = new MultiSpans();\n          multiSpans.addSpans(existingSpans);\n          multiSpans.addSpans(cachedSpans);\n          result.put(queryTerm.bytes(), multiSpans);\n        }\n      } else {\n        result.put(queryTerm.bytes(), cachedSpans);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eef54e3d232eae0e9fc18d75e9b0c3d9ce04b571","date":1515642580,"type":4,"author":"David Smiley","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/PhraseHelper#getTermToSpans(SpanQuery,LeafReaderContext,int,Map[BytesRef,Spans]).mjava","sourceNew":null,"sourceOld":"  // code extracted & refactored from WSTE.extractWeightedSpanTerms()\n  private void getTermToSpans(SpanQuery spanQuery, LeafReaderContext readerContext,\n                              int doc, Map<BytesRef, Spans> result)\n      throws IOException {\n    // note: in WSTE there was some field specific looping that seemed pointless so that isn't here.\n    final IndexSearcher searcher = new IndexSearcher(readerContext.reader());\n    searcher.setQueryCache(null);\n    if (willRewrite) {\n      spanQuery = (SpanQuery) searcher.rewrite(spanQuery); // searcher.rewrite loops till done\n    }\n\n    // Get the underlying query terms\n    TreeSet<Term> termSet = new FieldFilteringTermSet(); // sorted so we can loop over results in order shortly...\n    searcher.createWeight(spanQuery, ScoreMode.COMPLETE_NO_SCORES, 1.0f).extractTerms(termSet);//needsScores==false\n\n    // Get Spans by running the query against the reader\n    // TODO it might make sense to re-use/cache the Spans instance, to advance forward between docs\n    SpanWeight spanWeight = (SpanWeight) searcher.createNormalizedWeight(spanQuery, ScoreMode.COMPLETE_NO_SCORES);\n    Spans spans = spanWeight.getSpans(readerContext, SpanWeight.Postings.POSITIONS);\n    if (spans == null) {\n      return;\n    }\n    TwoPhaseIterator twoPhaseIterator = spans.asTwoPhaseIterator();\n    if (twoPhaseIterator != null) {\n      if (twoPhaseIterator.approximation().advance(doc) != doc || !twoPhaseIterator.matches()) {\n        return;\n      }\n    } else if (spans.advance(doc) != doc) { // preposition, and return doing nothing if find none\n      return;\n    }\n\n    // Consume the Spans into a cache.  This instance is used as a source for multiple cloned copies.\n    // It's important we do this and not re-use the same original Spans instance since these will be iterated\n    // independently later on; sometimes in ways that prevents sharing the original Spans.\n    CachedSpans cachedSpansSource = new CachedSpans(spans); // consumes spans for this doc only and caches\n    spans = null;// we don't use it below\n\n    // Map terms to a Spans instance (aggregate if necessary)\n    for (final Term queryTerm : termSet) {\n      // note: we expect that at least one query term will pass these filters. This is because the collected\n      //   spanQuery list were already filtered by these conditions.\n      if (positionInsensitiveTerms.contains(queryTerm)) {\n        continue;\n      }\n      // copy-constructor refers to same data (shallow) but has iteration state from the beginning\n      CachedSpans cachedSpans = new CachedSpans(cachedSpansSource);\n      // Add the span to whatever span may or may not exist\n      Spans existingSpans = result.get(queryTerm.bytes());\n      if (existingSpans != null) {\n        if (existingSpans instanceof MultiSpans) {\n          ((MultiSpans) existingSpans).addSpans(cachedSpans);\n        } else { // upgrade to MultiSpans\n          MultiSpans multiSpans = new MultiSpans();\n          multiSpans.addSpans(existingSpans);\n          multiSpans.addSpans(cachedSpans);\n          result.put(queryTerm.bytes(), multiSpans);\n        }\n      } else {\n        result.put(queryTerm.bytes(), cachedSpans);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b94236357aaa22b76c10629851fe4e376e0cea82","date":1516710914,"type":4,"author":"Karl Wright","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/PhraseHelper#getTermToSpans(SpanQuery,LeafReaderContext,int,Map[BytesRef,Spans]).mjava","sourceNew":null,"sourceOld":"  // code extracted & refactored from WSTE.extractWeightedSpanTerms()\n  private void getTermToSpans(SpanQuery spanQuery, LeafReaderContext readerContext,\n                              int doc, Map<BytesRef, Spans> result)\n      throws IOException {\n    // note: in WSTE there was some field specific looping that seemed pointless so that isn't here.\n    final IndexSearcher searcher = new IndexSearcher(readerContext.reader());\n    searcher.setQueryCache(null);\n    if (willRewrite) {\n      spanQuery = (SpanQuery) searcher.rewrite(spanQuery); // searcher.rewrite loops till done\n    }\n\n    // Get the underlying query terms\n    TreeSet<Term> termSet = new FieldFilteringTermSet(); // sorted so we can loop over results in order shortly...\n    searcher.createWeight(spanQuery, ScoreMode.COMPLETE_NO_SCORES, 1.0f).extractTerms(termSet);//needsScores==false\n\n    // Get Spans by running the query against the reader\n    // TODO it might make sense to re-use/cache the Spans instance, to advance forward between docs\n    SpanWeight spanWeight = (SpanWeight) searcher.createNormalizedWeight(spanQuery, ScoreMode.COMPLETE_NO_SCORES);\n    Spans spans = spanWeight.getSpans(readerContext, SpanWeight.Postings.POSITIONS);\n    if (spans == null) {\n      return;\n    }\n    TwoPhaseIterator twoPhaseIterator = spans.asTwoPhaseIterator();\n    if (twoPhaseIterator != null) {\n      if (twoPhaseIterator.approximation().advance(doc) != doc || !twoPhaseIterator.matches()) {\n        return;\n      }\n    } else if (spans.advance(doc) != doc) { // preposition, and return doing nothing if find none\n      return;\n    }\n\n    // Consume the Spans into a cache.  This instance is used as a source for multiple cloned copies.\n    // It's important we do this and not re-use the same original Spans instance since these will be iterated\n    // independently later on; sometimes in ways that prevents sharing the original Spans.\n    CachedSpans cachedSpansSource = new CachedSpans(spans); // consumes spans for this doc only and caches\n    spans = null;// we don't use it below\n\n    // Map terms to a Spans instance (aggregate if necessary)\n    for (final Term queryTerm : termSet) {\n      // note: we expect that at least one query term will pass these filters. This is because the collected\n      //   spanQuery list were already filtered by these conditions.\n      if (positionInsensitiveTerms.contains(queryTerm)) {\n        continue;\n      }\n      // copy-constructor refers to same data (shallow) but has iteration state from the beginning\n      CachedSpans cachedSpans = new CachedSpans(cachedSpansSource);\n      // Add the span to whatever span may or may not exist\n      Spans existingSpans = result.get(queryTerm.bytes());\n      if (existingSpans != null) {\n        if (existingSpans instanceof MultiSpans) {\n          ((MultiSpans) existingSpans).addSpans(cachedSpans);\n        } else { // upgrade to MultiSpans\n          MultiSpans multiSpans = new MultiSpans();\n          multiSpans.addSpans(existingSpans);\n          multiSpans.addSpans(cachedSpans);\n          result.put(queryTerm.bytes(), multiSpans);\n        }\n      } else {\n        result.put(queryTerm.bytes(), cachedSpans);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"5675b3bced0b155f0ff8001ce2e1e502be7c92f6":["1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2"],"b94236357aaa22b76c10629851fe4e376e0cea82":["417142ff08fda9cf0b72d5133e63097a166c6458","eef54e3d232eae0e9fc18d75e9b0c3d9ce04b571"],"ee8992dea79b51ba77a3fd1a5c0f94a5f0b8808d":["1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2","5675b3bced0b155f0ff8001ce2e1e502be7c92f6"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"eef54e3d232eae0e9fc18d75e9b0c3d9ce04b571":["417142ff08fda9cf0b72d5133e63097a166c6458"],"9856095f7afb5a607bf5e65077615ed91273508c":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","ee8992dea79b51ba77a3fd1a5c0f94a5f0b8808d"],"9fc47cb7b4346802411bb432f501ed0673d7119e":["ee8992dea79b51ba77a3fd1a5c0f94a5f0b8808d"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b94236357aaa22b76c10629851fe4e376e0cea82"],"417142ff08fda9cf0b72d5133e63097a166c6458":["ee8992dea79b51ba77a3fd1a5c0f94a5f0b8808d","9fc47cb7b4346802411bb432f501ed0673d7119e"]},"commit2Childs":{"5675b3bced0b155f0ff8001ce2e1e502be7c92f6":["ee8992dea79b51ba77a3fd1a5c0f94a5f0b8808d"],"b94236357aaa22b76c10629851fe4e376e0cea82":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ee8992dea79b51ba77a3fd1a5c0f94a5f0b8808d":["9856095f7afb5a607bf5e65077615ed91273508c","9fc47cb7b4346802411bb432f501ed0673d7119e","417142ff08fda9cf0b72d5133e63097a166c6458"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2":["5675b3bced0b155f0ff8001ce2e1e502be7c92f6","ee8992dea79b51ba77a3fd1a5c0f94a5f0b8808d","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"eef54e3d232eae0e9fc18d75e9b0c3d9ce04b571":["b94236357aaa22b76c10629851fe4e376e0cea82"],"9856095f7afb5a607bf5e65077615ed91273508c":[],"9fc47cb7b4346802411bb432f501ed0673d7119e":["417142ff08fda9cf0b72d5133e63097a166c6458"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["9856095f7afb5a607bf5e65077615ed91273508c"],"417142ff08fda9cf0b72d5133e63097a166c6458":["b94236357aaa22b76c10629851fe4e376e0cea82","eef54e3d232eae0e9fc18d75e9b0c3d9ce04b571"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["9856095f7afb5a607bf5e65077615ed91273508c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}