{"path":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(LeafReader,Directory,Directory,Directory,Analyzer,String...).mjava","commits":[{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":0,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(LeafReader,Directory,Directory,Directory,Analyzer,String...).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(LeafReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            doc.add(new Field(fieldName, originalIndex.document(scoreDoc.doc).getField(fieldName).stringValue(), ft));\n          }\n        } else {\n          for (StorableField storableField : originalIndex.document(scoreDoc.doc).getFields()) {\n            if (storableField.readerValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            } else if (storableField.binaryValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            } else if (storableField.stringValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            } else if (storableField.numericValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"805a01fd60dd540fc1326f9886d8cc985647f38e","date":1430733992,"type":3,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(LeafReader,Directory,Directory,Directory,Analyzer,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(LeafReader,Directory,Directory,Directory,Analyzer,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(LeafReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        StoredDocument document = originalIndex.document(scoreDoc.doc);\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            StorableField field = document.getField(fieldName);\n            if (field != null) {\n              doc.add(new Field(fieldName, field.stringValue(), ft));\n            }\n          }\n        } else {\n          for (StorableField storableField : document.getFields()) {\n            if (storableField.readerValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            } else if (storableField.binaryValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            } else if (storableField.stringValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            } else if (storableField.numericValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(LeafReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            doc.add(new Field(fieldName, originalIndex.document(scoreDoc.doc).getField(fieldName).stringValue(), ft));\n          }\n        } else {\n          for (StorableField storableField : originalIndex.document(scoreDoc.doc).getFields()) {\n            if (storableField.readerValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            } else if (storableField.binaryValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            } else if (storableField.stringValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            } else if (storableField.numericValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2c707e50d5d9db6929b753b3c0b0254186ee0986","date":1445602910,"type":3,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(LeafReader,Directory,Directory,Directory,Analyzer,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(LeafReader,Directory,Directory,Directory,Analyzer,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(LeafReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        StoredDocument document = originalIndex.document(scoreDoc.doc);\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            StorableField field = document.getField(fieldName);\n            if (field != null) {\n              doc.add(new Field(fieldName, field.stringValue(), ft));\n            }\n          }\n        } else {\n          for (StorableField storableField : document.getFields()) {\n            if (storableField.readerValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            } else if (storableField.binaryValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            } else if (storableField.stringValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            } else if (storableField.numericValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(LeafReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        StoredDocument document = originalIndex.document(scoreDoc.doc);\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            StorableField field = document.getField(fieldName);\n            if (field != null) {\n              doc.add(new Field(fieldName, field.stringValue(), ft));\n            }\n          }\n        } else {\n          for (StorableField storableField : document.getFields()) {\n            if (storableField.readerValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            } else if (storableField.binaryValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            } else if (storableField.stringValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            } else if (storableField.numericValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","date":1453060490,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(LeafReader,Directory,Directory,Directory,Analyzer,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(LeafReader,Directory,Directory,Directory,Analyzer,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(LeafReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        Document document = originalIndex.document(scoreDoc.doc);\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            IndexableField field = document.getField(fieldName);\n            if (field != null) {\n              doc.add(new Field(fieldName, field.stringValue(), ft));\n            }\n          }\n        } else {\n          for (IndexableField field : document.getFields()) {\n            if (field.readerValue() != null) {\n              doc.add(new Field(field.name(), field.readerValue(), ft));\n            } else if (field.binaryValue() != null) {\n              doc.add(new Field(field.name(), field.binaryValue(), ft));\n            } else if (field.stringValue() != null) {\n              doc.add(new Field(field.name(), field.stringValue(), ft));\n            } else if (field.numericValue() != null) {\n              doc.add(new Field(field.name(), field.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(LeafReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        StoredDocument document = originalIndex.document(scoreDoc.doc);\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            StorableField field = document.getField(fieldName);\n            if (field != null) {\n              doc.add(new Field(fieldName, field.stringValue(), ft));\n            }\n          }\n        } else {\n          for (StorableField storableField : document.getFields()) {\n            if (storableField.readerValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.readerValue(), ft));\n            } else if (storableField.binaryValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.binaryValue(), ft));\n            } else if (storableField.stringValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.stringValue(), ft));\n            } else if (storableField.numericValue() != null) {\n              doc.add(new Field(storableField.name(), storableField.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4e2d06d853b2be7aee1c9d69a6b36d26410459a9","date":1460361562,"type":5,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(LeafReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(LeafReader,Directory,Directory,Directory,Analyzer,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       names of the field used as the label for classification\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(LeafReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // try to get the exact no. of existing classes\n    Terms terms = originalIndex.terms(classFieldName);\n    long noOfClasses = -1;\n    if (terms != null) {\n      noOfClasses = terms.size();\n\n    }\n    if (noOfClasses == -1) {\n      noOfClasses = 10000; // fallback\n    }\n\n    HashMap<String, UninvertingReader.Type> mapping = new HashMap<>();\n    mapping.put(classFieldName, UninvertingReader.Type.SORTED);\n    UninvertingReader uninvertingReader = new UninvertingReader(originalIndex, mapping);\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(uninvertingReader);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, (int) noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      uninvertingReader.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(LeafReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        Document document = originalIndex.document(scoreDoc.doc);\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            IndexableField field = document.getField(fieldName);\n            if (field != null) {\n              doc.add(new Field(fieldName, field.stringValue(), ft));\n            }\n          }\n        } else {\n          for (IndexableField field : document.getFields()) {\n            if (field.readerValue() != null) {\n              doc.add(new Field(field.name(), field.readerValue(), ft));\n            } else if (field.binaryValue() != null) {\n              doc.add(new Field(field.name(), field.binaryValue(), ft));\n            } else if (field.stringValue() != null) {\n              doc.add(new Field(field.name(), field.stringValue(), ft));\n            } else if (field.numericValue() != null) {\n              doc.add(new Field(field.name(), field.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["f8f3dce1d4820d9634c1a6a46cd50ac13cf0f5a6"],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"805a01fd60dd540fc1326f9886d8cc985647f38e":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"2c707e50d5d9db6929b753b3c0b0254186ee0986":["805a01fd60dd540fc1326f9886d8cc985647f38e"],"4e2d06d853b2be7aee1c9d69a6b36d26410459a9":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["4e2d06d853b2be7aee1c9d69a6b36d26410459a9"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["2c707e50d5d9db6929b753b3c0b0254186ee0986"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"805a01fd60dd540fc1326f9886d8cc985647f38e":["2c707e50d5d9db6929b753b3c0b0254186ee0986"],"2c707e50d5d9db6929b753b3c0b0254186ee0986":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["805a01fd60dd540fc1326f9886d8cc985647f38e"],"4e2d06d853b2be7aee1c9d69a6b36d26410459a9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["4e2d06d853b2be7aee1c9d69a6b36d26410459a9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}