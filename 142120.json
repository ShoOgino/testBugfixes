{"path":"lucene/sandbox/src/test/org/apache/lucene/sandbox/postingshighlight/TestPostingsHighlighterRanking#testRanking().mjava","commits":[{"id":"43ff047e697f5b71d06c7eec1406226951c59b80","date":1356561472,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/sandbox/src/test/org/apache/lucene/sandbox/postingshighlight/TestPostingsHighlighterRanking#testRanking().mjava","pathOld":"/dev/null","sourceNew":"  /** \n   * indexes a bunch of gibberish, and then highlights top(n).\n   * asserts that top(n) highlights is a subset of top(n+1) up to some max N\n   */\n  // TODO: this only tests single-valued fields. we should also index multiple values per field!\n  public void testRanking() throws Exception {\n    // number of documents: we will check each one\n    final int numDocs = atLeast(100);\n    // number of top-N snippets, we will check 1 .. N\n    final int maxTopN = 5;\n    // maximum number of elements to put in a sentence.\n    final int maxSentenceLength = 10;\n    // maximum number of sentences in a document\n    final int maxNumSentences = 20;\n    \n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir);\n    Document document = new Document();\n    Field id = new StringField(\"id\", \"\", Field.Store.NO);\n    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);\n    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Field body = new Field(\"body\", \"\", offsetsType);\n    document.add(id);\n    document.add(body);\n    \n    for (int i = 0; i < numDocs; i++) {;\n      StringBuilder bodyText = new StringBuilder();\n      int numSentences = _TestUtil.nextInt(random(), 1, maxNumSentences);\n      for (int j = 0; j < numSentences; j++) {\n        bodyText.append(newSentence(random(), maxSentenceLength));\n      }\n      body.setStringValue(bodyText.toString());\n      id.setStringValue(Integer.toString(i));\n      iw.addDocument(document);\n    }\n    \n    IndexReader ir = iw.getReader();\n    IndexSearcher searcher = newSearcher(ir);\n    for (int i = 0; i < numDocs; i++) {\n      checkDocument(searcher, i, maxTopN);\n    }\n    iw.close();\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c7030e0d184253ce4a2a2e7ab578a4036e344246","date":1357219775,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/sandbox/src/test/org/apache/lucene/sandbox/postingshighlight/TestPostingsHighlighterRanking#testRanking().mjava","pathOld":"lucene/sandbox/src/test/org/apache/lucene/sandbox/postingshighlight/TestPostingsHighlighterRanking#testRanking().mjava","sourceNew":"  /** \n   * indexes a bunch of gibberish, and then highlights top(n).\n   * asserts that top(n) highlights is a subset of top(n+1) up to some max N\n   */\n  // TODO: this only tests single-valued fields. we should also index multiple values per field!\n  public void testRanking() throws Exception {\n    // number of documents: we will check each one\n    final int numDocs = atLeast(100);\n    // number of top-N snippets, we will check 1 .. N\n    final int maxTopN = 5;\n    // maximum number of elements to put in a sentence.\n    final int maxSentenceLength = 10;\n    // maximum number of sentences in a document\n    final int maxNumSentences = 20;\n    \n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));\n    Document document = new Document();\n    Field id = new StringField(\"id\", \"\", Field.Store.NO);\n    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);\n    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Field body = new Field(\"body\", \"\", offsetsType);\n    document.add(id);\n    document.add(body);\n    \n    for (int i = 0; i < numDocs; i++) {;\n      StringBuilder bodyText = new StringBuilder();\n      int numSentences = _TestUtil.nextInt(random(), 1, maxNumSentences);\n      for (int j = 0; j < numSentences; j++) {\n        bodyText.append(newSentence(random(), maxSentenceLength));\n      }\n      body.setStringValue(bodyText.toString());\n      id.setStringValue(Integer.toString(i));\n      iw.addDocument(document);\n    }\n    \n    IndexReader ir = iw.getReader();\n    IndexSearcher searcher = newSearcher(ir);\n    for (int i = 0; i < numDocs; i++) {\n      checkDocument(searcher, i, maxTopN);\n    }\n    iw.close();\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** \n   * indexes a bunch of gibberish, and then highlights top(n).\n   * asserts that top(n) highlights is a subset of top(n+1) up to some max N\n   */\n  // TODO: this only tests single-valued fields. we should also index multiple values per field!\n  public void testRanking() throws Exception {\n    // number of documents: we will check each one\n    final int numDocs = atLeast(100);\n    // number of top-N snippets, we will check 1 .. N\n    final int maxTopN = 5;\n    // maximum number of elements to put in a sentence.\n    final int maxSentenceLength = 10;\n    // maximum number of sentences in a document\n    final int maxNumSentences = 20;\n    \n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir);\n    Document document = new Document();\n    Field id = new StringField(\"id\", \"\", Field.Store.NO);\n    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);\n    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Field body = new Field(\"body\", \"\", offsetsType);\n    document.add(id);\n    document.add(body);\n    \n    for (int i = 0; i < numDocs; i++) {;\n      StringBuilder bodyText = new StringBuilder();\n      int numSentences = _TestUtil.nextInt(random(), 1, maxNumSentences);\n      for (int j = 0; j < numSentences; j++) {\n        bodyText.append(newSentence(random(), maxSentenceLength));\n      }\n      body.setStringValue(bodyText.toString());\n      id.setStringValue(Integer.toString(i));\n      iw.addDocument(document);\n    }\n    \n    IndexReader ir = iw.getReader();\n    IndexSearcher searcher = newSearcher(ir);\n    for (int i = 0; i < numDocs; i++) {\n      checkDocument(searcher, i, maxTopN);\n    }\n    iw.close();\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4b3d16cba9355e2e97962eb1c441bbd0b6735c15","date":1357426290,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking#testRanking().mjava","pathOld":"lucene/sandbox/src/test/org/apache/lucene/sandbox/postingshighlight/TestPostingsHighlighterRanking#testRanking().mjava","sourceNew":"  /** \n   * indexes a bunch of gibberish, and then highlights top(n).\n   * asserts that top(n) highlights is a subset of top(n+1) up to some max N\n   */\n  // TODO: this only tests single-valued fields. we should also index multiple values per field!\n  public void testRanking() throws Exception {\n    // number of documents: we will check each one\n    final int numDocs = atLeast(100);\n    // number of top-N snippets, we will check 1 .. N\n    final int maxTopN = 5;\n    // maximum number of elements to put in a sentence.\n    final int maxSentenceLength = 10;\n    // maximum number of sentences in a document\n    final int maxNumSentences = 20;\n    \n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));\n    Document document = new Document();\n    Field id = new StringField(\"id\", \"\", Field.Store.NO);\n    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);\n    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Field body = new Field(\"body\", \"\", offsetsType);\n    document.add(id);\n    document.add(body);\n    \n    for (int i = 0; i < numDocs; i++) {;\n      StringBuilder bodyText = new StringBuilder();\n      int numSentences = _TestUtil.nextInt(random(), 1, maxNumSentences);\n      for (int j = 0; j < numSentences; j++) {\n        bodyText.append(newSentence(random(), maxSentenceLength));\n      }\n      body.setStringValue(bodyText.toString());\n      id.setStringValue(Integer.toString(i));\n      iw.addDocument(document);\n    }\n    \n    IndexReader ir = iw.getReader();\n    IndexSearcher searcher = newSearcher(ir);\n    for (int i = 0; i < numDocs; i++) {\n      checkDocument(searcher, i, maxTopN);\n    }\n    iw.close();\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** \n   * indexes a bunch of gibberish, and then highlights top(n).\n   * asserts that top(n) highlights is a subset of top(n+1) up to some max N\n   */\n  // TODO: this only tests single-valued fields. we should also index multiple values per field!\n  public void testRanking() throws Exception {\n    // number of documents: we will check each one\n    final int numDocs = atLeast(100);\n    // number of top-N snippets, we will check 1 .. N\n    final int maxTopN = 5;\n    // maximum number of elements to put in a sentence.\n    final int maxSentenceLength = 10;\n    // maximum number of sentences in a document\n    final int maxNumSentences = 20;\n    \n    Directory dir = newDirectory();\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));\n    Document document = new Document();\n    Field id = new StringField(\"id\", \"\", Field.Store.NO);\n    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);\n    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Field body = new Field(\"body\", \"\", offsetsType);\n    document.add(id);\n    document.add(body);\n    \n    for (int i = 0; i < numDocs; i++) {;\n      StringBuilder bodyText = new StringBuilder();\n      int numSentences = _TestUtil.nextInt(random(), 1, maxNumSentences);\n      for (int j = 0; j < numSentences; j++) {\n        bodyText.append(newSentence(random(), maxSentenceLength));\n      }\n      body.setStringValue(bodyText.toString());\n      id.setStringValue(Integer.toString(i));\n      iw.addDocument(document);\n    }\n    \n    IndexReader ir = iw.getReader();\n    IndexSearcher searcher = newSearcher(ir);\n    for (int i = 0; i < numDocs; i++) {\n      checkDocument(searcher, i, maxTopN);\n    }\n    iw.close();\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"4b3d16cba9355e2e97962eb1c441bbd0b6735c15":["c7030e0d184253ce4a2a2e7ab578a4036e344246"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"c7030e0d184253ce4a2a2e7ab578a4036e344246":["43ff047e697f5b71d06c7eec1406226951c59b80"],"43ff047e697f5b71d06c7eec1406226951c59b80":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["4b3d16cba9355e2e97962eb1c441bbd0b6735c15"]},"commit2Childs":{"4b3d16cba9355e2e97962eb1c441bbd0b6735c15":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["43ff047e697f5b71d06c7eec1406226951c59b80"],"c7030e0d184253ce4a2a2e7ab578a4036e344246":["4b3d16cba9355e2e97962eb1c441bbd0b6735c15"],"43ff047e697f5b71d06c7eec1406226951c59b80":["c7030e0d184253ce4a2a2e7ab578a4036e344246"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}