{"path":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#deleteLastDocs(int).mjava","commits":[{"id":"bc1841e9449be30dd7bcb15d6247b4eb5c83a07b","date":1584454718,"type":1,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#deleteLastDocs(int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#deleteDocID(int).mjava","sourceNew":"  // This method marks the last N docs as deleted. This is used\n  // in the case of a non-aborting exception. There are several cases\n  // where we fail a document ie. due to an exception during analysis\n  // that causes the doc to be rejected but won't cause the DWPT to be\n  // stale nor the entire IW to abort and shutdown. In such a case\n  // we only mark these docs as deleted and turn it into a livedocs\n  // during flush\n  private void deleteLastDocs(int docCount) {\n    for (int docId = numDocsInRAM - docCount; docId < numDocsInRAM; docId++) {\n      pendingUpdates.addDocID(docId);\n    }\n    // NOTE: we do not trigger flush here.  This is\n    // potentially a RAM leak, if you have an app that tries\n    // to add docs but every single doc always hits a\n    // non-aborting exception.  Allowing a flush here gets\n    // very messy because we are only invoked when handling\n    // exceptions so to do this properly, while handling an\n    // exception we'd have to go off and flush new deletes\n    // which is risky (likely would hit some other\n    // confounding exception).\n  }\n\n","sourceOld":"  // Buffer a specific docID for deletion. Currently only\n  // used when we hit an exception when adding a document\n  void deleteDocID(int docIDUpto) {\n    pendingUpdates.addDocID(docIDUpto);\n    // NOTE: we do not trigger flush here.  This is\n    // potentially a RAM leak, if you have an app that tries\n    // to add docs but every single doc always hits a\n    // non-aborting exception.  Allowing a flush here gets\n    // very messy because we are only invoked when handling\n    // exceptions so to do this properly, while handling an\n    // exception we'd have to go off and flush new deletes\n    // which is risky (likely would hit some other\n    // confounding exception).\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7aff85e853fc8018761caa9a7803b1db411db8c","date":1586893039,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#deleteLastDocs(int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#deleteLastDocs(int).mjava","sourceNew":"  // This method marks the last N docs as deleted. This is used\n  // in the case of a non-aborting exception. There are several cases\n  // where we fail a document ie. due to an exception during analysis\n  // that causes the doc to be rejected but won't cause the DWPT to be\n  // stale nor the entire IW to abort and shutdown. In such a case\n  // we only mark these docs as deleted and turn it into a livedocs\n  // during flush\n  private void deleteLastDocs(int docCount) {\n    int from = numDocsInRAM-docCount;\n    int to = numDocsInRAM;\n    int size = deleteDocIDs.length;\n    deleteDocIDs = ArrayUtil.grow(deleteDocIDs, numDeletedDocIds + (to-from));\n    for (int docId = from; docId < to; docId++) {\n      deleteDocIDs[numDeletedDocIds++] = docId;\n    }\n    bytesUsed.addAndGet((deleteDocIDs.length - size) * Integer.SIZE);\n    // NOTE: we do not trigger flush here.  This is\n    // potentially a RAM leak, if you have an app that tries\n    // to add docs but every single doc always hits a\n    // non-aborting exception.  Allowing a flush here gets\n    // very messy because we are only invoked when handling\n    // exceptions so to do this properly, while handling an\n    // exception we'd have to go off and flush new deletes\n    // which is risky (likely would hit some other\n    // confounding exception).\n  }\n\n","sourceOld":"  // This method marks the last N docs as deleted. This is used\n  // in the case of a non-aborting exception. There are several cases\n  // where we fail a document ie. due to an exception during analysis\n  // that causes the doc to be rejected but won't cause the DWPT to be\n  // stale nor the entire IW to abort and shutdown. In such a case\n  // we only mark these docs as deleted and turn it into a livedocs\n  // during flush\n  private void deleteLastDocs(int docCount) {\n    for (int docId = numDocsInRAM - docCount; docId < numDocsInRAM; docId++) {\n      pendingUpdates.addDocID(docId);\n    }\n    // NOTE: we do not trigger flush here.  This is\n    // potentially a RAM leak, if you have an app that tries\n    // to add docs but every single doc always hits a\n    // non-aborting exception.  Allowing a flush here gets\n    // very messy because we are only invoked when handling\n    // exceptions so to do this properly, while handling an\n    // exception we'd have to go off and flush new deletes\n    // which is risky (likely would hit some other\n    // confounding exception).\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6926606ec5e0dd8d4ec79166d39a3b4ddb862bf4","date":1599581893,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#deleteLastDocs(int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#deleteLastDocs(int).mjava","sourceNew":"  // This method marks the last N docs as deleted. This is used\n  // in the case of a non-aborting exception. There are several cases\n  // where we fail a document ie. due to an exception during analysis\n  // that causes the doc to be rejected but won't cause the DWPT to be\n  // stale nor the entire IW to abort and shutdown. In such a case\n  // we only mark these docs as deleted and turn it into a livedocs\n  // during flush\n  private void deleteLastDocs(int docCount) {\n    int from = numDocsInRAM-docCount;\n    int to = numDocsInRAM;\n    int size = deleteDocIDs.length;\n    deleteDocIDs = ArrayUtil.grow(deleteDocIDs, numDeletedDocIds + (to-from));\n    for (int docId = from; docId < to; docId++) {\n      deleteDocIDs[numDeletedDocIds++] = docId;\n    }\n    bytesUsed.addAndGet((deleteDocIDs.length - size) * Integer.BYTES);\n    // NOTE: we do not trigger flush here.  This is\n    // potentially a RAM leak, if you have an app that tries\n    // to add docs but every single doc always hits a\n    // non-aborting exception.  Allowing a flush here gets\n    // very messy because we are only invoked when handling\n    // exceptions so to do this properly, while handling an\n    // exception we'd have to go off and flush new deletes\n    // which is risky (likely would hit some other\n    // confounding exception).\n  }\n\n","sourceOld":"  // This method marks the last N docs as deleted. This is used\n  // in the case of a non-aborting exception. There are several cases\n  // where we fail a document ie. due to an exception during analysis\n  // that causes the doc to be rejected but won't cause the DWPT to be\n  // stale nor the entire IW to abort and shutdown. In such a case\n  // we only mark these docs as deleted and turn it into a livedocs\n  // during flush\n  private void deleteLastDocs(int docCount) {\n    int from = numDocsInRAM-docCount;\n    int to = numDocsInRAM;\n    int size = deleteDocIDs.length;\n    deleteDocIDs = ArrayUtil.grow(deleteDocIDs, numDeletedDocIds + (to-from));\n    for (int docId = from; docId < to; docId++) {\n      deleteDocIDs[numDeletedDocIds++] = docId;\n    }\n    bytesUsed.addAndGet((deleteDocIDs.length - size) * Integer.SIZE);\n    // NOTE: we do not trigger flush here.  This is\n    // potentially a RAM leak, if you have an app that tries\n    // to add docs but every single doc always hits a\n    // non-aborting exception.  Allowing a flush here gets\n    // very messy because we are only invoked when handling\n    // exceptions so to do this properly, while handling an\n    // exception we'd have to go off and flush new deletes\n    // which is risky (likely would hit some other\n    // confounding exception).\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"49f1924bd448393fbdfef8b5ebed799f938169d3","date":1600069616,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#deleteLastDocs(int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#deleteLastDocs(int).mjava","sourceNew":"  // This method marks the last N docs as deleted. This is used\n  // in the case of a non-aborting exception. There are several cases\n  // where we fail a document ie. due to an exception during analysis\n  // that causes the doc to be rejected but won't cause the DWPT to be\n  // stale nor the entire IW to abort and shutdown. In such a case\n  // we only mark these docs as deleted and turn it into a livedocs\n  // during flush\n  private void deleteLastDocs(int docCount) {\n    int from = numDocsInRAM-docCount;\n    int to = numDocsInRAM;\n    deleteDocIDs = ArrayUtil.grow(deleteDocIDs, numDeletedDocIds + (to-from));\n    for (int docId = from; docId < to; docId++) {\n      deleteDocIDs[numDeletedDocIds++] = docId;\n    }\n    // NOTE: we do not trigger flush here.  This is\n    // potentially a RAM leak, if you have an app that tries\n    // to add docs but every single doc always hits a\n    // non-aborting exception.  Allowing a flush here gets\n    // very messy because we are only invoked when handling\n    // exceptions so to do this properly, while handling an\n    // exception we'd have to go off and flush new deletes\n    // which is risky (likely would hit some other\n    // confounding exception).\n  }\n\n","sourceOld":"  // This method marks the last N docs as deleted. This is used\n  // in the case of a non-aborting exception. There are several cases\n  // where we fail a document ie. due to an exception during analysis\n  // that causes the doc to be rejected but won't cause the DWPT to be\n  // stale nor the entire IW to abort and shutdown. In such a case\n  // we only mark these docs as deleted and turn it into a livedocs\n  // during flush\n  private void deleteLastDocs(int docCount) {\n    int from = numDocsInRAM-docCount;\n    int to = numDocsInRAM;\n    int size = deleteDocIDs.length;\n    deleteDocIDs = ArrayUtil.grow(deleteDocIDs, numDeletedDocIds + (to-from));\n    for (int docId = from; docId < to; docId++) {\n      deleteDocIDs[numDeletedDocIds++] = docId;\n    }\n    bytesUsed.addAndGet((deleteDocIDs.length - size) * Integer.BYTES);\n    // NOTE: we do not trigger flush here.  This is\n    // potentially a RAM leak, if you have an app that tries\n    // to add docs but every single doc always hits a\n    // non-aborting exception.  Allowing a flush here gets\n    // very messy because we are only invoked when handling\n    // exceptions so to do this properly, while handling an\n    // exception we'd have to go off and flush new deletes\n    // which is risky (likely would hit some other\n    // confounding exception).\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0dcf8f79417865e5028d753e669fae06457e8369","date":1600073240,"type":3,"author":"noblepaul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#deleteLastDocs(int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#deleteLastDocs(int).mjava","sourceNew":"  // This method marks the last N docs as deleted. This is used\n  // in the case of a non-aborting exception. There are several cases\n  // where we fail a document ie. due to an exception during analysis\n  // that causes the doc to be rejected but won't cause the DWPT to be\n  // stale nor the entire IW to abort and shutdown. In such a case\n  // we only mark these docs as deleted and turn it into a livedocs\n  // during flush\n  private void deleteLastDocs(int docCount) {\n    int from = numDocsInRAM-docCount;\n    int to = numDocsInRAM;\n    deleteDocIDs = ArrayUtil.grow(deleteDocIDs, numDeletedDocIds + (to-from));\n    for (int docId = from; docId < to; docId++) {\n      deleteDocIDs[numDeletedDocIds++] = docId;\n    }\n    // NOTE: we do not trigger flush here.  This is\n    // potentially a RAM leak, if you have an app that tries\n    // to add docs but every single doc always hits a\n    // non-aborting exception.  Allowing a flush here gets\n    // very messy because we are only invoked when handling\n    // exceptions so to do this properly, while handling an\n    // exception we'd have to go off and flush new deletes\n    // which is risky (likely would hit some other\n    // confounding exception).\n  }\n\n","sourceOld":"  // This method marks the last N docs as deleted. This is used\n  // in the case of a non-aborting exception. There are several cases\n  // where we fail a document ie. due to an exception during analysis\n  // that causes the doc to be rejected but won't cause the DWPT to be\n  // stale nor the entire IW to abort and shutdown. In such a case\n  // we only mark these docs as deleted and turn it into a livedocs\n  // during flush\n  private void deleteLastDocs(int docCount) {\n    int from = numDocsInRAM-docCount;\n    int to = numDocsInRAM;\n    int size = deleteDocIDs.length;\n    deleteDocIDs = ArrayUtil.grow(deleteDocIDs, numDeletedDocIds + (to-from));\n    for (int docId = from; docId < to; docId++) {\n      deleteDocIDs[numDeletedDocIds++] = docId;\n    }\n    bytesUsed.addAndGet((deleteDocIDs.length - size) * Integer.BYTES);\n    // NOTE: we do not trigger flush here.  This is\n    // potentially a RAM leak, if you have an app that tries\n    // to add docs but every single doc always hits a\n    // non-aborting exception.  Allowing a flush here gets\n    // very messy because we are only invoked when handling\n    // exceptions so to do this properly, while handling an\n    // exception we'd have to go off and flush new deletes\n    // which is risky (likely would hit some other\n    // confounding exception).\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"6926606ec5e0dd8d4ec79166d39a3b4ddb862bf4":["e7aff85e853fc8018761caa9a7803b1db411db8c"],"49f1924bd448393fbdfef8b5ebed799f938169d3":["6926606ec5e0dd8d4ec79166d39a3b4ddb862bf4"],"e7aff85e853fc8018761caa9a7803b1db411db8c":["bc1841e9449be30dd7bcb15d6247b4eb5c83a07b"],"0dcf8f79417865e5028d753e669fae06457e8369":["6926606ec5e0dd8d4ec79166d39a3b4ddb862bf4","49f1924bd448393fbdfef8b5ebed799f938169d3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"bc1841e9449be30dd7bcb15d6247b4eb5c83a07b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0dcf8f79417865e5028d753e669fae06457e8369"]},"commit2Childs":{"6926606ec5e0dd8d4ec79166d39a3b4ddb862bf4":["49f1924bd448393fbdfef8b5ebed799f938169d3","0dcf8f79417865e5028d753e669fae06457e8369"],"49f1924bd448393fbdfef8b5ebed799f938169d3":["0dcf8f79417865e5028d753e669fae06457e8369"],"e7aff85e853fc8018761caa9a7803b1db411db8c":["6926606ec5e0dd8d4ec79166d39a3b4ddb862bf4"],"0dcf8f79417865e5028d753e669fae06457e8369":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["bc1841e9449be30dd7bcb15d6247b4eb5c83a07b"],"bc1841e9449be30dd7bcb15d6247b4eb5c83a07b":["e7aff85e853fc8018761caa9a7803b1db411db8c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}