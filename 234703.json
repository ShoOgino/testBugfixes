{"path":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(AtomicReader,BytesRef).mjava","commits":[{"id":"da6d5ac19a80d65b1e864251f155d30960353b7e","date":1327881054,"type":1,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(AtomicReader,BytesRef).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(AtomicIndexReader,BytesRef).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final AtomicReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Fields fields = reader.fields();\n    if (fields == null) {\n      // No terms\n      return;\n    }\n    final Terms terms = fields.terms(field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = reader.getLiveDocs();\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum, false);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = docsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  // Call this only once (if you subclass!)\n  protected void uninvert(final AtomicIndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Fields fields = reader.fields();\n    if (fields == null) {\n      // No terms\n      return;\n    }\n    final Terms terms = fields.terms(field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = reader.getLiveDocs();\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum, false);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = docsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":1,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(AtomicReader,BytesRef).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final AtomicReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Fields fields = reader.fields();\n    if (fields == null) {\n      // No terms\n      return;\n    }\n    final Terms terms = fields.terms(field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = reader.getLiveDocs();\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum, false);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = docsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Fields fields = reader.fields();\n    if (fields == null) {\n      // No terms\n      return;\n    }\n    final Terms terms = fields.terms(field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = reader.getLiveDocs();\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum, false);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = docsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocTermOrds#uninvert(AtomicReader,BytesRef).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(AtomicReader,BytesRef).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final AtomicReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Fields fields = reader.fields();\n    if (fields == null) {\n      // No terms\n      return;\n    }\n    final Terms terms = fields.terms(field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = reader.getLiveDocs();\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum, false);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = docsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  // Call this only once (if you subclass!)\n  protected void uninvert(final AtomicReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Fields fields = reader.fields();\n    if (fields == null) {\n      // No terms\n      return;\n    }\n    final Terms terms = fields.terms(field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator(null);\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits liveDocs = reader.getLiveDocs();\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(liveDocs, docsEnum, false);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = docsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"da6d5ac19a80d65b1e864251f155d30960353b7e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5cab9a86bd67202d20b6adc463008c8e982b070a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","da6d5ac19a80d65b1e864251f155d30960353b7e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"da6d5ac19a80d65b1e864251f155d30960353b7e":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["da6d5ac19a80d65b1e864251f155d30960353b7e","5cab9a86bd67202d20b6adc463008c8e982b070a"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}