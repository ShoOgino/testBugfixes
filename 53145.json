{"path":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","commits":[{"id":"307cff5af2b00f126fdf9d3435b75d5ed4d0f402","date":1305370109,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n      \n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[] {\"a\", \"b\", \"c\", \"d\"};\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      final IndexSearcher s = new IndexSearcher(r);\n\n      for(int searchIter=0;searchIter<100;searchIter++) {\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: searchIter=\" + searchIter);\n        }\n\n        final String searchTerm = contentStrings[random.nextInt(contentStrings.length)];\n        final boolean fillFields = random.nextBoolean();\n        final boolean getScores = random.nextBoolean();\n        final boolean getMaxScores = random.nextBoolean();\n        final Sort groupSort = getRandomSort();\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n\n        final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n        final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n        final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n        //final int groupOffset = 0;\n\n        final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random.nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup);\n        }\n\n        final FirstPassGroupingCollector c1 = new FirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        if (doCache) {\n          final double maxCacheMB = random.nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          c = cCache = new CachingCollector(c1, true, maxCacheMB);\n        } else {\n          c = c1;\n          cCache = null;\n        }\n        s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n        final Collection<SearchGroup> topGroups = c1.getTopGroups(groupOffset, fillFields);\n        final TopGroups groupsResult;\n\n        if (topGroups != null) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n\n          final SecondPassGroupingCollector c2 = new SecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n          } else {\n            s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n          }\n        \n          groupsResult = c2.getTopGroups(docOffset);\n        } else {\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n\n        final TopGroups expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n        try {\n          // NOTE: intentional but temporary field cache insanity!\n          assertEquals(FieldCache.DEFAULT.getInts(r, \"id\"), expectedGroups, groupsResult);\n        } finally {\n          FieldCache.DEFAULT.purge(r);\n        }\n      }\n\n      r.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["a78a90fc9701e511308346ea29f4f5e548bb39fe","fa0f44f887719e97183771e977cfc4bfb485b766","9274621789ce990dbfef455dabdf026bb3184821"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c5f61d6a2927b52517a31a8bf022549d33b1dfec","date":1305652854,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[] {\"a\", \"b\", \"c\", \"d\"};\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      final IndexSearcher s = new IndexSearcher(r);\n\n      for(int searchIter=0;searchIter<100;searchIter++) {\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: searchIter=\" + searchIter);\n        }\n\n        final String searchTerm = contentStrings[random.nextInt(contentStrings.length)];\n        final boolean fillFields = random.nextBoolean();\n        final boolean getScores = random.nextBoolean();\n        final boolean getMaxScores = random.nextBoolean();\n        final Sort groupSort = getRandomSort();\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n\n        final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n        final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n        final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n        //final int groupOffset = 0;\n\n        final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random.nextBoolean();\n        final boolean doAllGroups = random.nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups);\n        }\n\n        final AllGroupsCollector allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = new AllGroupsCollector(\"group\");\n        } else {\n          allGroupsCollector = null;\n        }\n\n        final FirstPassGroupingCollector c1 = new FirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        if (doCache) {\n          final double maxCacheMB = random.nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n\n          if (doAllGroups) {\n            cCache = new CachingCollector(c1, true, maxCacheMB);\n            c = MultiCollector.wrap(cCache, allGroupsCollector);\n          } else {\n            c = cCache = new CachingCollector(c1, true, maxCacheMB);\n          }\n        } else if (doAllGroups) {\n          c = MultiCollector.wrap(c1, allGroupsCollector);\n          cCache = null;\n        } else {\n          c = c1;\n          cCache = null;\n        }\n        s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n        final Collection<SearchGroup> topGroups = c1.getTopGroups(groupOffset, fillFields);\n        final TopGroups groupsResult;\n\n        if (topGroups != null) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n\n          final SecondPassGroupingCollector c2 = new SecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n          } else {\n            s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n          }\n\n          if (doAllGroups) {\n            TopGroups tempTopGroups = c2.getTopGroups(docOffset);\n            groupsResult = new TopGroups(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = c2.getTopGroups(docOffset);\n          }\n        } else {\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n\n        final TopGroups expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n        try {\n          // NOTE: intentional but temporary field cache insanity!\n          assertEquals(FieldCache.DEFAULT.getInts(r, \"id\"), expectedGroups, groupsResult);\n        } finally {\n          FieldCache.DEFAULT.purge(r);\n        }\n      }\n\n      r.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n      \n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[] {\"a\", \"b\", \"c\", \"d\"};\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      final IndexSearcher s = new IndexSearcher(r);\n\n      for(int searchIter=0;searchIter<100;searchIter++) {\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: searchIter=\" + searchIter);\n        }\n\n        final String searchTerm = contentStrings[random.nextInt(contentStrings.length)];\n        final boolean fillFields = random.nextBoolean();\n        final boolean getScores = random.nextBoolean();\n        final boolean getMaxScores = random.nextBoolean();\n        final Sort groupSort = getRandomSort();\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n\n        final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n        final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n        final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n        //final int groupOffset = 0;\n\n        final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random.nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup);\n        }\n\n        final FirstPassGroupingCollector c1 = new FirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        if (doCache) {\n          final double maxCacheMB = random.nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          c = cCache = new CachingCollector(c1, true, maxCacheMB);\n        } else {\n          c = c1;\n          cCache = null;\n        }\n        s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n        final Collection<SearchGroup> topGroups = c1.getTopGroups(groupOffset, fillFields);\n        final TopGroups groupsResult;\n\n        if (topGroups != null) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n\n          final SecondPassGroupingCollector c2 = new SecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n          } else {\n            s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n          }\n        \n          groupsResult = c2.getTopGroups(docOffset);\n        } else {\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n\n        final TopGroups expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n        try {\n          // NOTE: intentional but temporary field cache insanity!\n          assertEquals(FieldCache.DEFAULT.getInts(r, \"id\"), expectedGroups, groupsResult);\n        } finally {\n          FieldCache.DEFAULT.purge(r);\n        }\n      }\n\n      r.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6703e55954db440ab8a2bc2a615c4fa9f66b602b","date":1305691248,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[] {\"a\", \"b\", \"c\", \"d\"};\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      final IndexSearcher s = new IndexSearcher(r);\n\n      for(int searchIter=0;searchIter<100;searchIter++) {\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: searchIter=\" + searchIter);\n        }\n\n        final String searchTerm = contentStrings[random.nextInt(contentStrings.length)];\n        final boolean fillFields = random.nextBoolean();\n        final boolean getScores = random.nextBoolean();\n        final boolean getMaxScores = random.nextBoolean();\n        final Sort groupSort = getRandomSort();\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n\n        final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n        final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n        final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n        //final int groupOffset = 0;\n\n        final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random.nextBoolean();\n        final boolean doAllGroups = random.nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups);\n        }\n\n        final AllGroupsCollector allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = new AllGroupsCollector(\"group\");\n        } else {\n          allGroupsCollector = null;\n        }\n\n        final FirstPassGroupingCollector c1 = new FirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        if (doCache) {\n          final double maxCacheMB = random.nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n\n          if (doAllGroups) {\n            cCache = CachingCollector.create(c1, true, maxCacheMB);\n            c = MultiCollector.wrap(cCache, allGroupsCollector);\n          } else {\n            c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n          }\n        } else if (doAllGroups) {\n          c = MultiCollector.wrap(c1, allGroupsCollector);\n          cCache = null;\n        } else {\n          c = c1;\n          cCache = null;\n        }\n        s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n        final Collection<SearchGroup> topGroups = c1.getTopGroups(groupOffset, fillFields);\n        final TopGroups groupsResult;\n\n        if (topGroups != null) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n\n          final SecondPassGroupingCollector c2 = new SecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n          } else {\n            s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n          }\n\n          if (doAllGroups) {\n            TopGroups tempTopGroups = c2.getTopGroups(docOffset);\n            groupsResult = new TopGroups(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = c2.getTopGroups(docOffset);\n          }\n        } else {\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n\n        final TopGroups expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n        try {\n          // NOTE: intentional but temporary field cache insanity!\n          assertEquals(FieldCache.DEFAULT.getInts(r, \"id\"), expectedGroups, groupsResult);\n        } finally {\n          FieldCache.DEFAULT.purge(r);\n        }\n      }\n\n      r.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[] {\"a\", \"b\", \"c\", \"d\"};\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      final IndexSearcher s = new IndexSearcher(r);\n\n      for(int searchIter=0;searchIter<100;searchIter++) {\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: searchIter=\" + searchIter);\n        }\n\n        final String searchTerm = contentStrings[random.nextInt(contentStrings.length)];\n        final boolean fillFields = random.nextBoolean();\n        final boolean getScores = random.nextBoolean();\n        final boolean getMaxScores = random.nextBoolean();\n        final Sort groupSort = getRandomSort();\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n\n        final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n        final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n        final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n        //final int groupOffset = 0;\n\n        final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random.nextBoolean();\n        final boolean doAllGroups = random.nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups);\n        }\n\n        final AllGroupsCollector allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = new AllGroupsCollector(\"group\");\n        } else {\n          allGroupsCollector = null;\n        }\n\n        final FirstPassGroupingCollector c1 = new FirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        if (doCache) {\n          final double maxCacheMB = random.nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n\n          if (doAllGroups) {\n            cCache = new CachingCollector(c1, true, maxCacheMB);\n            c = MultiCollector.wrap(cCache, allGroupsCollector);\n          } else {\n            c = cCache = new CachingCollector(c1, true, maxCacheMB);\n          }\n        } else if (doAllGroups) {\n          c = MultiCollector.wrap(c1, allGroupsCollector);\n          cCache = null;\n        } else {\n          c = c1;\n          cCache = null;\n        }\n        s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n        final Collection<SearchGroup> topGroups = c1.getTopGroups(groupOffset, fillFields);\n        final TopGroups groupsResult;\n\n        if (topGroups != null) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n\n          final SecondPassGroupingCollector c2 = new SecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n          } else {\n            s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n          }\n\n          if (doAllGroups) {\n            TopGroups tempTopGroups = c2.getTopGroups(docOffset);\n            groupsResult = new TopGroups(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = c2.getTopGroups(docOffset);\n          }\n        } else {\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n\n        final TopGroups expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n        try {\n          // NOTE: intentional but temporary field cache insanity!\n          assertEquals(FieldCache.DEFAULT.getInts(r, \"id\"), expectedGroups, groupsResult);\n        } finally {\n          FieldCache.DEFAULT.purge(r);\n        }\n      }\n\n      r.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c3a8a449466c1ff7ce2274fe73dab487256964b4","date":1305735867,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[] {\"a\", \"b\", \"c\", \"d\"};\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      final IndexSearcher s = new IndexSearcher(r);\n\n      for(int searchIter=0;searchIter<100;searchIter++) {\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: searchIter=\" + searchIter);\n        }\n\n        final String searchTerm = contentStrings[random.nextInt(contentStrings.length)];\n        final boolean fillFields = random.nextBoolean();\n        final boolean getScores = random.nextBoolean();\n        final boolean getMaxScores = random.nextBoolean();\n        final Sort groupSort = getRandomSort();\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n\n        final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n        final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n        final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n        //final int groupOffset = 0;\n\n        final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random.nextBoolean();\n        final boolean doAllGroups = random.nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups);\n        }\n\n        final AllGroupsCollector allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = new AllGroupsCollector(\"group\");\n        } else {\n          allGroupsCollector = null;\n        }\n\n        final FirstPassGroupingCollector c1 = new FirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        if (doCache) {\n          final double maxCacheMB = random.nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n\n          if (doAllGroups) {\n            cCache = CachingCollector.create(c1, true, maxCacheMB);\n            c = MultiCollector.wrap(cCache, allGroupsCollector);\n          } else {\n            c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n          }\n        } else if (doAllGroups) {\n          c = MultiCollector.wrap(c1, allGroupsCollector);\n          cCache = null;\n        } else {\n          c = c1;\n          cCache = null;\n        }\n        s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n        final Collection<SearchGroup> topGroups = c1.getTopGroups(groupOffset, fillFields);\n        final TopGroups groupsResult;\n\n        if (topGroups != null) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n\n          final SecondPassGroupingCollector c2 = new SecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n          } else {\n            s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n          }\n\n          if (doAllGroups) {\n            TopGroups tempTopGroups = c2.getTopGroups(docOffset);\n            groupsResult = new TopGroups(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = c2.getTopGroups(docOffset);\n          }\n        } else {\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n\n        final TopGroups expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n        try {\n          // NOTE: intentional but temporary field cache insanity!\n          assertEquals(FieldCache.DEFAULT.getInts(r, \"id\"), expectedGroups, groupsResult);\n        } finally {\n          FieldCache.DEFAULT.purge(r);\n        }\n      }\n\n      r.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"669ab70ac675a9fdd757b28a1a6ca63f667c7188","date":1305744699,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[] {\"a\", \"b\", \"c\", \"d\"};\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      final IndexSearcher s = new IndexSearcher(r);\n\n      for(int searchIter=0;searchIter<100;searchIter++) {\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: searchIter=\" + searchIter);\n        }\n\n        final String searchTerm = contentStrings[random.nextInt(contentStrings.length)];\n        final boolean fillFields = random.nextBoolean();\n        final boolean getScores = random.nextBoolean();\n        final boolean getMaxScores = random.nextBoolean();\n        final Sort groupSort = getRandomSort();\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n\n        final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n        final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n        final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n        //final int groupOffset = 0;\n\n        final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random.nextBoolean();\n        final boolean doAllGroups = random.nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups);\n        }\n\n        final AllGroupsCollector allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = new AllGroupsCollector(\"group\");\n        } else {\n          allGroupsCollector = null;\n        }\n\n        final FirstPassGroupingCollector c1 = new FirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final boolean useWrappingCollector = random.nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random.nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n\n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(false, true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n            if (doAllGroups) {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n            }\n          }\n        }\n\n        final Collection<SearchGroup> topGroups = c1.getTopGroups(groupOffset, fillFields);\n        final TopGroups groupsResult;\n\n        if (topGroups != null) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n\n          final SecondPassGroupingCollector c2 = new SecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n          } else {\n            s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n          }\n\n          if (doAllGroups) {\n            TopGroups tempTopGroups = c2.getTopGroups(docOffset);\n            groupsResult = new TopGroups(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = c2.getTopGroups(docOffset);\n          }\n        } else {\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n\n        final TopGroups expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n        try {\n          // NOTE: intentional but temporary field cache insanity!\n          assertEquals(FieldCache.DEFAULT.getInts(r, \"id\"), expectedGroups, groupsResult);\n        } finally {\n          FieldCache.DEFAULT.purge(r);\n        }\n      }\n\n      r.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[] {\"a\", \"b\", \"c\", \"d\"};\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      final IndexSearcher s = new IndexSearcher(r);\n\n      for(int searchIter=0;searchIter<100;searchIter++) {\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: searchIter=\" + searchIter);\n        }\n\n        final String searchTerm = contentStrings[random.nextInt(contentStrings.length)];\n        final boolean fillFields = random.nextBoolean();\n        final boolean getScores = random.nextBoolean();\n        final boolean getMaxScores = random.nextBoolean();\n        final Sort groupSort = getRandomSort();\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n\n        final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n        final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n        final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n        //final int groupOffset = 0;\n\n        final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random.nextBoolean();\n        final boolean doAllGroups = random.nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups);\n        }\n\n        final AllGroupsCollector allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = new AllGroupsCollector(\"group\");\n        } else {\n          allGroupsCollector = null;\n        }\n\n        final FirstPassGroupingCollector c1 = new FirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        if (doCache) {\n          final double maxCacheMB = random.nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n\n          if (doAllGroups) {\n            cCache = CachingCollector.create(c1, true, maxCacheMB);\n            c = MultiCollector.wrap(cCache, allGroupsCollector);\n          } else {\n            c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n          }\n        } else if (doAllGroups) {\n          c = MultiCollector.wrap(c1, allGroupsCollector);\n          cCache = null;\n        } else {\n          c = c1;\n          cCache = null;\n        }\n        s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n        final Collection<SearchGroup> topGroups = c1.getTopGroups(groupOffset, fillFields);\n        final TopGroups groupsResult;\n\n        if (topGroups != null) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n\n          final SecondPassGroupingCollector c2 = new SecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n          } else {\n            s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n          }\n\n          if (doAllGroups) {\n            TopGroups tempTopGroups = c2.getTopGroups(docOffset);\n            groupsResult = new TopGroups(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = c2.getTopGroups(docOffset);\n          }\n        } else {\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n\n        final TopGroups expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n        try {\n          // NOTE: intentional but temporary field cache insanity!\n          assertEquals(FieldCache.DEFAULT.getInts(r, \"id\"), expectedGroups, groupsResult);\n        } finally {\n          FieldCache.DEFAULT.purge(r);\n        }\n      }\n\n      r.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":0,"author":"Steven Rowe","isMerge":true,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[] {\"a\", \"b\", \"c\", \"d\"};\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      final IndexSearcher s = new IndexSearcher(r);\n\n      for(int searchIter=0;searchIter<100;searchIter++) {\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: searchIter=\" + searchIter);\n        }\n\n        final String searchTerm = contentStrings[random.nextInt(contentStrings.length)];\n        final boolean fillFields = random.nextBoolean();\n        final boolean getScores = random.nextBoolean();\n        final boolean getMaxScores = random.nextBoolean();\n        final Sort groupSort = getRandomSort();\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n\n        final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n        final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n        final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n        //final int groupOffset = 0;\n\n        final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random.nextBoolean();\n        final boolean doAllGroups = random.nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups);\n        }\n\n        final AllGroupsCollector allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = new AllGroupsCollector(\"group\");\n        } else {\n          allGroupsCollector = null;\n        }\n\n        final FirstPassGroupingCollector c1 = new FirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final boolean useWrappingCollector = random.nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random.nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n\n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(false, true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n            if (doAllGroups) {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n            }\n          }\n        }\n\n        final Collection<SearchGroup> topGroups = c1.getTopGroups(groupOffset, fillFields);\n        final TopGroups groupsResult;\n\n        if (topGroups != null) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n\n          final SecondPassGroupingCollector c2 = new SecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n          } else {\n            s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n          }\n\n          if (doAllGroups) {\n            TopGroups tempTopGroups = c2.getTopGroups(docOffset);\n            groupsResult = new TopGroups(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = c2.getTopGroups(docOffset);\n          }\n        } else {\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n\n        final TopGroups expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n        try {\n          // NOTE: intentional but temporary field cache insanity!\n          assertEquals(FieldCache.DEFAULT.getInts(r, \"id\"), expectedGroups, groupsResult);\n        } finally {\n          FieldCache.DEFAULT.purge(r);\n        }\n      }\n\n      r.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a","date":1306150983,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[] {\"a\", \"b\", \"c\", \"d\"};\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      final IndexSearcher s = new IndexSearcher(r);\n\n      for(int searchIter=0;searchIter<100;searchIter++) {\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: searchIter=\" + searchIter);\n        }\n\n        final String searchTerm = contentStrings[random.nextInt(contentStrings.length)];\n        final boolean fillFields = random.nextBoolean();\n        final boolean getScores = random.nextBoolean();\n        final boolean getMaxScores = random.nextBoolean();\n        final Sort groupSort = getRandomSort();\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n\n        final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n        final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n        final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n        //final int groupOffset = 0;\n\n        final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random.nextBoolean();\n        final boolean doAllGroups = random.nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups);\n        }\n\n        final AllGroupsCollector allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = new AllGroupsCollector(\"group\");\n        } else {\n          allGroupsCollector = null;\n        }\n\n        final FirstPassGroupingCollector c1 = new FirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final boolean useWrappingCollector = random.nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random.nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n\n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(false, true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n            if (doAllGroups) {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n            }\n          }\n        }\n\n        final Collection<SearchGroup> topGroups = c1.getTopGroups(groupOffset, fillFields);\n        final TopGroups groupsResult;\n\n        if (topGroups != null) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n\n          final SecondPassGroupingCollector c2 = new SecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n          } else {\n            s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n          }\n\n          if (doAllGroups) {\n            TopGroups tempTopGroups = c2.getTopGroups(docOffset);\n            groupsResult = new TopGroups(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = c2.getTopGroups(docOffset);\n          }\n        } else {\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n\n        final TopGroups expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n        try {\n          // NOTE: intentional but temporary field cache insanity!\n          assertEquals(FieldCache.DEFAULT.getInts(r, \"id\"), expectedGroups, groupsResult);\n        } finally {\n          FieldCache.DEFAULT.purge(r);\n        }\n      }\n\n      r.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[] {\"a\", \"b\", \"c\", \"d\"};\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      final IndexSearcher s = new IndexSearcher(r);\n\n      for(int searchIter=0;searchIter<100;searchIter++) {\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: searchIter=\" + searchIter);\n        }\n\n        final String searchTerm = contentStrings[random.nextInt(contentStrings.length)];\n        final boolean fillFields = random.nextBoolean();\n        final boolean getScores = random.nextBoolean();\n        final boolean getMaxScores = random.nextBoolean();\n        final Sort groupSort = getRandomSort();\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n\n        final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n        final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n        final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n        //final int groupOffset = 0;\n\n        final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random.nextBoolean();\n        final boolean doAllGroups = random.nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups);\n        }\n\n        final AllGroupsCollector allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = new AllGroupsCollector(\"group\");\n        } else {\n          allGroupsCollector = null;\n        }\n\n        final FirstPassGroupingCollector c1 = new FirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        if (doCache) {\n          final double maxCacheMB = random.nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n\n          if (doAllGroups) {\n            cCache = CachingCollector.create(c1, true, maxCacheMB);\n            c = MultiCollector.wrap(cCache, allGroupsCollector);\n          } else {\n            c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n          }\n        } else if (doAllGroups) {\n          c = MultiCollector.wrap(c1, allGroupsCollector);\n          cCache = null;\n        } else {\n          c = c1;\n          cCache = null;\n        }\n        s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n        final Collection<SearchGroup> topGroups = c1.getTopGroups(groupOffset, fillFields);\n        final TopGroups groupsResult;\n\n        if (topGroups != null) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n\n          final SecondPassGroupingCollector c2 = new SecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n          } else {\n            s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n          }\n\n          if (doAllGroups) {\n            TopGroups tempTopGroups = c2.getTopGroups(docOffset);\n            groupsResult = new TopGroups(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = c2.getTopGroups(docOffset);\n          }\n        } else {\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n\n        final TopGroups expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n        try {\n          // NOTE: intentional but temporary field cache insanity!\n          assertEquals(FieldCache.DEFAULT.getInts(r, \"id\"), expectedGroups, groupsResult);\n        } finally {\n          FieldCache.DEFAULT.purge(r);\n        }\n      }\n\n      r.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3ce36a160d1241ae9c70e109dc3fdfdfb009674a","date":1307033216,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[] {\"a\", \"b\", \"c\", \"d\"};\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      final Directory dir2 = newDirectory();\n      final IndexReader r2 = getDocBlockReader(dir2, groupDocs);\n      final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n\n      final IndexSearcher s = new IndexSearcher(r);\n      final IndexSearcher s2 = new IndexSearcher(r2);\n\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n      try {\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = contentStrings[random.nextInt(contentStrings.length)];\n          final boolean fillFields = random.nextBoolean();\n          final boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 4;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups);\n          }\n\n          final AllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new AllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final FirstPassGroupingCollector c1 = new FirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final SecondPassGroupingCollector c2 = new SecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc);\n                }\n              }\n            }\n          }\n          // NOTE: intentional but temporary field cache insanity!\n          assertEquals(docIDToID, expectedGroups, groupsResult, true);\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final AllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new AllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          final TopGroups tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        FieldCache.DEFAULT.purge(r2);\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[] {\"a\", \"b\", \"c\", \"d\"};\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      final IndexSearcher s = new IndexSearcher(r);\n\n      for(int searchIter=0;searchIter<100;searchIter++) {\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: searchIter=\" + searchIter);\n        }\n\n        final String searchTerm = contentStrings[random.nextInt(contentStrings.length)];\n        final boolean fillFields = random.nextBoolean();\n        final boolean getScores = random.nextBoolean();\n        final boolean getMaxScores = random.nextBoolean();\n        final Sort groupSort = getRandomSort();\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n\n        final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n        final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n        final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n        //final int groupOffset = 0;\n\n        final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random.nextBoolean();\n        final boolean doAllGroups = random.nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups);\n        }\n\n        final AllGroupsCollector allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = new AllGroupsCollector(\"group\");\n        } else {\n          allGroupsCollector = null;\n        }\n\n        final FirstPassGroupingCollector c1 = new FirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final boolean useWrappingCollector = random.nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random.nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n\n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(false, true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n            if (doAllGroups) {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n            }\n          }\n        }\n\n        final Collection<SearchGroup> topGroups = c1.getTopGroups(groupOffset, fillFields);\n        final TopGroups groupsResult;\n\n        if (topGroups != null) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n\n          final SecondPassGroupingCollector c2 = new SecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n          } else {\n            s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n          }\n\n          if (doAllGroups) {\n            TopGroups tempTopGroups = c2.getTopGroups(docOffset);\n            groupsResult = new TopGroups(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = c2.getTopGroups(docOffset);\n          }\n        } else {\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n\n        final TopGroups expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n        try {\n          // NOTE: intentional but temporary field cache insanity!\n          assertEquals(FieldCache.DEFAULT.getInts(r, \"id\"), expectedGroups, groupsResult);\n        } finally {\n          FieldCache.DEFAULT.purge(r);\n        }\n      }\n\n      r.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d62f5453a200cec2cbb60148de159dbf55591e9d","date":1307049300,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[] {\"a\", \"b\", \"c\", \"d\"};\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      final Directory dir2 = newDirectory();\n      final IndexReader r2 = getDocBlockReader(dir2, groupDocs);\n      final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n\n      final IndexSearcher s = new IndexSearcher(r);\n      final IndexSearcher s2 = new IndexSearcher(r2);\n\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n      try {\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = contentStrings[random.nextInt(contentStrings.length)];\n          final boolean fillFields = random.nextBoolean();\n          final boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 4;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups);\n          }\n\n          final AllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new AllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final FirstPassGroupingCollector c1 = new FirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final SecondPassGroupingCollector c2 = new SecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc);\n                }\n              }\n            }\n          }\n          // NOTE: intentional but temporary field cache insanity!\n          assertEquals(docIDToID, expectedGroups, groupsResult, true);\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final AllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new AllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          final TopGroups tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        FieldCache.DEFAULT.purge(r2);\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[] {\"a\", \"b\", \"c\", \"d\"};\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      final IndexSearcher s = new IndexSearcher(r);\n\n      for(int searchIter=0;searchIter<100;searchIter++) {\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: searchIter=\" + searchIter);\n        }\n\n        final String searchTerm = contentStrings[random.nextInt(contentStrings.length)];\n        final boolean fillFields = random.nextBoolean();\n        final boolean getScores = random.nextBoolean();\n        final boolean getMaxScores = random.nextBoolean();\n        final Sort groupSort = getRandomSort();\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n\n        final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n        final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n        final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n        //final int groupOffset = 0;\n\n        final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random.nextBoolean();\n        final boolean doAllGroups = random.nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups);\n        }\n\n        final AllGroupsCollector allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = new AllGroupsCollector(\"group\");\n        } else {\n          allGroupsCollector = null;\n        }\n\n        final FirstPassGroupingCollector c1 = new FirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final boolean useWrappingCollector = random.nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random.nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n\n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(false, true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n            if (doAllGroups) {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n            }\n          }\n        }\n\n        final Collection<SearchGroup> topGroups = c1.getTopGroups(groupOffset, fillFields);\n        final TopGroups groupsResult;\n\n        if (topGroups != null) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n\n          final SecondPassGroupingCollector c2 = new SecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n          } else {\n            s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n          }\n\n          if (doAllGroups) {\n            TopGroups tempTopGroups = c2.getTopGroups(docOffset);\n            groupsResult = new TopGroups(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = c2.getTopGroups(docOffset);\n          }\n        } else {\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n\n        final TopGroups expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n        try {\n          // NOTE: intentional but temporary field cache insanity!\n          assertEquals(FieldCache.DEFAULT.getInts(r, \"id\"), expectedGroups, groupsResult);\n        } finally {\n          FieldCache.DEFAULT.purge(r);\n        }\n      }\n\n      r.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1fa60a501961bce2ff07ee1cde7c78699025547e","date":1307054117,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[] {\"a\", \"b\", \"c\", \"d\"};\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      final Directory dir2 = newDirectory();\n      final IndexReader r2 = getDocBlockReader(dir2, groupDocs);\n      final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n\n      final IndexSearcher s = new IndexSearcher(r);\n      final IndexSearcher s2 = new IndexSearcher(r2);\n\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n      try {\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = contentStrings[random.nextInt(contentStrings.length)];\n          final boolean fillFields = random.nextBoolean();\n          final boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 4;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc);\n                }\n              }\n            }\n          }\n          // NOTE: intentional but temporary field cache insanity!\n          assertEquals(docIDToID, expectedGroups, groupsResult, true);\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups<BytesRef>(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        FieldCache.DEFAULT.purge(r2);\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[] {\"a\", \"b\", \"c\", \"d\"};\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      final Directory dir2 = newDirectory();\n      final IndexReader r2 = getDocBlockReader(dir2, groupDocs);\n      final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n\n      final IndexSearcher s = new IndexSearcher(r);\n      final IndexSearcher s2 = new IndexSearcher(r2);\n\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n      try {\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = contentStrings[random.nextInt(contentStrings.length)];\n          final boolean fillFields = random.nextBoolean();\n          final boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 4;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups);\n          }\n\n          final AllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new AllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final FirstPassGroupingCollector c1 = new FirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final SecondPassGroupingCollector c2 = new SecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc);\n                }\n              }\n            }\n          }\n          // NOTE: intentional but temporary field cache insanity!\n          assertEquals(docIDToID, expectedGroups, groupsResult, true);\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final AllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new AllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          final TopGroups tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        FieldCache.DEFAULT.purge(r2);\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c715a0f99152be7566591f323c6c5a25725a1bcb","date":1307118449,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[] {\"a\", \"b\", \"c\", \"d\"};\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      final Directory dir2 = newDirectory();\n      final IndexReader r2 = getDocBlockReader(dir2, groupDocs);\n      final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n\n      final IndexSearcher s = new IndexSearcher(r);\n      final IndexSearcher s2 = new IndexSearcher(r2);\n\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n      try {\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = contentStrings[random.nextInt(contentStrings.length)];\n          final boolean fillFields = random.nextBoolean();\n          final boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 4;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc);\n                }\n              }\n            }\n          }\n          // NOTE: intentional but temporary field cache insanity!\n          assertEquals(docIDToID, expectedGroups, groupsResult, true);\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups<BytesRef>(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        FieldCache.DEFAULT.purge(r2);\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[] {\"a\", \"b\", \"c\", \"d\"};\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      final IndexSearcher s = new IndexSearcher(r);\n\n      for(int searchIter=0;searchIter<100;searchIter++) {\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: searchIter=\" + searchIter);\n        }\n\n        final String searchTerm = contentStrings[random.nextInt(contentStrings.length)];\n        final boolean fillFields = random.nextBoolean();\n        final boolean getScores = random.nextBoolean();\n        final boolean getMaxScores = random.nextBoolean();\n        final Sort groupSort = getRandomSort();\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n\n        final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n        final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n        final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n        //final int groupOffset = 0;\n\n        final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random.nextBoolean();\n        final boolean doAllGroups = random.nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups);\n        }\n\n        final AllGroupsCollector allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = new AllGroupsCollector(\"group\");\n        } else {\n          allGroupsCollector = null;\n        }\n\n        final FirstPassGroupingCollector c1 = new FirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final boolean useWrappingCollector = random.nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random.nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n\n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(false, true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n            if (doAllGroups) {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n            }\n          }\n        }\n\n        final Collection<SearchGroup> topGroups = c1.getTopGroups(groupOffset, fillFields);\n        final TopGroups groupsResult;\n\n        if (topGroups != null) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n\n          final SecondPassGroupingCollector c2 = new SecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n          } else {\n            s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n          }\n\n          if (doAllGroups) {\n            TopGroups tempTopGroups = c2.getTopGroups(docOffset);\n            groupsResult = new TopGroups(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = c2.getTopGroups(docOffset);\n          }\n        } else {\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n\n        final TopGroups expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n        try {\n          // NOTE: intentional but temporary field cache insanity!\n          assertEquals(FieldCache.DEFAULT.getInts(r, \"id\"), expectedGroups, groupsResult);\n        } finally {\n          FieldCache.DEFAULT.purge(r);\n        }\n      }\n\n      r.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6b861c0fdfa4d005c70848c9121655e9dc704f96","date":1307129511,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader r2 = null;\n      Directory dir2 = null;\n\n      try {\n        final IndexSearcher s = new IndexSearcher(r);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dir2 = newDirectory();\n        r2 = getDocBlockReader(dir2, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n        final IndexSearcher s2 = new IndexSearcher(r2);\n\n        // Reader2 only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<Float,Float> scoreMap = new HashMap<Float,Float>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + s2.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = s2.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToID2[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToID2[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID2[hit.doc]);\n            scoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 4;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, getScores);\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups<BytesRef>(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    groupDocsHits.groupSortValues[groupSortIDX] = scoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = scoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false, getScores);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (r2 != null) {\n          FieldCache.DEFAULT.purge(r2);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[] {\"a\", \"b\", \"c\", \"d\"};\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      final Directory dir2 = newDirectory();\n      final IndexReader r2 = getDocBlockReader(dir2, groupDocs);\n      final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n\n      final IndexSearcher s = new IndexSearcher(r);\n      final IndexSearcher s2 = new IndexSearcher(r2);\n\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n      try {\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = contentStrings[random.nextInt(contentStrings.length)];\n          final boolean fillFields = random.nextBoolean();\n          final boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 4;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc);\n                }\n              }\n            }\n          }\n          // NOTE: intentional but temporary field cache insanity!\n          assertEquals(docIDToID, expectedGroups, groupsResult, true);\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups<BytesRef>(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        FieldCache.DEFAULT.purge(r2);\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821","d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c03daa6ddcb4768a702115ec63799cab5fff3d92","date":1307140842,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader r2 = null;\n      Directory dir2 = null;\n\n      try {\n        final IndexSearcher s = new IndexSearcher(r);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dir2 = newDirectory();\n        r2 = getDocBlockReader(dir2, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n        final IndexSearcher s2 = new IndexSearcher(r2);\n\n        // Reader2 only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<Float,Float> scoreMap = new HashMap<Float,Float>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + s2.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = s2.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToID2[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToID2[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID2[hit.doc]);\n            scoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 4;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, getScores);\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups<BytesRef>(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    groupDocsHits.groupSortValues[groupSortIDX] = scoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = scoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false, getScores);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (r2 != null) {\n          FieldCache.DEFAULT.purge(r2);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[] {\"a\", \"b\", \"c\", \"d\"};\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      final Directory dir2 = newDirectory();\n      final IndexReader r2 = getDocBlockReader(dir2, groupDocs);\n      final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n\n      final IndexSearcher s = new IndexSearcher(r);\n      final IndexSearcher s2 = new IndexSearcher(r2);\n\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n      try {\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = contentStrings[random.nextInt(contentStrings.length)];\n          final boolean fillFields = random.nextBoolean();\n          final boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 4;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc);\n                }\n              }\n            }\n          }\n          // NOTE: intentional but temporary field cache insanity!\n          assertEquals(docIDToID, expectedGroups, groupsResult, true);\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups<BytesRef>(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        FieldCache.DEFAULT.purge(r2);\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d4d1b712a120ae5e1d1a6fddf8eaa6f642264ace","date":1307195917,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader r2 = null;\n      Directory dir2 = null;\n\n      try {\n        final IndexSearcher s = new IndexSearcher(r);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dir2 = newDirectory();\n        r2 = getDocBlockReader(dir2, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n        final IndexSearcher s2 = new IndexSearcher(r2);\n\n        // Reader2 only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + s2.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = s2.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToID2[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToID2[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToID2[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 4;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, getScores);\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups<BytesRef>(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false, getScores);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (r2 != null) {\n          FieldCache.DEFAULT.purge(r2);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader r2 = null;\n      Directory dir2 = null;\n\n      try {\n        final IndexSearcher s = new IndexSearcher(r);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dir2 = newDirectory();\n        r2 = getDocBlockReader(dir2, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n        final IndexSearcher s2 = new IndexSearcher(r2);\n\n        // Reader2 only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<Float,Float> scoreMap = new HashMap<Float,Float>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + s2.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = s2.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToID2[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToID2[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID2[hit.doc]);\n            scoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 4;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, getScores);\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups<BytesRef>(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    groupDocsHits.groupSortValues[groupSortIDX] = scoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = scoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false, getScores);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (r2 != null) {\n          FieldCache.DEFAULT.purge(r2);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1e7c99bd45fa88a3d93a03fdd773053bef72268e","date":1307218088,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader r2 = null;\n      Directory dir2 = null;\n\n      try {\n        final IndexSearcher s = new IndexSearcher(r);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dir2 = newDirectory();\n        r2 = getDocBlockReader(dir2, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n        final IndexSearcher s2 = new IndexSearcher(r2);\n\n        // Reader2 only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + s2.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = s2.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToID2[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToID2[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToID2[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 4;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, getScores);\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups<BytesRef>(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false, getScores);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (r2 != null) {\n          FieldCache.DEFAULT.purge(r2);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[] {\"a\", \"b\", \"c\", \"d\"};\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      final Directory dir2 = newDirectory();\n      final IndexReader r2 = getDocBlockReader(dir2, groupDocs);\n      final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n\n      final IndexSearcher s = new IndexSearcher(r);\n      final IndexSearcher s2 = new IndexSearcher(r2);\n\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n      try {\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = contentStrings[random.nextInt(contentStrings.length)];\n          final boolean fillFields = random.nextBoolean();\n          final boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 4;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups);\n          }\n\n          final AllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new AllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final FirstPassGroupingCollector c1 = new FirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final SecondPassGroupingCollector c2 = new SecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc);\n                }\n              }\n            }\n          }\n          // NOTE: intentional but temporary field cache insanity!\n          assertEquals(docIDToID, expectedGroups, groupsResult, true);\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final AllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new AllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          final TopGroups tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        FieldCache.DEFAULT.purge(r2);\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"77cf4379b2824f6ea34b091c495d6e95c38ff9e2","date":1307610475,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader r2 = null;\n      Directory dir2 = null;\n\n      try {\n        final IndexSearcher s = new IndexSearcher(r);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dir2 = newDirectory();\n        r2 = getDocBlockReader(dir2, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n        final IndexSearcher s2 = new IndexSearcher(r2);\n\n        // Reader2 only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + s2.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = s2.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToID2[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToID2[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToID2[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 4;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, getScores);\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups<BytesRef>(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false, getScores);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (r2 != null) {\n          FieldCache.DEFAULT.purge(r2);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader r2 = null;\n      Directory dir2 = null;\n\n      try {\n        final IndexSearcher s = new IndexSearcher(r);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dir2 = newDirectory();\n        r2 = getDocBlockReader(dir2, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n        final IndexSearcher s2 = new IndexSearcher(r2);\n\n        // Reader2 only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<Float,Float> scoreMap = new HashMap<Float,Float>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + s2.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = s2.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToID2[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToID2[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID2[hit.doc]);\n            scoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 4;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, getScores);\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups<BytesRef>(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    groupDocsHits.groupSortValues[groupSortIDX] = scoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = scoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false, getScores);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (r2 != null) {\n          FieldCache.DEFAULT.purge(r2);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d4aa50b979cd392e00e5bc0f23f78cbd106cb968","date":1308150768,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader r2 = null;\n      Directory dir2 = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dir2 = newDirectory();\n        r2 = getDocBlockReader(dir2, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n        final IndexSearcher s2 = newSearcher(r2);\n        final ShardState shards2 = new ShardState(s2);\n\n        // Reader2 only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + s2.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = s2.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToID2[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToID2[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToID2[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          // Search top reader:\n          final Query q = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(q, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups:\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for(SearchGroup<BytesRef> groupx : topGroups) {\n                System.out.println(\"    \" + groupToString(groupx.groupValue) + \" sort=\" + Arrays.toString(groupx.sortValues));\n              }\n            }\n          }\n          \n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, q, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores);\n\n          // Confirm merged shards match: \n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups<BytesRef>(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(s2, shards2.subSearchers, q, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false, true, true, getScores);\n          assertEquals(docIDToID2, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores);\n        }\n        s.close();\n        s2.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (r2 != null) {\n          FieldCache.DEFAULT.purge(r2);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader r2 = null;\n      Directory dir2 = null;\n\n      try {\n        final IndexSearcher s = new IndexSearcher(r);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dir2 = newDirectory();\n        r2 = getDocBlockReader(dir2, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n        final IndexSearcher s2 = new IndexSearcher(r2);\n\n        // Reader2 only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + s2.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = s2.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToID2[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToID2[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToID2[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 4;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, getScores);\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups<BytesRef>(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false, getScores);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (r2 != null) {\n          FieldCache.DEFAULT.purge(r2);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0a2903ea38ae3e636b93a08c52a5e37ae939cf6b","date":1308291005,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader r2 = null;\n      Directory dir2 = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dir2 = newDirectory();\n        r2 = getDocBlockReader(dir2, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n        final IndexSearcher s2 = newSearcher(r2);\n        final ShardState shards2 = new ShardState(s2);\n\n        // Reader2 only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + s2.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = s2.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToID2[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToID2[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToID2[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          // Search top reader:\n          final Query q = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(q, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups:\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for(SearchGroup<BytesRef> groupx : topGroups) {\n                System.out.println(\"    \" + groupToString(groupx.groupValue) + \" sort=\" + Arrays.toString(groupx.sortValues));\n              }\n            }\n          }\n          \n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, q, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores);\n\n          // Confirm merged shards match: \n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups<BytesRef>(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(s2, shards2.subSearchers, q, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false, true, true, getScores);\n          assertEquals(docIDToID2, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores);\n        }\n        s.close();\n        s2.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (r2 != null) {\n          FieldCache.DEFAULT.purge(r2);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader r2 = null;\n      Directory dir2 = null;\n\n      try {\n        final IndexSearcher s = new IndexSearcher(r);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dir2 = newDirectory();\n        r2 = getDocBlockReader(dir2, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n        final IndexSearcher s2 = new IndexSearcher(r2);\n\n        // Reader2 only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + s2.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = s2.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToID2[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToID2[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToID2[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 4;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          s.search(new TermQuery(new Term(\"content\", searchTerm)), c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, getScores);\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups<BytesRef>(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false, getScores);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (r2 != null) {\n          FieldCache.DEFAULT.purge(r2);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"090a0320e4de4a3674376aef96b9701f47564f86","date":1308707325,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader r2 = null;\n      Directory dir2 = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dir2 = newDirectory();\n        r2 = getDocBlockReader(dir2, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n        final IndexSearcher s2 = newSearcher(r2);\n        final ShardState shards2 = new ShardState(s2);\n\n        // Reader2 only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + s2.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = s2.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToID2[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToID2[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToID2[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          // Search top reader:\n          final Query q = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(q, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups:\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for(SearchGroup<BytesRef> groupx : topGroups) {\n                System.out.println(\"    \" + groupToString(groupx.groupValue) + \" sort=\" + Arrays.toString(groupx.sortValues));\n              }\n            }\n          }\n          \n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, q, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores);\n\n          // Confirm merged shards match: \n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups<BytesRef>(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(s2, shards2.subSearchers, q, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false, true, true, getScores);\n          assertEquals(docIDToID2, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores);\n        }\n        s.close();\n        s2.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (r2 != null) {\n          FieldCache.DEFAULT.purge(r2);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader r2 = null;\n      Directory dir2 = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dir2 = newDirectory();\n        r2 = getDocBlockReader(dir2, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n        final IndexSearcher s2 = newSearcher(r2);\n        final ShardState shards2 = new ShardState(s2);\n\n        // Reader2 only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + s2.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = s2.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToID2[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToID2[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToID2[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          // Search top reader:\n          final Query q = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(q, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups:\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for(SearchGroup<BytesRef> groupx : topGroups) {\n                System.out.println(\"    \" + groupToString(groupx.groupValue) + \" sort=\" + Arrays.toString(groupx.sortValues));\n              }\n            }\n          }\n          \n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, q, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores);\n\n          // Confirm merged shards match: \n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups<BytesRef>(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(s2, shards2.subSearchers, q, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false, true, true, getScores);\n          assertEquals(docIDToID2, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores);\n        }\n        s.close();\n        s2.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (r2 != null) {\n          FieldCache.DEFAULT.purge(r2);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2553b00f699380c64959ccb27991289aae87be2e","date":1309290151,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader r2 = null;\n      Directory dir2 = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dir2 = newDirectory();\n        r2 = getDocBlockReader(dir2, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n        final IndexSearcher s2 = newSearcher(r2);\n        final ShardState shards2 = new ShardState(s2);\n\n        // Reader2 only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + s2.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = s2.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToID2[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToID2[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToID2[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          // Search top reader:\n          final Query q = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(q, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups:\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for(SearchGroup<BytesRef> groupx : topGroups) {\n                System.out.println(\"    \" + groupToString(groupx.groupValue) + \" sort=\" + Arrays.toString(groupx.sortValues));\n              }\n            }\n          }\n          \n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, q, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores);\n\n          // Confirm merged shards match: \n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups<BytesRef>(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(s2, shards2.subSearchers, q, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false, true, true, getScores);\n          assertEquals(docIDToID2, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores);\n        }\n        s.close();\n        s2.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (r2 != null) {\n          FieldCache.DEFAULT.purge(r2);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader r2 = null;\n      Directory dir2 = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dir2 = newDirectory();\n        r2 = getDocBlockReader(dir2, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n        final IndexSearcher s2 = newSearcher(r2);\n        final ShardState shards2 = new ShardState(s2);\n\n        // Reader2 only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + s2.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = s2.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToID2[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToID2[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToID2[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          // Search top reader:\n          final Query q = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(q, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups:\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for(SearchGroup<BytesRef> groupx : topGroups) {\n                System.out.println(\"    \" + groupToString(groupx.groupValue) + \" sort=\" + Arrays.toString(groupx.sortValues));\n              }\n            }\n          }\n          \n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, q, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores);\n\n          // Confirm merged shards match: \n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups<BytesRef>(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(s2, shards2.subSearchers, q, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false, true, true, getScores);\n          assertEquals(docIDToID2, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores);\n        }\n        s.close();\n        s2.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (r2 != null) {\n          FieldCache.DEFAULT.purge(r2);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader r2 = null;\n      Directory dir2 = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dir2 = newDirectory();\n        r2 = getDocBlockReader(dir2, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n        final IndexSearcher s2 = newSearcher(r2);\n        final ShardState shards2 = new ShardState(s2);\n\n        // Reader2 only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + s2.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = s2.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToID2[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToID2[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToID2[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          // Search top reader:\n          final Query q = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(q, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups:\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for(SearchGroup<BytesRef> groupx : topGroups) {\n                System.out.println(\"    \" + groupToString(groupx.groupValue) + \" sort=\" + Arrays.toString(groupx.sortValues));\n              }\n            }\n          }\n          \n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, q, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores);\n\n          // Confirm merged shards match: \n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups<BytesRef>(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(s2, shards2.subSearchers, q, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false, true, true, getScores);\n          assertEquals(docIDToID2, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores);\n        }\n        s.close();\n        s2.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (r2 != null) {\n          FieldCache.DEFAULT.purge(r2);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader r2 = null;\n      Directory dir2 = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dir2 = newDirectory();\n        r2 = getDocBlockReader(dir2, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n        final IndexSearcher s2 = newSearcher(r2);\n        final ShardState shards2 = new ShardState(s2);\n\n        // Reader2 only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + s2.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = s2.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToID2[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToID2[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToID2[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          // Search top reader:\n          final Query q = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(q, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups:\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for(SearchGroup<BytesRef> groupx : topGroups) {\n                System.out.println(\"    \" + groupToString(groupx.groupValue) + \" sort=\" + Arrays.toString(groupx.sortValues));\n              }\n            }\n          }\n          \n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, q, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores);\n\n          // Confirm merged shards match: \n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups<BytesRef>(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(s2, shards2.subSearchers, q, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false, true, true, getScores);\n          assertEquals(docIDToID2, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores);\n        }\n        s.close();\n        s2.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (r2 != null) {\n          FieldCache.DEFAULT.purge(r2);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader r2 = null;\n      Directory dir2 = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dir2 = newDirectory();\n        r2 = getDocBlockReader(dir2, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n        final IndexSearcher s2 = newSearcher(r2);\n        final ShardState shards2 = new ShardState(s2);\n\n        // Reader2 only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + s2.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = s2.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToID2[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToID2[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToID2[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          // Search top reader:\n          final Query q = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(q, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups:\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for(SearchGroup<BytesRef> groupx : topGroups) {\n                System.out.println(\"    \" + groupToString(groupx.groupValue) + \" sort=\" + Arrays.toString(groupx.sortValues));\n              }\n            }\n          }\n          \n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, q, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores);\n\n          // Confirm merged shards match: \n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups<BytesRef>(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(s2, shards2.subSearchers, q, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false, true, true, getScores);\n          assertEquals(docIDToID2, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores);\n        }\n        s.close();\n        s2.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (r2 != null) {\n          FieldCache.DEFAULT.purge(r2);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", Field.Index.NOT_ANALYZED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", Field.Index.ANALYZED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader r2 = null;\n      Directory dir2 = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dir2 = newDirectory();\n        r2 = getDocBlockReader(dir2, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n        final IndexSearcher s2 = newSearcher(r2);\n        final ShardState shards2 = new ShardState(s2);\n\n        // Reader2 only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + s2.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = s2.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToID2[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToID2[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToID2[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          // Search top reader:\n          final Query q = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(q, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups:\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for(SearchGroup<BytesRef> groupx : topGroups) {\n                System.out.println(\"    \" + groupToString(groupx.groupValue) + \" sort=\" + Arrays.toString(groupx.sortValues));\n              }\n            }\n          }\n          \n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, q, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores);\n\n          // Confirm merged shards match: \n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups<BytesRef>(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(s2, shards2.subSearchers, q, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false, true, true, getScores);\n          assertEquals(docIDToID2, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores);\n        }\n        s.close();\n        s2.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (r2 != null) {\n          FieldCache.DEFAULT.purge(r2);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7cb194976386e349893169fee3c2aa6de3a83fd1","date":1317819143,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\");\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n          \n          // Get 1st pass top groups using shards\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          final TopGroups<BytesRef> groupsResult;\n          if (topGroups != null) {\n\n            // Get 2nd pass grouped result:\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n              \n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores);\n\n          // Confirm merged shards match: \n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores);\n        }\n        s.close();\n        sBlocks.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader r2 = null;\n      Directory dir2 = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dir2 = newDirectory();\n        r2 = getDocBlockReader(dir2, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToID2 = FieldCache.DEFAULT.getInts(r2, \"id\");\n\n        final IndexSearcher s2 = newSearcher(r2);\n        final ShardState shards2 = new ShardState(s2);\n\n        // Reader2 only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + s2.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = s2.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToID2[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToID2[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToID2[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          // Search top reader:\n          final Query q = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(q, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c1);\n              if (doAllGroups) {\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), allGroupsCollector);\n              }\n            }\n          }\n\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n          final TopGroups groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups:\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for(SearchGroup<BytesRef> groupx : topGroups) {\n                System.out.println(\"    \" + groupToString(groupx.groupValue) + \" sort=\" + Arrays.toString(groupx.sortValues));\n              }\n            }\n          }\n          \n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, q, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n              }\n            } else {\n              s.search(new TermQuery(new Term(\"content\", searchTerm)), c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups\");\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()));\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores);\n\n          // Confirm merged shards match: \n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          s2.search(new TermQuery(new Term(\"content\", searchTerm)), c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroups2 = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups groupsResult2;\n          if (doAllGroups && tempTopGroups2 != null) {\n            assertEquals((int) tempTopGroups2.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResult2 = new TopGroups<BytesRef>(tempTopGroups2, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResult2 = tempTopGroups2;\n          }\n\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(s2, shards2.subSearchers, q, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID2, expectedGroups, groupsResult2, false, true, true, getScores);\n          assertEquals(docIDToID2, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores);\n        }\n        s.close();\n        s2.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (r2 != null) {\n          FieldCache.DEFAULT.purge(r2);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      r2.close();\n      dir2.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821","d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8b48c85d1bf438ef65fbc1abe44f4e2c04a43e00","date":1317931776,"type":3,"author":"Martijn van Groningen","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\");\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n          \n          // Get 1st pass top groups using shards\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n              \n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores);\n\n          // Confirm merged shards match: \n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores);\n        }\n        s.close();\n        sBlocks.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\");\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final TermAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = new TermAllGroupsCollector(\"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final TermFirstPassGroupingCollector c1 = new TermFirstPassGroupingCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n        \n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = c1.getTopGroups(groupOffset, fillFields);\n\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n          \n          // Get 1st pass top groups using shards\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          final TopGroups<BytesRef> groupsResult;\n          if (topGroups != null) {\n\n            // Get 2nd pass grouped result:\n            final TermSecondPassGroupingCollector c2 = new TermSecondPassGroupingCollector(\"group\", topGroups, groupSort, docSort, docOffset+docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = c2.getTopGroups(docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = c2.getTopGroups(docOffset);\n            }\n          } else {\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n              \n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores);\n\n          // Confirm merged shards match: \n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores);\n        }\n        s.close();\n        sBlocks.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821","d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f4c236b7b2d1cb61683f5ba02d09249fdb42a7fd","date":1320399315,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\");\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n          \n          // Get 1st pass top groups using shards\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n              \n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores);\n\n          // Confirm merged shards match: \n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores);\n        }\n        s.close();\n        sBlocks.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\");\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n          \n          // Get 1st pass top groups using shards\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n              \n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores);\n\n          // Confirm merged shards match: \n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores);\n        }\n        s.close();\n        sBlocks.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"382fe3a6ca9745891afebda9b9a57cc158305545","date":1320952430,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n          \n          // Get 1st pass top groups using shards\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n              \n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores);\n\n          // Confirm merged shards match: \n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores);\n        }\n        s.close();\n        sBlocks.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\");\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\");\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n          \n          // Get 1st pass top groups using shards\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n              \n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores);\n\n          // Confirm merged shards match: \n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores);\n        }\n        s.close();\n        sBlocks.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4739c84c362b9673ab5ed3e038ff760c718c30c8","date":1322161679,"type":3,"author":"Martijn van Groningen","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n//          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n          randomValue = _TestUtil.randomSimpleString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      IndexDocValuesField idvGroupField = new IndexDocValuesField(\"group\");\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytes(new BytesRef(groupDoc.group), ValueType.BYTES_VAR_SORTED);\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n        s.close();\n        sBlocks.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    for(int iter=0;iter<3;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        groups.add(new BytesRef(_TestUtil.randomRealisticUnicodeString(random)));\n        //groups.add(new BytesRef(_TestUtil.randomSimpleString(random)));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\" + random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n        \n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);              \n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);              \n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n        \n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n          \n          // Get 1st pass top groups using shards\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n              \n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores);\n\n          // Confirm merged shards match: \n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query, groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores);\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores);\n        }\n        s.close();\n        sBlocks.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821","d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"49bb0892656968105a772d1ae683bc9757260889","date":1322227723,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n//          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n          randomValue = _TestUtil.randomSimpleString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      IndexDocValuesField idvGroupField = new IndexDocValuesField(\"group\");\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytes(BytesRef.deepCopyOf(groupDoc.group), ValueType.BYTES_VAR_SORTED);\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n        s.close();\n        sBlocks.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n//          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n          randomValue = _TestUtil.randomSimpleString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      IndexDocValuesField idvGroupField = new IndexDocValuesField(\"group\");\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytes(new BytesRef(groupDoc.group), ValueType.BYTES_VAR_SORTED);\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n        s.close();\n        sBlocks.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ac5f46eec1c9387389e11f06fe4f207dd18734ca","date":1323364699,"type":3,"author":"Martijn van Groningen","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      IndexDocValuesField idvGroupField = new IndexDocValuesField(\"group\");\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytes(BytesRef.deepCopyOf(groupDoc.group), ValueType.BYTES_VAR_SORTED);\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n        s.close();\n        sBlocks.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n//          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n          randomValue = _TestUtil.randomSimpleString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      IndexDocValuesField idvGroupField = new IndexDocValuesField(\"group\");\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytes(BytesRef.deepCopyOf(groupDoc.group), ValueType.BYTES_VAR_SORTED);\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n        s.close();\n        sBlocks.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"efc85580874d26ca9617f9ad61628d56f6859762","date":1323367273,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      IndexDocValuesField idvGroupField = new IndexDocValuesField(\"group\");\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytes(BytesRef.deepCopyOf(groupDoc.group), ValueType.BYTES_VAR_SORTED);\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n        s.close();\n        sBlocks.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n//          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n          randomValue = _TestUtil.randomSimpleString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      IndexDocValuesField idvGroupField = new IndexDocValuesField(\"group\");\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytes(BytesRef.deepCopyOf(groupDoc.group), ValueType.BYTES_VAR_SORTED);\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n        s.close();\n        sBlocks.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","date":1323437438,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      IndexDocValuesField idvGroupField = new IndexDocValuesField(\"group\");\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytes(BytesRef.deepCopyOf(groupDoc.group), ValueType.BYTES_VAR_SORTED);\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n        s.close();\n        sBlocks.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n//          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n          randomValue = _TestUtil.randomSimpleString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      IndexDocValuesField idvGroupField = new IndexDocValuesField(\"group\");\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytes(BytesRef.deepCopyOf(groupDoc.group), ValueType.BYTES_VAR_SORTED);\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n        s.close();\n        sBlocks.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1e59c344a45b9502f40ec44f5fe4e20ed2291dbe","date":1323449025,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\");\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytes(BytesRef.deepCopyOf(groupDoc.group), Type.BYTES_VAR_SORTED);\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n        s.close();\n        sBlocks.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      IndexDocValuesField idvGroupField = new IndexDocValuesField(\"group\");\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytes(BytesRef.deepCopyOf(groupDoc.group), ValueType.BYTES_VAR_SORTED);\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n        s.close();\n        sBlocks.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["fa0f44f887719e97183771e977cfc4bfb485b766"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0e7c2454a6a8237bfd0e953f5b940838408c9055","date":1323649300,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      IndexDocValuesField idvGroupField = new IndexDocValuesField(\"group\");\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytes(BytesRef.deepCopyOf(groupDoc.group), ValueType.BYTES_VAR_SORTED);\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      IndexDocValuesField idvGroupField = new IndexDocValuesField(\"group\");\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytes(BytesRef.deepCopyOf(groupDoc.group), ValueType.BYTES_VAR_SORTED);\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n        s.close();\n        sBlocks.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","date":1323720782,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\");\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytes(BytesRef.deepCopyOf(groupDoc.group), Type.BYTES_VAR_SORTED);\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\");\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytes(BytesRef.deepCopyOf(groupDoc.group), Type.BYTES_VAR_SORTED);\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n        s.close();\n        sBlocks.close();\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d638301ad1cfcae567b681b893bc8781f0ee48a5","date":1323801546,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\");\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytes(BytesRef.deepCopyOf(groupDoc.group), Type.BYTES_VAR_SORTED);\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      IndexDocValuesField idvGroupField = new IndexDocValuesField(\"group\");\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytes(BytesRef.deepCopyOf(groupDoc.group), ValueType.BYTES_VAR_SORTED);\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fa0f44f887719e97183771e977cfc4bfb485b766","date":1326668713,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\");\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\");\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytes(BytesRef.deepCopyOf(groupDoc.group), Type.BYTES_VAR_SORTED);\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":["307cff5af2b00f126fdf9d3435b75d5ed4d0f402","1e59c344a45b9502f40ec44f5fe4e20ed2291dbe"],"bugIntro":["a78a90fc9701e511308346ea29f4f5e548bb39fe","cd659803551ebd8ca09b9e4ad7abd18d3d558f9d"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2dd6ecb8250c497ed227653279d6a4f470bfbb31","date":1326814483,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowMultiReaderWrapper(r), \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowMultiReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(r, \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(rBlocks, \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7528ec8c6e88061e2e6af98c4ae1f72a30f180b2","date":1327854270,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        // nocommit FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          // nocommit FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowMultiReaderWrapper(r), \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowMultiReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8cd232a36e691d3af7035460733f822fb1a5239a","date":1327962711,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        // TODO: FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          // TODO: FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        // nocommit FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          // nocommit FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        // TODO: FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          // TODO: FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final IndexReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowMultiReaderWrapper(r), \"id\", false);\n      IndexReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowMultiReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8fa38e5ecc85303dce7ded93b3cc9a48b3d546d9","date":1327969999,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        // TODO: FieldCache.DEFAULT.purge(r);\n        if (rBlocks != null) {\n          // TODO: FieldCache.DEFAULT.purge(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a78a90fc9701e511308346ea29f4f5e548bb39fe","date":1329489995,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericField id = new NumericField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setValue(groupDoc.sort1.utf8ToString());\n        sort2.setValue(groupDoc.sort2.utf8ToString());\n        content.setValue(groupDoc.content);\n        id.setValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":["307cff5af2b00f126fdf9d3435b75d5ed4d0f402","fa0f44f887719e97183771e977cfc4bfb485b766"],"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6b588d7000deacb0a01f30746b91644112b94326","date":1331201456,"type":3,"author":"Martijn van Groningen","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6005b05c19356dfca18f39979caeeb6b85bc88bb","date":1331204804,"type":3,"author":"Martijn van Groningen","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821","d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"38e3b736c7ca086d61b7dbb841c905ee115490da","date":1331657018,"type":3,"author":"Ryan McKinley","isMerge":true,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings(\"unchecked\")\n          final TopGroups<BytesRef> tempTopGroupsBlocks = c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random, 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random, 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random, 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random);\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random, 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random.nextInt(3)).append(' ');\n        final int fakeCount = random.nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random,\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random)));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random.nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random.nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random.nextInt(groups.size())),\n                                               groups.get(random.nextInt(groups.size())),\n                                               contentStrings[random.nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random.nextInt(3);\n          final boolean fillFields = random.nextBoolean();\n          boolean getScores = random.nextBoolean();\n          final boolean getMaxScores = random.nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random, 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random, 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random, 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random, 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random.nextBoolean();\n          final boolean doAllGroups = random.nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random.nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random.nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"3ce36a160d1241ae9c70e109dc3fdfdfb009674a":["669ab70ac675a9fdd757b28a1a6ca63f667c7188"],"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":["49bb0892656968105a772d1ae683bc9757260889","ac5f46eec1c9387389e11f06fe4f207dd18734ca"],"4739c84c362b9673ab5ed3e038ff760c718c30c8":["382fe3a6ca9745891afebda9b9a57cc158305545"],"d62f5453a200cec2cbb60148de159dbf55591e9d":["a3776dccca01c11e7046323cfad46a3b4a471233","3ce36a160d1241ae9c70e109dc3fdfdfb009674a"],"382fe3a6ca9745891afebda9b9a57cc158305545":["f4c236b7b2d1cb61683f5ba02d09249fdb42a7fd"],"0a2903ea38ae3e636b93a08c52a5e37ae939cf6b":["1e7c99bd45fa88a3d93a03fdd773053bef72268e","d4aa50b979cd392e00e5bc0f23f78cbd106cb968"],"7cb194976386e349893169fee3c2aa6de3a83fd1":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"7528ec8c6e88061e2e6af98c4ae1f72a30f180b2":["2dd6ecb8250c497ed227653279d6a4f470bfbb31"],"0e7c2454a6a8237bfd0e953f5b940838408c9055":["ac5f46eec1c9387389e11f06fe4f207dd18734ca"],"6b588d7000deacb0a01f30746b91644112b94326":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["090a0320e4de4a3674376aef96b9701f47564f86"],"c03daa6ddcb4768a702115ec63799cab5fff3d92":["c715a0f99152be7566591f323c6c5a25725a1bcb","6b861c0fdfa4d005c70848c9121655e9dc704f96"],"c5f61d6a2927b52517a31a8bf022549d33b1dfec":["307cff5af2b00f126fdf9d3435b75d5ed4d0f402"],"38e3b736c7ca086d61b7dbb841c905ee115490da":["a78a90fc9701e511308346ea29f4f5e548bb39fe","6005b05c19356dfca18f39979caeeb6b85bc88bb"],"8fa38e5ecc85303dce7ded93b3cc9a48b3d546d9":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"d4aa50b979cd392e00e5bc0f23f78cbd106cb968":["d4d1b712a120ae5e1d1a6fddf8eaa6f642264ace"],"2553b00f699380c64959ccb27991289aae87be2e":["0a2903ea38ae3e636b93a08c52a5e37ae939cf6b","090a0320e4de4a3674376aef96b9701f47564f86"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["d4aa50b979cd392e00e5bc0f23f78cbd106cb968","090a0320e4de4a3674376aef96b9701f47564f86"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"d4d1b712a120ae5e1d1a6fddf8eaa6f642264ace":["6b861c0fdfa4d005c70848c9121655e9dc704f96"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","669ab70ac675a9fdd757b28a1a6ca63f667c7188"],"1e59c344a45b9502f40ec44f5fe4e20ed2291dbe":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":["c03daa6ddcb4768a702115ec63799cab5fff3d92","d4d1b712a120ae5e1d1a6fddf8eaa6f642264ace"],"49bb0892656968105a772d1ae683bc9757260889":["4739c84c362b9673ab5ed3e038ff760c718c30c8"],"5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a":["c3a8a449466c1ff7ce2274fe73dab487256964b4","669ab70ac675a9fdd757b28a1a6ca63f667c7188"],"6005b05c19356dfca18f39979caeeb6b85bc88bb":["6b588d7000deacb0a01f30746b91644112b94326"],"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":["1e59c344a45b9502f40ec44f5fe4e20ed2291dbe","0e7c2454a6a8237bfd0e953f5b940838408c9055"],"c715a0f99152be7566591f323c6c5a25725a1bcb":["5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a","1fa60a501961bce2ff07ee1cde7c78699025547e"],"1fa60a501961bce2ff07ee1cde7c78699025547e":["3ce36a160d1241ae9c70e109dc3fdfdfb009674a"],"d638301ad1cfcae567b681b893bc8781f0ee48a5":["0e7c2454a6a8237bfd0e953f5b940838408c9055","93ccd971aca7fb61b7f1b946e44714cfc80bfc7c"],"a78a90fc9701e511308346ea29f4f5e548bb39fe":["8fa38e5ecc85303dce7ded93b3cc9a48b3d546d9"],"6703e55954db440ab8a2bc2a615c4fa9f66b602b":["c5f61d6a2927b52517a31a8bf022549d33b1dfec"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"fa0f44f887719e97183771e977cfc4bfb485b766":["d638301ad1cfcae567b681b893bc8781f0ee48a5"],"ac5f46eec1c9387389e11f06fe4f207dd18734ca":["49bb0892656968105a772d1ae683bc9757260889"],"efc85580874d26ca9617f9ad61628d56f6859762":["49bb0892656968105a772d1ae683bc9757260889","ac5f46eec1c9387389e11f06fe4f207dd18734ca"],"f4c236b7b2d1cb61683f5ba02d09249fdb42a7fd":["8b48c85d1bf438ef65fbc1abe44f4e2c04a43e00"],"6b861c0fdfa4d005c70848c9121655e9dc704f96":["1fa60a501961bce2ff07ee1cde7c78699025547e"],"8cd232a36e691d3af7035460733f822fb1a5239a":["7528ec8c6e88061e2e6af98c4ae1f72a30f180b2"],"8b48c85d1bf438ef65fbc1abe44f4e2c04a43e00":["7cb194976386e349893169fee3c2aa6de3a83fd1"],"2dd6ecb8250c497ed227653279d6a4f470bfbb31":["fa0f44f887719e97183771e977cfc4bfb485b766"],"090a0320e4de4a3674376aef96b9701f47564f86":["d4aa50b979cd392e00e5bc0f23f78cbd106cb968"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["2dd6ecb8250c497ed227653279d6a4f470bfbb31","8cd232a36e691d3af7035460733f822fb1a5239a"],"1e7c99bd45fa88a3d93a03fdd773053bef72268e":["d62f5453a200cec2cbb60148de159dbf55591e9d","d4d1b712a120ae5e1d1a6fddf8eaa6f642264ace"],"c3a8a449466c1ff7ce2274fe73dab487256964b4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","6703e55954db440ab8a2bc2a615c4fa9f66b602b"],"307cff5af2b00f126fdf9d3435b75d5ed4d0f402":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["6005b05c19356dfca18f39979caeeb6b85bc88bb"],"669ab70ac675a9fdd757b28a1a6ca63f667c7188":["6703e55954db440ab8a2bc2a615c4fa9f66b602b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b89678825b68eccaf09e6ab71675fc0b0af1e099"]},"commit2Childs":{"3ce36a160d1241ae9c70e109dc3fdfdfb009674a":["d62f5453a200cec2cbb60148de159dbf55591e9d","1fa60a501961bce2ff07ee1cde7c78699025547e"],"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":["1e59c344a45b9502f40ec44f5fe4e20ed2291dbe"],"4739c84c362b9673ab5ed3e038ff760c718c30c8":["49bb0892656968105a772d1ae683bc9757260889"],"d62f5453a200cec2cbb60148de159dbf55591e9d":["1e7c99bd45fa88a3d93a03fdd773053bef72268e"],"382fe3a6ca9745891afebda9b9a57cc158305545":["4739c84c362b9673ab5ed3e038ff760c718c30c8"],"0a2903ea38ae3e636b93a08c52a5e37ae939cf6b":["2553b00f699380c64959ccb27991289aae87be2e"],"7cb194976386e349893169fee3c2aa6de3a83fd1":["8b48c85d1bf438ef65fbc1abe44f4e2c04a43e00"],"7528ec8c6e88061e2e6af98c4ae1f72a30f180b2":["8cd232a36e691d3af7035460733f822fb1a5239a"],"0e7c2454a6a8237bfd0e953f5b940838408c9055":["93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","d638301ad1cfcae567b681b893bc8781f0ee48a5"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["7cb194976386e349893169fee3c2aa6de3a83fd1"],"6b588d7000deacb0a01f30746b91644112b94326":["6005b05c19356dfca18f39979caeeb6b85bc88bb"],"c03daa6ddcb4768a702115ec63799cab5fff3d92":["77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"c5f61d6a2927b52517a31a8bf022549d33b1dfec":["6703e55954db440ab8a2bc2a615c4fa9f66b602b"],"38e3b736c7ca086d61b7dbb841c905ee115490da":[],"8fa38e5ecc85303dce7ded93b3cc9a48b3d546d9":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"d4aa50b979cd392e00e5bc0f23f78cbd106cb968":["0a2903ea38ae3e636b93a08c52a5e37ae939cf6b","d083e83f225b11e5fdd900e83d26ddb385b6955c","090a0320e4de4a3674376aef96b9701f47564f86"],"2553b00f699380c64959ccb27991289aae87be2e":[],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d4d1b712a120ae5e1d1a6fddf8eaa6f642264ace":["d4aa50b979cd392e00e5bc0f23f78cbd106cb968","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","1e7c99bd45fa88a3d93a03fdd773053bef72268e"],"a3776dccca01c11e7046323cfad46a3b4a471233":["d62f5453a200cec2cbb60148de159dbf55591e9d"],"1e59c344a45b9502f40ec44f5fe4e20ed2291dbe":["93ccd971aca7fb61b7f1b946e44714cfc80bfc7c"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":[],"49bb0892656968105a772d1ae683bc9757260889":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","ac5f46eec1c9387389e11f06fe4f207dd18734ca","efc85580874d26ca9617f9ad61628d56f6859762"],"5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a":["c715a0f99152be7566591f323c6c5a25725a1bcb"],"6005b05c19356dfca18f39979caeeb6b85bc88bb":["38e3b736c7ca086d61b7dbb841c905ee115490da","629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":["d638301ad1cfcae567b681b893bc8781f0ee48a5"],"c715a0f99152be7566591f323c6c5a25725a1bcb":["c03daa6ddcb4768a702115ec63799cab5fff3d92"],"1fa60a501961bce2ff07ee1cde7c78699025547e":["c715a0f99152be7566591f323c6c5a25725a1bcb","6b861c0fdfa4d005c70848c9121655e9dc704f96"],"d638301ad1cfcae567b681b893bc8781f0ee48a5":["fa0f44f887719e97183771e977cfc4bfb485b766"],"a78a90fc9701e511308346ea29f4f5e548bb39fe":["6b588d7000deacb0a01f30746b91644112b94326","38e3b736c7ca086d61b7dbb841c905ee115490da"],"6703e55954db440ab8a2bc2a615c4fa9f66b602b":["c3a8a449466c1ff7ce2274fe73dab487256964b4","669ab70ac675a9fdd757b28a1a6ca63f667c7188"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a3776dccca01c11e7046323cfad46a3b4a471233","c3a8a449466c1ff7ce2274fe73dab487256964b4","307cff5af2b00f126fdf9d3435b75d5ed4d0f402"],"fa0f44f887719e97183771e977cfc4bfb485b766":["2dd6ecb8250c497ed227653279d6a4f470bfbb31"],"ac5f46eec1c9387389e11f06fe4f207dd18734ca":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","0e7c2454a6a8237bfd0e953f5b940838408c9055","efc85580874d26ca9617f9ad61628d56f6859762"],"efc85580874d26ca9617f9ad61628d56f6859762":[],"f4c236b7b2d1cb61683f5ba02d09249fdb42a7fd":["382fe3a6ca9745891afebda9b9a57cc158305545"],"6b861c0fdfa4d005c70848c9121655e9dc704f96":["c03daa6ddcb4768a702115ec63799cab5fff3d92","d4d1b712a120ae5e1d1a6fddf8eaa6f642264ace"],"8cd232a36e691d3af7035460733f822fb1a5239a":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"8b48c85d1bf438ef65fbc1abe44f4e2c04a43e00":["f4c236b7b2d1cb61683f5ba02d09249fdb42a7fd"],"1e7c99bd45fa88a3d93a03fdd773053bef72268e":["0a2903ea38ae3e636b93a08c52a5e37ae939cf6b"],"2dd6ecb8250c497ed227653279d6a4f470bfbb31":["7528ec8c6e88061e2e6af98c4ae1f72a30f180b2","5cab9a86bd67202d20b6adc463008c8e982b070a"],"090a0320e4de4a3674376aef96b9701f47564f86":["1509f151d7692d84fae414b2b799ac06ba60fcb4","2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["8fa38e5ecc85303dce7ded93b3cc9a48b3d546d9"],"c3a8a449466c1ff7ce2274fe73dab487256964b4":["5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a"],"307cff5af2b00f126fdf9d3435b75d5ed4d0f402":["c5f61d6a2927b52517a31a8bf022549d33b1dfec"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"669ab70ac675a9fdd757b28a1a6ca63f667c7188":["3ce36a160d1241ae9c70e109dc3fdfdfb009674a","a3776dccca01c11e7046323cfad46a3b4a471233","5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["38e3b736c7ca086d61b7dbb841c905ee115490da","2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","efc85580874d26ca9617f9ad61628d56f6859762","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}