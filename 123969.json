{"path":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldHighlighter#highlightOffsetsEnums(OffsetsEnum).mjava","commits":[{"id":"8764ca7bb74ee716c839b9545a93ec4a578c2005","date":1517564468,"type":1,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldHighlighter#highlightOffsetsEnums(OffsetsEnum).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldHighlighter#highlightOffsetsEnums(List[OffsetsEnum]).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  protected Passage[] highlightOffsetsEnums(OffsetsEnum off)\n      throws IOException {\n\n    final int contentLength = this.breakIterator.getText().getEndIndex();\n\n    if (off.nextPosition() == false) {\n      return new Passage[0];\n    }\n\n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(Math.min(64, maxPassages + 1), (left, right) -> {\n      if (left.getScore() < right.getScore()) {\n        return -1;\n      } else if (left.getScore() > right.getScore()) {\n        return 1;\n      } else {\n        return left.getStartOffset() - right.getStartOffset();\n      }\n    });\n    Passage passage = new Passage(); // the current passage in-progress.  Will either get reset or added to queue.\n\n    do {\n      int start = off.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = off.endOffset();\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      // See if this term should be part of a new passage.\n      if (start >= passage.getEndOffset()) {\n        passage = maybeAddPassage(passageQueue, passageScorer, passage, contentLength);\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          break;\n        }\n        // advance breakIterator\n        passage.setStartOffset(Math.max(this.breakIterator.preceding(start + 1), 0));\n        passage.setEndOffset(Math.min(this.breakIterator.following(start), contentLength));\n      }\n      // Add this term to the passage.\n      BytesRef term = off.getTerm();// a reference; safe to refer to\n      assert term != null;\n      passage.addMatch(start, end, term, off.freq());\n    } while (off.nextPosition());\n    maybeAddPassage(passageQueue, passageScorer, passage, contentLength);\n\n    Passage[] passages = passageQueue.toArray(new Passage[passageQueue.size()]);\n    // sort in ascending order\n    Arrays.sort(passages, Comparator.comparingInt(Passage::getStartOffset));\n    return passages;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  protected Passage[] highlightOffsetsEnums(List<OffsetsEnum> offsetsEnums)\n      throws IOException {\n    PassageScorer scorer = passageScorer;\n    BreakIterator breakIterator = this.breakIterator;\n    final int contentLength = breakIterator.getText().getEndIndex();\n\n    //TODO consider moving this part to an aggregate OffsetsEnum subclass so we have one enum that already has its weight\n    PriorityQueue<OffsetsEnum> offsetsEnumQueue = new PriorityQueue<>(offsetsEnums.size() + 1);\n    for (OffsetsEnum off : offsetsEnums) {\n      off.setWeight(scorer.weight(contentLength, off.freq()));\n      if (off.nextPosition()) {// go to first position\n        offsetsEnumQueue.add(off);\n      }\n    }\n    offsetsEnumQueue.add(new OffsetsEnum.OfPostings(new BytesRef(), EMPTY)); // a sentinel for termination\n\n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(Math.min(64, maxPassages + 1), (left, right) -> {\n      if (left.getScore() < right.getScore()) {\n        return -1;\n      } else if (left.getScore() > right.getScore()) {\n        return 1;\n      } else {\n        return left.getStartOffset() - right.getStartOffset();\n      }\n    });\n    Passage passage = new Passage(); // the current passage in-progress.  Will either get reset or added to queue.\n\n    OffsetsEnum off;\n    while ((off = offsetsEnumQueue.poll()) != null) {\n      int start = off.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = off.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid\n      // hits may exist (they are sorted by start). so we pretend like we never\n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      // See if this term should be part of a new passage.\n      if (start >= passage.getEndOffset()) {\n        if (passage.getStartOffset() >= 0) { // true if this passage has terms; otherwise couldn't find any (yet)\n          // finalize passage\n          passage.setScore(passage.getScore() * scorer.norm(passage.getStartOffset()));\n          // new sentence: first add 'passage' to queue\n          if (passageQueue.size() == maxPassages && passage.getScore() < passageQueue.peek().getScore()) {\n            passage.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(passage);\n            if (passageQueue.size() > maxPassages) {\n              passage = passageQueue.poll();\n              passage.reset();\n            } else {\n              passage = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          break;\n        }\n        // advance breakIterator\n        passage.setStartOffset(Math.max(breakIterator.preceding(start + 1), 0));\n        passage.setEndOffset(Math.min(breakIterator.following(start), contentLength));\n      }\n      // Add this term to the passage.\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = off.getTerm();// a reference; safe to refer to\n        assert term != null;\n        passage.addMatch(start, end, term);\n        // see if there are multiple occurrences of this term in this passage. If so, add them.\n        if (!off.nextPosition()) {\n          break; // No more in the entire text. Already removed from pq; move on\n        }\n        start = off.startOffset();\n        end = off.endOffset();\n        if (start >= passage.getEndOffset() || end > contentLength) { // it's beyond this passage\n          offsetsEnumQueue.offer(off);\n          break;\n        }\n      }\n      passage.setScore(passage.getScore() + off.getWeight() * scorer.tf(tf, passage.getEndOffset() - passage.getStartOffset()));\n    }\n\n    Passage[] passages = passageQueue.toArray(new Passage[passageQueue.size()]);\n    for (Passage p : passages) {\n      p.sort();\n    }\n    // sort in ascending order\n    Arrays.sort(passages, Comparator.comparingInt(Passage::getStartOffset));\n    return passages;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9ab9796b8ce55058e483d2f195ac9b1942fcf478","date":1577858220,"type":3,"author":"Nándor Mátravölgyi","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldHighlighter#highlightOffsetsEnums(OffsetsEnum).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldHighlighter#highlightOffsetsEnums(OffsetsEnum).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  protected Passage[] highlightOffsetsEnums(OffsetsEnum off)\n      throws IOException {\n\n    final int contentLength = this.breakIterator.getText().getEndIndex();\n\n    if (off.nextPosition() == false) {\n      return new Passage[0];\n    }\n\n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(Math.min(64, maxPassages + 1), (left, right) -> {\n      if (left.getScore() < right.getScore()) {\n        return -1;\n      } else if (left.getScore() > right.getScore()) {\n        return 1;\n      } else {\n        return left.getStartOffset() - right.getStartOffset();\n      }\n    });\n    Passage passage = new Passage(); // the current passage in-progress.  Will either get reset or added to queue.\n    int lastPassageEnd = 0;\n\n    do {\n      int start = off.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = off.endOffset();\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      // See if this term should be part of a new passage.\n      if (start >= passage.getEndOffset()) {\n        passage = maybeAddPassage(passageQueue, passageScorer, passage, contentLength);\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          break;\n        }\n        // find fragment from the middle of the match, so the result's length may be closer to fragsize\n        final int center = start + (end - start) / 2;\n        // advance breakIterator\n        passage.setStartOffset(Math.min(start, Math.max(this.breakIterator.preceding(Math.max(start + 1, center)), lastPassageEnd)));\n        lastPassageEnd = Math.max(end, Math.min(this.breakIterator.following(Math.min(end - 1, center)), contentLength));\n        passage.setEndOffset(lastPassageEnd);\n      }\n      // Add this term to the passage.\n      BytesRef term = off.getTerm();// a reference; safe to refer to\n      assert term != null;\n      passage.addMatch(start, end, term, off.freq());\n    } while (off.nextPosition());\n    maybeAddPassage(passageQueue, passageScorer, passage, contentLength);\n\n    Passage[] passages = passageQueue.toArray(new Passage[passageQueue.size()]);\n    // sort in ascending order\n    Arrays.sort(passages, Comparator.comparingInt(Passage::getStartOffset));\n    return passages;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  protected Passage[] highlightOffsetsEnums(OffsetsEnum off)\n      throws IOException {\n\n    final int contentLength = this.breakIterator.getText().getEndIndex();\n\n    if (off.nextPosition() == false) {\n      return new Passage[0];\n    }\n\n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(Math.min(64, maxPassages + 1), (left, right) -> {\n      if (left.getScore() < right.getScore()) {\n        return -1;\n      } else if (left.getScore() > right.getScore()) {\n        return 1;\n      } else {\n        return left.getStartOffset() - right.getStartOffset();\n      }\n    });\n    Passage passage = new Passage(); // the current passage in-progress.  Will either get reset or added to queue.\n\n    do {\n      int start = off.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = off.endOffset();\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      // See if this term should be part of a new passage.\n      if (start >= passage.getEndOffset()) {\n        passage = maybeAddPassage(passageQueue, passageScorer, passage, contentLength);\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          break;\n        }\n        // advance breakIterator\n        passage.setStartOffset(Math.max(this.breakIterator.preceding(start + 1), 0));\n        passage.setEndOffset(Math.min(this.breakIterator.following(start), contentLength));\n      }\n      // Add this term to the passage.\n      BytesRef term = off.getTerm();// a reference; safe to refer to\n      assert term != null;\n      passage.addMatch(start, end, term, off.freq());\n    } while (off.nextPosition());\n    maybeAddPassage(passageQueue, passageScorer, passage, contentLength);\n\n    Passage[] passages = passageQueue.toArray(new Passage[passageQueue.size()]);\n    // sort in ascending order\n    Arrays.sort(passages, Comparator.comparingInt(Passage::getStartOffset));\n    return passages;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c606b777c4250f3f3f6f66d659c7c4c403679b71","date":1577958559,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldHighlighter#highlightOffsetsEnums(OffsetsEnum).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldHighlighter#highlightOffsetsEnums(OffsetsEnum).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  protected Passage[] highlightOffsetsEnums(OffsetsEnum off)\n      throws IOException {\n\n    final int contentLength = this.breakIterator.getText().getEndIndex();\n\n    if (off.nextPosition() == false) {\n      return new Passage[0];\n    }\n\n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(Math.min(64, maxPassages + 1), (left, right) -> {\n      if (left.getScore() < right.getScore()) {\n        return -1;\n      } else if (left.getScore() > right.getScore()) {\n        return 1;\n      } else {\n        return left.getStartOffset() - right.getStartOffset();\n      }\n    });\n    Passage passage = new Passage(); // the current passage in-progress.  Will either get reset or added to queue.\n    int lastPassageEnd = 0;\n\n    do {\n      int start = off.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = off.endOffset();\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      // See if this term should be part of a new passage.\n      if (start >= passage.getEndOffset()) {\n        passage = maybeAddPassage(passageQueue, passageScorer, passage, contentLength);\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          break;\n        }\n        // find fragment from the middle of the match, so the result's length may be closer to fragsize\n        final int center = start + (end - start) / 2;\n        // advance breakIterator\n        passage.setStartOffset(Math.min(start, Math.max(this.breakIterator.preceding(Math.max(start + 1, center)), lastPassageEnd)));\n        lastPassageEnd = Math.max(end, Math.min(this.breakIterator.following(Math.min(end - 1, center)), contentLength));\n        passage.setEndOffset(lastPassageEnd);\n      }\n      // Add this term to the passage.\n      BytesRef term = off.getTerm();// a reference; safe to refer to\n      assert term != null;\n      passage.addMatch(start, end, term, off.freq());\n    } while (off.nextPosition());\n    maybeAddPassage(passageQueue, passageScorer, passage, contentLength);\n\n    Passage[] passages = passageQueue.toArray(new Passage[passageQueue.size()]);\n    // sort in ascending order\n    Arrays.sort(passages, Comparator.comparingInt(Passage::getStartOffset));\n    return passages;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  protected Passage[] highlightOffsetsEnums(OffsetsEnum off)\n      throws IOException {\n\n    final int contentLength = this.breakIterator.getText().getEndIndex();\n\n    if (off.nextPosition() == false) {\n      return new Passage[0];\n    }\n\n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(Math.min(64, maxPassages + 1), (left, right) -> {\n      if (left.getScore() < right.getScore()) {\n        return -1;\n      } else if (left.getScore() > right.getScore()) {\n        return 1;\n      } else {\n        return left.getStartOffset() - right.getStartOffset();\n      }\n    });\n    Passage passage = new Passage(); // the current passage in-progress.  Will either get reset or added to queue.\n\n    do {\n      int start = off.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = off.endOffset();\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      // See if this term should be part of a new passage.\n      if (start >= passage.getEndOffset()) {\n        passage = maybeAddPassage(passageQueue, passageScorer, passage, contentLength);\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          break;\n        }\n        // advance breakIterator\n        passage.setStartOffset(Math.max(this.breakIterator.preceding(start + 1), 0));\n        passage.setEndOffset(Math.min(this.breakIterator.following(start), contentLength));\n      }\n      // Add this term to the passage.\n      BytesRef term = off.getTerm();// a reference; safe to refer to\n      assert term != null;\n      passage.addMatch(start, end, term, off.freq());\n    } while (off.nextPosition());\n    maybeAddPassage(passageQueue, passageScorer, passage, contentLength);\n\n    Passage[] passages = passageQueue.toArray(new Passage[passageQueue.size()]);\n    // sort in ascending order\n    Arrays.sort(passages, Comparator.comparingInt(Passage::getStartOffset));\n    return passages;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"9ab9796b8ce55058e483d2f195ac9b1942fcf478":["8764ca7bb74ee716c839b9545a93ec4a578c2005"],"c606b777c4250f3f3f6f66d659c7c4c403679b71":["8764ca7bb74ee716c839b9545a93ec4a578c2005","9ab9796b8ce55058e483d2f195ac9b1942fcf478"],"8764ca7bb74ee716c839b9545a93ec4a578c2005":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9ab9796b8ce55058e483d2f195ac9b1942fcf478"]},"commit2Childs":{"9ab9796b8ce55058e483d2f195ac9b1942fcf478":["c606b777c4250f3f3f6f66d659c7c4c403679b71","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c606b777c4250f3f3f6f66d659c7c4c403679b71":[],"8764ca7bb74ee716c839b9545a93ec4a578c2005":["9ab9796b8ce55058e483d2f195ac9b1942fcf478","c606b777c4250f3f3f6f66d659c7c4c403679b71"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["8764ca7bb74ee716c839b9545a93ec4a578c2005"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c606b777c4250f3f3f6f66d659c7c4c403679b71","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}