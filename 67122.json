{"path":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","sourceNew":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = new MockRAMDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.CREATE)\n          .setMaxBufferedDocs(2));\n        ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(101);\n        Document doc = new Document();\n        doc.add(new Field(\"field\", \"aaa\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }      \n\n      dir.close();\n    }\n\n","sourceOld":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = new MockRAMDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.CREATE)\n          .setMaxBufferedDocs(2));\n        ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(101);\n        Document doc = new Document();\n        doc.add(new Field(\"field\", \"aaa\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }      \n\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7ab99e8c71442b92c320e218141dee04a9b91ce8","date":1269203801,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","sourceNew":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = new MockRAMDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT))\n          .setOpenMode(OpenMode.CREATE).setMaxBufferedDocs(2));\n        ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(101);\n        Document doc = new Document();\n        doc.add(new Field(\"field\", \"aaa\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }      \n\n      dir.close();\n    }\n\n","sourceOld":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = new MockRAMDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.CREATE)\n          .setMaxBufferedDocs(2));\n        ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(101);\n        Document doc = new Document();\n        doc.add(new Field(\"field\", \"aaa\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }      \n\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d572389229127c297dd1fa5ce4758e1cec41e799","date":1273610938,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","sourceNew":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = new MockRAMDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer())\n          .setOpenMode(OpenMode.CREATE).setMaxBufferedDocs(2));\n        ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(101);\n        Document doc = new Document();\n        doc.add(new Field(\"field\", \"aaa\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }      \n\n      dir.close();\n    }\n\n","sourceOld":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = new MockRAMDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT))\n          .setOpenMode(OpenMode.CREATE).setMaxBufferedDocs(2));\n        ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(101);\n        Document doc = new Document();\n        doc.add(new Field(\"field\", \"aaa\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }      \n\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b21422ff1d1d56499dec481f193b402e5e8def5b","date":1281472367,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","sourceNew":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = new MockRAMDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(random,\n          TEST_VERSION_CURRENT, new MockAnalyzer())\n          .setOpenMode(OpenMode.CREATE).setMaxBufferedDocs(2));\n        ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(101);\n        Document doc = new Document();\n        doc.add(new Field(\"field\", \"aaa\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }      \n\n      dir.close();\n    }\n\n","sourceOld":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = new MockRAMDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer())\n          .setOpenMode(OpenMode.CREATE).setMaxBufferedDocs(2));\n        ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(101);\n        Document doc = new Document();\n        doc.add(new Field(\"field\", \"aaa\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }      \n\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c","date":1281646583,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","sourceNew":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory(random);\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(random,\n          TEST_VERSION_CURRENT, new MockAnalyzer())\n          .setOpenMode(OpenMode.CREATE).setMaxBufferedDocs(2));\n        ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(101);\n        Document doc = new Document();\n        doc.add(new Field(\"field\", \"aaa\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }      \n\n      dir.close();\n    }\n\n","sourceOld":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = new MockRAMDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(random,\n          TEST_VERSION_CURRENT, new MockAnalyzer())\n          .setOpenMode(OpenMode.CREATE).setMaxBufferedDocs(2));\n        ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(101);\n        Document doc = new Document();\n        doc.add(new Field(\"field\", \"aaa\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }      \n\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1f653cfcf159baeaafe5d01682a911e95bba4012","date":1284122058,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","sourceNew":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer())\n          .setOpenMode(OpenMode.CREATE).setMaxBufferedDocs(2));\n        ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(101);\n        Document doc = new Document();\n        doc.add(new Field(\"field\", \"aaa\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }      \n\n      dir.close();\n    }\n\n","sourceOld":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory(random);\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(random,\n          TEST_VERSION_CURRENT, new MockAnalyzer())\n          .setOpenMode(OpenMode.CREATE).setMaxBufferedDocs(2));\n        ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(101);\n        Document doc = new Document();\n        doc.add(new Field(\"field\", \"aaa\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }      \n\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"132903c28af3aa6f67284b78de91c0f0a99488c2","date":1284282129,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","sourceNew":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer())\n          .setOpenMode(OpenMode.CREATE).setMaxBufferedDocs(2));\n        ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(101);\n        Document doc = new Document();\n        doc.add(newField(\"field\", \"aaa\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }      \n\n      dir.close();\n    }\n\n","sourceOld":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer())\n          .setOpenMode(OpenMode.CREATE).setMaxBufferedDocs(2));\n        ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(101);\n        Document doc = new Document();\n        doc.add(new Field(\"field\", \"aaa\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }      \n\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4e8cc373c801e54cec75daf9f52792cb4b17f536","date":1291116159,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","sourceNew":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(\n            dir,\n            newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n                setOpenMode(OpenMode.CREATE).\n                setMaxBufferedDocs(2).\n                setMergePolicy(newLogMergePolicy(101))\n        );\n        Document doc = new Document();\n        doc.add(newField(\"field\", \"aaa\", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }      \n\n      dir.close();\n    }\n\n","sourceOld":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer())\n          .setOpenMode(OpenMode.CREATE).setMaxBufferedDocs(2));\n        ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(101);\n        Document doc = new Document();\n        doc.add(newField(\"field\", \"aaa\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }      \n\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":["b60dc5742a5f801078cc6b89d827ff7b38ddb815"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3bb13258feba31ab676502787ab2e1779f129b7a","date":1291596436,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","sourceNew":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(\n            dir,\n            newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n                setOpenMode(OpenMode.CREATE).\n                setMaxBufferedDocs(2).\n                setMergePolicy(newLogMergePolicy(101))\n        );\n        Document doc = new Document();\n        doc.add(newField(\"field\", \"aaa\", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }      \n\n      dir.close();\n    }\n\n","sourceOld":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer())\n          .setOpenMode(OpenMode.CREATE).setMaxBufferedDocs(2));\n        ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(101);\n        Document doc = new Document();\n        doc.add(newField(\"field\", \"aaa\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }      \n\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","sourceNew":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(\n            dir,\n            newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n                setOpenMode(OpenMode.CREATE).\n                setMaxBufferedDocs(2).\n                setMergePolicy(newLogMergePolicy(101))\n        );\n        Document doc = new Document();\n        doc.add(newField(\"field\", \"aaa\", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }\n\n      dir.close();\n    }\n\n","sourceOld":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = new MockRAMDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer())\n          .setOpenMode(OpenMode.CREATE).setMaxBufferedDocs(2));\n        ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(101);\n        Document doc = new Document();\n        doc.add(new Field(\"field\", \"aaa\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }      \n\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","sourceNew":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(\n            dir,\n            newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                setOpenMode(OpenMode.CREATE).\n                setMaxBufferedDocs(2).\n                setMergePolicy(newLogMergePolicy(101))\n        );\n        Document doc = new Document();\n        doc.add(newField(\"field\", \"aaa\", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }      \n\n      dir.close();\n    }\n\n","sourceOld":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(\n            dir,\n            newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n                setOpenMode(OpenMode.CREATE).\n                setMaxBufferedDocs(2).\n                setMergePolicy(newLogMergePolicy(101))\n        );\n        Document doc = new Document();\n        doc.add(newField(\"field\", \"aaa\", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }      \n\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","sourceNew":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(\n            dir,\n            newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                setOpenMode(OpenMode.CREATE).\n                setMaxBufferedDocs(2).\n                setMergePolicy(newLogMergePolicy(101))\n        );\n        Document doc = new Document();\n        doc.add(newField(\"field\", \"aaa\", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }\n\n      dir.close();\n    }\n\n","sourceOld":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(\n            dir,\n            newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n                setOpenMode(OpenMode.CREATE).\n                setMaxBufferedDocs(2).\n                setMergePolicy(newLogMergePolicy(101))\n        );\n        Document doc = new Document();\n        doc.add(newField(\"field\", \"aaa\", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }\n\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","sourceNew":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(\n            dir,\n            newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                setOpenMode(OpenMode.CREATE).\n                setMaxBufferedDocs(2).\n                setMergePolicy(newLogMergePolicy(101))\n        );\n        Document doc = new Document();\n        doc.add(newField(\"field\", \"aaa\", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }\n\n      dir.close();\n    }\n\n","sourceOld":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(\n            dir,\n            newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                setOpenMode(OpenMode.CREATE).\n                setMaxBufferedDocs(2).\n                setMergePolicy(newLogMergePolicy(101))\n        );\n        Document doc = new Document();\n        doc.add(newField(\"field\", \"aaa\", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }      \n\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","sourceNew":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(\n            dir,\n            newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                setOpenMode(OpenMode.CREATE).\n                setMaxBufferedDocs(2).\n                setMergePolicy(newLogMergePolicy(101))\n        );\n        Document doc = new Document();\n        doc.add(newField(\"field\", \"aaa\", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }\n\n      dir.close();\n    }\n\n","sourceOld":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(\n            dir,\n            newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n                setOpenMode(OpenMode.CREATE).\n                setMaxBufferedDocs(2).\n                setMergePolicy(newLogMergePolicy(101))\n        );\n        Document doc = new Document();\n        doc.add(newField(\"field\", \"aaa\", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }      \n\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b60dc5742a5f801078cc6b89d827ff7b38ddb815","date":1305825810,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","sourceNew":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(\n            dir,\n            newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                setOpenMode(OpenMode.CREATE).\n                setMaxBufferedDocs(2).\n                setMergePolicy(newLogMergePolicy(51))\n        );\n        Document doc = new Document();\n        doc.add(newField(\"field\", \"aaa\", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<100;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }\n\n      dir.close();\n    }\n\n","sourceOld":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(\n            dir,\n            newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                setOpenMode(OpenMode.CREATE).\n                setMaxBufferedDocs(2).\n                setMergePolicy(newLogMergePolicy(101))\n        );\n        Document doc = new Document();\n        doc.add(newField(\"field\", \"aaa\", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }\n\n      dir.close();\n    }\n\n","bugFix":["b1405362241b561f5590ff4a87d5d6e173bcd9cf","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","sourceNew":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(\n            dir,\n            newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                setOpenMode(OpenMode.CREATE).\n                setMaxBufferedDocs(2).\n                setMergePolicy(newLogMergePolicy(51))\n        );\n        Document doc = new Document();\n        doc.add(newField(\"field\", \"aaa\", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<100;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }\n\n      dir.close();\n    }\n\n","sourceOld":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(\n            dir,\n            newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n                setOpenMode(OpenMode.CREATE).\n                setMaxBufferedDocs(2).\n                setMergePolicy(newLogMergePolicy(101))\n        );\n        Document doc = new Document();\n        doc.add(newField(\"field\", \"aaa\", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }      \n\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a","date":1306150983,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","sourceNew":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(\n            dir,\n            newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                setOpenMode(OpenMode.CREATE).\n                setMaxBufferedDocs(2).\n                setMergePolicy(newLogMergePolicy(51))\n        );\n        Document doc = new Document();\n        doc.add(newField(\"field\", \"aaa\", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<100;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }\n\n      dir.close();\n    }\n\n","sourceOld":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(\n            dir,\n            newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                setOpenMode(OpenMode.CREATE).\n                setMaxBufferedDocs(2).\n                setMergePolicy(newLogMergePolicy(101))\n        );\n        Document doc = new Document();\n        doc.add(newField(\"field\", \"aaa\", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<200;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }\n\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ff6fd241dc6610f7f81b62e3ba4cedf105939623","date":1307331653,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","sourceNew":null,"sourceOld":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(\n            dir,\n            newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                setOpenMode(OpenMode.CREATE).\n                setMaxBufferedDocs(2).\n                setMergePolicy(newLogMergePolicy(51))\n        );\n        Document doc = new Document();\n        doc.add(newField(\"field\", \"aaa\", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<100;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }\n\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"79c2cb24929f2649a8875fb629086171f914d5ce","date":1307332717,"type":4,"author":"Steven Rowe","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","sourceNew":null,"sourceOld":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(\n            dir,\n            newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                setOpenMode(OpenMode.CREATE).\n                setMaxBufferedDocs(2).\n                setMergePolicy(newLogMergePolicy(51))\n        );\n        Document doc = new Document();\n        doc.add(newField(\"field\", \"aaa\", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<100;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }\n\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"77cf4379b2824f6ea34b091c495d6e95c38ff9e2","date":1307610475,"type":4,"author":"Simon Willnauer","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testBackgroundOptimize().mjava","sourceNew":null,"sourceOld":"    // Test calling optimize(false) whereby optimize is kicked\n    // off but we don't wait for it to finish (but\n    // writer.close()) does wait\n    public void testBackgroundOptimize() throws IOException {\n\n      Directory dir = newDirectory();\n      for(int pass=0;pass<2;pass++) {\n        IndexWriter writer = new IndexWriter(\n            dir,\n            newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                setOpenMode(OpenMode.CREATE).\n                setMaxBufferedDocs(2).\n                setMergePolicy(newLogMergePolicy(51))\n        );\n        Document doc = new Document();\n        doc.add(newField(\"field\", \"aaa\", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n        for(int i=0;i<100;i++)\n          writer.addDocument(doc);\n        writer.optimize(false);\n\n        if (0 == pass) {\n          writer.close();\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(reader.isOptimized());\n          reader.close();\n        } else {\n          // Get another segment to flush so we can verify it is\n          // NOT included in the optimization\n          writer.addDocument(doc);\n          writer.addDocument(doc);\n          writer.close();\n\n          IndexReader reader = IndexReader.open(dir, true);\n          assertTrue(!reader.isOptimized());\n          reader.close();\n\n          SegmentInfos infos = new SegmentInfos();\n          infos.read(dir);\n          assertEquals(2, infos.size());\n        }\n      }\n\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ff6fd241dc6610f7f81b62e3ba4cedf105939623":["b60dc5742a5f801078cc6b89d827ff7b38ddb815"],"5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a":["135621f3a0670a9394eb563224a3b76cc4dddc0f","b60dc5742a5f801078cc6b89d827ff7b38ddb815"],"b60dc5742a5f801078cc6b89d827ff7b38ddb815":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["b21422ff1d1d56499dec481f193b402e5e8def5b"],"7ab99e8c71442b92c320e218141dee04a9b91ce8":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["3bb13258feba31ab676502787ab2e1779f129b7a","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"d572389229127c297dd1fa5ce4758e1cec41e799":["7ab99e8c71442b92c320e218141dee04a9b91ce8"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["f2c5f0cb44df114db4228c8f77861714b5cabaea","962d04139994fce5193143ef35615499a9a96d78"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"962d04139994fce5193143ef35615499a9a96d78":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["d572389229127c297dd1fa5ce4758e1cec41e799","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["4e8cc373c801e54cec75daf9f52792cb4b17f536"],"79c2cb24929f2649a8875fb629086171f914d5ce":["a3776dccca01c11e7046323cfad46a3b4a471233","ff6fd241dc6610f7f81b62e3ba4cedf105939623"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"a3776dccca01c11e7046323cfad46a3b4a471233":["4e8cc373c801e54cec75daf9f52792cb4b17f536","b60dc5742a5f801078cc6b89d827ff7b38ddb815"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":["5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a","ff6fd241dc6610f7f81b62e3ba4cedf105939623"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["d572389229127c297dd1fa5ce4758e1cec41e799"],"3bb13258feba31ab676502787ab2e1779f129b7a":["132903c28af3aa6f67284b78de91c0f0a99488c2","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["ff6fd241dc6610f7f81b62e3ba4cedf105939623"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"ff6fd241dc6610f7f81b62e3ba4cedf105939623":["79c2cb24929f2649a8875fb629086171f914d5ce","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a":["77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"b60dc5742a5f801078cc6b89d827ff7b38ddb815":["ff6fd241dc6610f7f81b62e3ba4cedf105939623","5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a","a3776dccca01c11e7046323cfad46a3b4a471233"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"7ab99e8c71442b92c320e218141dee04a9b91ce8":["d572389229127c297dd1fa5ce4758e1cec41e799"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["b60dc5742a5f801078cc6b89d827ff7b38ddb815","135621f3a0670a9394eb563224a3b76cc4dddc0f"],"d572389229127c297dd1fa5ce4758e1cec41e799":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","b21422ff1d1d56499dec481f193b402e5e8def5b"],"962d04139994fce5193143ef35615499a9a96d78":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["3bb13258feba31ab676502787ab2e1779f129b7a","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["962d04139994fce5193143ef35615499a9a96d78"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["b3e06be49006ecac364d39d12b9c9f74882f9b9f","962d04139994fce5193143ef35615499a9a96d78"],"79c2cb24929f2649a8875fb629086171f914d5ce":[],"1f653cfcf159baeaafe5d01682a911e95bba4012":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"a3776dccca01c11e7046323cfad46a3b4a471233":["79c2cb24929f2649a8875fb629086171f914d5ce"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":[],"b21422ff1d1d56499dec481f193b402e5e8def5b":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"3bb13258feba31ab676502787ab2e1779f129b7a":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["7ab99e8c71442b92c320e218141dee04a9b91ce8"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","f2c5f0cb44df114db4228c8f77861714b5cabaea","a3776dccca01c11e7046323cfad46a3b4a471233","3bb13258feba31ab676502787ab2e1779f129b7a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["79c2cb24929f2649a8875fb629086171f914d5ce","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}