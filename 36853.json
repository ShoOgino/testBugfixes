{"path":"lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","commits":[{"id":"99cf56f3a650b908f7017a72f9d23940418f8a52","date":1284891529,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/standard/StandardTermsDictReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","sourceNew":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(BytesRef term, boolean useCache) throws IOException {\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copy(cachedState);\n            seekPending = true;\n            bytesReader.term.copy(term);\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (state.ord != -1) {\n          // we are positioned\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          }\n\n          if (cmp < 0 &&\n              fieldIndexReader.nextIndexTerm(state.ord, indexResult) &&\n              termComp.compare(indexResult.term, term) > 0) {\n            // Optimization: requested term is within the\n            // same index block we are now in; skip seeking\n            // (but do scanning):\n            doSeek = false;\n          }\n        }\n\n        // Used only for assert:\n        final long startOrd;\n\n        if (doSeek) {\n\n          // As index to find biggest index term that's <=\n          // our text:\n          fieldIndexReader.getIndexOffset(term, indexResult);\n\n          in.seek(indexResult.offset);\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexResult.term);\n          \n          state.ord = indexResult.position-1;\n          assert state.ord >= -1: \"ord=\" + state.ord + \" pos=\" + indexResult.position;\n\n          startOrd = indexResult.position;\n        } else {\n          startOrd = -1;\n        }\n\n        // Now scan:\n        while(next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n\n            if (doSeek && useCache) {\n              // Store in cache\n              FieldAndTerm entryKey = new FieldAndTerm(fieldTerm);\n              cachedState = (TermState) state.clone();\n              // this is fp after current term\n              cachedState.filePointer = in.getFilePointer();\n              termsCache.put(entryKey, cachedState);\n            }\n              \n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert state.ord == startOrd || !fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true): \"state.ord=\" + state.ord + \" startOrd=\" + startOrd + \" ir.isIndexTerm=\" + fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true) + \" state.docFreq=\" + state.docFreq;\n        }\n\n        return SeekStatus.END;\n      }\n\n","sourceOld":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(BytesRef term, boolean useCache) throws IOException {\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copy(cachedState);\n            seekPending = true;\n            bytesReader.term.copy(term);\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (state.ord != -1) {\n          // we are positioned\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          }\n\n          if (cmp < 0 &&\n              fieldIndexReader.nextIndexTerm(state.ord, indexResult) &&\n              termComp.compare(indexResult.term, term) > 0) {\n            // Optimization: requested term is within the\n            // same index block we are now in; skip seeking\n            // (but do scanning):\n            doSeek = false;\n          }\n        }\n\n        // Used only for assert:\n        final long startOrd;\n\n        if (doSeek) {\n\n          // As index to find biggest index term that's <=\n          // our text:\n          fieldIndexReader.getIndexOffset(term, indexResult);\n\n          in.seek(indexResult.offset);\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexResult.term);\n          \n          state.ord = indexResult.position-1;\n          assert state.ord >= -1: \"ord=\" + state.ord + \" pos=\" + indexResult.position;\n\n          startOrd = indexResult.position;\n        } else {\n          startOrd = -1;\n        }\n\n        // Now scan:\n        while(next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n\n            if (doSeek && useCache) {\n              // Store in cache\n              FieldAndTerm entryKey = new FieldAndTerm(fieldTerm);\n              cachedState = (TermState) state.clone();\n              // this is fp after current term\n              cachedState.filePointer = in.getFilePointer();\n              termsCache.put(entryKey, cachedState);\n            }\n              \n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert state.ord == startOrd || !fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true): \"state.ord=\" + state.ord + \" startOrd=\" + startOrd + \" ir.isIndexTerm=\" + fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true) + \" state.docFreq=\" + state.docFreq;\n        }\n\n        return SeekStatus.END;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":0,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","pathOld":"/dev/null","sourceNew":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(BytesRef term, boolean useCache) throws IOException {\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copy(cachedState);\n            seekPending = true;\n            bytesReader.term.copy(term);\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (state.ord != -1) {\n          // we are positioned\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          }\n\n          if (cmp < 0 &&\n              fieldIndexReader.nextIndexTerm(state.ord, indexResult) &&\n              termComp.compare(indexResult.term, term) > 0) {\n            // Optimization: requested term is within the\n            // same index block we are now in; skip seeking\n            // (but do scanning):\n            doSeek = false;\n          }\n        }\n\n        // Used only for assert:\n        final long startOrd;\n\n        if (doSeek) {\n\n          // As index to find biggest index term that's <=\n          // our text:\n          fieldIndexReader.getIndexOffset(term, indexResult);\n\n          in.seek(indexResult.offset);\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexResult.term);\n          \n          state.ord = indexResult.position-1;\n          assert state.ord >= -1: \"ord=\" + state.ord + \" pos=\" + indexResult.position;\n\n          startOrd = indexResult.position;\n        } else {\n          startOrd = -1;\n        }\n\n        // Now scan:\n        while(next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n\n            if (doSeek && useCache) {\n              // Store in cache\n              FieldAndTerm entryKey = new FieldAndTerm(fieldTerm);\n              cachedState = (TermState) state.clone();\n              // this is fp after current term\n              cachedState.filePointer = in.getFilePointer();\n              termsCache.put(entryKey, cachedState);\n            }\n              \n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert state.ord == startOrd || !fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true): \"state.ord=\" + state.ord + \" startOrd=\" + startOrd + \" ir.isIndexTerm=\" + fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true) + \" state.docFreq=\" + state.docFreq;\n        }\n\n        return SeekStatus.END;\n      }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"56da903869515527852ee21ea7ef7bfe414cd40d","date":1294224724,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","sourceNew":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(BytesRef term, boolean useCache) throws IOException {\n\n        if (indexEnum == null) {\n          throw new IllegalStateException(\"terms index was not loaded\");\n        }\n        \n        //System.out.println(\"te.seek term=\" + fieldInfo.name + \":\" + term.utf8ToString() + \" current=\" + term().utf8ToString() + \" useCache=\" + useCache + \" this=\"  + this);\n\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copy(cachedState);\n            seekPending = true;\n            positioned = false;\n            bytesReader.term.copy(term);\n            //System.out.println(\"  cached!\");\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (positioned) {\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          } else if (cmp < 0) {\n\n            if (seekPending) {\n              seekPending = false;\n              in.seek(state.filePointer);\n              indexEnum.seek(bytesReader.term);\n              didIndexNext = false;\n            }\n\n            // Target term is after current term\n            if (!didIndexNext) {\n              if (indexEnum.next() == -1) {\n                nextIndexTerm = null;\n              } else {\n                nextIndexTerm = indexEnum.term();\n              }\n              //System.out.println(\"  now do index next() nextIndexTerm=\" + (nextIndexTerm == null ? \"null\" : nextIndexTerm.utf8ToString()));\n              didIndexNext = true;\n            }\n\n            if (nextIndexTerm == null || termComp.compare(term, nextIndexTerm) < 0) {\n              // Optimization: requested term is within the\n              // same index block we are now in; skip seeking\n              // (but do scanning):\n              doSeek = false;\n              //System.out.println(\"  skip seek: nextIndexTerm=\" + nextIndexTerm);\n            }\n          }\n        }\n\n        if (doSeek) {\n\n          positioned = true;\n\n          // Ask terms index to find biggest index term that's <=\n          // our text:\n          in.seek(indexEnum.seek(term));\n          didIndexNext = false;\n          if (doOrd) {\n            state.ord = indexEnum.ord()-1;\n          }\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexEnum.term());\n          //System.out.println(\"  doSeek term=\" + indexEnum.term().utf8ToString() + \" vs target=\" + term.utf8ToString());\n        } else {\n          //System.out.println(\"  skip seek\");\n        }\n\n        assert startSeek();\n\n        // Now scan:\n        while (next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n            // Done!\n            if (useCache) {\n              // Store in cache\n              FieldAndTerm entryKey = new FieldAndTerm(fieldTerm);\n              cachedState = (TermState) state.clone();\n              // this is fp after current term\n              cachedState.filePointer = in.getFilePointer();\n              termsCache.put(entryKey, cachedState);\n            }\n\n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert checkSeekScan();\n        }\n\n        positioned = false;\n        return SeekStatus.END;\n      }\n\n","sourceOld":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(BytesRef term, boolean useCache) throws IOException {\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copy(cachedState);\n            seekPending = true;\n            bytesReader.term.copy(term);\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (state.ord != -1) {\n          // we are positioned\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          }\n\n          if (cmp < 0 &&\n              fieldIndexReader.nextIndexTerm(state.ord, indexResult) &&\n              termComp.compare(indexResult.term, term) > 0) {\n            // Optimization: requested term is within the\n            // same index block we are now in; skip seeking\n            // (but do scanning):\n            doSeek = false;\n          }\n        }\n\n        // Used only for assert:\n        final long startOrd;\n\n        if (doSeek) {\n\n          // As index to find biggest index term that's <=\n          // our text:\n          fieldIndexReader.getIndexOffset(term, indexResult);\n\n          in.seek(indexResult.offset);\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexResult.term);\n          \n          state.ord = indexResult.position-1;\n          assert state.ord >= -1: \"ord=\" + state.ord + \" pos=\" + indexResult.position;\n\n          startOrd = indexResult.position;\n        } else {\n          startOrd = -1;\n        }\n\n        // Now scan:\n        while(next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n\n            if (doSeek && useCache) {\n              // Store in cache\n              FieldAndTerm entryKey = new FieldAndTerm(fieldTerm);\n              cachedState = (TermState) state.clone();\n              // this is fp after current term\n              cachedState.filePointer = in.getFilePointer();\n              termsCache.put(entryKey, cachedState);\n            }\n              \n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert state.ord == startOrd || !fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true): \"state.ord=\" + state.ord + \" startOrd=\" + startOrd + \" ir.isIndexTerm=\" + fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true) + \" state.docFreq=\" + state.docFreq;\n        }\n\n        return SeekStatus.END;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70ad682703b8585f5d0a637efec044d57ec05efb","date":1294259117,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","sourceNew":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(BytesRef term, boolean useCache) throws IOException {\n\n        if (indexEnum == null) {\n          throw new IllegalStateException(\"terms index was not loaded\");\n        }\n        \n        //System.out.println(\"te.seek term=\" + fieldInfo.name + \":\" + term.utf8ToString() + \" current=\" + term().utf8ToString() + \" useCache=\" + useCache + \" this=\"  + this);\n\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copy(cachedState);\n            seekPending = true;\n            positioned = false;\n            bytesReader.term.copy(term);\n            //System.out.println(\"  cached!\");\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (positioned) {\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          } else if (cmp < 0) {\n\n            if (seekPending) {\n              seekPending = false;\n              in.seek(state.filePointer);\n              indexEnum.seek(bytesReader.term);\n              didIndexNext = false;\n            }\n\n            // Target term is after current term\n            if (!didIndexNext) {\n              if (indexEnum.next() == -1) {\n                nextIndexTerm = null;\n              } else {\n                nextIndexTerm = indexEnum.term();\n              }\n              //System.out.println(\"  now do index next() nextIndexTerm=\" + (nextIndexTerm == null ? \"null\" : nextIndexTerm.utf8ToString()));\n              didIndexNext = true;\n            }\n\n            if (nextIndexTerm == null || termComp.compare(term, nextIndexTerm) < 0) {\n              // Optimization: requested term is within the\n              // same index block we are now in; skip seeking\n              // (but do scanning):\n              doSeek = false;\n              //System.out.println(\"  skip seek: nextIndexTerm=\" + nextIndexTerm);\n            }\n          }\n        }\n\n        if (doSeek) {\n\n          positioned = true;\n\n          // Ask terms index to find biggest index term that's <=\n          // our text:\n          in.seek(indexEnum.seek(term));\n          didIndexNext = false;\n          if (doOrd) {\n            state.ord = indexEnum.ord()-1;\n          }\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexEnum.term());\n          //System.out.println(\"  doSeek term=\" + indexEnum.term().utf8ToString() + \" vs target=\" + term.utf8ToString());\n        } else {\n          //System.out.println(\"  skip seek\");\n        }\n\n        assert startSeek();\n\n        // Now scan:\n        while (next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n            // Done!\n            if (useCache) {\n              // Store in cache\n              FieldAndTerm entryKey = new FieldAndTerm(fieldTerm);\n              cachedState = (TermState) state.clone();\n              // this is fp after current term\n              cachedState.filePointer = in.getFilePointer();\n              termsCache.put(entryKey, cachedState);\n            }\n\n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert checkSeekScan();\n        }\n\n        positioned = false;\n        return SeekStatus.END;\n      }\n\n","sourceOld":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(BytesRef term, boolean useCache) throws IOException {\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copy(cachedState);\n            seekPending = true;\n            bytesReader.term.copy(term);\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (state.ord != -1) {\n          // we are positioned\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          }\n\n          if (cmp < 0 &&\n              fieldIndexReader.nextIndexTerm(state.ord, indexResult) &&\n              termComp.compare(indexResult.term, term) > 0) {\n            // Optimization: requested term is within the\n            // same index block we are now in; skip seeking\n            // (but do scanning):\n            doSeek = false;\n          }\n        }\n\n        // Used only for assert:\n        final long startOrd;\n\n        if (doSeek) {\n\n          // As index to find biggest index term that's <=\n          // our text:\n          fieldIndexReader.getIndexOffset(term, indexResult);\n\n          in.seek(indexResult.offset);\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexResult.term);\n          \n          state.ord = indexResult.position-1;\n          assert state.ord >= -1: \"ord=\" + state.ord + \" pos=\" + indexResult.position;\n\n          startOrd = indexResult.position;\n        } else {\n          startOrd = -1;\n        }\n\n        // Now scan:\n        while(next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n\n            if (doSeek && useCache) {\n              // Store in cache\n              FieldAndTerm entryKey = new FieldAndTerm(fieldTerm);\n              cachedState = (TermState) state.clone();\n              // this is fp after current term\n              cachedState.filePointer = in.getFilePointer();\n              termsCache.put(entryKey, cachedState);\n            }\n              \n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert state.ord == startOrd || !fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true): \"state.ord=\" + state.ord + \" startOrd=\" + startOrd + \" ir.isIndexTerm=\" + fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true) + \" state.docFreq=\" + state.docFreq;\n        }\n\n        return SeekStatus.END;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3d53c3f769ca0f9e7434937b792877770271aecf","date":1294785129,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","sourceNew":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(BytesRef term, boolean useCache) throws IOException {\n\n        if (indexEnum == null) {\n          throw new IllegalStateException(\"terms index was not loaded\");\n        }\n        \n        //System.out.println(\"te.seek term=\" + fieldInfo.name + \":\" + term.utf8ToString() + \" current=\" + term().utf8ToString() + \" useCache=\" + useCache + \" this=\"  + this);\n\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copyFrom(cachedState);\n            seekPending = true;\n            positioned = false;\n            bytesReader.term.copy(term);\n            //System.out.println(\"  cached!\");\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (positioned) {\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          } else if (cmp < 0) {\n\n            if (seekPending) {\n              seekPending = false;\n              in.seek(state.filePointer);\n              indexEnum.seek(bytesReader.term);\n              didIndexNext = false;\n            }\n\n            // Target term is after current term\n            if (!didIndexNext) {\n              if (indexEnum.next() == -1) {\n                nextIndexTerm = null;\n              } else {\n                nextIndexTerm = indexEnum.term();\n              }\n              //System.out.println(\"  now do index next() nextIndexTerm=\" + (nextIndexTerm == null ? \"null\" : nextIndexTerm.utf8ToString()));\n              didIndexNext = true;\n            }\n\n            if (nextIndexTerm == null || termComp.compare(term, nextIndexTerm) < 0) {\n              // Optimization: requested term is within the\n              // same index block we are now in; skip seeking\n              // (but do scanning):\n              doSeek = false;\n              //System.out.println(\"  skip seek: nextIndexTerm=\" + nextIndexTerm);\n            }\n          }\n        }\n\n        if (doSeek) {\n\n          positioned = true;\n\n          // Ask terms index to find biggest index term that's <=\n          // our text:\n          in.seek(indexEnum.seek(term));\n          didIndexNext = false;\n          if (doOrd) {\n            state.ord = indexEnum.ord()-1;\n          }\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexEnum.term());\n          //System.out.println(\"  doSeek term=\" + indexEnum.term().utf8ToString() + \" vs target=\" + term.utf8ToString());\n        } else {\n          //System.out.println(\"  skip seek\");\n        }\n\n        assert startSeek();\n\n        // Now scan:\n        while (next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n            // Done!\n            if (useCache) {\n              // Store in cache\n              FieldAndTerm entryKey = new FieldAndTerm(fieldTerm);\n              cachedState = (TermState) state.clone();\n              // this is fp after current term\n              cachedState.filePointer = in.getFilePointer();\n              termsCache.put(entryKey, cachedState);\n            }\n\n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert checkSeekScan();\n        }\n\n        positioned = false;\n        return SeekStatus.END;\n      }\n\n","sourceOld":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(BytesRef term, boolean useCache) throws IOException {\n\n        if (indexEnum == null) {\n          throw new IllegalStateException(\"terms index was not loaded\");\n        }\n        \n        //System.out.println(\"te.seek term=\" + fieldInfo.name + \":\" + term.utf8ToString() + \" current=\" + term().utf8ToString() + \" useCache=\" + useCache + \" this=\"  + this);\n\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copy(cachedState);\n            seekPending = true;\n            positioned = false;\n            bytesReader.term.copy(term);\n            //System.out.println(\"  cached!\");\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (positioned) {\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          } else if (cmp < 0) {\n\n            if (seekPending) {\n              seekPending = false;\n              in.seek(state.filePointer);\n              indexEnum.seek(bytesReader.term);\n              didIndexNext = false;\n            }\n\n            // Target term is after current term\n            if (!didIndexNext) {\n              if (indexEnum.next() == -1) {\n                nextIndexTerm = null;\n              } else {\n                nextIndexTerm = indexEnum.term();\n              }\n              //System.out.println(\"  now do index next() nextIndexTerm=\" + (nextIndexTerm == null ? \"null\" : nextIndexTerm.utf8ToString()));\n              didIndexNext = true;\n            }\n\n            if (nextIndexTerm == null || termComp.compare(term, nextIndexTerm) < 0) {\n              // Optimization: requested term is within the\n              // same index block we are now in; skip seeking\n              // (but do scanning):\n              doSeek = false;\n              //System.out.println(\"  skip seek: nextIndexTerm=\" + nextIndexTerm);\n            }\n          }\n        }\n\n        if (doSeek) {\n\n          positioned = true;\n\n          // Ask terms index to find biggest index term that's <=\n          // our text:\n          in.seek(indexEnum.seek(term));\n          didIndexNext = false;\n          if (doOrd) {\n            state.ord = indexEnum.ord()-1;\n          }\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexEnum.term());\n          //System.out.println(\"  doSeek term=\" + indexEnum.term().utf8ToString() + \" vs target=\" + term.utf8ToString());\n        } else {\n          //System.out.println(\"  skip seek\");\n        }\n\n        assert startSeek();\n\n        // Now scan:\n        while (next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n            // Done!\n            if (useCache) {\n              // Store in cache\n              FieldAndTerm entryKey = new FieldAndTerm(fieldTerm);\n              cachedState = (TermState) state.clone();\n              // this is fp after current term\n              cachedState.filePointer = in.getFilePointer();\n              termsCache.put(entryKey, cachedState);\n            }\n\n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert checkSeekScan();\n        }\n\n        positioned = false;\n        return SeekStatus.END;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fa1a999d6674423e5c4ac858b410283f6fe03f20","date":1294868331,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","sourceNew":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(final BytesRef term, final boolean useCache) throws IOException {\n\n        if (indexEnum == null) {\n          throw new IllegalStateException(\"terms index was not loaded\");\n        }\n        \n        //System.out.println(\"te.seek term=\" + fieldInfo.name + \":\" + term.utf8ToString() + \" current=\" + term().utf8ToString() + \" useCache=\" + useCache + \" this=\"  + this);\n\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copyFrom(cachedState);\n            setTermState(term, state);\n            positioned = false;\n            //System.out.println(\"  cached!\");\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (positioned) {\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          } else if (cmp < 0) {\n\n            if (seekPending) {\n              seekPending = false;\n              in.seek(state.filePointer);\n              indexEnum.seek(bytesReader.term);\n              didIndexNext = false;\n            }\n\n            // Target term is after current term\n            if (!didIndexNext) {\n              if (indexEnum.next() == -1) {\n                nextIndexTerm = null;\n              } else {\n                nextIndexTerm = indexEnum.term();\n              }\n              //System.out.println(\"  now do index next() nextIndexTerm=\" + (nextIndexTerm == null ? \"null\" : nextIndexTerm.utf8ToString()));\n              didIndexNext = true;\n            }\n\n            if (nextIndexTerm == null || termComp.compare(term, nextIndexTerm) < 0) {\n              // Optimization: requested term is within the\n              // same index block we are now in; skip seeking\n              // (but do scanning):\n              doSeek = false;\n              //System.out.println(\"  skip seek: nextIndexTerm=\" + nextIndexTerm);\n            }\n          }\n        }\n\n        if (doSeek) {\n\n          positioned = true;\n\n          // Ask terms index to find biggest index term that's <=\n          // our text:\n          in.seek(indexEnum.seek(term));\n          didIndexNext = false;\n          if (doOrd) {\n            state.ord = indexEnum.ord()-1;\n          }\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexEnum.term());\n          //System.out.println(\"  doSeek term=\" + indexEnum.term().utf8ToString() + \" vs target=\" + term.utf8ToString());\n        } else {\n          //System.out.println(\"  skip seek\");\n        }\n\n        assert startSeek();\n\n        // Now scan:\n        while (next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n            // Done!\n            if (useCache) {\n              cacheTerm(fieldTerm);\n            }\n\n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert checkSeekScan();\n        }\n\n        positioned = false;\n        return SeekStatus.END;\n      }\n\n","sourceOld":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(BytesRef term, boolean useCache) throws IOException {\n\n        if (indexEnum == null) {\n          throw new IllegalStateException(\"terms index was not loaded\");\n        }\n        \n        //System.out.println(\"te.seek term=\" + fieldInfo.name + \":\" + term.utf8ToString() + \" current=\" + term().utf8ToString() + \" useCache=\" + useCache + \" this=\"  + this);\n\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copyFrom(cachedState);\n            seekPending = true;\n            positioned = false;\n            bytesReader.term.copy(term);\n            //System.out.println(\"  cached!\");\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (positioned) {\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          } else if (cmp < 0) {\n\n            if (seekPending) {\n              seekPending = false;\n              in.seek(state.filePointer);\n              indexEnum.seek(bytesReader.term);\n              didIndexNext = false;\n            }\n\n            // Target term is after current term\n            if (!didIndexNext) {\n              if (indexEnum.next() == -1) {\n                nextIndexTerm = null;\n              } else {\n                nextIndexTerm = indexEnum.term();\n              }\n              //System.out.println(\"  now do index next() nextIndexTerm=\" + (nextIndexTerm == null ? \"null\" : nextIndexTerm.utf8ToString()));\n              didIndexNext = true;\n            }\n\n            if (nextIndexTerm == null || termComp.compare(term, nextIndexTerm) < 0) {\n              // Optimization: requested term is within the\n              // same index block we are now in; skip seeking\n              // (but do scanning):\n              doSeek = false;\n              //System.out.println(\"  skip seek: nextIndexTerm=\" + nextIndexTerm);\n            }\n          }\n        }\n\n        if (doSeek) {\n\n          positioned = true;\n\n          // Ask terms index to find biggest index term that's <=\n          // our text:\n          in.seek(indexEnum.seek(term));\n          didIndexNext = false;\n          if (doOrd) {\n            state.ord = indexEnum.ord()-1;\n          }\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexEnum.term());\n          //System.out.println(\"  doSeek term=\" + indexEnum.term().utf8ToString() + \" vs target=\" + term.utf8ToString());\n        } else {\n          //System.out.println(\"  skip seek\");\n        }\n\n        assert startSeek();\n\n        // Now scan:\n        while (next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n            // Done!\n            if (useCache) {\n              // Store in cache\n              FieldAndTerm entryKey = new FieldAndTerm(fieldTerm);\n              cachedState = (TermState) state.clone();\n              // this is fp after current term\n              cachedState.filePointer = in.getFilePointer();\n              termsCache.put(entryKey, cachedState);\n            }\n\n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert checkSeekScan();\n        }\n\n        positioned = false;\n        return SeekStatus.END;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","sourceNew":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(final BytesRef term, final boolean useCache) throws IOException {\n\n        if (indexEnum == null) {\n          throw new IllegalStateException(\"terms index was not loaded\");\n        }\n        \n        //System.out.println(\"te.seek term=\" + fieldInfo.name + \":\" + term.utf8ToString() + \" current=\" + term().utf8ToString() + \" useCache=\" + useCache + \" this=\"  + this);\n\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copyFrom(cachedState);\n            setTermState(term, state);\n            positioned = false;\n            //System.out.println(\"  cached!\");\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (positioned) {\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          } else if (cmp < 0) {\n\n            if (seekPending) {\n              seekPending = false;\n              in.seek(state.filePointer);\n              indexEnum.seek(bytesReader.term);\n              didIndexNext = false;\n            }\n\n            // Target term is after current term\n            if (!didIndexNext) {\n              if (indexEnum.next() == -1) {\n                nextIndexTerm = null;\n              } else {\n                nextIndexTerm = indexEnum.term();\n              }\n              //System.out.println(\"  now do index next() nextIndexTerm=\" + (nextIndexTerm == null ? \"null\" : nextIndexTerm.utf8ToString()));\n              didIndexNext = true;\n            }\n\n            if (nextIndexTerm == null || termComp.compare(term, nextIndexTerm) < 0) {\n              // Optimization: requested term is within the\n              // same index block we are now in; skip seeking\n              // (but do scanning):\n              doSeek = false;\n              //System.out.println(\"  skip seek: nextIndexTerm=\" + nextIndexTerm);\n            }\n          }\n        }\n\n        if (doSeek) {\n\n          positioned = true;\n\n          // Ask terms index to find biggest index term that's <=\n          // our text:\n          in.seek(indexEnum.seek(term));\n          didIndexNext = false;\n          if (doOrd) {\n            state.ord = indexEnum.ord()-1;\n          }\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexEnum.term());\n          //System.out.println(\"  doSeek term=\" + indexEnum.term().utf8ToString() + \" vs target=\" + term.utf8ToString());\n        } else {\n          //System.out.println(\"  skip seek\");\n        }\n\n        assert startSeek();\n\n        // Now scan:\n        while (next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n            // Done!\n            if (useCache) {\n              cacheTerm(fieldTerm);\n            }\n\n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert checkSeekScan();\n        }\n\n        positioned = false;\n        return SeekStatus.END;\n      }\n\n","sourceOld":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(BytesRef term, boolean useCache) throws IOException {\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copy(cachedState);\n            seekPending = true;\n            bytesReader.term.copy(term);\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (state.ord != -1) {\n          // we are positioned\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          }\n\n          if (cmp < 0 &&\n              fieldIndexReader.nextIndexTerm(state.ord, indexResult) &&\n              termComp.compare(indexResult.term, term) > 0) {\n            // Optimization: requested term is within the\n            // same index block we are now in; skip seeking\n            // (but do scanning):\n            doSeek = false;\n          }\n        }\n\n        // Used only for assert:\n        final long startOrd;\n\n        if (doSeek) {\n\n          // As index to find biggest index term that's <=\n          // our text:\n          fieldIndexReader.getIndexOffset(term, indexResult);\n\n          in.seek(indexResult.offset);\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexResult.term);\n          \n          state.ord = indexResult.position-1;\n          assert state.ord >= -1: \"ord=\" + state.ord + \" pos=\" + indexResult.position;\n\n          startOrd = indexResult.position;\n        } else {\n          startOrd = -1;\n        }\n\n        // Now scan:\n        while(next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n\n            if (doSeek && useCache) {\n              // Store in cache\n              FieldAndTerm entryKey = new FieldAndTerm(fieldTerm);\n              cachedState = (TermState) state.clone();\n              // this is fp after current term\n              cachedState.filePointer = in.getFilePointer();\n              termsCache.put(entryKey, cachedState);\n            }\n              \n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert state.ord == startOrd || !fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true): \"state.ord=\" + state.ord + \" startOrd=\" + startOrd + \" ir.isIndexTerm=\" + fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true) + \" state.docFreq=\" + state.docFreq;\n        }\n\n        return SeekStatus.END;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b1add9ddc0005b07550d4350720aac22dc9886b3","date":1295549635,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","sourceNew":"      @Override\n      public SeekStatus seek(final BytesRef target, final boolean useCache) throws IOException {\n\n        if (indexEnum == null) {\n          throw new IllegalStateException(\"terms index was not loaded\");\n        }\n        \n        //System.out.println(\"BTR.seek seg=\" + segment + \" target=\" + fieldInfo.name + \":\" + target.utf8ToString() + \" \" + target + \" current=\" + term().utf8ToString() + \" \" + term() + \" useCache=\" + useCache + \" indexIsCurrent=\" + indexIsCurrent + \" didIndexNext=\" + didIndexNext + \" seekPending=\" + seekPending + \" divisor=\" + indexReader.getDivisor() + \" this=\"  + this);\n        /*\n        if (didIndexNext) {\n          if (nextIndexTerm == null) {\n            //System.out.println(\"  nextIndexTerm=null\");\n          } else {\n            //System.out.println(\"  nextIndexTerm=\" + nextIndexTerm.utf8ToString());\n          }\n        }\n        */\n\n        // Check cache\n        if (useCache) {\n          fieldTerm.term = target;\n          // TODO: should we differentiate \"frozen\"\n          // TermState (ie one that was cloned and\n          // cached/returned by termState()) from the\n          // malleable (primary) one?\n          final TermState cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            seekPending = true;\n            //System.out.println(\"  cached!\");\n            seek(target, cachedState);\n            //System.out.println(\"  term=\" + term.utf8ToString());\n            return SeekStatus.FOUND;\n          }\n        }\n\n        boolean doSeek = true;\n\n        // See if we can avoid seeking, because target term\n        // is after current term but before next index term:\n        if (indexIsCurrent) {\n\n          final int cmp = termComp.compare(term, target);\n\n          if (cmp == 0) {\n            // Already at the requested term\n            return SeekStatus.FOUND;\n          } else if (cmp < 0) {\n\n            // Target term is after current term\n            if (!didIndexNext) {\n              if (indexEnum.next() == -1) {\n                nextIndexTerm = null;\n              } else {\n                nextIndexTerm = indexEnum.term();\n              }\n              //System.out.println(\"  now do index next() nextIndexTerm=\" + (nextIndexTerm == null ? \"null\" : nextIndexTerm.utf8ToString()));\n              didIndexNext = true;\n            }\n\n            if (nextIndexTerm == null || termComp.compare(target, nextIndexTerm) < 0) {\n              // Optimization: requested term is within the\n              // same term block we are now in; skip seeking\n              // (but do scanning):\n              doSeek = false;\n              //System.out.println(\"  skip seek: nextIndexTerm=\" + (nextIndexTerm == null ? \"null\" : nextIndexTerm.utf8ToString()));\n            }\n          }\n        }\n\n        if (doSeek) {\n          //System.out.println(\"  seek\");\n\n          // Ask terms index to find biggest indexed term (=\n          // first term in a block) that's <= our text:\n          in.seek(indexEnum.seek(target));\n          boolean result = nextBlock();\n\n          // Block must exist since, at least, the indexed term\n          // is in the block:\n          assert result;\n\n          indexIsCurrent = true;\n          didIndexNext = false;\n          blocksSinceSeek = 0;          \n\n          if (doOrd) {\n            state.ord = indexEnum.ord()-1;\n          }\n\n          // NOTE: the first _next() after an index seek is\n          // a bit wasteful, since it redundantly reads some\n          // suffix bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // next()ing over an index term we'd have to\n          // special case it:\n          term.copy(indexEnum.term());\n          //System.out.println(\"  seek: term=\" + term.utf8ToString());\n        } else {\n          ////System.out.println(\"  skip seek\");\n        }\n\n        seekPending = false;\n\n        // Now scan:\n        while (_next() != null) {\n          final int cmp = termComp.compare(term, target);\n          if (cmp == 0) {\n            // Match!\n            if (useCache) {\n              // Store in cache\n              decodeMetaData();\n              termsCache.put(new FieldAndTerm(fieldTerm), (BlockTermState) state.clone());\n            }\n            //System.out.println(\"  FOUND\");\n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            //System.out.println(\"  NOT_FOUND term=\" + term.utf8ToString());\n            return SeekStatus.NOT_FOUND;\n          }\n          \n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert indexIsCurrent;\n        }\n\n        indexIsCurrent = false;\n        //System.out.println(\"  END\");\n        return SeekStatus.END;\n      }\n\n","sourceOld":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(final BytesRef term, final boolean useCache) throws IOException {\n\n        if (indexEnum == null) {\n          throw new IllegalStateException(\"terms index was not loaded\");\n        }\n        \n        //System.out.println(\"te.seek term=\" + fieldInfo.name + \":\" + term.utf8ToString() + \" current=\" + term().utf8ToString() + \" useCache=\" + useCache + \" this=\"  + this);\n\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copyFrom(cachedState);\n            setTermState(term, state);\n            positioned = false;\n            //System.out.println(\"  cached!\");\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (positioned) {\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          } else if (cmp < 0) {\n\n            if (seekPending) {\n              seekPending = false;\n              in.seek(state.filePointer);\n              indexEnum.seek(bytesReader.term);\n              didIndexNext = false;\n            }\n\n            // Target term is after current term\n            if (!didIndexNext) {\n              if (indexEnum.next() == -1) {\n                nextIndexTerm = null;\n              } else {\n                nextIndexTerm = indexEnum.term();\n              }\n              //System.out.println(\"  now do index next() nextIndexTerm=\" + (nextIndexTerm == null ? \"null\" : nextIndexTerm.utf8ToString()));\n              didIndexNext = true;\n            }\n\n            if (nextIndexTerm == null || termComp.compare(term, nextIndexTerm) < 0) {\n              // Optimization: requested term is within the\n              // same index block we are now in; skip seeking\n              // (but do scanning):\n              doSeek = false;\n              //System.out.println(\"  skip seek: nextIndexTerm=\" + nextIndexTerm);\n            }\n          }\n        }\n\n        if (doSeek) {\n\n          positioned = true;\n\n          // Ask terms index to find biggest index term that's <=\n          // our text:\n          in.seek(indexEnum.seek(term));\n          didIndexNext = false;\n          if (doOrd) {\n            state.ord = indexEnum.ord()-1;\n          }\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexEnum.term());\n          //System.out.println(\"  doSeek term=\" + indexEnum.term().utf8ToString() + \" vs target=\" + term.utf8ToString());\n        } else {\n          //System.out.println(\"  skip seek\");\n        }\n\n        assert startSeek();\n\n        // Now scan:\n        while (next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n            // Done!\n            if (useCache) {\n              cacheTerm(fieldTerm);\n            }\n\n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert checkSeekScan();\n        }\n\n        positioned = false;\n        return SeekStatus.END;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e79a6d080bdd5b2a8f56342cf571b5476de04180","date":1295638686,"type":4,"author":"Michael Busch","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","sourceNew":null,"sourceOld":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(final BytesRef term, final boolean useCache) throws IOException {\n\n        if (indexEnum == null) {\n          throw new IllegalStateException(\"terms index was not loaded\");\n        }\n        \n        //System.out.println(\"te.seek term=\" + fieldInfo.name + \":\" + term.utf8ToString() + \" current=\" + term().utf8ToString() + \" useCache=\" + useCache + \" this=\"  + this);\n\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copyFrom(cachedState);\n            setTermState(term, state);\n            positioned = false;\n            //System.out.println(\"  cached!\");\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (positioned) {\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          } else if (cmp < 0) {\n\n            if (seekPending) {\n              seekPending = false;\n              in.seek(state.filePointer);\n              indexEnum.seek(bytesReader.term);\n              didIndexNext = false;\n            }\n\n            // Target term is after current term\n            if (!didIndexNext) {\n              if (indexEnum.next() == -1) {\n                nextIndexTerm = null;\n              } else {\n                nextIndexTerm = indexEnum.term();\n              }\n              //System.out.println(\"  now do index next() nextIndexTerm=\" + (nextIndexTerm == null ? \"null\" : nextIndexTerm.utf8ToString()));\n              didIndexNext = true;\n            }\n\n            if (nextIndexTerm == null || termComp.compare(term, nextIndexTerm) < 0) {\n              // Optimization: requested term is within the\n              // same index block we are now in; skip seeking\n              // (but do scanning):\n              doSeek = false;\n              //System.out.println(\"  skip seek: nextIndexTerm=\" + nextIndexTerm);\n            }\n          }\n        }\n\n        if (doSeek) {\n\n          positioned = true;\n\n          // Ask terms index to find biggest index term that's <=\n          // our text:\n          in.seek(indexEnum.seek(term));\n          didIndexNext = false;\n          if (doOrd) {\n            state.ord = indexEnum.ord()-1;\n          }\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexEnum.term());\n          //System.out.println(\"  doSeek term=\" + indexEnum.term().utf8ToString() + \" vs target=\" + term.utf8ToString());\n        } else {\n          //System.out.println(\"  skip seek\");\n        }\n\n        assert startSeek();\n\n        // Now scan:\n        while (next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n            // Done!\n            if (useCache) {\n              cacheTerm(fieldTerm);\n            }\n\n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert checkSeekScan();\n        }\n\n        positioned = false;\n        return SeekStatus.END;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":4,"author":"Simon Willnauer","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","sourceNew":null,"sourceOld":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(BytesRef term, boolean useCache) throws IOException {\n\n        if (indexEnum == null) {\n          throw new IllegalStateException(\"terms index was not loaded\");\n        }\n        \n        //System.out.println(\"te.seek term=\" + fieldInfo.name + \":\" + term.utf8ToString() + \" current=\" + term().utf8ToString() + \" useCache=\" + useCache + \" this=\"  + this);\n\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copy(cachedState);\n            seekPending = true;\n            positioned = false;\n            bytesReader.term.copy(term);\n            //System.out.println(\"  cached!\");\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (positioned) {\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          } else if (cmp < 0) {\n\n            if (seekPending) {\n              seekPending = false;\n              in.seek(state.filePointer);\n              indexEnum.seek(bytesReader.term);\n              didIndexNext = false;\n            }\n\n            // Target term is after current term\n            if (!didIndexNext) {\n              if (indexEnum.next() == -1) {\n                nextIndexTerm = null;\n              } else {\n                nextIndexTerm = indexEnum.term();\n              }\n              //System.out.println(\"  now do index next() nextIndexTerm=\" + (nextIndexTerm == null ? \"null\" : nextIndexTerm.utf8ToString()));\n              didIndexNext = true;\n            }\n\n            if (nextIndexTerm == null || termComp.compare(term, nextIndexTerm) < 0) {\n              // Optimization: requested term is within the\n              // same index block we are now in; skip seeking\n              // (but do scanning):\n              doSeek = false;\n              //System.out.println(\"  skip seek: nextIndexTerm=\" + nextIndexTerm);\n            }\n          }\n        }\n\n        if (doSeek) {\n\n          positioned = true;\n\n          // Ask terms index to find biggest index term that's <=\n          // our text:\n          in.seek(indexEnum.seek(term));\n          didIndexNext = false;\n          if (doOrd) {\n            state.ord = indexEnum.ord()-1;\n          }\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexEnum.term());\n          //System.out.println(\"  doSeek term=\" + indexEnum.term().utf8ToString() + \" vs target=\" + term.utf8ToString());\n        } else {\n          //System.out.println(\"  skip seek\");\n        }\n\n        assert startSeek();\n\n        // Now scan:\n        while (next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n            // Done!\n            if (useCache) {\n              // Store in cache\n              FieldAndTerm entryKey = new FieldAndTerm(fieldTerm);\n              cachedState = (TermState) state.clone();\n              // this is fp after current term\n              cachedState.filePointer = in.getFilePointer();\n              termsCache.put(entryKey, cachedState);\n            }\n\n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert checkSeekScan();\n        }\n\n        positioned = false;\n        return SeekStatus.END;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"70ad682703b8585f5d0a637efec044d57ec05efb":["99cf56f3a650b908f7017a72f9d23940418f8a52","56da903869515527852ee21ea7ef7bfe414cd40d"],"56da903869515527852ee21ea7ef7bfe414cd40d":["99cf56f3a650b908f7017a72f9d23940418f8a52"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["70ad682703b8585f5d0a637efec044d57ec05efb","b1add9ddc0005b07550d4350720aac22dc9886b3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3d53c3f769ca0f9e7434937b792877770271aecf":["56da903869515527852ee21ea7ef7bfe414cd40d"],"fa1a999d6674423e5c4ac858b410283f6fe03f20":["3d53c3f769ca0f9e7434937b792877770271aecf"],"b1add9ddc0005b07550d4350720aac22dc9886b3":["fa1a999d6674423e5c4ac858b410283f6fe03f20"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","99cf56f3a650b908f7017a72f9d23940418f8a52"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","fa1a999d6674423e5c4ac858b410283f6fe03f20"],"e79a6d080bdd5b2a8f56342cf571b5476de04180":["868da859b43505d9d2a023bfeae6dd0c795f5295","b1add9ddc0005b07550d4350720aac22dc9886b3"],"99cf56f3a650b908f7017a72f9d23940418f8a52":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b1add9ddc0005b07550d4350720aac22dc9886b3"]},"commit2Childs":{"70ad682703b8585f5d0a637efec044d57ec05efb":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"56da903869515527852ee21ea7ef7bfe414cd40d":["70ad682703b8585f5d0a637efec044d57ec05efb","3d53c3f769ca0f9e7434937b792877770271aecf"],"29ef99d61cda9641b6250bf9567329a6e65f901d":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","99cf56f3a650b908f7017a72f9d23940418f8a52"],"3d53c3f769ca0f9e7434937b792877770271aecf":["fa1a999d6674423e5c4ac858b410283f6fe03f20"],"fa1a999d6674423e5c4ac858b410283f6fe03f20":["b1add9ddc0005b07550d4350720aac22dc9886b3","868da859b43505d9d2a023bfeae6dd0c795f5295"],"b1add9ddc0005b07550d4350720aac22dc9886b3":["29ef99d61cda9641b6250bf9567329a6e65f901d","e79a6d080bdd5b2a8f56342cf571b5476de04180","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["868da859b43505d9d2a023bfeae6dd0c795f5295"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["e79a6d080bdd5b2a8f56342cf571b5476de04180"],"e79a6d080bdd5b2a8f56342cf571b5476de04180":[],"99cf56f3a650b908f7017a72f9d23940418f8a52":["70ad682703b8585f5d0a637efec044d57ec05efb","56da903869515527852ee21ea7ef7bfe414cd40d","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["29ef99d61cda9641b6250bf9567329a6e65f901d","e79a6d080bdd5b2a8f56342cf571b5476de04180","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}