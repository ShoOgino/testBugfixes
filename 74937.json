{"path":"sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/RobotExclusionFilter#handleRequest(Message).mjava","commits":[{"id":"05d36e0b328ec96237035fbcca240e73631396e5","date":1020520725,"type":0,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/RobotExclusionFilter#handleRequest(Message).mjava","pathOld":"/dev/null","sourceNew":"    /**\n     * method that handles each URL request<p>\n     *\n     * This method will get the robots.txt file the first time a server is\n     * requested. See the description above.\n     *\n     * @param message\n     *      the (URL)Message\n     * @return\n     *      the original message or NULL if this host had a disallow on that URL\n     * @link{http://info.webcrawler.com/mak/projects/robots/norobots.html})\n     */\n\n    public Message handleRequest(Message message)\n    {\n        //log.logThreadSafe(\"handleRequest: got message: \" + message);\n        try\n        {\n            // assert message instanceof URLMessage;\n            URLMessage urlMsg = ((URLMessage) message);\n            URL url = urlMsg.getUrl();\n            //assert url != null;\n            HostInfo h = hostManager.getHostInfo(url.getHost());\n            if (!h.isRobotTxtChecked() && !h.isLoadingRobotsTxt())\n            {\n                log.logThreadSafe(\"handleRequest: starting to get robots.txt\");\n                // probably this results in Race Conditions here\n\n                rePool.doTask(new RobotExclusionTask(h), new Integer(h.id));\n                h.setLoadingRobotsTxt(true);\n            }\n\n            synchronized (h)\n            {\n                // isLoading...() and queuedRequest.insert() must be atomic\n                if (h.isLoadingRobotsTxt())\n                {\n\n                    //log.logThreadSafe(\"handleRequest: other thread is loading\");\n                    // assert h.queuedRequests != null\n                    h.queuedRequests.insert(message);\n                    // not thread safe\n                    log.logThreadSafe(\"handleRequest: queued file \" + url);\n                    return null;\n                }\n            }\n\n            //log.logThreadSafe(\"handleRequest: no thread is loading; robots.txt loaded\");\n            //log.logThreadSafe(\"handleRequest: checking if allowed\");\n            String path = url.getPath();\n            if (path == null || path.equals(\"\"))\n            {\n                path = \"/\";\n            }\n\n            if (h.isAllowed(path))\n            {\n                // log.logThreadSafe(\"handleRequest: file \" + urlMsg.getURLString() + \" ok\");\n                return message;\n            }\n            log.logThreadSafe(\"handleRequest: file \" + urlMsg.getURLString() + \" filtered\");\n            this.filtered++;\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n        return null;\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"35a1a3e95f2bf9aed45087ffee78298452d7c234","date":1024322368,"type":3,"author":"cmarschner","isMerge":false,"pathNew":"sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/RobotExclusionFilter#handleRequest(Message).mjava","pathOld":"sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/RobotExclusionFilter#handleRequest(Message).mjava","sourceNew":"    /**\n     * method that handles each URL request<p>\n     *\n     * This method will get the robots.txt file the first time a server is\n     * requested. See the description above.\n     *\n     * @param message\n     *      the (URL)Message\n     * @return\n     *      the original message or NULL if this host had a disallow on that URL\n     * @link{http://info.webcrawler.com/mak/projects/robots/norobots.html})\n     */\n\n    public Message handleRequest(Message message)\n    {\n        //log.logThreadSafe(\"handleRequest: got message: \" + message);\n        try\n        {\n            // assert message instanceof URLMessage;\n            URLMessage urlMsg = ((URLMessage) message);\n            URL url = urlMsg.getUrl();\n            //assert url != null;\n            HostInfo h = hostManager.getHostInfo(url.getHost().toLowerCase());\n            if (!h.isRobotTxtChecked() && !h.isLoadingRobotsTxt())\n            {\n                log.logThreadSafe(\"handleRequest: starting to get robots.txt\");\n                // probably this results in Race Conditions here\n\n                rePool.doTask(new RobotExclusionTask(h), new Integer(h.getId()));\n                h.setLoadingRobotsTxt(true);\n            }\n\n            synchronized (h)\n            {\n                // isLoading...() and queuedRequest.insert() must be atomic\n                if (h.isLoadingRobotsTxt())\n                {\n\n                    //log.logThreadSafe(\"handleRequest: other thread is loading\");\n                    // assert h.queuedRequests != null\n                    h.insertIntoQueue(message);\n                    // not thread safe\n                    log.logThreadSafe(\"handleRequest: queued file \" + url);\n                    return null;\n                }\n            }\n\n            //log.logThreadSafe(\"handleRequest: no thread is loading; robots.txt loaded\");\n            //log.logThreadSafe(\"handleRequest: checking if allowed\");\n            String path = url.getPath();\n            if (path == null || path.equals(\"\"))\n            {\n                path = \"/\";\n            }\n\n            if (h.isAllowed(path))\n            {\n                // log.logThreadSafe(\"handleRequest: file \" + urlMsg.getURLString() + \" ok\");\n                return message;\n            }\n            log.logThreadSafe(\"handleRequest: file \" + urlMsg.getURLString() + \" filtered\");\n            this.filtered++;\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n        return null;\n    }\n\n","sourceOld":"    /**\n     * method that handles each URL request<p>\n     *\n     * This method will get the robots.txt file the first time a server is\n     * requested. See the description above.\n     *\n     * @param message\n     *      the (URL)Message\n     * @return\n     *      the original message or NULL if this host had a disallow on that URL\n     * @link{http://info.webcrawler.com/mak/projects/robots/norobots.html})\n     */\n\n    public Message handleRequest(Message message)\n    {\n        //log.logThreadSafe(\"handleRequest: got message: \" + message);\n        try\n        {\n            // assert message instanceof URLMessage;\n            URLMessage urlMsg = ((URLMessage) message);\n            URL url = urlMsg.getUrl();\n            //assert url != null;\n            HostInfo h = hostManager.getHostInfo(url.getHost());\n            if (!h.isRobotTxtChecked() && !h.isLoadingRobotsTxt())\n            {\n                log.logThreadSafe(\"handleRequest: starting to get robots.txt\");\n                // probably this results in Race Conditions here\n\n                rePool.doTask(new RobotExclusionTask(h), new Integer(h.id));\n                h.setLoadingRobotsTxt(true);\n            }\n\n            synchronized (h)\n            {\n                // isLoading...() and queuedRequest.insert() must be atomic\n                if (h.isLoadingRobotsTxt())\n                {\n\n                    //log.logThreadSafe(\"handleRequest: other thread is loading\");\n                    // assert h.queuedRequests != null\n                    h.queuedRequests.insert(message);\n                    // not thread safe\n                    log.logThreadSafe(\"handleRequest: queued file \" + url);\n                    return null;\n                }\n            }\n\n            //log.logThreadSafe(\"handleRequest: no thread is loading; robots.txt loaded\");\n            //log.logThreadSafe(\"handleRequest: checking if allowed\");\n            String path = url.getPath();\n            if (path == null || path.equals(\"\"))\n            {\n                path = \"/\";\n            }\n\n            if (h.isAllowed(path))\n            {\n                // log.logThreadSafe(\"handleRequest: file \" + urlMsg.getURLString() + \" ok\");\n                return message;\n            }\n            log.logThreadSafe(\"handleRequest: file \" + urlMsg.getURLString() + \" filtered\");\n            this.filtered++;\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n        return null;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0badbfbc9869e6d3ce976803c01c6150b925393","date":1035299707,"type":3,"author":"cmarschner","isMerge":false,"pathNew":"sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/RobotExclusionFilter#handleRequest(Message).mjava","pathOld":"sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/RobotExclusionFilter#handleRequest(Message).mjava","sourceNew":"    /**\n     * method that handles each URL request<p>\n     *\n     * This method will get the robots.txt file the first time a server is\n     * requested. See the description above.\n     *\n     * @param message\n     *      the (URL)Message\n     * @return\n     *      the original message or NULL if this host had a disallow on that URL\n     * @link{http://info.webcrawler.com/mak/projects/robots/norobots.html})\n     */\n\n    public Message handleRequest(Message message)\n    {\n        //log.logThreadSafe(\"handleRequest: got message: \" + message);\n        try\n        {\n            // assert message instanceof URLMessage;\n            URLMessage urlMsg = ((URLMessage) message);\n            URL url = urlMsg.getUrl();\n//            String urlString = urlMsg.getNormalizedURLString();\n//            URL nUrl = new URL(urlString);\n            //assert url != null;\n            HostInfo h = hostManager.getHostInfo(url.getHost());\n            synchronized (h)\n            {\n                if (!h.isRobotTxtChecked() && !h.isLoadingRobotsTxt())\n                {\n                    log.logThreadSafe(\"handleRequest: starting to get robots.txt\");\n                    // probably this results in Race Conditions here\n\n                    rePool.doTask(new RobotExclusionTask(h), new Integer(h.getId()));\n                    h.setLoadingRobotsTxt(true);\n                }\n\n                // isLoading...() and queuedRequest.insert() must be atomic\n                if (h.isLoadingRobotsTxt())\n                {\n\n                    //log.logThreadSafe(\"handleRequest: other thread is loading\");\n                    // assert h.queuedRequests != null\n                    h.insertIntoQueue(message);\n                    // not thread safe\n                    log.logThreadSafe(\"handleRequest: queued file \" + url);\n                    return null;\n                }\n            }\n\n            //log.logThreadSafe(\"handleRequest: no thread is loading; robots.txt loaded\");\n            //log.logThreadSafe(\"handleRequest: checking if allowed\");\n            String path = url.getPath();\n            if (path == null || path.equals(\"\"))\n            {\n                path = \"/\";\n            }\n\n            if (h.isAllowed(path))\n            {\n                // log.logThreadSafe(\"handleRequest: file \" + urlMsg.getURLString() + \" ok\");\n                return message;\n            }\n            log.logThreadSafe(\"handleRequest: file \" + urlMsg.getURLString() + \" filtered\");\n            this.filtered++;\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n        return null;\n    }\n\n","sourceOld":"    /**\n     * method that handles each URL request<p>\n     *\n     * This method will get the robots.txt file the first time a server is\n     * requested. See the description above.\n     *\n     * @param message\n     *      the (URL)Message\n     * @return\n     *      the original message or NULL if this host had a disallow on that URL\n     * @link{http://info.webcrawler.com/mak/projects/robots/norobots.html})\n     */\n\n    public Message handleRequest(Message message)\n    {\n        //log.logThreadSafe(\"handleRequest: got message: \" + message);\n        try\n        {\n            // assert message instanceof URLMessage;\n            URLMessage urlMsg = ((URLMessage) message);\n            URL url = urlMsg.getUrl();\n            //assert url != null;\n            HostInfo h = hostManager.getHostInfo(url.getHost().toLowerCase());\n            if (!h.isRobotTxtChecked() && !h.isLoadingRobotsTxt())\n            {\n                log.logThreadSafe(\"handleRequest: starting to get robots.txt\");\n                // probably this results in Race Conditions here\n\n                rePool.doTask(new RobotExclusionTask(h), new Integer(h.getId()));\n                h.setLoadingRobotsTxt(true);\n            }\n\n            synchronized (h)\n            {\n                // isLoading...() and queuedRequest.insert() must be atomic\n                if (h.isLoadingRobotsTxt())\n                {\n\n                    //log.logThreadSafe(\"handleRequest: other thread is loading\");\n                    // assert h.queuedRequests != null\n                    h.insertIntoQueue(message);\n                    // not thread safe\n                    log.logThreadSafe(\"handleRequest: queued file \" + url);\n                    return null;\n                }\n            }\n\n            //log.logThreadSafe(\"handleRequest: no thread is loading; robots.txt loaded\");\n            //log.logThreadSafe(\"handleRequest: checking if allowed\");\n            String path = url.getPath();\n            if (path == null || path.equals(\"\"))\n            {\n                path = \"/\";\n            }\n\n            if (h.isAllowed(path))\n            {\n                // log.logThreadSafe(\"handleRequest: file \" + urlMsg.getURLString() + \" ok\");\n                return message;\n            }\n            log.logThreadSafe(\"handleRequest: file \" + urlMsg.getURLString() + \" filtered\");\n            this.filtered++;\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n        return null;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"afc16d717d9ed1a8e45371668ca6de674164d624","date":1103345442,"type":4,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"/dev/null","pathOld":"sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/RobotExclusionFilter#handleRequest(Message).mjava","sourceNew":null,"sourceOld":"    /**\n     * method that handles each URL request<p>\n     *\n     * This method will get the robots.txt file the first time a server is\n     * requested. See the description above.\n     *\n     * @param message\n     *      the (URL)Message\n     * @return\n     *      the original message or NULL if this host had a disallow on that URL\n     * @link{http://info.webcrawler.com/mak/projects/robots/norobots.html})\n     */\n\n    public Message handleRequest(Message message)\n    {\n        //log.logThreadSafe(\"handleRequest: got message: \" + message);\n        try\n        {\n            // assert message instanceof URLMessage;\n            URLMessage urlMsg = ((URLMessage) message);\n            URL url = urlMsg.getUrl();\n//            String urlString = urlMsg.getNormalizedURLString();\n//            URL nUrl = new URL(urlString);\n            //assert url != null;\n            HostInfo h = hostManager.getHostInfo(url.getHost());\n            synchronized (h)\n            {\n                if (!h.isRobotTxtChecked() && !h.isLoadingRobotsTxt())\n                {\n                    log.logThreadSafe(\"handleRequest: starting to get robots.txt\");\n                    // probably this results in Race Conditions here\n\n                    rePool.doTask(new RobotExclusionTask(h), new Integer(h.getId()));\n                    h.setLoadingRobotsTxt(true);\n                }\n\n                // isLoading...() and queuedRequest.insert() must be atomic\n                if (h.isLoadingRobotsTxt())\n                {\n\n                    //log.logThreadSafe(\"handleRequest: other thread is loading\");\n                    // assert h.queuedRequests != null\n                    h.insertIntoQueue(message);\n                    // not thread safe\n                    log.logThreadSafe(\"handleRequest: queued file \" + url);\n                    return null;\n                }\n            }\n\n            //log.logThreadSafe(\"handleRequest: no thread is loading; robots.txt loaded\");\n            //log.logThreadSafe(\"handleRequest: checking if allowed\");\n            String path = url.getPath();\n            if (path == null || path.equals(\"\"))\n            {\n                path = \"/\";\n            }\n\n            if (h.isAllowed(path))\n            {\n                // log.logThreadSafe(\"handleRequest: file \" + urlMsg.getURLString() + \" ok\");\n                return message;\n            }\n            log.logThreadSafe(\"handleRequest: file \" + urlMsg.getURLString() + \" filtered\");\n            this.filtered++;\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n        return null;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b0badbfbc9869e6d3ce976803c01c6150b925393":["35a1a3e95f2bf9aed45087ffee78298452d7c234"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"afc16d717d9ed1a8e45371668ca6de674164d624":["b0badbfbc9869e6d3ce976803c01c6150b925393"],"05d36e0b328ec96237035fbcca240e73631396e5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"35a1a3e95f2bf9aed45087ffee78298452d7c234":["05d36e0b328ec96237035fbcca240e73631396e5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["afc16d717d9ed1a8e45371668ca6de674164d624"]},"commit2Childs":{"b0badbfbc9869e6d3ce976803c01c6150b925393":["afc16d717d9ed1a8e45371668ca6de674164d624"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["05d36e0b328ec96237035fbcca240e73631396e5"],"afc16d717d9ed1a8e45371668ca6de674164d624":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"05d36e0b328ec96237035fbcca240e73631396e5":["35a1a3e95f2bf9aed45087ffee78298452d7c234"],"35a1a3e95f2bf9aed45087ffee78298452d7c234":["b0badbfbc9869e6d3ce976803c01c6150b925393"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}