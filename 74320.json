{"path":"lucene/sandbox/src/test/org/apache/lucene/sandbox/postingshighlight/TestPostingsHighlighterRanking.FakePassageFormatter#format(Passage[],String).mjava","commits":[{"id":"43ff047e697f5b71d06c7eec1406226951c59b80","date":1356561472,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/sandbox/src/test/org/apache/lucene/sandbox/postingshighlight/TestPostingsHighlighterRanking.FakePassageFormatter#format(Passage[],String).mjava","pathOld":"/dev/null","sourceNew":"    @Override\n    public String format(Passage passages[], String content) {\n      for (Passage p : passages) {\n        // verify some basics about the passage\n        assertTrue(p.getScore() >= 0);\n        assertTrue(p.getNumMatches() > 0);\n        assertTrue(p.getStartOffset() >= 0);\n        assertTrue(p.getStartOffset() <= content.length());\n        // we use a very simple analyzer. so we can assert the matches are correct\n        for (int i = 0; i < p.getNumMatches(); i++) {\n          Term term = p.getMatchTerms()[i];\n          assertEquals(\"body\", term.field());\n          int matchStart = p.getMatchStarts()[i];\n          assertTrue(matchStart >= 0);\n          int matchEnd = p.getMatchEnds()[i];\n          assertTrue(matchEnd >= 0);\n          // single character terms\n          assertEquals(matchStart+1, matchEnd);\n          // and the offsets must be correct...\n          BytesRef bytes = term.bytes();\n          assertEquals(1, bytes.length);\n          assertEquals((char)bytes.bytes[bytes.offset], Character.toLowerCase(content.charAt(matchStart)));\n        }\n        // record just the start/end offset for simplicity\n        seen.add(new Pair(p.getStartOffset(), p.getEndOffset()));\n      }\n      return \"bogus!!!!!!\";\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4b3d16cba9355e2e97962eb1c441bbd0b6735c15","date":1357426290,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking.FakePassageFormatter#format(Passage[],String).mjava","pathOld":"lucene/sandbox/src/test/org/apache/lucene/sandbox/postingshighlight/TestPostingsHighlighterRanking.FakePassageFormatter#format(Passage[],String).mjava","sourceNew":"    @Override\n    public String format(Passage passages[], String content) {\n      for (Passage p : passages) {\n        // verify some basics about the passage\n        assertTrue(p.getScore() >= 0);\n        assertTrue(p.getNumMatches() > 0);\n        assertTrue(p.getStartOffset() >= 0);\n        assertTrue(p.getStartOffset() <= content.length());\n        // we use a very simple analyzer. so we can assert the matches are correct\n        for (int i = 0; i < p.getNumMatches(); i++) {\n          Term term = p.getMatchTerms()[i];\n          assertEquals(\"body\", term.field());\n          int matchStart = p.getMatchStarts()[i];\n          assertTrue(matchStart >= 0);\n          int matchEnd = p.getMatchEnds()[i];\n          assertTrue(matchEnd >= 0);\n          // single character terms\n          assertEquals(matchStart+1, matchEnd);\n          // and the offsets must be correct...\n          BytesRef bytes = term.bytes();\n          assertEquals(1, bytes.length);\n          assertEquals((char)bytes.bytes[bytes.offset], Character.toLowerCase(content.charAt(matchStart)));\n        }\n        // record just the start/end offset for simplicity\n        seen.add(new Pair(p.getStartOffset(), p.getEndOffset()));\n      }\n      return \"bogus!!!!!!\";\n    }\n\n","sourceOld":"    @Override\n    public String format(Passage passages[], String content) {\n      for (Passage p : passages) {\n        // verify some basics about the passage\n        assertTrue(p.getScore() >= 0);\n        assertTrue(p.getNumMatches() > 0);\n        assertTrue(p.getStartOffset() >= 0);\n        assertTrue(p.getStartOffset() <= content.length());\n        // we use a very simple analyzer. so we can assert the matches are correct\n        for (int i = 0; i < p.getNumMatches(); i++) {\n          Term term = p.getMatchTerms()[i];\n          assertEquals(\"body\", term.field());\n          int matchStart = p.getMatchStarts()[i];\n          assertTrue(matchStart >= 0);\n          int matchEnd = p.getMatchEnds()[i];\n          assertTrue(matchEnd >= 0);\n          // single character terms\n          assertEquals(matchStart+1, matchEnd);\n          // and the offsets must be correct...\n          BytesRef bytes = term.bytes();\n          assertEquals(1, bytes.length);\n          assertEquals((char)bytes.bytes[bytes.offset], Character.toLowerCase(content.charAt(matchStart)));\n        }\n        // record just the start/end offset for simplicity\n        seen.add(new Pair(p.getStartOffset(), p.getEndOffset()));\n      }\n      return \"bogus!!!!!!\";\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"4b3d16cba9355e2e97962eb1c441bbd0b6735c15":["43ff047e697f5b71d06c7eec1406226951c59b80"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"43ff047e697f5b71d06c7eec1406226951c59b80":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["4b3d16cba9355e2e97962eb1c441bbd0b6735c15"]},"commit2Childs":{"4b3d16cba9355e2e97962eb1c441bbd0b6735c15":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["43ff047e697f5b71d06c7eec1406226951c59b80"],"43ff047e697f5b71d06c7eec1406226951c59b80":["4b3d16cba9355e2e97962eb1c441bbd0b6735c15"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}