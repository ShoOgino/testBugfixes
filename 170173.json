{"path":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/UnifiedHighlighter#highlightFieldsAsObjects(String[],Query,int[],int[]).mjava","commits":[{"id":"1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2","date":1475611903,"type":0,"author":"David Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/UnifiedHighlighter#highlightFieldsAsObjects(String[],Query,int[],int[]).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Expert: highlights the top-N passages from multiple fields,\n   * for the provided int[] docids, to custom Object as\n   * returned by the {@link PassageFormatter}.  Use\n   * this API to render to something other than String.\n   *\n   * @param fieldsIn      field names to highlight. Must have a stored string value.\n   * @param query         query to highlight.\n   * @param docIdsIn      containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to\n   *                      form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets\n   * corresponding to the documents in <code>docIdsIn</code>.\n   * If no highlights were found for a document, the\n   * first {@code maxPassages} from the field will\n   * be returned.\n   * @throws IOException              if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without\n   *                                  {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  protected Map<String, Object[]> highlightFieldsAsObjects(String[] fieldsIn, Query query, int[] docIdsIn,\n                                                           int[] maxPassagesIn) throws IOException {\n    if (fieldsIn.length < 1) {\n      throw new IllegalArgumentException(\"fieldsIn must not be empty\");\n    }\n    if (fieldsIn.length != maxPassagesIn.length) {\n      throw new IllegalArgumentException(\"invalid number of maxPassagesIn\");\n    }\n    if (searcher == null) {\n      throw new IllegalStateException(\"This method requires that an indexSearcher was passed in the \"\n          + \"constructor.  Perhaps you mean to call highlightWithoutSearcher?\");\n    }\n\n    // Sort docs & fields for sequential i/o\n\n    // Sort doc IDs w/ index to original order: (copy input arrays since we sort in-place)\n    int[] docIds = new int[docIdsIn.length];\n    int[] docInIndexes = new int[docIds.length]; // fill in ascending order; points into docIdsIn[]\n    copyAndSortDocIdsWithIndex(docIdsIn, docIds, docInIndexes); // latter 2 are \"out\" params\n\n    // Sort fields w/ maxPassages pair: (copy input arrays since we sort in-place)\n    final String fields[] = new String[fieldsIn.length];\n    final int maxPassages[] = new int[maxPassagesIn.length];\n    copyAndSortFieldsWithMaxPassages(fieldsIn, maxPassagesIn, fields, maxPassages); // latter 2 are \"out\" params\n\n    // Init field highlighters (where most of the highlight logic lives, and on a per field basis)\n    SortedSet<Term> queryTerms = extractTerms(query);\n    FieldHighlighter[] fieldHighlighters = new FieldHighlighter[fields.length];\n    int numTermVectors = 0;\n    int numPostings = 0;\n    for (int f = 0; f < fields.length; f++) {\n      FieldHighlighter fieldHighlighter = getFieldHighlighter(fields[f], query, queryTerms, maxPassages[f]);\n      fieldHighlighters[f] = fieldHighlighter;\n\n      switch (fieldHighlighter.getOffsetSource()) {\n        case TERM_VECTORS:\n          numTermVectors++;\n          break;\n        case POSTINGS:\n          numPostings++;\n          break;\n        case POSTINGS_WITH_TERM_VECTORS:\n          numTermVectors++;\n          numPostings++;\n          break;\n        case ANALYSIS:\n        case NONE_NEEDED:\n        default:\n          //do nothing\n          break;\n      }\n    }\n\n    int cacheCharsThreshold = calculateOptimalCacheCharsThreshold(numTermVectors, numPostings);\n\n    IndexReader indexReaderWithTermVecCache =\n        (numTermVectors >= 2) ? TermVectorReusingLeafReader.wrap(searcher.getIndexReader()) : null;\n\n    // [fieldIdx][docIdInIndex] of highlightDoc result\n    Object[][] highlightDocsInByField = new Object[fields.length][docIds.length];\n    // Highlight in doc batches determined by loadFieldValues (consumes from docIdIter)\n    DocIdSetIterator docIdIter = asDocIdSetIterator(docIds);\n    for (int batchDocIdx = 0; batchDocIdx < docIds.length; ) {\n      // Load the field values of the first batch of document(s) (note: commonly all docs are in this batch)\n      List<CharSequence[]> fieldValsByDoc =\n          loadFieldValues(fields, docIdIter, cacheCharsThreshold);\n      //    the size of the above list is the size of the batch (num of docs in the batch)\n\n      // Highlight in per-field order first, then by doc (better I/O pattern)\n      for (int fieldIdx = 0; fieldIdx < fields.length; fieldIdx++) {\n        Object[] resultByDocIn = highlightDocsInByField[fieldIdx];//parallel to docIdsIn\n        FieldHighlighter fieldHighlighter = fieldHighlighters[fieldIdx];\n        for (int docIdx = batchDocIdx; docIdx - batchDocIdx < fieldValsByDoc.size(); docIdx++) {\n          int docId = docIds[docIdx];//sorted order\n          CharSequence content = fieldValsByDoc.get(docIdx - batchDocIdx)[fieldIdx];\n          if (content == null) {\n            continue;\n          }\n          IndexReader indexReader =\n              (fieldHighlighter.getOffsetSource() == OffsetSource.TERM_VECTORS\n                  && indexReaderWithTermVecCache != null)\n                  ? indexReaderWithTermVecCache\n                  : searcher.getIndexReader();\n          int docInIndex = docInIndexes[docIdx];//original input order\n          assert resultByDocIn[docInIndex] == null;\n          resultByDocIn[docInIndex] =\n              fieldHighlighter\n                  .highlightFieldForDoc(indexReader, docId, content.toString());\n        }\n\n      }\n\n      batchDocIdx += fieldValsByDoc.size();\n    }\n    assert docIdIter.docID() == DocIdSetIterator.NO_MORE_DOCS\n        || docIdIter.nextDoc() == DocIdSetIterator.NO_MORE_DOCS;\n\n    // TODO reconsider the return type; since this is an \"advanced\" method, lets not return a Map?  Notice the only\n    //    caller simply iterates it to build another structure.\n\n    // field -> object highlights parallel to docIdsIn\n    Map<String, Object[]> resultMap = new HashMap<>(fields.length);\n    for (int f = 0; f < fields.length; f++) {\n      resultMap.put(fields[f], highlightDocsInByField[f]);\n    }\n    return resultMap;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/UnifiedHighlighter#highlightFieldsAsObjects(String[],Query,int[],int[]).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Expert: highlights the top-N passages from multiple fields,\n   * for the provided int[] docids, to custom Object as\n   * returned by the {@link PassageFormatter}.  Use\n   * this API to render to something other than String.\n   *\n   * @param fieldsIn      field names to highlight. Must have a stored string value.\n   * @param query         query to highlight.\n   * @param docIdsIn      containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to\n   *                      form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets\n   * corresponding to the documents in <code>docIdsIn</code>.\n   * If no highlights were found for a document, the\n   * first {@code maxPassages} from the field will\n   * be returned.\n   * @throws IOException              if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without\n   *                                  {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  protected Map<String, Object[]> highlightFieldsAsObjects(String[] fieldsIn, Query query, int[] docIdsIn,\n                                                           int[] maxPassagesIn) throws IOException {\n    if (fieldsIn.length < 1) {\n      throw new IllegalArgumentException(\"fieldsIn must not be empty\");\n    }\n    if (fieldsIn.length != maxPassagesIn.length) {\n      throw new IllegalArgumentException(\"invalid number of maxPassagesIn\");\n    }\n    if (searcher == null) {\n      throw new IllegalStateException(\"This method requires that an indexSearcher was passed in the \"\n          + \"constructor.  Perhaps you mean to call highlightWithoutSearcher?\");\n    }\n\n    // Sort docs & fields for sequential i/o\n\n    // Sort doc IDs w/ index to original order: (copy input arrays since we sort in-place)\n    int[] docIds = new int[docIdsIn.length];\n    int[] docInIndexes = new int[docIds.length]; // fill in ascending order; points into docIdsIn[]\n    copyAndSortDocIdsWithIndex(docIdsIn, docIds, docInIndexes); // latter 2 are \"out\" params\n\n    // Sort fields w/ maxPassages pair: (copy input arrays since we sort in-place)\n    final String fields[] = new String[fieldsIn.length];\n    final int maxPassages[] = new int[maxPassagesIn.length];\n    copyAndSortFieldsWithMaxPassages(fieldsIn, maxPassagesIn, fields, maxPassages); // latter 2 are \"out\" params\n\n    // Init field highlighters (where most of the highlight logic lives, and on a per field basis)\n    SortedSet<Term> queryTerms = extractTerms(query);\n    FieldHighlighter[] fieldHighlighters = new FieldHighlighter[fields.length];\n    int numTermVectors = 0;\n    int numPostings = 0;\n    for (int f = 0; f < fields.length; f++) {\n      FieldHighlighter fieldHighlighter = getFieldHighlighter(fields[f], query, queryTerms, maxPassages[f]);\n      fieldHighlighters[f] = fieldHighlighter;\n\n      switch (fieldHighlighter.getOffsetSource()) {\n        case TERM_VECTORS:\n          numTermVectors++;\n          break;\n        case POSTINGS:\n          numPostings++;\n          break;\n        case POSTINGS_WITH_TERM_VECTORS:\n          numTermVectors++;\n          numPostings++;\n          break;\n        case ANALYSIS:\n        case NONE_NEEDED:\n        default:\n          //do nothing\n          break;\n      }\n    }\n\n    int cacheCharsThreshold = calculateOptimalCacheCharsThreshold(numTermVectors, numPostings);\n\n    IndexReader indexReaderWithTermVecCache =\n        (numTermVectors >= 2) ? TermVectorReusingLeafReader.wrap(searcher.getIndexReader()) : null;\n\n    // [fieldIdx][docIdInIndex] of highlightDoc result\n    Object[][] highlightDocsInByField = new Object[fields.length][docIds.length];\n    // Highlight in doc batches determined by loadFieldValues (consumes from docIdIter)\n    DocIdSetIterator docIdIter = asDocIdSetIterator(docIds);\n    for (int batchDocIdx = 0; batchDocIdx < docIds.length; ) {\n      // Load the field values of the first batch of document(s) (note: commonly all docs are in this batch)\n      List<CharSequence[]> fieldValsByDoc =\n          loadFieldValues(fields, docIdIter, cacheCharsThreshold);\n      //    the size of the above list is the size of the batch (num of docs in the batch)\n\n      // Highlight in per-field order first, then by doc (better I/O pattern)\n      for (int fieldIdx = 0; fieldIdx < fields.length; fieldIdx++) {\n        Object[] resultByDocIn = highlightDocsInByField[fieldIdx];//parallel to docIdsIn\n        FieldHighlighter fieldHighlighter = fieldHighlighters[fieldIdx];\n        for (int docIdx = batchDocIdx; docIdx - batchDocIdx < fieldValsByDoc.size(); docIdx++) {\n          int docId = docIds[docIdx];//sorted order\n          CharSequence content = fieldValsByDoc.get(docIdx - batchDocIdx)[fieldIdx];\n          if (content == null) {\n            continue;\n          }\n          IndexReader indexReader =\n              (fieldHighlighter.getOffsetSource() == OffsetSource.TERM_VECTORS\n                  && indexReaderWithTermVecCache != null)\n                  ? indexReaderWithTermVecCache\n                  : searcher.getIndexReader();\n          int docInIndex = docInIndexes[docIdx];//original input order\n          assert resultByDocIn[docInIndex] == null;\n          resultByDocIn[docInIndex] =\n              fieldHighlighter\n                  .highlightFieldForDoc(indexReader, docId, content.toString());\n        }\n\n      }\n\n      batchDocIdx += fieldValsByDoc.size();\n    }\n    assert docIdIter.docID() == DocIdSetIterator.NO_MORE_DOCS\n        || docIdIter.nextDoc() == DocIdSetIterator.NO_MORE_DOCS;\n\n    // TODO reconsider the return type; since this is an \"advanced\" method, lets not return a Map?  Notice the only\n    //    caller simply iterates it to build another structure.\n\n    // field -> object highlights parallel to docIdsIn\n    Map<String, Object[]> resultMap = new HashMap<>(fields.length);\n    for (int f = 0; f < fields.length; f++) {\n      resultMap.put(fields[f], highlightDocsInByField[f]);\n    }\n    return resultMap;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5675b3bced0b155f0ff8001ce2e1e502be7c92f6","date":1480972317,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/UnifiedHighlighter#highlightFieldsAsObjects(String[],Query,int[],int[]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/UnifiedHighlighter#highlightFieldsAsObjects(String[],Query,int[],int[]).mjava","sourceNew":"  /**\n   * Expert: highlights the top-N passages from multiple fields,\n   * for the provided int[] docids, to custom Object as\n   * returned by the {@link PassageFormatter}.  Use\n   * this API to render to something other than String.\n   *\n   * @param fieldsIn      field names to highlight. Must have a stored string value.\n   * @param query         query to highlight.\n   * @param docIdsIn      containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to\n   *                      form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets\n   * corresponding to the documents in <code>docIdsIn</code>.\n   * If no highlights were found for a document, the\n   * first {@code maxPassages} from the field will\n   * be returned.\n   * @throws IOException              if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without\n   *                                  {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  protected Map<String, Object[]> highlightFieldsAsObjects(String[] fieldsIn, Query query, int[] docIdsIn,\n                                                           int[] maxPassagesIn) throws IOException {\n    if (fieldsIn.length < 1) {\n      throw new IllegalArgumentException(\"fieldsIn must not be empty\");\n    }\n    if (fieldsIn.length != maxPassagesIn.length) {\n      throw new IllegalArgumentException(\"invalid number of maxPassagesIn\");\n    }\n    if (searcher == null) {\n      throw new IllegalStateException(\"This method requires that an indexSearcher was passed in the \"\n          + \"constructor.  Perhaps you mean to call highlightWithoutSearcher?\");\n    }\n\n    // Sort docs & fields for sequential i/o\n\n    // Sort doc IDs w/ index to original order: (copy input arrays since we sort in-place)\n    int[] docIds = new int[docIdsIn.length];\n    int[] docInIndexes = new int[docIds.length]; // fill in ascending order; points into docIdsIn[]\n    copyAndSortDocIdsWithIndex(docIdsIn, docIds, docInIndexes); // latter 2 are \"out\" params\n\n    // Sort fields w/ maxPassages pair: (copy input arrays since we sort in-place)\n    final String fields[] = new String[fieldsIn.length];\n    final int maxPassages[] = new int[maxPassagesIn.length];\n    copyAndSortFieldsWithMaxPassages(fieldsIn, maxPassagesIn, fields, maxPassages); // latter 2 are \"out\" params\n\n    // Init field highlighters (where most of the highlight logic lives, and on a per field basis)\n    Set<Term> queryTerms = extractTerms(query);\n    FieldHighlighter[] fieldHighlighters = new FieldHighlighter[fields.length];\n    int numTermVectors = 0;\n    int numPostings = 0;\n    for (int f = 0; f < fields.length; f++) {\n      FieldHighlighter fieldHighlighter = getFieldHighlighter(fields[f], query, queryTerms, maxPassages[f]);\n      fieldHighlighters[f] = fieldHighlighter;\n\n      switch (fieldHighlighter.getOffsetSource()) {\n        case TERM_VECTORS:\n          numTermVectors++;\n          break;\n        case POSTINGS:\n          numPostings++;\n          break;\n        case POSTINGS_WITH_TERM_VECTORS:\n          numTermVectors++;\n          numPostings++;\n          break;\n        case ANALYSIS:\n        case NONE_NEEDED:\n        default:\n          //do nothing\n          break;\n      }\n    }\n\n    int cacheCharsThreshold = calculateOptimalCacheCharsThreshold(numTermVectors, numPostings);\n\n    IndexReader indexReaderWithTermVecCache =\n        (numTermVectors >= 2) ? TermVectorReusingLeafReader.wrap(searcher.getIndexReader()) : null;\n\n    // [fieldIdx][docIdInIndex] of highlightDoc result\n    Object[][] highlightDocsInByField = new Object[fields.length][docIds.length];\n    // Highlight in doc batches determined by loadFieldValues (consumes from docIdIter)\n    DocIdSetIterator docIdIter = asDocIdSetIterator(docIds);\n    for (int batchDocIdx = 0; batchDocIdx < docIds.length; ) {\n      // Load the field values of the first batch of document(s) (note: commonly all docs are in this batch)\n      List<CharSequence[]> fieldValsByDoc =\n          loadFieldValues(fields, docIdIter, cacheCharsThreshold);\n      //    the size of the above list is the size of the batch (num of docs in the batch)\n\n      // Highlight in per-field order first, then by doc (better I/O pattern)\n      for (int fieldIdx = 0; fieldIdx < fields.length; fieldIdx++) {\n        Object[] resultByDocIn = highlightDocsInByField[fieldIdx];//parallel to docIdsIn\n        FieldHighlighter fieldHighlighter = fieldHighlighters[fieldIdx];\n        for (int docIdx = batchDocIdx; docIdx - batchDocIdx < fieldValsByDoc.size(); docIdx++) {\n          int docId = docIds[docIdx];//sorted order\n          CharSequence content = fieldValsByDoc.get(docIdx - batchDocIdx)[fieldIdx];\n          if (content == null) {\n            continue;\n          }\n          IndexReader indexReader =\n              (fieldHighlighter.getOffsetSource() == OffsetSource.TERM_VECTORS\n                  && indexReaderWithTermVecCache != null)\n                  ? indexReaderWithTermVecCache\n                  : searcher.getIndexReader();\n          int docInIndex = docInIndexes[docIdx];//original input order\n          assert resultByDocIn[docInIndex] == null;\n          resultByDocIn[docInIndex] =\n              fieldHighlighter\n                  .highlightFieldForDoc(indexReader, docId, content.toString());\n        }\n\n      }\n\n      batchDocIdx += fieldValsByDoc.size();\n    }\n    assert docIdIter.docID() == DocIdSetIterator.NO_MORE_DOCS\n        || docIdIter.nextDoc() == DocIdSetIterator.NO_MORE_DOCS;\n\n    // TODO reconsider the return type; since this is an \"advanced\" method, lets not return a Map?  Notice the only\n    //    caller simply iterates it to build another structure.\n\n    // field -> object highlights parallel to docIdsIn\n    Map<String, Object[]> resultMap = new HashMap<>(fields.length);\n    for (int f = 0; f < fields.length; f++) {\n      resultMap.put(fields[f], highlightDocsInByField[f]);\n    }\n    return resultMap;\n  }\n\n","sourceOld":"  /**\n   * Expert: highlights the top-N passages from multiple fields,\n   * for the provided int[] docids, to custom Object as\n   * returned by the {@link PassageFormatter}.  Use\n   * this API to render to something other than String.\n   *\n   * @param fieldsIn      field names to highlight. Must have a stored string value.\n   * @param query         query to highlight.\n   * @param docIdsIn      containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to\n   *                      form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets\n   * corresponding to the documents in <code>docIdsIn</code>.\n   * If no highlights were found for a document, the\n   * first {@code maxPassages} from the field will\n   * be returned.\n   * @throws IOException              if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without\n   *                                  {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  protected Map<String, Object[]> highlightFieldsAsObjects(String[] fieldsIn, Query query, int[] docIdsIn,\n                                                           int[] maxPassagesIn) throws IOException {\n    if (fieldsIn.length < 1) {\n      throw new IllegalArgumentException(\"fieldsIn must not be empty\");\n    }\n    if (fieldsIn.length != maxPassagesIn.length) {\n      throw new IllegalArgumentException(\"invalid number of maxPassagesIn\");\n    }\n    if (searcher == null) {\n      throw new IllegalStateException(\"This method requires that an indexSearcher was passed in the \"\n          + \"constructor.  Perhaps you mean to call highlightWithoutSearcher?\");\n    }\n\n    // Sort docs & fields for sequential i/o\n\n    // Sort doc IDs w/ index to original order: (copy input arrays since we sort in-place)\n    int[] docIds = new int[docIdsIn.length];\n    int[] docInIndexes = new int[docIds.length]; // fill in ascending order; points into docIdsIn[]\n    copyAndSortDocIdsWithIndex(docIdsIn, docIds, docInIndexes); // latter 2 are \"out\" params\n\n    // Sort fields w/ maxPassages pair: (copy input arrays since we sort in-place)\n    final String fields[] = new String[fieldsIn.length];\n    final int maxPassages[] = new int[maxPassagesIn.length];\n    copyAndSortFieldsWithMaxPassages(fieldsIn, maxPassagesIn, fields, maxPassages); // latter 2 are \"out\" params\n\n    // Init field highlighters (where most of the highlight logic lives, and on a per field basis)\n    SortedSet<Term> queryTerms = extractTerms(query);\n    FieldHighlighter[] fieldHighlighters = new FieldHighlighter[fields.length];\n    int numTermVectors = 0;\n    int numPostings = 0;\n    for (int f = 0; f < fields.length; f++) {\n      FieldHighlighter fieldHighlighter = getFieldHighlighter(fields[f], query, queryTerms, maxPassages[f]);\n      fieldHighlighters[f] = fieldHighlighter;\n\n      switch (fieldHighlighter.getOffsetSource()) {\n        case TERM_VECTORS:\n          numTermVectors++;\n          break;\n        case POSTINGS:\n          numPostings++;\n          break;\n        case POSTINGS_WITH_TERM_VECTORS:\n          numTermVectors++;\n          numPostings++;\n          break;\n        case ANALYSIS:\n        case NONE_NEEDED:\n        default:\n          //do nothing\n          break;\n      }\n    }\n\n    int cacheCharsThreshold = calculateOptimalCacheCharsThreshold(numTermVectors, numPostings);\n\n    IndexReader indexReaderWithTermVecCache =\n        (numTermVectors >= 2) ? TermVectorReusingLeafReader.wrap(searcher.getIndexReader()) : null;\n\n    // [fieldIdx][docIdInIndex] of highlightDoc result\n    Object[][] highlightDocsInByField = new Object[fields.length][docIds.length];\n    // Highlight in doc batches determined by loadFieldValues (consumes from docIdIter)\n    DocIdSetIterator docIdIter = asDocIdSetIterator(docIds);\n    for (int batchDocIdx = 0; batchDocIdx < docIds.length; ) {\n      // Load the field values of the first batch of document(s) (note: commonly all docs are in this batch)\n      List<CharSequence[]> fieldValsByDoc =\n          loadFieldValues(fields, docIdIter, cacheCharsThreshold);\n      //    the size of the above list is the size of the batch (num of docs in the batch)\n\n      // Highlight in per-field order first, then by doc (better I/O pattern)\n      for (int fieldIdx = 0; fieldIdx < fields.length; fieldIdx++) {\n        Object[] resultByDocIn = highlightDocsInByField[fieldIdx];//parallel to docIdsIn\n        FieldHighlighter fieldHighlighter = fieldHighlighters[fieldIdx];\n        for (int docIdx = batchDocIdx; docIdx - batchDocIdx < fieldValsByDoc.size(); docIdx++) {\n          int docId = docIds[docIdx];//sorted order\n          CharSequence content = fieldValsByDoc.get(docIdx - batchDocIdx)[fieldIdx];\n          if (content == null) {\n            continue;\n          }\n          IndexReader indexReader =\n              (fieldHighlighter.getOffsetSource() == OffsetSource.TERM_VECTORS\n                  && indexReaderWithTermVecCache != null)\n                  ? indexReaderWithTermVecCache\n                  : searcher.getIndexReader();\n          int docInIndex = docInIndexes[docIdx];//original input order\n          assert resultByDocIn[docInIndex] == null;\n          resultByDocIn[docInIndex] =\n              fieldHighlighter\n                  .highlightFieldForDoc(indexReader, docId, content.toString());\n        }\n\n      }\n\n      batchDocIdx += fieldValsByDoc.size();\n    }\n    assert docIdIter.docID() == DocIdSetIterator.NO_MORE_DOCS\n        || docIdIter.nextDoc() == DocIdSetIterator.NO_MORE_DOCS;\n\n    // TODO reconsider the return type; since this is an \"advanced\" method, lets not return a Map?  Notice the only\n    //    caller simply iterates it to build another structure.\n\n    // field -> object highlights parallel to docIdsIn\n    Map<String, Object[]> resultMap = new HashMap<>(fields.length);\n    for (int f = 0; f < fields.length; f++) {\n      resultMap.put(fields[f], highlightDocsInByField[f]);\n    }\n    return resultMap;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ee8992dea79b51ba77a3fd1a5c0f94a5f0b8808d","date":1481116359,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/UnifiedHighlighter#highlightFieldsAsObjects(String[],Query,int[],int[]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/UnifiedHighlighter#highlightFieldsAsObjects(String[],Query,int[],int[]).mjava","sourceNew":"  /**\n   * Expert: highlights the top-N passages from multiple fields,\n   * for the provided int[] docids, to custom Object as\n   * returned by the {@link PassageFormatter}.  Use\n   * this API to render to something other than String.\n   *\n   * @param fieldsIn      field names to highlight. Must have a stored string value.\n   * @param query         query to highlight.\n   * @param docIdsIn      containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to\n   *                      form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets\n   * corresponding to the documents in <code>docIdsIn</code>.\n   * If no highlights were found for a document, the\n   * first {@code maxPassages} from the field will\n   * be returned.\n   * @throws IOException              if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without\n   *                                  {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  protected Map<String, Object[]> highlightFieldsAsObjects(String[] fieldsIn, Query query, int[] docIdsIn,\n                                                           int[] maxPassagesIn) throws IOException {\n    if (fieldsIn.length < 1) {\n      throw new IllegalArgumentException(\"fieldsIn must not be empty\");\n    }\n    if (fieldsIn.length != maxPassagesIn.length) {\n      throw new IllegalArgumentException(\"invalid number of maxPassagesIn\");\n    }\n    if (searcher == null) {\n      throw new IllegalStateException(\"This method requires that an indexSearcher was passed in the \"\n          + \"constructor.  Perhaps you mean to call highlightWithoutSearcher?\");\n    }\n\n    // Sort docs & fields for sequential i/o\n\n    // Sort doc IDs w/ index to original order: (copy input arrays since we sort in-place)\n    int[] docIds = new int[docIdsIn.length];\n    int[] docInIndexes = new int[docIds.length]; // fill in ascending order; points into docIdsIn[]\n    copyAndSortDocIdsWithIndex(docIdsIn, docIds, docInIndexes); // latter 2 are \"out\" params\n\n    // Sort fields w/ maxPassages pair: (copy input arrays since we sort in-place)\n    final String fields[] = new String[fieldsIn.length];\n    final int maxPassages[] = new int[maxPassagesIn.length];\n    copyAndSortFieldsWithMaxPassages(fieldsIn, maxPassagesIn, fields, maxPassages); // latter 2 are \"out\" params\n\n    // Init field highlighters (where most of the highlight logic lives, and on a per field basis)\n    Set<Term> queryTerms = extractTerms(query);\n    FieldHighlighter[] fieldHighlighters = new FieldHighlighter[fields.length];\n    int numTermVectors = 0;\n    int numPostings = 0;\n    for (int f = 0; f < fields.length; f++) {\n      FieldHighlighter fieldHighlighter = getFieldHighlighter(fields[f], query, queryTerms, maxPassages[f]);\n      fieldHighlighters[f] = fieldHighlighter;\n\n      switch (fieldHighlighter.getOffsetSource()) {\n        case TERM_VECTORS:\n          numTermVectors++;\n          break;\n        case POSTINGS:\n          numPostings++;\n          break;\n        case POSTINGS_WITH_TERM_VECTORS:\n          numTermVectors++;\n          numPostings++;\n          break;\n        case ANALYSIS:\n        case NONE_NEEDED:\n        default:\n          //do nothing\n          break;\n      }\n    }\n\n    int cacheCharsThreshold = calculateOptimalCacheCharsThreshold(numTermVectors, numPostings);\n\n    IndexReader indexReaderWithTermVecCache =\n        (numTermVectors >= 2) ? TermVectorReusingLeafReader.wrap(searcher.getIndexReader()) : null;\n\n    // [fieldIdx][docIdInIndex] of highlightDoc result\n    Object[][] highlightDocsInByField = new Object[fields.length][docIds.length];\n    // Highlight in doc batches determined by loadFieldValues (consumes from docIdIter)\n    DocIdSetIterator docIdIter = asDocIdSetIterator(docIds);\n    for (int batchDocIdx = 0; batchDocIdx < docIds.length; ) {\n      // Load the field values of the first batch of document(s) (note: commonly all docs are in this batch)\n      List<CharSequence[]> fieldValsByDoc =\n          loadFieldValues(fields, docIdIter, cacheCharsThreshold);\n      //    the size of the above list is the size of the batch (num of docs in the batch)\n\n      // Highlight in per-field order first, then by doc (better I/O pattern)\n      for (int fieldIdx = 0; fieldIdx < fields.length; fieldIdx++) {\n        Object[] resultByDocIn = highlightDocsInByField[fieldIdx];//parallel to docIdsIn\n        FieldHighlighter fieldHighlighter = fieldHighlighters[fieldIdx];\n        for (int docIdx = batchDocIdx; docIdx - batchDocIdx < fieldValsByDoc.size(); docIdx++) {\n          int docId = docIds[docIdx];//sorted order\n          CharSequence content = fieldValsByDoc.get(docIdx - batchDocIdx)[fieldIdx];\n          if (content == null) {\n            continue;\n          }\n          IndexReader indexReader =\n              (fieldHighlighter.getOffsetSource() == OffsetSource.TERM_VECTORS\n                  && indexReaderWithTermVecCache != null)\n                  ? indexReaderWithTermVecCache\n                  : searcher.getIndexReader();\n          int docInIndex = docInIndexes[docIdx];//original input order\n          assert resultByDocIn[docInIndex] == null;\n          resultByDocIn[docInIndex] =\n              fieldHighlighter\n                  .highlightFieldForDoc(indexReader, docId, content.toString());\n        }\n\n      }\n\n      batchDocIdx += fieldValsByDoc.size();\n    }\n    assert docIdIter.docID() == DocIdSetIterator.NO_MORE_DOCS\n        || docIdIter.nextDoc() == DocIdSetIterator.NO_MORE_DOCS;\n\n    // TODO reconsider the return type; since this is an \"advanced\" method, lets not return a Map?  Notice the only\n    //    caller simply iterates it to build another structure.\n\n    // field -> object highlights parallel to docIdsIn\n    Map<String, Object[]> resultMap = new HashMap<>(fields.length);\n    for (int f = 0; f < fields.length; f++) {\n      resultMap.put(fields[f], highlightDocsInByField[f]);\n    }\n    return resultMap;\n  }\n\n","sourceOld":"  /**\n   * Expert: highlights the top-N passages from multiple fields,\n   * for the provided int[] docids, to custom Object as\n   * returned by the {@link PassageFormatter}.  Use\n   * this API to render to something other than String.\n   *\n   * @param fieldsIn      field names to highlight. Must have a stored string value.\n   * @param query         query to highlight.\n   * @param docIdsIn      containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to\n   *                      form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets\n   * corresponding to the documents in <code>docIdsIn</code>.\n   * If no highlights were found for a document, the\n   * first {@code maxPassages} from the field will\n   * be returned.\n   * @throws IOException              if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without\n   *                                  {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  protected Map<String, Object[]> highlightFieldsAsObjects(String[] fieldsIn, Query query, int[] docIdsIn,\n                                                           int[] maxPassagesIn) throws IOException {\n    if (fieldsIn.length < 1) {\n      throw new IllegalArgumentException(\"fieldsIn must not be empty\");\n    }\n    if (fieldsIn.length != maxPassagesIn.length) {\n      throw new IllegalArgumentException(\"invalid number of maxPassagesIn\");\n    }\n    if (searcher == null) {\n      throw new IllegalStateException(\"This method requires that an indexSearcher was passed in the \"\n          + \"constructor.  Perhaps you mean to call highlightWithoutSearcher?\");\n    }\n\n    // Sort docs & fields for sequential i/o\n\n    // Sort doc IDs w/ index to original order: (copy input arrays since we sort in-place)\n    int[] docIds = new int[docIdsIn.length];\n    int[] docInIndexes = new int[docIds.length]; // fill in ascending order; points into docIdsIn[]\n    copyAndSortDocIdsWithIndex(docIdsIn, docIds, docInIndexes); // latter 2 are \"out\" params\n\n    // Sort fields w/ maxPassages pair: (copy input arrays since we sort in-place)\n    final String fields[] = new String[fieldsIn.length];\n    final int maxPassages[] = new int[maxPassagesIn.length];\n    copyAndSortFieldsWithMaxPassages(fieldsIn, maxPassagesIn, fields, maxPassages); // latter 2 are \"out\" params\n\n    // Init field highlighters (where most of the highlight logic lives, and on a per field basis)\n    SortedSet<Term> queryTerms = extractTerms(query);\n    FieldHighlighter[] fieldHighlighters = new FieldHighlighter[fields.length];\n    int numTermVectors = 0;\n    int numPostings = 0;\n    for (int f = 0; f < fields.length; f++) {\n      FieldHighlighter fieldHighlighter = getFieldHighlighter(fields[f], query, queryTerms, maxPassages[f]);\n      fieldHighlighters[f] = fieldHighlighter;\n\n      switch (fieldHighlighter.getOffsetSource()) {\n        case TERM_VECTORS:\n          numTermVectors++;\n          break;\n        case POSTINGS:\n          numPostings++;\n          break;\n        case POSTINGS_WITH_TERM_VECTORS:\n          numTermVectors++;\n          numPostings++;\n          break;\n        case ANALYSIS:\n        case NONE_NEEDED:\n        default:\n          //do nothing\n          break;\n      }\n    }\n\n    int cacheCharsThreshold = calculateOptimalCacheCharsThreshold(numTermVectors, numPostings);\n\n    IndexReader indexReaderWithTermVecCache =\n        (numTermVectors >= 2) ? TermVectorReusingLeafReader.wrap(searcher.getIndexReader()) : null;\n\n    // [fieldIdx][docIdInIndex] of highlightDoc result\n    Object[][] highlightDocsInByField = new Object[fields.length][docIds.length];\n    // Highlight in doc batches determined by loadFieldValues (consumes from docIdIter)\n    DocIdSetIterator docIdIter = asDocIdSetIterator(docIds);\n    for (int batchDocIdx = 0; batchDocIdx < docIds.length; ) {\n      // Load the field values of the first batch of document(s) (note: commonly all docs are in this batch)\n      List<CharSequence[]> fieldValsByDoc =\n          loadFieldValues(fields, docIdIter, cacheCharsThreshold);\n      //    the size of the above list is the size of the batch (num of docs in the batch)\n\n      // Highlight in per-field order first, then by doc (better I/O pattern)\n      for (int fieldIdx = 0; fieldIdx < fields.length; fieldIdx++) {\n        Object[] resultByDocIn = highlightDocsInByField[fieldIdx];//parallel to docIdsIn\n        FieldHighlighter fieldHighlighter = fieldHighlighters[fieldIdx];\n        for (int docIdx = batchDocIdx; docIdx - batchDocIdx < fieldValsByDoc.size(); docIdx++) {\n          int docId = docIds[docIdx];//sorted order\n          CharSequence content = fieldValsByDoc.get(docIdx - batchDocIdx)[fieldIdx];\n          if (content == null) {\n            continue;\n          }\n          IndexReader indexReader =\n              (fieldHighlighter.getOffsetSource() == OffsetSource.TERM_VECTORS\n                  && indexReaderWithTermVecCache != null)\n                  ? indexReaderWithTermVecCache\n                  : searcher.getIndexReader();\n          int docInIndex = docInIndexes[docIdx];//original input order\n          assert resultByDocIn[docInIndex] == null;\n          resultByDocIn[docInIndex] =\n              fieldHighlighter\n                  .highlightFieldForDoc(indexReader, docId, content.toString());\n        }\n\n      }\n\n      batchDocIdx += fieldValsByDoc.size();\n    }\n    assert docIdIter.docID() == DocIdSetIterator.NO_MORE_DOCS\n        || docIdIter.nextDoc() == DocIdSetIterator.NO_MORE_DOCS;\n\n    // TODO reconsider the return type; since this is an \"advanced\" method, lets not return a Map?  Notice the only\n    //    caller simply iterates it to build another structure.\n\n    // field -> object highlights parallel to docIdsIn\n    Map<String, Object[]> resultMap = new HashMap<>(fields.length);\n    for (int f = 0; f < fields.length; f++) {\n      resultMap.put(fields[f], highlightDocsInByField[f]);\n    }\n    return resultMap;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9856095f7afb5a607bf5e65077615ed91273508c","date":1481837697,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/UnifiedHighlighter#highlightFieldsAsObjects(String[],Query,int[],int[]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/UnifiedHighlighter#highlightFieldsAsObjects(String[],Query,int[],int[]).mjava","sourceNew":"  /**\n   * Expert: highlights the top-N passages from multiple fields,\n   * for the provided int[] docids, to custom Object as\n   * returned by the {@link PassageFormatter}.  Use\n   * this API to render to something other than String.\n   *\n   * @param fieldsIn      field names to highlight. Must have a stored string value.\n   * @param query         query to highlight.\n   * @param docIdsIn      containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to\n   *                      form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets\n   * corresponding to the documents in <code>docIdsIn</code>.\n   * If no highlights were found for a document, the\n   * first {@code maxPassages} from the field will\n   * be returned.\n   * @throws IOException              if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without\n   *                                  {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  protected Map<String, Object[]> highlightFieldsAsObjects(String[] fieldsIn, Query query, int[] docIdsIn,\n                                                           int[] maxPassagesIn) throws IOException {\n    if (fieldsIn.length < 1) {\n      throw new IllegalArgumentException(\"fieldsIn must not be empty\");\n    }\n    if (fieldsIn.length != maxPassagesIn.length) {\n      throw new IllegalArgumentException(\"invalid number of maxPassagesIn\");\n    }\n    if (searcher == null) {\n      throw new IllegalStateException(\"This method requires that an indexSearcher was passed in the \"\n          + \"constructor.  Perhaps you mean to call highlightWithoutSearcher?\");\n    }\n\n    // Sort docs & fields for sequential i/o\n\n    // Sort doc IDs w/ index to original order: (copy input arrays since we sort in-place)\n    int[] docIds = new int[docIdsIn.length];\n    int[] docInIndexes = new int[docIds.length]; // fill in ascending order; points into docIdsIn[]\n    copyAndSortDocIdsWithIndex(docIdsIn, docIds, docInIndexes); // latter 2 are \"out\" params\n\n    // Sort fields w/ maxPassages pair: (copy input arrays since we sort in-place)\n    final String fields[] = new String[fieldsIn.length];\n    final int maxPassages[] = new int[maxPassagesIn.length];\n    copyAndSortFieldsWithMaxPassages(fieldsIn, maxPassagesIn, fields, maxPassages); // latter 2 are \"out\" params\n\n    // Init field highlighters (where most of the highlight logic lives, and on a per field basis)\n    Set<Term> queryTerms = extractTerms(query);\n    FieldHighlighter[] fieldHighlighters = new FieldHighlighter[fields.length];\n    int numTermVectors = 0;\n    int numPostings = 0;\n    for (int f = 0; f < fields.length; f++) {\n      FieldHighlighter fieldHighlighter = getFieldHighlighter(fields[f], query, queryTerms, maxPassages[f]);\n      fieldHighlighters[f] = fieldHighlighter;\n\n      switch (fieldHighlighter.getOffsetSource()) {\n        case TERM_VECTORS:\n          numTermVectors++;\n          break;\n        case POSTINGS:\n          numPostings++;\n          break;\n        case POSTINGS_WITH_TERM_VECTORS:\n          numTermVectors++;\n          numPostings++;\n          break;\n        case ANALYSIS:\n        case NONE_NEEDED:\n        default:\n          //do nothing\n          break;\n      }\n    }\n\n    int cacheCharsThreshold = calculateOptimalCacheCharsThreshold(numTermVectors, numPostings);\n\n    IndexReader indexReaderWithTermVecCache =\n        (numTermVectors >= 2) ? TermVectorReusingLeafReader.wrap(searcher.getIndexReader()) : null;\n\n    // [fieldIdx][docIdInIndex] of highlightDoc result\n    Object[][] highlightDocsInByField = new Object[fields.length][docIds.length];\n    // Highlight in doc batches determined by loadFieldValues (consumes from docIdIter)\n    DocIdSetIterator docIdIter = asDocIdSetIterator(docIds);\n    for (int batchDocIdx = 0; batchDocIdx < docIds.length; ) {\n      // Load the field values of the first batch of document(s) (note: commonly all docs are in this batch)\n      List<CharSequence[]> fieldValsByDoc =\n          loadFieldValues(fields, docIdIter, cacheCharsThreshold);\n      //    the size of the above list is the size of the batch (num of docs in the batch)\n\n      // Highlight in per-field order first, then by doc (better I/O pattern)\n      for (int fieldIdx = 0; fieldIdx < fields.length; fieldIdx++) {\n        Object[] resultByDocIn = highlightDocsInByField[fieldIdx];//parallel to docIdsIn\n        FieldHighlighter fieldHighlighter = fieldHighlighters[fieldIdx];\n        for (int docIdx = batchDocIdx; docIdx - batchDocIdx < fieldValsByDoc.size(); docIdx++) {\n          int docId = docIds[docIdx];//sorted order\n          CharSequence content = fieldValsByDoc.get(docIdx - batchDocIdx)[fieldIdx];\n          if (content == null) {\n            continue;\n          }\n          IndexReader indexReader =\n              (fieldHighlighter.getOffsetSource() == OffsetSource.TERM_VECTORS\n                  && indexReaderWithTermVecCache != null)\n                  ? indexReaderWithTermVecCache\n                  : searcher.getIndexReader();\n          int docInIndex = docInIndexes[docIdx];//original input order\n          assert resultByDocIn[docInIndex] == null;\n          resultByDocIn[docInIndex] =\n              fieldHighlighter\n                  .highlightFieldForDoc(indexReader, docId, content.toString());\n        }\n\n      }\n\n      batchDocIdx += fieldValsByDoc.size();\n    }\n    assert docIdIter.docID() == DocIdSetIterator.NO_MORE_DOCS\n        || docIdIter.nextDoc() == DocIdSetIterator.NO_MORE_DOCS;\n\n    // TODO reconsider the return type; since this is an \"advanced\" method, lets not return a Map?  Notice the only\n    //    caller simply iterates it to build another structure.\n\n    // field -> object highlights parallel to docIdsIn\n    Map<String, Object[]> resultMap = new HashMap<>(fields.length);\n    for (int f = 0; f < fields.length; f++) {\n      resultMap.put(fields[f], highlightDocsInByField[f]);\n    }\n    return resultMap;\n  }\n\n","sourceOld":"  /**\n   * Expert: highlights the top-N passages from multiple fields,\n   * for the provided int[] docids, to custom Object as\n   * returned by the {@link PassageFormatter}.  Use\n   * this API to render to something other than String.\n   *\n   * @param fieldsIn      field names to highlight. Must have a stored string value.\n   * @param query         query to highlight.\n   * @param docIdsIn      containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to\n   *                      form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets\n   * corresponding to the documents in <code>docIdsIn</code>.\n   * If no highlights were found for a document, the\n   * first {@code maxPassages} from the field will\n   * be returned.\n   * @throws IOException              if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without\n   *                                  {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  protected Map<String, Object[]> highlightFieldsAsObjects(String[] fieldsIn, Query query, int[] docIdsIn,\n                                                           int[] maxPassagesIn) throws IOException {\n    if (fieldsIn.length < 1) {\n      throw new IllegalArgumentException(\"fieldsIn must not be empty\");\n    }\n    if (fieldsIn.length != maxPassagesIn.length) {\n      throw new IllegalArgumentException(\"invalid number of maxPassagesIn\");\n    }\n    if (searcher == null) {\n      throw new IllegalStateException(\"This method requires that an indexSearcher was passed in the \"\n          + \"constructor.  Perhaps you mean to call highlightWithoutSearcher?\");\n    }\n\n    // Sort docs & fields for sequential i/o\n\n    // Sort doc IDs w/ index to original order: (copy input arrays since we sort in-place)\n    int[] docIds = new int[docIdsIn.length];\n    int[] docInIndexes = new int[docIds.length]; // fill in ascending order; points into docIdsIn[]\n    copyAndSortDocIdsWithIndex(docIdsIn, docIds, docInIndexes); // latter 2 are \"out\" params\n\n    // Sort fields w/ maxPassages pair: (copy input arrays since we sort in-place)\n    final String fields[] = new String[fieldsIn.length];\n    final int maxPassages[] = new int[maxPassagesIn.length];\n    copyAndSortFieldsWithMaxPassages(fieldsIn, maxPassagesIn, fields, maxPassages); // latter 2 are \"out\" params\n\n    // Init field highlighters (where most of the highlight logic lives, and on a per field basis)\n    SortedSet<Term> queryTerms = extractTerms(query);\n    FieldHighlighter[] fieldHighlighters = new FieldHighlighter[fields.length];\n    int numTermVectors = 0;\n    int numPostings = 0;\n    for (int f = 0; f < fields.length; f++) {\n      FieldHighlighter fieldHighlighter = getFieldHighlighter(fields[f], query, queryTerms, maxPassages[f]);\n      fieldHighlighters[f] = fieldHighlighter;\n\n      switch (fieldHighlighter.getOffsetSource()) {\n        case TERM_VECTORS:\n          numTermVectors++;\n          break;\n        case POSTINGS:\n          numPostings++;\n          break;\n        case POSTINGS_WITH_TERM_VECTORS:\n          numTermVectors++;\n          numPostings++;\n          break;\n        case ANALYSIS:\n        case NONE_NEEDED:\n        default:\n          //do nothing\n          break;\n      }\n    }\n\n    int cacheCharsThreshold = calculateOptimalCacheCharsThreshold(numTermVectors, numPostings);\n\n    IndexReader indexReaderWithTermVecCache =\n        (numTermVectors >= 2) ? TermVectorReusingLeafReader.wrap(searcher.getIndexReader()) : null;\n\n    // [fieldIdx][docIdInIndex] of highlightDoc result\n    Object[][] highlightDocsInByField = new Object[fields.length][docIds.length];\n    // Highlight in doc batches determined by loadFieldValues (consumes from docIdIter)\n    DocIdSetIterator docIdIter = asDocIdSetIterator(docIds);\n    for (int batchDocIdx = 0; batchDocIdx < docIds.length; ) {\n      // Load the field values of the first batch of document(s) (note: commonly all docs are in this batch)\n      List<CharSequence[]> fieldValsByDoc =\n          loadFieldValues(fields, docIdIter, cacheCharsThreshold);\n      //    the size of the above list is the size of the batch (num of docs in the batch)\n\n      // Highlight in per-field order first, then by doc (better I/O pattern)\n      for (int fieldIdx = 0; fieldIdx < fields.length; fieldIdx++) {\n        Object[] resultByDocIn = highlightDocsInByField[fieldIdx];//parallel to docIdsIn\n        FieldHighlighter fieldHighlighter = fieldHighlighters[fieldIdx];\n        for (int docIdx = batchDocIdx; docIdx - batchDocIdx < fieldValsByDoc.size(); docIdx++) {\n          int docId = docIds[docIdx];//sorted order\n          CharSequence content = fieldValsByDoc.get(docIdx - batchDocIdx)[fieldIdx];\n          if (content == null) {\n            continue;\n          }\n          IndexReader indexReader =\n              (fieldHighlighter.getOffsetSource() == OffsetSource.TERM_VECTORS\n                  && indexReaderWithTermVecCache != null)\n                  ? indexReaderWithTermVecCache\n                  : searcher.getIndexReader();\n          int docInIndex = docInIndexes[docIdx];//original input order\n          assert resultByDocIn[docInIndex] == null;\n          resultByDocIn[docInIndex] =\n              fieldHighlighter\n                  .highlightFieldForDoc(indexReader, docId, content.toString());\n        }\n\n      }\n\n      batchDocIdx += fieldValsByDoc.size();\n    }\n    assert docIdIter.docID() == DocIdSetIterator.NO_MORE_DOCS\n        || docIdIter.nextDoc() == DocIdSetIterator.NO_MORE_DOCS;\n\n    // TODO reconsider the return type; since this is an \"advanced\" method, lets not return a Map?  Notice the only\n    //    caller simply iterates it to build another structure.\n\n    // field -> object highlights parallel to docIdsIn\n    Map<String, Object[]> resultMap = new HashMap<>(fields.length);\n    for (int f = 0; f < fields.length; f++) {\n      resultMap.put(fields[f], highlightDocsInByField[f]);\n    }\n    return resultMap;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"795822cce6616d4035b5a4bdbb6c113ea2f715ba","date":1535599765,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/UnifiedHighlighter#highlightFieldsAsObjects(String[],Query,int[],int[]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/UnifiedHighlighter#highlightFieldsAsObjects(String[],Query,int[],int[]).mjava","sourceNew":"  /**\n   * Expert: highlights the top-N passages from multiple fields,\n   * for the provided int[] docids, to custom Object as\n   * returned by the {@link PassageFormatter}.  Use\n   * this API to render to something other than String.\n   *\n   * @param fieldsIn      field names to highlight. Must have a stored string value.\n   * @param query         query to highlight.\n   * @param docIdsIn      containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to\n   *                      form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets\n   * corresponding to the documents in <code>docIdsIn</code>.\n   * If no highlights were found for a document, the\n   * first {@code maxPassages} from the field will\n   * be returned.\n   * @throws IOException              if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without\n   *                                  {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  protected Map<String, Object[]> highlightFieldsAsObjects(String[] fieldsIn, Query query, int[] docIdsIn,\n                                                           int[] maxPassagesIn) throws IOException {\n    if (fieldsIn.length < 1) {\n      throw new IllegalArgumentException(\"fieldsIn must not be empty\");\n    }\n    if (fieldsIn.length != maxPassagesIn.length) {\n      throw new IllegalArgumentException(\"invalid number of maxPassagesIn\");\n    }\n    if (searcher == null) {\n      throw new IllegalStateException(\"This method requires that an indexSearcher was passed in the \"\n          + \"constructor.  Perhaps you mean to call highlightWithoutSearcher?\");\n    }\n\n    // Sort docs & fields for sequential i/o\n\n    // Sort doc IDs w/ index to original order: (copy input arrays since we sort in-place)\n    int[] docIds = new int[docIdsIn.length];\n    int[] docInIndexes = new int[docIds.length]; // fill in ascending order; points into docIdsIn[]\n    copyAndSortDocIdsWithIndex(docIdsIn, docIds, docInIndexes); // latter 2 are \"out\" params\n\n    // Sort fields w/ maxPassages pair: (copy input arrays since we sort in-place)\n    final String fields[] = new String[fieldsIn.length];\n    final int maxPassages[] = new int[maxPassagesIn.length];\n    copyAndSortFieldsWithMaxPassages(fieldsIn, maxPassagesIn, fields, maxPassages); // latter 2 are \"out\" params\n\n    // Init field highlighters (where most of the highlight logic lives, and on a per field basis)\n    Set<Term> queryTerms = extractTerms(query);\n    FieldHighlighter[] fieldHighlighters = new FieldHighlighter[fields.length];\n    int numTermVectors = 0;\n    int numPostings = 0;\n    for (int f = 0; f < fields.length; f++) {\n      FieldHighlighter fieldHighlighter = getFieldHighlighter(fields[f], query, queryTerms, maxPassages[f]);\n      fieldHighlighters[f] = fieldHighlighter;\n\n      switch (fieldHighlighter.getOffsetSource()) {\n        case TERM_VECTORS:\n          numTermVectors++;\n          break;\n        case POSTINGS:\n          numPostings++;\n          break;\n        case POSTINGS_WITH_TERM_VECTORS:\n          numTermVectors++;\n          numPostings++;\n          break;\n        case ANALYSIS:\n        case NONE_NEEDED:\n        default:\n          //do nothing\n          break;\n      }\n    }\n\n    int cacheCharsThreshold = calculateOptimalCacheCharsThreshold(numTermVectors, numPostings);\n\n    IndexReader indexReaderWithTermVecCache =\n        (numTermVectors >= 2) ? TermVectorReusingLeafReader.wrap(searcher.getIndexReader()) : null;\n\n    // [fieldIdx][docIdInIndex] of highlightDoc result\n    Object[][] highlightDocsInByField = new Object[fields.length][docIds.length];\n    // Highlight in doc batches determined by loadFieldValues (consumes from docIdIter)\n    DocIdSetIterator docIdIter = asDocIdSetIterator(docIds);\n    for (int batchDocIdx = 0; batchDocIdx < docIds.length; ) {\n      // Load the field values of the first batch of document(s) (note: commonly all docs are in this batch)\n      List<CharSequence[]> fieldValsByDoc =\n          loadFieldValues(fields, docIdIter, cacheCharsThreshold);\n      //    the size of the above list is the size of the batch (num of docs in the batch)\n\n      // Highlight in per-field order first, then by doc (better I/O pattern)\n      for (int fieldIdx = 0; fieldIdx < fields.length; fieldIdx++) {\n        Object[] resultByDocIn = highlightDocsInByField[fieldIdx];//parallel to docIdsIn\n        FieldHighlighter fieldHighlighter = fieldHighlighters[fieldIdx];\n        for (int docIdx = batchDocIdx; docIdx - batchDocIdx < fieldValsByDoc.size(); docIdx++) {\n          int docId = docIds[docIdx];//sorted order\n          CharSequence content = fieldValsByDoc.get(docIdx - batchDocIdx)[fieldIdx];\n          if (content == null) {\n            continue;\n          }\n          IndexReader indexReader =\n              (fieldHighlighter.getOffsetSource() == OffsetSource.TERM_VECTORS\n                  && indexReaderWithTermVecCache != null)\n                  ? indexReaderWithTermVecCache\n                  : searcher.getIndexReader();\n          final LeafReader leafReader;\n          if (indexReader instanceof LeafReader) {\n            leafReader = (LeafReader) indexReader;\n          } else {\n            List<LeafReaderContext> leaves = indexReader.leaves();\n            LeafReaderContext leafReaderContext = leaves.get(ReaderUtil.subIndex(docId, leaves));\n            leafReader = leafReaderContext.reader();\n            docId -= leafReaderContext.docBase; // adjust 'doc' to be within this leaf reader\n          }\n          int docInIndex = docInIndexes[docIdx];//original input order\n          assert resultByDocIn[docInIndex] == null;\n          resultByDocIn[docInIndex] =\n              fieldHighlighter\n                  .highlightFieldForDoc(leafReader, docId, content.toString());\n        }\n\n      }\n\n      batchDocIdx += fieldValsByDoc.size();\n    }\n    assert docIdIter.docID() == DocIdSetIterator.NO_MORE_DOCS\n        || docIdIter.nextDoc() == DocIdSetIterator.NO_MORE_DOCS;\n\n    // TODO reconsider the return type; since this is an \"advanced\" method, lets not return a Map?  Notice the only\n    //    caller simply iterates it to build another structure.\n\n    // field -> object highlights parallel to docIdsIn\n    Map<String, Object[]> resultMap = new HashMap<>(fields.length);\n    for (int f = 0; f < fields.length; f++) {\n      resultMap.put(fields[f], highlightDocsInByField[f]);\n    }\n    return resultMap;\n  }\n\n","sourceOld":"  /**\n   * Expert: highlights the top-N passages from multiple fields,\n   * for the provided int[] docids, to custom Object as\n   * returned by the {@link PassageFormatter}.  Use\n   * this API to render to something other than String.\n   *\n   * @param fieldsIn      field names to highlight. Must have a stored string value.\n   * @param query         query to highlight.\n   * @param docIdsIn      containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to\n   *                      form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets\n   * corresponding to the documents in <code>docIdsIn</code>.\n   * If no highlights were found for a document, the\n   * first {@code maxPassages} from the field will\n   * be returned.\n   * @throws IOException              if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without\n   *                                  {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  protected Map<String, Object[]> highlightFieldsAsObjects(String[] fieldsIn, Query query, int[] docIdsIn,\n                                                           int[] maxPassagesIn) throws IOException {\n    if (fieldsIn.length < 1) {\n      throw new IllegalArgumentException(\"fieldsIn must not be empty\");\n    }\n    if (fieldsIn.length != maxPassagesIn.length) {\n      throw new IllegalArgumentException(\"invalid number of maxPassagesIn\");\n    }\n    if (searcher == null) {\n      throw new IllegalStateException(\"This method requires that an indexSearcher was passed in the \"\n          + \"constructor.  Perhaps you mean to call highlightWithoutSearcher?\");\n    }\n\n    // Sort docs & fields for sequential i/o\n\n    // Sort doc IDs w/ index to original order: (copy input arrays since we sort in-place)\n    int[] docIds = new int[docIdsIn.length];\n    int[] docInIndexes = new int[docIds.length]; // fill in ascending order; points into docIdsIn[]\n    copyAndSortDocIdsWithIndex(docIdsIn, docIds, docInIndexes); // latter 2 are \"out\" params\n\n    // Sort fields w/ maxPassages pair: (copy input arrays since we sort in-place)\n    final String fields[] = new String[fieldsIn.length];\n    final int maxPassages[] = new int[maxPassagesIn.length];\n    copyAndSortFieldsWithMaxPassages(fieldsIn, maxPassagesIn, fields, maxPassages); // latter 2 are \"out\" params\n\n    // Init field highlighters (where most of the highlight logic lives, and on a per field basis)\n    Set<Term> queryTerms = extractTerms(query);\n    FieldHighlighter[] fieldHighlighters = new FieldHighlighter[fields.length];\n    int numTermVectors = 0;\n    int numPostings = 0;\n    for (int f = 0; f < fields.length; f++) {\n      FieldHighlighter fieldHighlighter = getFieldHighlighter(fields[f], query, queryTerms, maxPassages[f]);\n      fieldHighlighters[f] = fieldHighlighter;\n\n      switch (fieldHighlighter.getOffsetSource()) {\n        case TERM_VECTORS:\n          numTermVectors++;\n          break;\n        case POSTINGS:\n          numPostings++;\n          break;\n        case POSTINGS_WITH_TERM_VECTORS:\n          numTermVectors++;\n          numPostings++;\n          break;\n        case ANALYSIS:\n        case NONE_NEEDED:\n        default:\n          //do nothing\n          break;\n      }\n    }\n\n    int cacheCharsThreshold = calculateOptimalCacheCharsThreshold(numTermVectors, numPostings);\n\n    IndexReader indexReaderWithTermVecCache =\n        (numTermVectors >= 2) ? TermVectorReusingLeafReader.wrap(searcher.getIndexReader()) : null;\n\n    // [fieldIdx][docIdInIndex] of highlightDoc result\n    Object[][] highlightDocsInByField = new Object[fields.length][docIds.length];\n    // Highlight in doc batches determined by loadFieldValues (consumes from docIdIter)\n    DocIdSetIterator docIdIter = asDocIdSetIterator(docIds);\n    for (int batchDocIdx = 0; batchDocIdx < docIds.length; ) {\n      // Load the field values of the first batch of document(s) (note: commonly all docs are in this batch)\n      List<CharSequence[]> fieldValsByDoc =\n          loadFieldValues(fields, docIdIter, cacheCharsThreshold);\n      //    the size of the above list is the size of the batch (num of docs in the batch)\n\n      // Highlight in per-field order first, then by doc (better I/O pattern)\n      for (int fieldIdx = 0; fieldIdx < fields.length; fieldIdx++) {\n        Object[] resultByDocIn = highlightDocsInByField[fieldIdx];//parallel to docIdsIn\n        FieldHighlighter fieldHighlighter = fieldHighlighters[fieldIdx];\n        for (int docIdx = batchDocIdx; docIdx - batchDocIdx < fieldValsByDoc.size(); docIdx++) {\n          int docId = docIds[docIdx];//sorted order\n          CharSequence content = fieldValsByDoc.get(docIdx - batchDocIdx)[fieldIdx];\n          if (content == null) {\n            continue;\n          }\n          IndexReader indexReader =\n              (fieldHighlighter.getOffsetSource() == OffsetSource.TERM_VECTORS\n                  && indexReaderWithTermVecCache != null)\n                  ? indexReaderWithTermVecCache\n                  : searcher.getIndexReader();\n          int docInIndex = docInIndexes[docIdx];//original input order\n          assert resultByDocIn[docInIndex] == null;\n          resultByDocIn[docInIndex] =\n              fieldHighlighter\n                  .highlightFieldForDoc(indexReader, docId, content.toString());\n        }\n\n      }\n\n      batchDocIdx += fieldValsByDoc.size();\n    }\n    assert docIdIter.docID() == DocIdSetIterator.NO_MORE_DOCS\n        || docIdIter.nextDoc() == DocIdSetIterator.NO_MORE_DOCS;\n\n    // TODO reconsider the return type; since this is an \"advanced\" method, lets not return a Map?  Notice the only\n    //    caller simply iterates it to build another structure.\n\n    // field -> object highlights parallel to docIdsIn\n    Map<String, Object[]> resultMap = new HashMap<>(fields.length);\n    for (int f = 0; f < fields.length; f++) {\n      resultMap.put(fields[f], highlightDocsInByField[f]);\n    }\n    return resultMap;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c37ab80ad12b466f3dc92e4baa7b0cbf9aded429","date":1590107358,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/UnifiedHighlighter#highlightFieldsAsObjects(String[],Query,int[],int[]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/UnifiedHighlighter#highlightFieldsAsObjects(String[],Query,int[],int[]).mjava","sourceNew":"  /**\n   * Expert: highlights the top-N passages from multiple fields,\n   * for the provided int[] docids, to custom Object as\n   * returned by the {@link PassageFormatter}.  Use\n   * this API to render to something other than String.\n   *\n   * @param fieldsIn      field names to highlight. Must have a stored string value.\n   * @param query         query to highlight.\n   * @param docIdsIn      containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to\n   *                      form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets\n   * corresponding to the documents in <code>docIdsIn</code>.\n   * If no highlights were found for a document, the\n   * first {@code maxPassages} from the field will\n   * be returned.\n   * @throws IOException              if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without\n   *                                  {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  protected Map<String, Object[]> highlightFieldsAsObjects(String[] fieldsIn, Query query, int[] docIdsIn,\n                                                           int[] maxPassagesIn) throws IOException {\n    if (fieldsIn.length < 1) {\n      throw new IllegalArgumentException(\"fieldsIn must not be empty\");\n    }\n    if (fieldsIn.length != maxPassagesIn.length) {\n      throw new IllegalArgumentException(\"invalid number of maxPassagesIn\");\n    }\n    if (searcher == null) {\n      throw new IllegalStateException(\"This method requires that an indexSearcher was passed in the \"\n          + \"constructor.  Perhaps you mean to call highlightWithoutSearcher?\");\n    }\n\n    // Sort docs & fields for sequential i/o\n\n    // Sort doc IDs w/ index to original order: (copy input arrays since we sort in-place)\n    int[] docIds = new int[docIdsIn.length];\n    int[] docInIndexes = new int[docIds.length]; // fill in ascending order; points into docIdsIn[]\n    copyAndSortDocIdsWithIndex(docIdsIn, docIds, docInIndexes); // latter 2 are \"out\" params\n\n    // Sort fields w/ maxPassages pair: (copy input arrays since we sort in-place)\n    final String fields[] = new String[fieldsIn.length];\n    final int maxPassages[] = new int[maxPassagesIn.length];\n    copyAndSortFieldsWithMaxPassages(fieldsIn, maxPassagesIn, fields, maxPassages); // latter 2 are \"out\" params\n\n    // Init field highlighters (where most of the highlight logic lives, and on a per field basis)\n    Set<Term> queryTerms = extractTerms(query);\n    FieldHighlighter[] fieldHighlighters = new FieldHighlighter[fields.length];\n    int numTermVectors = 0;\n    int numPostings = 0;\n    for (int f = 0; f < fields.length; f++) {\n      FieldHighlighter fieldHighlighter = getFieldHighlighter(fields[f], query, queryTerms, maxPassages[f]);\n      fieldHighlighters[f] = fieldHighlighter;\n\n      switch (fieldHighlighter.getOffsetSource()) {\n        case TERM_VECTORS:\n          numTermVectors++;\n          break;\n        case POSTINGS:\n          numPostings++;\n          break;\n        case POSTINGS_WITH_TERM_VECTORS:\n          numTermVectors++;\n          numPostings++;\n          break;\n        case ANALYSIS:\n        case NONE_NEEDED:\n        default:\n          //do nothing\n          break;\n      }\n    }\n\n    int cacheCharsThreshold = calculateOptimalCacheCharsThreshold(numTermVectors, numPostings);\n\n    IndexReader indexReaderWithTermVecCache =\n        (numTermVectors >= 2) ? TermVectorReusingLeafReader.wrap(searcher.getIndexReader()) : null;\n\n    // [fieldIdx][docIdInIndex] of highlightDoc result\n    Object[][] highlightDocsInByField = new Object[fields.length][docIds.length];\n    // Highlight in doc batches determined by loadFieldValues (consumes from docIdIter)\n    DocIdSetIterator docIdIter = asDocIdSetIterator(docIds);\n    for (int batchDocIdx = 0; batchDocIdx < docIds.length; ) {\n      // Load the field values of the first batch of document(s) (note: commonly all docs are in this batch)\n      List<CharSequence[]> fieldValsByDoc =\n          loadFieldValues(fields, docIdIter, cacheCharsThreshold);\n      //    the size of the above list is the size of the batch (num of docs in the batch)\n\n      // Highlight in per-field order first, then by doc (better I/O pattern)\n      for (int fieldIdx = 0; fieldIdx < fields.length; fieldIdx++) {\n        Object[] resultByDocIn = highlightDocsInByField[fieldIdx];//parallel to docIdsIn\n        FieldHighlighter fieldHighlighter = fieldHighlighters[fieldIdx];\n        for (int docIdx = batchDocIdx; docIdx - batchDocIdx < fieldValsByDoc.size(); docIdx++) {\n          int docId = docIds[docIdx];//sorted order\n          CharSequence content = fieldValsByDoc.get(docIdx - batchDocIdx)[fieldIdx];\n          if (content == null) {\n            continue;\n          }\n          IndexReader indexReader =\n              (fieldHighlighter.getOffsetSource() == OffsetSource.TERM_VECTORS\n                  && indexReaderWithTermVecCache != null)\n                  ? indexReaderWithTermVecCache\n                  : searcher.getIndexReader();\n          final LeafReader leafReader;\n          if (indexReader instanceof LeafReader) {\n            leafReader = (LeafReader) indexReader;\n          } else {\n            List<LeafReaderContext> leaves = indexReader.leaves();\n            LeafReaderContext leafReaderContext = leaves.get(ReaderUtil.subIndex(docId, leaves));\n            leafReader = leafReaderContext.reader();\n            docId -= leafReaderContext.docBase; // adjust 'doc' to be within this leaf reader\n          }\n          int docInIndex = docInIndexes[docIdx];//original input order\n          assert resultByDocIn[docInIndex] == null;\n          resultByDocIn[docInIndex] =\n              fieldHighlighter\n                  .highlightFieldForDoc(leafReader, docId, content.toString());\n        }\n\n      }\n\n      batchDocIdx += fieldValsByDoc.size();\n    }\n    IOUtils.close(indexReaderWithTermVecCache);\n    assert docIdIter.docID() == DocIdSetIterator.NO_MORE_DOCS\n        || docIdIter.nextDoc() == DocIdSetIterator.NO_MORE_DOCS;\n\n    // TODO reconsider the return type; since this is an \"advanced\" method, lets not return a Map?  Notice the only\n    //    caller simply iterates it to build another structure.\n\n    // field -> object highlights parallel to docIdsIn\n    Map<String, Object[]> resultMap = new HashMap<>(fields.length);\n    for (int f = 0; f < fields.length; f++) {\n      resultMap.put(fields[f], highlightDocsInByField[f]);\n    }\n    return resultMap;\n  }\n\n","sourceOld":"  /**\n   * Expert: highlights the top-N passages from multiple fields,\n   * for the provided int[] docids, to custom Object as\n   * returned by the {@link PassageFormatter}.  Use\n   * this API to render to something other than String.\n   *\n   * @param fieldsIn      field names to highlight. Must have a stored string value.\n   * @param query         query to highlight.\n   * @param docIdsIn      containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to\n   *                      form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets\n   * corresponding to the documents in <code>docIdsIn</code>.\n   * If no highlights were found for a document, the\n   * first {@code maxPassages} from the field will\n   * be returned.\n   * @throws IOException              if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without\n   *                                  {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  protected Map<String, Object[]> highlightFieldsAsObjects(String[] fieldsIn, Query query, int[] docIdsIn,\n                                                           int[] maxPassagesIn) throws IOException {\n    if (fieldsIn.length < 1) {\n      throw new IllegalArgumentException(\"fieldsIn must not be empty\");\n    }\n    if (fieldsIn.length != maxPassagesIn.length) {\n      throw new IllegalArgumentException(\"invalid number of maxPassagesIn\");\n    }\n    if (searcher == null) {\n      throw new IllegalStateException(\"This method requires that an indexSearcher was passed in the \"\n          + \"constructor.  Perhaps you mean to call highlightWithoutSearcher?\");\n    }\n\n    // Sort docs & fields for sequential i/o\n\n    // Sort doc IDs w/ index to original order: (copy input arrays since we sort in-place)\n    int[] docIds = new int[docIdsIn.length];\n    int[] docInIndexes = new int[docIds.length]; // fill in ascending order; points into docIdsIn[]\n    copyAndSortDocIdsWithIndex(docIdsIn, docIds, docInIndexes); // latter 2 are \"out\" params\n\n    // Sort fields w/ maxPassages pair: (copy input arrays since we sort in-place)\n    final String fields[] = new String[fieldsIn.length];\n    final int maxPassages[] = new int[maxPassagesIn.length];\n    copyAndSortFieldsWithMaxPassages(fieldsIn, maxPassagesIn, fields, maxPassages); // latter 2 are \"out\" params\n\n    // Init field highlighters (where most of the highlight logic lives, and on a per field basis)\n    Set<Term> queryTerms = extractTerms(query);\n    FieldHighlighter[] fieldHighlighters = new FieldHighlighter[fields.length];\n    int numTermVectors = 0;\n    int numPostings = 0;\n    for (int f = 0; f < fields.length; f++) {\n      FieldHighlighter fieldHighlighter = getFieldHighlighter(fields[f], query, queryTerms, maxPassages[f]);\n      fieldHighlighters[f] = fieldHighlighter;\n\n      switch (fieldHighlighter.getOffsetSource()) {\n        case TERM_VECTORS:\n          numTermVectors++;\n          break;\n        case POSTINGS:\n          numPostings++;\n          break;\n        case POSTINGS_WITH_TERM_VECTORS:\n          numTermVectors++;\n          numPostings++;\n          break;\n        case ANALYSIS:\n        case NONE_NEEDED:\n        default:\n          //do nothing\n          break;\n      }\n    }\n\n    int cacheCharsThreshold = calculateOptimalCacheCharsThreshold(numTermVectors, numPostings);\n\n    IndexReader indexReaderWithTermVecCache =\n        (numTermVectors >= 2) ? TermVectorReusingLeafReader.wrap(searcher.getIndexReader()) : null;\n\n    // [fieldIdx][docIdInIndex] of highlightDoc result\n    Object[][] highlightDocsInByField = new Object[fields.length][docIds.length];\n    // Highlight in doc batches determined by loadFieldValues (consumes from docIdIter)\n    DocIdSetIterator docIdIter = asDocIdSetIterator(docIds);\n    for (int batchDocIdx = 0; batchDocIdx < docIds.length; ) {\n      // Load the field values of the first batch of document(s) (note: commonly all docs are in this batch)\n      List<CharSequence[]> fieldValsByDoc =\n          loadFieldValues(fields, docIdIter, cacheCharsThreshold);\n      //    the size of the above list is the size of the batch (num of docs in the batch)\n\n      // Highlight in per-field order first, then by doc (better I/O pattern)\n      for (int fieldIdx = 0; fieldIdx < fields.length; fieldIdx++) {\n        Object[] resultByDocIn = highlightDocsInByField[fieldIdx];//parallel to docIdsIn\n        FieldHighlighter fieldHighlighter = fieldHighlighters[fieldIdx];\n        for (int docIdx = batchDocIdx; docIdx - batchDocIdx < fieldValsByDoc.size(); docIdx++) {\n          int docId = docIds[docIdx];//sorted order\n          CharSequence content = fieldValsByDoc.get(docIdx - batchDocIdx)[fieldIdx];\n          if (content == null) {\n            continue;\n          }\n          IndexReader indexReader =\n              (fieldHighlighter.getOffsetSource() == OffsetSource.TERM_VECTORS\n                  && indexReaderWithTermVecCache != null)\n                  ? indexReaderWithTermVecCache\n                  : searcher.getIndexReader();\n          final LeafReader leafReader;\n          if (indexReader instanceof LeafReader) {\n            leafReader = (LeafReader) indexReader;\n          } else {\n            List<LeafReaderContext> leaves = indexReader.leaves();\n            LeafReaderContext leafReaderContext = leaves.get(ReaderUtil.subIndex(docId, leaves));\n            leafReader = leafReaderContext.reader();\n            docId -= leafReaderContext.docBase; // adjust 'doc' to be within this leaf reader\n          }\n          int docInIndex = docInIndexes[docIdx];//original input order\n          assert resultByDocIn[docInIndex] == null;\n          resultByDocIn[docInIndex] =\n              fieldHighlighter\n                  .highlightFieldForDoc(leafReader, docId, content.toString());\n        }\n\n      }\n\n      batchDocIdx += fieldValsByDoc.size();\n    }\n    assert docIdIter.docID() == DocIdSetIterator.NO_MORE_DOCS\n        || docIdIter.nextDoc() == DocIdSetIterator.NO_MORE_DOCS;\n\n    // TODO reconsider the return type; since this is an \"advanced\" method, lets not return a Map?  Notice the only\n    //    caller simply iterates it to build another structure.\n\n    // field -> object highlights parallel to docIdsIn\n    Map<String, Object[]> resultMap = new HashMap<>(fields.length);\n    for (int f = 0; f < fields.length; f++) {\n      resultMap.put(fields[f], highlightDocsInByField[f]);\n    }\n    return resultMap;\n  }\n\n","bugFix":null,"bugIntro":["ac7dc8447caedf847d70ac1910dc1efaa36f0f68"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ac7dc8447caedf847d70ac1910dc1efaa36f0f68","date":1596059784,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/UnifiedHighlighter#highlightFieldsAsObjects(String[],Query,int[],int[]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/UnifiedHighlighter#highlightFieldsAsObjects(String[],Query,int[],int[]).mjava","sourceNew":"  /**\n   * Expert: highlights the top-N passages from multiple fields,\n   * for the provided int[] docids, to custom Object as\n   * returned by the {@link PassageFormatter}.  Use\n   * this API to render to something other than String.\n   *\n   * @param fieldsIn      field names to highlight. Must have a stored string value.\n   * @param query         query to highlight.\n   * @param docIdsIn      containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to\n   *                      form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets\n   * corresponding to the documents in <code>docIdsIn</code>.\n   * If no highlights were found for a document, the\n   * first {@code maxPassages} from the field will\n   * be returned.\n   * @throws IOException              if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without\n   *                                  {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  protected Map<String, Object[]> highlightFieldsAsObjects(String[] fieldsIn, Query query, int[] docIdsIn,\n                                                           int[] maxPassagesIn) throws IOException {\n    if (fieldsIn.length < 1) {\n      throw new IllegalArgumentException(\"fieldsIn must not be empty\");\n    }\n    if (fieldsIn.length != maxPassagesIn.length) {\n      throw new IllegalArgumentException(\"invalid number of maxPassagesIn\");\n    }\n    if (searcher == null) {\n      throw new IllegalStateException(\"This method requires that an indexSearcher was passed in the \"\n          + \"constructor.  Perhaps you mean to call highlightWithoutSearcher?\");\n    }\n\n    // Sort docs & fields for sequential i/o\n\n    // Sort doc IDs w/ index to original order: (copy input arrays since we sort in-place)\n    int[] docIds = new int[docIdsIn.length];\n    int[] docInIndexes = new int[docIds.length]; // fill in ascending order; points into docIdsIn[]\n    copyAndSortDocIdsWithIndex(docIdsIn, docIds, docInIndexes); // latter 2 are \"out\" params\n\n    // Sort fields w/ maxPassages pair: (copy input arrays since we sort in-place)\n    final String fields[] = new String[fieldsIn.length];\n    final int maxPassages[] = new int[maxPassagesIn.length];\n    copyAndSortFieldsWithMaxPassages(fieldsIn, maxPassagesIn, fields, maxPassages); // latter 2 are \"out\" params\n\n    // Init field highlighters (where most of the highlight logic lives, and on a per field basis)\n    Set<Term> queryTerms = extractTerms(query);\n    FieldHighlighter[] fieldHighlighters = new FieldHighlighter[fields.length];\n    int numTermVectors = 0;\n    int numPostings = 0;\n    for (int f = 0; f < fields.length; f++) {\n      FieldHighlighter fieldHighlighter = getFieldHighlighter(fields[f], query, queryTerms, maxPassages[f]);\n      fieldHighlighters[f] = fieldHighlighter;\n\n      switch (fieldHighlighter.getOffsetSource()) {\n        case TERM_VECTORS:\n          numTermVectors++;\n          break;\n        case POSTINGS:\n          numPostings++;\n          break;\n        case POSTINGS_WITH_TERM_VECTORS:\n          numTermVectors++;\n          numPostings++;\n          break;\n        case ANALYSIS:\n        case NONE_NEEDED:\n        default:\n          //do nothing\n          break;\n      }\n    }\n\n    int cacheCharsThreshold = calculateOptimalCacheCharsThreshold(numTermVectors, numPostings);\n\n    IndexReader indexReaderWithTermVecCache =\n        (numTermVectors >= 2) ? TermVectorReusingLeafReader.wrap(searcher.getIndexReader()) : null;\n\n    // [fieldIdx][docIdInIndex] of highlightDoc result\n    Object[][] highlightDocsInByField = new Object[fields.length][docIds.length];\n    // Highlight in doc batches determined by loadFieldValues (consumes from docIdIter)\n    DocIdSetIterator docIdIter = asDocIdSetIterator(docIds);\n    for (int batchDocIdx = 0; batchDocIdx < docIds.length; ) {\n      // Load the field values of the first batch of document(s) (note: commonly all docs are in this batch)\n      List<CharSequence[]> fieldValsByDoc =\n          loadFieldValues(fields, docIdIter, cacheCharsThreshold);\n      //    the size of the above list is the size of the batch (num of docs in the batch)\n\n      // Highlight in per-field order first, then by doc (better I/O pattern)\n      for (int fieldIdx = 0; fieldIdx < fields.length; fieldIdx++) {\n        Object[] resultByDocIn = highlightDocsInByField[fieldIdx];//parallel to docIdsIn\n        FieldHighlighter fieldHighlighter = fieldHighlighters[fieldIdx];\n        for (int docIdx = batchDocIdx; docIdx - batchDocIdx < fieldValsByDoc.size(); docIdx++) {\n          int docId = docIds[docIdx];//sorted order\n          CharSequence content = fieldValsByDoc.get(docIdx - batchDocIdx)[fieldIdx];\n          if (content == null) {\n            continue;\n          }\n          IndexReader indexReader =\n              (fieldHighlighter.getOffsetSource() == OffsetSource.TERM_VECTORS\n                  && indexReaderWithTermVecCache != null)\n                  ? indexReaderWithTermVecCache\n                  : searcher.getIndexReader();\n          final LeafReader leafReader;\n          if (indexReader instanceof LeafReader) {\n            leafReader = (LeafReader) indexReader;\n          } else {\n            List<LeafReaderContext> leaves = indexReader.leaves();\n            LeafReaderContext leafReaderContext = leaves.get(ReaderUtil.subIndex(docId, leaves));\n            leafReader = leafReaderContext.reader();\n            docId -= leafReaderContext.docBase; // adjust 'doc' to be within this leaf reader\n          }\n          int docInIndex = docInIndexes[docIdx];//original input order\n          assert resultByDocIn[docInIndex] == null;\n          resultByDocIn[docInIndex] =\n              fieldHighlighter\n                  .highlightFieldForDoc(leafReader, docId, content.toString());\n        }\n\n      }\n\n      batchDocIdx += fieldValsByDoc.size();\n    }\n    IOUtils.close(indexReaderWithTermVecCache); // FYI won't close underlying reader\n    assert docIdIter.docID() == DocIdSetIterator.NO_MORE_DOCS\n        || docIdIter.nextDoc() == DocIdSetIterator.NO_MORE_DOCS;\n\n    // TODO reconsider the return type; since this is an \"advanced\" method, lets not return a Map?  Notice the only\n    //    caller simply iterates it to build another structure.\n\n    // field -> object highlights parallel to docIdsIn\n    Map<String, Object[]> resultMap = new HashMap<>(fields.length);\n    for (int f = 0; f < fields.length; f++) {\n      resultMap.put(fields[f], highlightDocsInByField[f]);\n    }\n    return resultMap;\n  }\n\n","sourceOld":"  /**\n   * Expert: highlights the top-N passages from multiple fields,\n   * for the provided int[] docids, to custom Object as\n   * returned by the {@link PassageFormatter}.  Use\n   * this API to render to something other than String.\n   *\n   * @param fieldsIn      field names to highlight. Must have a stored string value.\n   * @param query         query to highlight.\n   * @param docIdsIn      containing the document IDs to highlight.\n   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to\n   *                      form the highlighted snippets.\n   * @return Map keyed on field name, containing the array of formatted snippets\n   * corresponding to the documents in <code>docIdsIn</code>.\n   * If no highlights were found for a document, the\n   * first {@code maxPassages} from the field will\n   * be returned.\n   * @throws IOException              if an I/O error occurred during processing\n   * @throws IllegalArgumentException if <code>field</code> was indexed without\n   *                                  {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}\n   */\n  protected Map<String, Object[]> highlightFieldsAsObjects(String[] fieldsIn, Query query, int[] docIdsIn,\n                                                           int[] maxPassagesIn) throws IOException {\n    if (fieldsIn.length < 1) {\n      throw new IllegalArgumentException(\"fieldsIn must not be empty\");\n    }\n    if (fieldsIn.length != maxPassagesIn.length) {\n      throw new IllegalArgumentException(\"invalid number of maxPassagesIn\");\n    }\n    if (searcher == null) {\n      throw new IllegalStateException(\"This method requires that an indexSearcher was passed in the \"\n          + \"constructor.  Perhaps you mean to call highlightWithoutSearcher?\");\n    }\n\n    // Sort docs & fields for sequential i/o\n\n    // Sort doc IDs w/ index to original order: (copy input arrays since we sort in-place)\n    int[] docIds = new int[docIdsIn.length];\n    int[] docInIndexes = new int[docIds.length]; // fill in ascending order; points into docIdsIn[]\n    copyAndSortDocIdsWithIndex(docIdsIn, docIds, docInIndexes); // latter 2 are \"out\" params\n\n    // Sort fields w/ maxPassages pair: (copy input arrays since we sort in-place)\n    final String fields[] = new String[fieldsIn.length];\n    final int maxPassages[] = new int[maxPassagesIn.length];\n    copyAndSortFieldsWithMaxPassages(fieldsIn, maxPassagesIn, fields, maxPassages); // latter 2 are \"out\" params\n\n    // Init field highlighters (where most of the highlight logic lives, and on a per field basis)\n    Set<Term> queryTerms = extractTerms(query);\n    FieldHighlighter[] fieldHighlighters = new FieldHighlighter[fields.length];\n    int numTermVectors = 0;\n    int numPostings = 0;\n    for (int f = 0; f < fields.length; f++) {\n      FieldHighlighter fieldHighlighter = getFieldHighlighter(fields[f], query, queryTerms, maxPassages[f]);\n      fieldHighlighters[f] = fieldHighlighter;\n\n      switch (fieldHighlighter.getOffsetSource()) {\n        case TERM_VECTORS:\n          numTermVectors++;\n          break;\n        case POSTINGS:\n          numPostings++;\n          break;\n        case POSTINGS_WITH_TERM_VECTORS:\n          numTermVectors++;\n          numPostings++;\n          break;\n        case ANALYSIS:\n        case NONE_NEEDED:\n        default:\n          //do nothing\n          break;\n      }\n    }\n\n    int cacheCharsThreshold = calculateOptimalCacheCharsThreshold(numTermVectors, numPostings);\n\n    IndexReader indexReaderWithTermVecCache =\n        (numTermVectors >= 2) ? TermVectorReusingLeafReader.wrap(searcher.getIndexReader()) : null;\n\n    // [fieldIdx][docIdInIndex] of highlightDoc result\n    Object[][] highlightDocsInByField = new Object[fields.length][docIds.length];\n    // Highlight in doc batches determined by loadFieldValues (consumes from docIdIter)\n    DocIdSetIterator docIdIter = asDocIdSetIterator(docIds);\n    for (int batchDocIdx = 0; batchDocIdx < docIds.length; ) {\n      // Load the field values of the first batch of document(s) (note: commonly all docs are in this batch)\n      List<CharSequence[]> fieldValsByDoc =\n          loadFieldValues(fields, docIdIter, cacheCharsThreshold);\n      //    the size of the above list is the size of the batch (num of docs in the batch)\n\n      // Highlight in per-field order first, then by doc (better I/O pattern)\n      for (int fieldIdx = 0; fieldIdx < fields.length; fieldIdx++) {\n        Object[] resultByDocIn = highlightDocsInByField[fieldIdx];//parallel to docIdsIn\n        FieldHighlighter fieldHighlighter = fieldHighlighters[fieldIdx];\n        for (int docIdx = batchDocIdx; docIdx - batchDocIdx < fieldValsByDoc.size(); docIdx++) {\n          int docId = docIds[docIdx];//sorted order\n          CharSequence content = fieldValsByDoc.get(docIdx - batchDocIdx)[fieldIdx];\n          if (content == null) {\n            continue;\n          }\n          IndexReader indexReader =\n              (fieldHighlighter.getOffsetSource() == OffsetSource.TERM_VECTORS\n                  && indexReaderWithTermVecCache != null)\n                  ? indexReaderWithTermVecCache\n                  : searcher.getIndexReader();\n          final LeafReader leafReader;\n          if (indexReader instanceof LeafReader) {\n            leafReader = (LeafReader) indexReader;\n          } else {\n            List<LeafReaderContext> leaves = indexReader.leaves();\n            LeafReaderContext leafReaderContext = leaves.get(ReaderUtil.subIndex(docId, leaves));\n            leafReader = leafReaderContext.reader();\n            docId -= leafReaderContext.docBase; // adjust 'doc' to be within this leaf reader\n          }\n          int docInIndex = docInIndexes[docIdx];//original input order\n          assert resultByDocIn[docInIndex] == null;\n          resultByDocIn[docInIndex] =\n              fieldHighlighter\n                  .highlightFieldForDoc(leafReader, docId, content.toString());\n        }\n\n      }\n\n      batchDocIdx += fieldValsByDoc.size();\n    }\n    IOUtils.close(indexReaderWithTermVecCache);\n    assert docIdIter.docID() == DocIdSetIterator.NO_MORE_DOCS\n        || docIdIter.nextDoc() == DocIdSetIterator.NO_MORE_DOCS;\n\n    // TODO reconsider the return type; since this is an \"advanced\" method, lets not return a Map?  Notice the only\n    //    caller simply iterates it to build another structure.\n\n    // field -> object highlights parallel to docIdsIn\n    Map<String, Object[]> resultMap = new HashMap<>(fields.length);\n    for (int f = 0; f < fields.length; f++) {\n      resultMap.put(fields[f], highlightDocsInByField[f]);\n    }\n    return resultMap;\n  }\n\n","bugFix":["c37ab80ad12b466f3dc92e4baa7b0cbf9aded429"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"5675b3bced0b155f0ff8001ce2e1e502be7c92f6":["1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2"],"ee8992dea79b51ba77a3fd1a5c0f94a5f0b8808d":["1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2","5675b3bced0b155f0ff8001ce2e1e502be7c92f6"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"ac7dc8447caedf847d70ac1910dc1efaa36f0f68":["c37ab80ad12b466f3dc92e4baa7b0cbf9aded429"],"9856095f7afb5a607bf5e65077615ed91273508c":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","ee8992dea79b51ba77a3fd1a5c0f94a5f0b8808d"],"795822cce6616d4035b5a4bdbb6c113ea2f715ba":["ee8992dea79b51ba77a3fd1a5c0f94a5f0b8808d"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["ac7dc8447caedf847d70ac1910dc1efaa36f0f68"],"c37ab80ad12b466f3dc92e4baa7b0cbf9aded429":["795822cce6616d4035b5a4bdbb6c113ea2f715ba"]},"commit2Childs":{"5675b3bced0b155f0ff8001ce2e1e502be7c92f6":["ee8992dea79b51ba77a3fd1a5c0f94a5f0b8808d"],"ee8992dea79b51ba77a3fd1a5c0f94a5f0b8808d":["9856095f7afb5a607bf5e65077615ed91273508c","795822cce6616d4035b5a4bdbb6c113ea2f715ba"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2":["5675b3bced0b155f0ff8001ce2e1e502be7c92f6","ee8992dea79b51ba77a3fd1a5c0f94a5f0b8808d","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"ac7dc8447caedf847d70ac1910dc1efaa36f0f68":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"9856095f7afb5a607bf5e65077615ed91273508c":[],"795822cce6616d4035b5a4bdbb6c113ea2f715ba":["c37ab80ad12b466f3dc92e4baa7b0cbf9aded429"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["9856095f7afb5a607bf5e65077615ed91273508c"],"c37ab80ad12b466f3dc92e4baa7b0cbf9aded429":["ac7dc8447caedf847d70ac1910dc1efaa36f0f68"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["9856095f7afb5a607bf5e65077615ed91273508c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}