{"path":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testMaxStartOffsetConsistency().mjava","commits":[{"id":"5d62e4938659e263e96ae8188e11aea8a940aea5","date":1430230314,"type":0,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testMaxStartOffsetConsistency().mjava","pathOld":"/dev/null","sourceNew":"  public void testMaxStartOffsetConsistency() throws IOException {\n    FieldType tvFieldType = new FieldType(TextField.TYPE_NOT_STORED);\n    tvFieldType.setStoreTermVectors(true);\n    tvFieldType.setStoreTermVectorOffsets(true);\n    tvFieldType.setStoreTermVectorPositions(true);\n\n    Directory dir = newDirectory();\n\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setEnableChecks(false);//we don't necessarily consume the whole stream because of limiting by startOffset\n    Document doc = new Document();\n    final String TEXT = \" f gg h\";\n    doc.add(new Field(\"fld_tv\", analyzer.tokenStream(\"fooFld\", TEXT), tvFieldType));\n    doc.add(new TextField(\"fld_notv\", analyzer.tokenStream(\"barFld\", TEXT)));\n\n    IndexReader reader;\n    try (RandomIndexWriter writer = new RandomIndexWriter(random(), dir)) {\n      writer.addDocument(doc);\n      reader = writer.getReader();\n    }\n    try {\n      Fields tvFields = reader.getTermVectors(0);\n      for (int maxStartOffset = -1; maxStartOffset <= TEXT.length(); maxStartOffset++) {\n        TokenStream tvStream = TokenSources.getTokenStream(\"fld_tv\", tvFields, TEXT, analyzer, maxStartOffset);\n        TokenStream anaStream = TokenSources.getTokenStream(\"fld_notv\", tvFields, TEXT, analyzer, maxStartOffset);\n\n        //assert have same tokens, none of which has a start offset > maxStartOffset\n        final OffsetAttribute tvOffAtt = tvStream.addAttribute(OffsetAttribute.class);\n        final OffsetAttribute anaOffAtt = anaStream.addAttribute(OffsetAttribute.class);\n        tvStream.reset();\n        anaStream.reset();\n        while (tvStream.incrementToken()) {\n          assertTrue(anaStream.incrementToken());\n          assertEquals(tvOffAtt.startOffset(), anaOffAtt.startOffset());\n          if (maxStartOffset >= 0)\n            assertTrue(tvOffAtt.startOffset() <= maxStartOffset);\n        }\n        assertTrue(anaStream.incrementToken() == false);\n        tvStream.end();\n        anaStream.end();\n        tvStream.close();\n        anaStream.close();\n      }\n\n    } finally {\n      reader.close();\n    }\n\n\n\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5d62e4938659e263e96ae8188e11aea8a940aea5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["5d62e4938659e263e96ae8188e11aea8a940aea5"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["5d62e4938659e263e96ae8188e11aea8a940aea5"],"5d62e4938659e263e96ae8188e11aea8a940aea5":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}