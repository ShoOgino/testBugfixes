{"path":"lucene/core/src/test/org/apache/lucene/search/TestLRUFilterCache#testFineGrainedStats().mjava","commits":[{"id":"296c9b37e42c78be4df12c05d295996c46e0d0ef","date":1420447527,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUFilterCache#testFineGrainedStats().mjava","pathOld":"/dev/null","sourceNew":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<Object, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUFilterCache filterCache = new LRUFilterCache(2, 10000000) {\n      @Override\n      protected void onHit(Object readerCoreKey, Filter filter) {\n        super.onHit(readerCoreKey, filter);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Filter filter) {\n        super.onMiss(readerCoreKey, filter);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onFilterCache(Filter filter, long ramBytesUsed) {\n        super.onFilterCache(filter, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onFilterEviction(Filter filter, long ramBytesUsed) {\n        super.onFilterEviction(filter, ramBytesUsed);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Filter filter = new QueryWrapperFilter(new TermQuery(new Term(\"color\", \"red\")));\n    final Filter filter2 = new QueryWrapperFilter(new TermQuery(new Term(\"color\", \"blue\")));\n    final Filter filter3 = new QueryWrapperFilter(new TermQuery(new Term(\"color\", \"green\")));\n\n    // search on searcher1\n    Filter cached = filterCache.doCache(filter, FilterCachingPolicy.ALWAYS_CACHE);\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(cached), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    cached = filterCache.doCache(filter2, FilterCachingPolicy.ALWAYS_CACHE);\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(cached), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    cached = filterCache.doCache(filter3, FilterCachingPolicy.ALWAYS_CACHE);\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(cached), 1);\n    }\n    assertEquals(segmentCount1, filterCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(filterCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUFilterCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(filterCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(filterCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    filterCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"60596f28be69b10c37a56a303c2dbea07b2ca4ba","date":1425060541,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestLRUFilterCache#testFineGrainedStats().mjava","sourceNew":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<Object, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUQueryCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","sourceOld":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<Object, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUFilterCache filterCache = new LRUFilterCache(2, 10000000) {\n      @Override\n      protected void onHit(Object readerCoreKey, Filter filter) {\n        super.onHit(readerCoreKey, filter);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Filter filter) {\n        super.onMiss(readerCoreKey, filter);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onFilterCache(Filter filter, long ramBytesUsed) {\n        super.onFilterCache(filter, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onFilterEviction(Filter filter, long ramBytesUsed) {\n        super.onFilterEviction(filter, ramBytesUsed);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Filter filter = new QueryWrapperFilter(new TermQuery(new Term(\"color\", \"red\")));\n    final Filter filter2 = new QueryWrapperFilter(new TermQuery(new Term(\"color\", \"blue\")));\n    final Filter filter3 = new QueryWrapperFilter(new TermQuery(new Term(\"color\", \"green\")));\n\n    // search on searcher1\n    Filter cached = filterCache.doCache(filter, FilterCachingPolicy.ALWAYS_CACHE);\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(cached), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    cached = filterCache.doCache(filter2, FilterCachingPolicy.ALWAYS_CACHE);\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(cached), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    cached = filterCache.doCache(filter3, FilterCachingPolicy.ALWAYS_CACHE);\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(cached), 1);\n    }\n    assertEquals(segmentCount1, filterCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(filterCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUFilterCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(filterCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(filterCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    filterCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":4,"author":"Ryan Ernst","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestLRUFilterCache#testFineGrainedStats().mjava","sourceNew":null,"sourceOld":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<Object, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUFilterCache filterCache = new LRUFilterCache(2, 10000000) {\n      @Override\n      protected void onHit(Object readerCoreKey, Filter filter) {\n        super.onHit(readerCoreKey, filter);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Filter filter) {\n        super.onMiss(readerCoreKey, filter);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onFilterCache(Filter filter, long ramBytesUsed) {\n        super.onFilterCache(filter, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onFilterEviction(Filter filter, long ramBytesUsed) {\n        super.onFilterEviction(filter, ramBytesUsed);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Filter filter = new QueryWrapperFilter(new TermQuery(new Term(\"color\", \"red\")));\n    final Filter filter2 = new QueryWrapperFilter(new TermQuery(new Term(\"color\", \"blue\")));\n    final Filter filter3 = new QueryWrapperFilter(new TermQuery(new Term(\"color\", \"green\")));\n\n    // search on searcher1\n    Filter cached = filterCache.doCache(filter, FilterCachingPolicy.ALWAYS_CACHE);\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(cached), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    cached = filterCache.doCache(filter2, FilterCachingPolicy.ALWAYS_CACHE);\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(cached), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    cached = filterCache.doCache(filter3, FilterCachingPolicy.ALWAYS_CACHE);\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(cached), 1);\n    }\n    assertEquals(segmentCount1, filterCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(filterCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUFilterCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(filterCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(filterCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    filterCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["296c9b37e42c78be4df12c05d295996c46e0d0ef","60596f28be69b10c37a56a303c2dbea07b2ca4ba"],"60596f28be69b10c37a56a303c2dbea07b2ca4ba":["296c9b37e42c78be4df12c05d295996c46e0d0ef"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["60596f28be69b10c37a56a303c2dbea07b2ca4ba"],"296c9b37e42c78be4df12c05d295996c46e0d0ef":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"60596f28be69b10c37a56a303c2dbea07b2ca4ba":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["296c9b37e42c78be4df12c05d295996c46e0d0ef"],"296c9b37e42c78be4df12c05d295996c46e0d0ef":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","60596f28be69b10c37a56a303c2dbea07b2ca4ba"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}