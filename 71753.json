{"path":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","commits":[{"id":"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","date":1306767085,"type":1,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","pathOld":"solr/contrib/dataimporthandler/src/main/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","sourceNew":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<Map<String, Object>>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (LOG.isDebugEnabled()) LOG.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          LOG.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformeation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            LOG.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, new XPathRecordReader.Handler() {\n            @SuppressWarnings(\"unchecked\")\n            public void handle(Map<String, Object> record, String xpath) {\n              rows.add(readRow(record, xpath));\n            }\n          });\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            LOG.warn(msg, e);\n            Map<String, Object> map = new HashMap<String, Object>();\n            map.put(SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            LOG.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","sourceOld":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<Map<String, Object>>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (LOG.isDebugEnabled()) LOG.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          LOG.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformeation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            LOG.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, new XPathRecordReader.Handler() {\n            @SuppressWarnings(\"unchecked\")\n            public void handle(Map<String, Object> record, String xpath) {\n              rows.add(readRow(record, xpath));\n            }\n          });\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            LOG.warn(msg, e);\n            Map<String, Object> map = new HashMap<String, Object>();\n            map.put(SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            LOG.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":1,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","pathOld":"solr/contrib/dataimporthandler/src/main/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","sourceNew":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<Map<String, Object>>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (LOG.isDebugEnabled()) LOG.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          LOG.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformeation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            LOG.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, new XPathRecordReader.Handler() {\n            @SuppressWarnings(\"unchecked\")\n            public void handle(Map<String, Object> record, String xpath) {\n              rows.add(readRow(record, xpath));\n            }\n          });\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            LOG.warn(msg, e);\n            Map<String, Object> map = new HashMap<String, Object>();\n            map.put(SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            LOG.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","sourceOld":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<Map<String, Object>>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (LOG.isDebugEnabled()) LOG.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          LOG.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformeation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            LOG.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, new XPathRecordReader.Handler() {\n            @SuppressWarnings(\"unchecked\")\n            public void handle(Map<String, Object> record, String xpath) {\n              rows.add(readRow(record, xpath));\n            }\n          });\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            LOG.warn(msg, e);\n            Map<String, Object> map = new HashMap<String, Object>();\n            map.put(SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            LOG.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7530de27b87b961b51f01bd1299b7004d46e8823","date":1355236261,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","pathOld":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","sourceNew":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<Map<String, Object>>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (LOG.isDebugEnabled()) LOG.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          LOG.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformeation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            LOG.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, new XPathRecordReader.Handler() {\n            @Override\n            @SuppressWarnings(\"unchecked\")\n            public void handle(Map<String, Object> record, String xpath) {\n              rows.add(readRow(record, xpath));\n            }\n          });\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            LOG.warn(msg, e);\n            Map<String, Object> map = new HashMap<String, Object>();\n            map.put(SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            LOG.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","sourceOld":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<Map<String, Object>>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (LOG.isDebugEnabled()) LOG.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          LOG.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformeation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            LOG.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, new XPathRecordReader.Handler() {\n            @SuppressWarnings(\"unchecked\")\n            public void handle(Map<String, Object> record, String xpath) {\n              rows.add(readRow(record, xpath));\n            }\n          });\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            LOG.warn(msg, e);\n            Map<String, Object> map = new HashMap<String, Object>();\n            map.put(SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            LOG.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","pathOld":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","sourceNew":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<Map<String, Object>>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (LOG.isDebugEnabled()) LOG.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          LOG.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformeation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            LOG.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, new XPathRecordReader.Handler() {\n            @Override\n            @SuppressWarnings(\"unchecked\")\n            public void handle(Map<String, Object> record, String xpath) {\n              rows.add(readRow(record, xpath));\n            }\n          });\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            LOG.warn(msg, e);\n            Map<String, Object> map = new HashMap<String, Object>();\n            map.put(SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            LOG.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","sourceOld":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<Map<String, Object>>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (LOG.isDebugEnabled()) LOG.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          LOG.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformeation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            LOG.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, new XPathRecordReader.Handler() {\n            @SuppressWarnings(\"unchecked\")\n            public void handle(Map<String, Object> record, String xpath) {\n              rows.add(readRow(record, xpath));\n            }\n          });\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            LOG.warn(msg, e);\n            Map<String, Object> map = new HashMap<String, Object>();\n            map.put(SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            LOG.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1b604003611eabbb4d3f0fb1f89d3b6a017f8faa","date":1386079993,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","pathOld":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","sourceNew":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<Map<String, Object>>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (LOG.isDebugEnabled()) LOG.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          LOG.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformeation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            LOG.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, new XPathRecordReader.Handler() {\n            @Override\n            @SuppressWarnings(\"unchecked\")\n            public void handle(Map<String, Object> record, String xpath) {\n              rows.add(readRow(record, xpath));\n            }\n          });\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            LOG.warn(msg, e);\n            Map<String, Object> map = new HashMap<String, Object>();\n            map.put(DocBuilder.SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            LOG.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","sourceOld":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<Map<String, Object>>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (LOG.isDebugEnabled()) LOG.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          LOG.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformeation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            LOG.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, new XPathRecordReader.Handler() {\n            @Override\n            @SuppressWarnings(\"unchecked\")\n            public void handle(Map<String, Object> record, String xpath) {\n              rows.add(readRow(record, xpath));\n            }\n          });\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            LOG.warn(msg, e);\n            Map<String, Object> map = new HashMap<String, Object>();\n            map.put(SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            LOG.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","bugFix":["0ed3f47d0f68ca5e5107d28c942fbd1185f44c62"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"74f45af4339b0daf7a95c820ab88c1aea74fbce0","date":1387475327,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","pathOld":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","sourceNew":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<Map<String, Object>>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (LOG.isDebugEnabled()) LOG.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          LOG.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformeation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            LOG.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, new XPathRecordReader.Handler() {\n            @Override\n            @SuppressWarnings(\"unchecked\")\n            public void handle(Map<String, Object> record, String xpath) {\n              rows.add(readRow(record, xpath));\n            }\n          });\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            LOG.warn(msg, e);\n            Map<String, Object> map = new HashMap<String, Object>();\n            map.put(DocBuilder.SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            LOG.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","sourceOld":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<Map<String, Object>>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (LOG.isDebugEnabled()) LOG.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          LOG.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformeation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            LOG.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, new XPathRecordReader.Handler() {\n            @Override\n            @SuppressWarnings(\"unchecked\")\n            public void handle(Map<String, Object> record, String xpath) {\n              rows.add(readRow(record, xpath));\n            }\n          });\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            LOG.warn(msg, e);\n            Map<String, Object> map = new HashMap<String, Object>();\n            map.put(SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            LOG.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","pathOld":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","sourceNew":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (LOG.isDebugEnabled()) LOG.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          LOG.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformeation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            LOG.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, new XPathRecordReader.Handler() {\n            @Override\n            @SuppressWarnings(\"unchecked\")\n            public void handle(Map<String, Object> record, String xpath) {\n              rows.add(readRow(record, xpath));\n            }\n          });\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            LOG.warn(msg, e);\n            Map<String, Object> map = new HashMap<>();\n            map.put(DocBuilder.SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            LOG.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","sourceOld":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<Map<String, Object>>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (LOG.isDebugEnabled()) LOG.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          LOG.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformeation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            LOG.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, new XPathRecordReader.Handler() {\n            @Override\n            @SuppressWarnings(\"unchecked\")\n            public void handle(Map<String, Object> record, String xpath) {\n              rows.add(readRow(record, xpath));\n            }\n          });\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            LOG.warn(msg, e);\n            Map<String, Object> map = new HashMap<String, Object>();\n            map.put(DocBuilder.SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            LOG.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a4c8935d422223aad3246883993521d01e52e823","date":1423089561,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","pathOld":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","sourceNew":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (LOG.isDebugEnabled()) LOG.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          LOG.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            LOG.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, new XPathRecordReader.Handler() {\n            @Override\n            @SuppressWarnings(\"unchecked\")\n            public void handle(Map<String, Object> record, String xpath) {\n              rows.add(readRow(record, xpath));\n            }\n          });\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            LOG.warn(msg, e);\n            Map<String, Object> map = new HashMap<>();\n            map.put(DocBuilder.SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            LOG.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","sourceOld":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (LOG.isDebugEnabled()) LOG.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          LOG.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformeation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            LOG.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, new XPathRecordReader.Handler() {\n            @Override\n            @SuppressWarnings(\"unchecked\")\n            public void handle(Map<String, Object> record, String xpath) {\n              rows.add(readRow(record, xpath));\n            }\n          });\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            LOG.warn(msg, e);\n            Map<String, Object> map = new HashMap<>();\n            map.put(DocBuilder.SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            LOG.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"68b2523bf6d81a99aa007384dc8a69a71fec1cce","date":1477560907,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","pathOld":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","sourceNew":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (LOG.isDebugEnabled()) LOG.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          LOG.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            LOG.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, (record, xpath) -> rows.add(readRow(record, xpath)));\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            LOG.warn(msg, e);\n            Map<String, Object> map = new HashMap<>();\n            map.put(DocBuilder.SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            LOG.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","sourceOld":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (LOG.isDebugEnabled()) LOG.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          LOG.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            LOG.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, new XPathRecordReader.Handler() {\n            @Override\n            @SuppressWarnings(\"unchecked\")\n            public void handle(Map<String, Object> record, String xpath) {\n              rows.add(readRow(record, xpath));\n            }\n          });\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            LOG.warn(msg, e);\n            Map<String, Object> map = new HashMap<>();\n            map.put(DocBuilder.SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            LOG.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"80d0e6d59ae23f4a6f30eaf40bfb40742300287f","date":1477598926,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","pathOld":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","sourceNew":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (LOG.isDebugEnabled()) LOG.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          LOG.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            LOG.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, (record, xpath) -> rows.add(readRow(record, xpath)));\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            LOG.warn(msg, e);\n            Map<String, Object> map = new HashMap<>();\n            map.put(DocBuilder.SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            LOG.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","sourceOld":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (LOG.isDebugEnabled()) LOG.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          LOG.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            LOG.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, new XPathRecordReader.Handler() {\n            @Override\n            @SuppressWarnings(\"unchecked\")\n            public void handle(Map<String, Object> record, String xpath) {\n              rows.add(readRow(record, xpath));\n            }\n          });\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            LOG.warn(msg, e);\n            Map<String, Object> map = new HashMap<>();\n            map.put(DocBuilder.SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            LOG.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e9c81f7e703d7ccca5bc78beb61253f0a8a22afd","date":1534976797,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","pathOld":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","sourceNew":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (log.isDebugEnabled()) log.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          log.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            log.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, (record, xpath) -> rows.add(readRow(record, xpath)));\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            log.warn(msg, e);\n            Map<String, Object> map = new HashMap<>();\n            map.put(DocBuilder.SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            log.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","sourceOld":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (LOG.isDebugEnabled()) LOG.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          LOG.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            LOG.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, (record, xpath) -> rows.add(readRow(record, xpath)));\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            LOG.warn(msg, e);\n            Map<String, Object> map = new HashMap<>();\n            map.put(DocBuilder.SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            LOG.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a5df378a6155dcc1f4d4ecdcbd8ea5bc058560e9","date":1574619880,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","pathOld":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (log.isDebugEnabled()) log.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          log.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            log.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, (record, xpath) -> rows.add(readRow(record, xpath)));\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            log.warn(msg, e);\n            Map<String, Object> map = new HashMap<>();\n            map.put(DocBuilder.SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            log.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","sourceOld":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (log.isDebugEnabled()) log.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          log.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            log.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, (record, xpath) -> rows.add(readRow(record, xpath)));\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            log.warn(msg, e);\n            Map<String, Object> map = new HashMap<>();\n            map.put(DocBuilder.SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            log.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bb9c3baacabd473e8ecd6c4948aabacead49b88e","date":1574700980,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","pathOld":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","sourceNew":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (log.isDebugEnabled()) log.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          log.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            log.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, (record, xpath) -> rows.add(readRow(record, xpath)));\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            log.warn(msg, e);\n            Map<String, Object> map = new HashMap<>();\n            map.put(DocBuilder.SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            log.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (log.isDebugEnabled()) log.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          log.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            log.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, (record, xpath) -> rows.add(readRow(record, xpath)));\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            log.warn(msg, e);\n            Map<String, Object> map = new HashMap<>();\n            map.put(DocBuilder.SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            log.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a966532d92cf9ba2856f15a8140151bb6b518e4b","date":1588290631,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","pathOld":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","sourceNew":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (log.isDebugEnabled()) {\n            log.debug(\"Skipping url : {}\", s, e);\n          }\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          log.warn(\"Failed for url : {}\", s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            log.warn(\"Failed for url : {}\", s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, (record, xpath) -> rows.add(readRow(record, xpath)));\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            log.warn(msg, e);\n            Map<String, Object> map = new HashMap<>();\n            map.put(DocBuilder.SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            log.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","sourceOld":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (log.isDebugEnabled()) log.debug(\"Skipping url : \" + s, e);\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          log.warn(\"Failed for url : \" + s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            log.warn(\"Failed for url : \" + s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, (record, xpath) -> rows.add(readRow(record, xpath)));\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            log.warn(msg, e);\n            Map<String, Object> map = new HashMap<>();\n            map.put(DocBuilder.SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            log.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"024e13388acbf5562fcbb77a129620982a5e2d79","date":1591531835,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","pathOld":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","sourceNew":"  @SuppressWarnings({\"unchecked\"})\n  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (log.isDebugEnabled()) {\n            log.debug(\"Skipping url : {}\", s, e);\n          }\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          log.warn(\"Failed for url : {}\", s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            log.warn(\"Failed for url : {}\", s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, (record, xpath) -> rows.add(readRow(record, xpath)));\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            log.warn(msg, e);\n            Map<String, Object> map = new HashMap<>();\n            map.put(DocBuilder.SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            log.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","sourceOld":"  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (log.isDebugEnabled()) {\n            log.debug(\"Skipping url : {}\", s, e);\n          }\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          log.warn(\"Failed for url : {}\", s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            log.warn(\"Failed for url : {}\", s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, (record, xpath) -> rows.add(readRow(record, xpath)));\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            log.warn(msg, e);\n            Map<String, Object> map = new HashMap<>();\n            map.put(DocBuilder.SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            log.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d2c8f5c46c2501b61e2f55eb7ee59e6c5372290b","date":1598712724,"type":4,"author":"Alexandre Rafalovitch","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor#initQuery(String).mjava","sourceNew":null,"sourceOld":"  @SuppressWarnings({\"unchecked\"})\n  private void initQuery(String s) {\n    Reader data = null;\n    try {\n      final List<Map<String, Object>> rows = new ArrayList<>();\n      try {\n        data = dataSource.getData(s);\n      } catch (Exception e) {\n        if (ABORT.equals(onError)) {\n          wrapAndThrow(SEVERE, e);\n        } else if (SKIP.equals(onError)) {\n          if (log.isDebugEnabled()) {\n            log.debug(\"Skipping url : {}\", s, e);\n          }\n          wrapAndThrow(DataImportHandlerException.SKIP, e);\n        } else {\n          log.warn(\"Failed for url : {}\", s, e);\n          rowIterator = Collections.EMPTY_LIST.iterator();\n          return;\n        }\n      }\n      if (xslTransformer != null) {\n        try {\n          SimpleCharArrayReader caw = new SimpleCharArrayReader();\n          xslTransformer.transform(new StreamSource(data),\n                  new StreamResult(caw));\n          data = caw.getReader();\n        } catch (TransformerException e) {\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, \"Exception in applying XSL Transformation\");\n          } else if (SKIP.equals(onError)) {\n            wrapAndThrow(DataImportHandlerException.SKIP, e);\n          } else {\n            log.warn(\"Failed for url : {}\", s, e);\n            rowIterator = Collections.EMPTY_LIST.iterator();\n            return;\n          }\n        }\n      }\n      if (streamRows) {\n        rowIterator = getRowIterator(data, s);\n      } else {\n        try {\n          xpathReader.streamRecords(data, (record, xpath) -> rows.add(readRow(record, xpath)));\n        } catch (Exception e) {\n          String msg = \"Parsing failed for xml, url:\" + s + \" rows processed:\" + rows.size();\n          if (rows.size() > 0) msg += \" last row: \" + rows.get(rows.size() - 1);\n          if (ABORT.equals(onError)) {\n            wrapAndThrow(SEVERE, e, msg);\n          } else if (SKIP.equals(onError)) {\n            log.warn(msg, e);\n            Map<String, Object> map = new HashMap<>();\n            map.put(DocBuilder.SKIP_DOC, Boolean.TRUE);\n            rows.add(map);\n          } else if (CONTINUE.equals(onError)) {\n            log.warn(msg, e);\n          }\n        }\n        rowIterator = rows.iterator();\n      }\n    } finally {\n      if (!streamRows) {\n        closeIt(data);\n      }\n\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["c26f00b574427b55127e869b935845554afde1fa","7530de27b87b961b51f01bd1299b7004d46e8823"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["1b604003611eabbb4d3f0fb1f89d3b6a017f8faa"],"024e13388acbf5562fcbb77a129620982a5e2d79":["a966532d92cf9ba2856f15a8140151bb6b518e4b"],"a5df378a6155dcc1f4d4ecdcbd8ea5bc058560e9":["e9c81f7e703d7ccca5bc78beb61253f0a8a22afd"],"c26f00b574427b55127e869b935845554afde1fa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","5128b7b3b73fedff05fdc5ea2e6be53c1020bb91"],"a966532d92cf9ba2856f15a8140151bb6b518e4b":["bb9c3baacabd473e8ecd6c4948aabacead49b88e"],"1b604003611eabbb4d3f0fb1f89d3b6a017f8faa":["7530de27b87b961b51f01bd1299b7004d46e8823"],"d2c8f5c46c2501b61e2f55eb7ee59e6c5372290b":["024e13388acbf5562fcbb77a129620982a5e2d79"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"80d0e6d59ae23f4a6f30eaf40bfb40742300287f":["a4c8935d422223aad3246883993521d01e52e823","68b2523bf6d81a99aa007384dc8a69a71fec1cce"],"bb9c3baacabd473e8ecd6c4948aabacead49b88e":["a5df378a6155dcc1f4d4ecdcbd8ea5bc058560e9"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":["7530de27b87b961b51f01bd1299b7004d46e8823","1b604003611eabbb4d3f0fb1f89d3b6a017f8faa"],"7530de27b87b961b51f01bd1299b7004d46e8823":["c26f00b574427b55127e869b935845554afde1fa"],"68b2523bf6d81a99aa007384dc8a69a71fec1cce":["a4c8935d422223aad3246883993521d01e52e823"],"a4c8935d422223aad3246883993521d01e52e823":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d2c8f5c46c2501b61e2f55eb7ee59e6c5372290b"],"e9c81f7e703d7ccca5bc78beb61253f0a8a22afd":["68b2523bf6d81a99aa007384dc8a69a71fec1cce"]},"commit2Childs":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":[],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["a4c8935d422223aad3246883993521d01e52e823"],"024e13388acbf5562fcbb77a129620982a5e2d79":["d2c8f5c46c2501b61e2f55eb7ee59e6c5372290b"],"a5df378a6155dcc1f4d4ecdcbd8ea5bc058560e9":["bb9c3baacabd473e8ecd6c4948aabacead49b88e"],"c26f00b574427b55127e869b935845554afde1fa":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","7530de27b87b961b51f01bd1299b7004d46e8823"],"a966532d92cf9ba2856f15a8140151bb6b518e4b":["024e13388acbf5562fcbb77a129620982a5e2d79"],"1b604003611eabbb4d3f0fb1f89d3b6a017f8faa":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"d2c8f5c46c2501b61e2f55eb7ee59e6c5372290b":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["c26f00b574427b55127e869b935845554afde1fa"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c26f00b574427b55127e869b935845554afde1fa","5128b7b3b73fedff05fdc5ea2e6be53c1020bb91"],"80d0e6d59ae23f4a6f30eaf40bfb40742300287f":[],"bb9c3baacabd473e8ecd6c4948aabacead49b88e":["a966532d92cf9ba2856f15a8140151bb6b518e4b"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":[],"7530de27b87b961b51f01bd1299b7004d46e8823":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","1b604003611eabbb4d3f0fb1f89d3b6a017f8faa","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"a4c8935d422223aad3246883993521d01e52e823":["80d0e6d59ae23f4a6f30eaf40bfb40742300287f","68b2523bf6d81a99aa007384dc8a69a71fec1cce"],"68b2523bf6d81a99aa007384dc8a69a71fec1cce":["80d0e6d59ae23f4a6f30eaf40bfb40742300287f","e9c81f7e703d7ccca5bc78beb61253f0a8a22afd"],"e9c81f7e703d7ccca5bc78beb61253f0a8a22afd":["a5df378a6155dcc1f4d4ecdcbd8ea5bc058560e9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","80d0e6d59ae23f4a6f30eaf40bfb40742300287f","74f45af4339b0daf7a95c820ab88c1aea74fbce0","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}