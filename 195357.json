{"path":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","commits":[{"id":"5c9841ea9df1e03b039e461a66bccbf584ef78ce","date":1164236101,"type":0,"author":"Wolfgang Hoschek","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token stream, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code>.\n   * <p>\n   * This can help improve performance in the presence of expensive Analyzer / TokenFilter chains.\n   * <p>\n   * Caveats: Caching only works if the methods equals() and hashCode() methods are properly \n   * implemented on the Reader passed to <code>tokenStream(String fieldName, Reader reader)</code>.\n   * Further, using caching on large Lucene documents can lead to out of memory exceptions.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        Pair pair = new Pair(fieldName, reader);\n        final ArrayList tokens = (ArrayList) cache.get(pair);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          cache.put(pair, tokens2);\n          return new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next() throws IOException {\n              Token token = input.next(); // from filter super class\n              if (token != null) tokens2.add(token);\n              return token;\n            }\n          };\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next() {\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7dcef0ee2aeb0b537bfe29114b738ac13deb4c87","date":1164236279,"type":3,"author":"Wolfgang Hoschek","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","sourceNew":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token stream, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code>.\n   * <p>\n   * This can help improve performance in the presence of expensive Analyzer / TokenFilter chains.\n   * <p>\n   * Caveats: \n   * 1) Caching only works if the methods equals() and hashCode() methods are properly \n   * implemented on the Reader passed to <code>tokenStream(String fieldName, Reader reader)</code>.\n   * 2) Caching the tokens of large Lucene documents can lead to out of memory exceptions. \n   * 3) The Token instances delivered by the underlying child analyzer must be immutable.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        Pair pair = new Pair(fieldName, reader);\n        final ArrayList tokens = (ArrayList) cache.get(pair);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          cache.put(pair, tokens2);\n          return new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next() throws IOException {\n              Token token = input.next(); // from filter super class\n              if (token != null) tokens2.add(token);\n              return token;\n            }\n          };\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next() {\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token stream, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code>.\n   * <p>\n   * This can help improve performance in the presence of expensive Analyzer / TokenFilter chains.\n   * <p>\n   * Caveats: Caching only works if the methods equals() and hashCode() methods are properly \n   * implemented on the Reader passed to <code>tokenStream(String fieldName, Reader reader)</code>.\n   * Further, using caching on large Lucene documents can lead to out of memory exceptions.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        Pair pair = new Pair(fieldName, reader);\n        final ArrayList tokens = (ArrayList) cache.get(pair);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          cache.put(pair, tokens2);\n          return new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next() throws IOException {\n              Token token = input.next(); // from filter super class\n              if (token != null) tokens2.add(token);\n              return token;\n            }\n          };\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next() {\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e294b09f8895bed713b23a472f55d3c67917d43d","date":1164237012,"type":3,"author":"Wolfgang Hoschek","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","sourceNew":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token stream, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code>.\n   * <p>\n   * This can help improve performance in the presence of expensive Analyzer / TokenFilter chains.\n   * <p>\n   * Caveats: \n   * 1) Caching only works if the methods equals() and hashCode() methods are properly \n   * implemented on the Reader passed to <code>tokenStream(String fieldName, Reader reader)</code>.\n   * 2) Caching the tokens of large Lucene documents can lead to out of memory exceptions. \n   * 3) The Token instances delivered by the underlying child analyzer must be immutable.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        Pair key = new Pair(fieldName, reader);\n        final ArrayList tokens = (ArrayList) cache.get(key);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          cache.put(key, tokens2);\n          return new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next() throws IOException {\n              Token token = input.next(); // from filter super class\n              if (token != null) tokens2.add(token);\n              return token;\n            }\n          };\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next() {\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token stream, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code>.\n   * <p>\n   * This can help improve performance in the presence of expensive Analyzer / TokenFilter chains.\n   * <p>\n   * Caveats: \n   * 1) Caching only works if the methods equals() and hashCode() methods are properly \n   * implemented on the Reader passed to <code>tokenStream(String fieldName, Reader reader)</code>.\n   * 2) Caching the tokens of large Lucene documents can lead to out of memory exceptions. \n   * 3) The Token instances delivered by the underlying child analyzer must be immutable.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        Pair pair = new Pair(fieldName, reader);\n        final ArrayList tokens = (ArrayList) cache.get(pair);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          cache.put(pair, tokens2);\n          return new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next() throws IOException {\n              Token token = input.next(); // from filter super class\n              if (token != null) tokens2.add(token);\n              return token;\n            }\n          };\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next() {\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b92e421d87170033ef8a6105a8f26581cff1f248","date":1164237139,"type":3,"author":"Wolfgang Hoschek","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","sourceNew":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token stream, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code>.\n   * <p>\n   * This can help improve performance in the presence of expensive Analyzer / TokenFilter chains.\n   * <p>\n   * Caveats: \n   * 1) Caching only works if the methods equals() and hashCode() are properly \n   * implemented on the Reader passed to <code>tokenStream(String fieldName, Reader reader)</code>.\n   * 2) Caching the tokens of large Lucene documents can lead to out of memory exceptions. \n   * 3) The Token instances delivered by the underlying child analyzer must be immutable.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        Pair key = new Pair(fieldName, reader);\n        final ArrayList tokens = (ArrayList) cache.get(key);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          cache.put(key, tokens2);\n          return new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next() throws IOException {\n              Token token = input.next(); // from filter super class\n              if (token != null) tokens2.add(token);\n              return token;\n            }\n          };\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next() {\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token stream, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code>.\n   * <p>\n   * This can help improve performance in the presence of expensive Analyzer / TokenFilter chains.\n   * <p>\n   * Caveats: \n   * 1) Caching only works if the methods equals() and hashCode() methods are properly \n   * implemented on the Reader passed to <code>tokenStream(String fieldName, Reader reader)</code>.\n   * 2) Caching the tokens of large Lucene documents can lead to out of memory exceptions. \n   * 3) The Token instances delivered by the underlying child analyzer must be immutable.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        Pair key = new Pair(fieldName, reader);\n        final ArrayList tokens = (ArrayList) cache.get(key);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          cache.put(key, tokens2);\n          return new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next() throws IOException {\n              Token token = input.next(); // from filter super class\n              if (token != null) tokens2.add(token);\n              return token;\n            }\n          };\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next() {\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"86fab45674bc40ecfc974abda9e8bfc19364a86a","date":1164237776,"type":3,"author":"Wolfgang Hoschek","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","sourceNew":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token stream, and delivers those cached tokens on subsequent matching calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code>.\n   * <p>\n   * This can help improve performance in the presence of expensive Analyzer / TokenFilter chains.\n   * <p>\n   * Caveats: \n   * 1) Caching only works if the methods equals() and hashCode() are properly \n   * implemented on the Reader passed to <code>tokenStream(String fieldName, Reader reader)</code>.\n   * 2) Caching the tokens of large Lucene documents can lead to out of memory exceptions. \n   * 3) The Token instances delivered by the underlying child analyzer must be immutable.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        Pair key = new Pair(fieldName, reader);\n        final ArrayList tokens = (ArrayList) cache.get(key);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          cache.put(key, tokens2);\n          return new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next() throws IOException {\n              Token token = input.next(); // from filter super class\n              if (token != null) tokens2.add(token);\n              return token;\n            }\n          };\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next() {\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token stream, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code>.\n   * <p>\n   * This can help improve performance in the presence of expensive Analyzer / TokenFilter chains.\n   * <p>\n   * Caveats: \n   * 1) Caching only works if the methods equals() and hashCode() are properly \n   * implemented on the Reader passed to <code>tokenStream(String fieldName, Reader reader)</code>.\n   * 2) Caching the tokens of large Lucene documents can lead to out of memory exceptions. \n   * 3) The Token instances delivered by the underlying child analyzer must be immutable.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        Pair key = new Pair(fieldName, reader);\n        final ArrayList tokens = (ArrayList) cache.get(key);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          cache.put(key, tokens2);\n          return new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next() throws IOException {\n              Token token = input.next(); // from filter super class\n              if (token != null) tokens2.add(token);\n              return token;\n            }\n          };\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next() {\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1e4f6aaa14836889ddc3c570c5637720aa36b01b","date":1164238182,"type":3,"author":"Wolfgang Hoschek","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","sourceNew":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token stream, and delivers those cached tokens on subsequent matching calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code>.\n   * <p>\n   * This can help improve performance in the presence of expensive Analyzer / TokenFilter chains.\n   * <p>\n   * Caveats: \n   * 2) Caching the tokens of large Lucene documents can lead to out of memory exceptions. \n   * 3) The Token instances delivered by the underlying child analyzer must be immutable.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList tokens = (ArrayList) cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          cache.put(fieldName, tokens2);\n          return new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next() throws IOException {\n              Token token = input.next(); // from filter super class\n              if (token != null) tokens2.add(token);\n              return token;\n            }\n          };\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next() {\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token stream, and delivers those cached tokens on subsequent matching calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code>.\n   * <p>\n   * This can help improve performance in the presence of expensive Analyzer / TokenFilter chains.\n   * <p>\n   * Caveats: \n   * 1) Caching only works if the methods equals() and hashCode() are properly \n   * implemented on the Reader passed to <code>tokenStream(String fieldName, Reader reader)</code>.\n   * 2) Caching the tokens of large Lucene documents can lead to out of memory exceptions. \n   * 3) The Token instances delivered by the underlying child analyzer must be immutable.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        Pair key = new Pair(fieldName, reader);\n        final ArrayList tokens = (ArrayList) cache.get(key);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          cache.put(key, tokens2);\n          return new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next() throws IOException {\n              Token token = input.next(); // from filter super class\n              if (token != null) tokens2.add(token);\n              return token;\n            }\n          };\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next() {\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a7df847add05971ad8e4209615386df687c3cbbe","date":1164241553,"type":3,"author":"Wolfgang Hoschek","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","sourceNew":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token stream, and delivers those cached tokens on subsequent matching calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code>.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * 2) Caching the tokens of large Lucene documents can lead to out of memory exceptions. \n   * 3) The Token instances delivered by the underlying child analyzer must be immutable.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList tokens = (ArrayList) cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          cache.put(fieldName, tokens2);\n          return new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next() throws IOException {\n              Token token = input.next(); // from filter super class\n              if (token != null) tokens2.add(token);\n              return token;\n            }\n          };\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next() {\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token stream, and delivers those cached tokens on subsequent matching calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code>.\n   * <p>\n   * This can help improve performance in the presence of expensive Analyzer / TokenFilter chains.\n   * <p>\n   * Caveats: \n   * 2) Caching the tokens of large Lucene documents can lead to out of memory exceptions. \n   * 3) The Token instances delivered by the underlying child analyzer must be immutable.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList tokens = (ArrayList) cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          cache.put(fieldName, tokens2);\n          return new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next() throws IOException {\n              Token token = input.next(); // from filter super class\n              if (token != null) tokens2.add(token);\n              return token;\n            }\n          };\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next() {\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"416a0fb892148aa9a6c24611491df4b727eaab06","date":1164241656,"type":3,"author":"Wolfgang Hoschek","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","sourceNew":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token stream, and delivers those cached tokens on subsequent matching calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code>.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * 1) Caching the tokens of large Lucene documents can lead to out of memory exceptions. \n   * 2) The Token instances delivered by the underlying child analyzer must be immutable.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList tokens = (ArrayList) cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          cache.put(fieldName, tokens2);\n          return new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next() throws IOException {\n              Token token = input.next(); // from filter super class\n              if (token != null) tokens2.add(token);\n              return token;\n            }\n          };\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next() {\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token stream, and delivers those cached tokens on subsequent matching calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code>.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * 2) Caching the tokens of large Lucene documents can lead to out of memory exceptions. \n   * 3) The Token instances delivered by the underlying child analyzer must be immutable.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList tokens = (ArrayList) cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          cache.put(fieldName, tokens2);\n          return new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next() throws IOException {\n              Token token = input.next(); // from filter super class\n              if (token != null) tokens2.add(token);\n              return token;\n            }\n          };\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next() {\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"377b6c04030498cf26062c99aa1d46b15118945b","date":1164649046,"type":4,"author":"Wolfgang Hoschek","isMerge":false,"pathNew":"/dev/null","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","sourceNew":null,"sourceOld":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token stream, and delivers those cached tokens on subsequent matching calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code>.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * 1) Caching the tokens of large Lucene documents can lead to out of memory exceptions. \n   * 2) The Token instances delivered by the underlying child analyzer must be immutable.\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList tokens = (ArrayList) cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          cache.put(fieldName, tokens2);\n          return new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next() throws IOException {\n              Token token = input.next(); // from filter super class\n              if (token != null) tokens2.add(token);\n              return token;\n            }\n          };\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next() {\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cbd4f37195fa1ffd923031c58ae7b82b9959a260","date":1164659132,"type":0,"author":"Wolfgang Hoschek","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token streams, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code> \n   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * <ul>\n   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> \n   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>\n   * <li>A caching analyzer instance must not be used for more than one document\n   * because the cache is not keyed on the Reader parameter.</li>\n   * </ul>\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList tokens = (ArrayList) cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          cache.put(fieldName, tokens2);\n          return new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next() throws IOException {\n              Token token = input.next(); // from filter super class\n              if (token != null) tokens2.add(token);\n              return token;\n            }\n          };\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next() {\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8c53928df49cc0a3b0f3b9df66e6bf9ce21a654a","date":1164659452,"type":3,"author":"Wolfgang Hoschek","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","sourceNew":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token streams, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code> \n   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * <ul>\n   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> \n   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>\n   * <li>The same caching analyzer instance must not be used for more than one document\n   * because the cache is not keyed on the Reader parameter.</li>\n   * </ul>\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList tokens = (ArrayList) cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          cache.put(fieldName, tokens2);\n          return new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next() throws IOException {\n              Token token = input.next(); // from filter super class\n              if (token != null) tokens2.add(token);\n              return token;\n            }\n          };\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next() {\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token streams, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code> \n   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * <ul>\n   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> \n   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>\n   * <li>A caching analyzer instance must not be used for more than one document\n   * because the cache is not keyed on the Reader parameter.</li>\n   * </ul>\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList tokens = (ArrayList) cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          cache.put(fieldName, tokens2);\n          return new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next() throws IOException {\n              Token token = input.next(); // from filter super class\n              if (token != null) tokens2.add(token);\n              return token;\n            }\n          };\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next() {\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fdc2cf9abcc3aff05c4860c448d70aa7a2999684","date":1164659685,"type":3,"author":"Wolfgang Hoschek","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","sourceNew":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token streams, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code> \n   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * <ul>\n   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> \n   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>\n   * <li>The same caching analyzer instance must not be used for more than one document\n   * because the cache is not keyed on the Reader parameter.</li>\n   * </ul>\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList tokens = (ArrayList) cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          TokenStream stream = new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next() throws IOException {\n              Token token = input.next(); // from filter super class\n              if (token != null) tokens2.add(token);\n              return token;\n            }\n          };\n          cache.put(fieldName, tokens2);\n          return stream;\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next() {\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token streams, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code> \n   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * <ul>\n   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> \n   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>\n   * <li>The same caching analyzer instance must not be used for more than one document\n   * because the cache is not keyed on the Reader parameter.</li>\n   * </ul>\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList tokens = (ArrayList) cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          cache.put(fieldName, tokens2);\n          return new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next() throws IOException {\n              Token token = input.next(); // from filter super class\n              if (token != null) tokens2.add(token);\n              return token;\n            }\n          };\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next() {\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b584dd842d8dafab32b398a7252ca91e666a4422","date":1164659789,"type":3,"author":"Wolfgang Hoschek","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","sourceNew":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token streams, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code> \n   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * <ul>\n   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> \n   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>\n   * <li>The same caching analyzer instance must not be used for more than one document\n   * because the cache is not keyed on the Reader parameter.</li>\n   * </ul>\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList tokens = (ArrayList) cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          TokenStream tokenStream = new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next() throws IOException {\n              Token token = input.next(); // from filter super class\n              if (token != null) tokens2.add(token);\n              return token;\n            }\n          };\n          \n          cache.put(fieldName, tokens2);\n          return tokenStream;\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next() {\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token streams, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code> \n   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * <ul>\n   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> \n   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>\n   * <li>The same caching analyzer instance must not be used for more than one document\n   * because the cache is not keyed on the Reader parameter.</li>\n   * </ul>\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList tokens = (ArrayList) cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          TokenStream stream = new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next() throws IOException {\n              Token token = input.next(); // from filter super class\n              if (token != null) tokens2.add(token);\n              return token;\n            }\n          };\n          cache.put(fieldName, tokens2);\n          return stream;\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next() {\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e2cb543b41c145f33390f460ee743d6693c9c6c","date":1219243087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","sourceNew":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token streams, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code> \n   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * <ul>\n   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> \n   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>\n   * <li>The same caching analyzer instance must not be used for more than one document\n   * because the cache is not keyed on the Reader parameter.</li>\n   * </ul>\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList tokens = (ArrayList) cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          TokenStream tokenStream = new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next(final Token reusableToken) throws IOException {\n              assert reusableToken != null;\n              Token nextToken = input.next(reusableToken); // from filter super class\n              if (nextToken != null) tokens2.add(nextToken.clone());\n              return nextToken;\n            }\n          };\n          \n          cache.put(fieldName, tokens2);\n          return tokenStream;\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next(Token token) {\n              assert token != null;\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token streams, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code> \n   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * <ul>\n   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> \n   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>\n   * <li>The same caching analyzer instance must not be used for more than one document\n   * because the cache is not keyed on the Reader parameter.</li>\n   * </ul>\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList tokens = (ArrayList) cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          TokenStream tokenStream = new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next() throws IOException {\n              Token token = input.next(); // from filter super class\n              if (token != null) tokens2.add(token);\n              return token;\n            }\n          };\n          \n          cache.put(fieldName, tokens2);\n          return tokenStream;\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next() {\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9b5756469957918cac40a831acec9cf01c8c2bb3","date":1249167152,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","sourceNew":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token streams, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code> \n   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * <ul>\n   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> \n   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>\n   * <li>The same caching analyzer instance must not be used for more than one document\n   * because the cache is not keyed on the Reader parameter.</li>\n   * </ul>\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList tokens = (ArrayList) cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          TokenStream tokenStream = new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public boolean incrementToken() throws IOException {\n              boolean hasNext = input.incrementToken();\n              if (hasNext) tokens2.add(captureState());\n              return hasNext;\n            }\n          };\n          \n          cache.put(fieldName, tokens2);\n          return tokenStream;\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public boolean incrementToken() {\n              if (!iter.hasNext()) return false;\n              restoreState((AttributeSource.State) iter.next());\n              return true;\n            }\n          };\n        }\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token streams, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code> \n   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * <ul>\n   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> \n   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>\n   * <li>The same caching analyzer instance must not be used for more than one document\n   * because the cache is not keyed on the Reader parameter.</li>\n   * </ul>\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList tokens = (ArrayList) cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          TokenStream tokenStream = new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public Token next(final Token reusableToken) throws IOException {\n              assert reusableToken != null;\n              Token nextToken = input.next(reusableToken); // from filter super class\n              if (nextToken != null) tokens2.add(nextToken.clone());\n              return nextToken;\n            }\n          };\n          \n          cache.put(fieldName, tokens2);\n          return tokenStream;\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public Token next(Token token) {\n              assert token != null;\n              if (!iter.hasNext()) return null;\n              return (Token) iter.next();\n            }\n          };\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d68e5c46e6a5ebdf4dafec4a123344092b915cc0","date":1256752193,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"contrib/wordnet/src/java/org/apache/lucene/wordnet/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","sourceNew":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token streams, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code> \n   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * <ul>\n   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> \n   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>\n   * <li>The same caching analyzer instance must not be used for more than one document\n   * because the cache is not keyed on the Reader parameter.</li>\n   * </ul>\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList tokens = (ArrayList) cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          TokenStream tokenStream = new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public boolean incrementToken() throws IOException {\n              boolean hasNext = input.incrementToken();\n              if (hasNext) tokens2.add(captureState());\n              return hasNext;\n            }\n          };\n          \n          cache.put(fieldName, tokens2);\n          return tokenStream;\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public boolean incrementToken() {\n              if (!iter.hasNext()) return false;\n              restoreState((AttributeSource.State) iter.next());\n              return true;\n            }\n          };\n        }\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token streams, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code> \n   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * <ul>\n   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> \n   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>\n   * <li>The same caching analyzer instance must not be used for more than one document\n   * because the cache is not keyed on the Reader parameter.</li>\n   * </ul>\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList tokens = (ArrayList) cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          TokenStream tokenStream = new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public boolean incrementToken() throws IOException {\n              boolean hasNext = input.incrementToken();\n              if (hasNext) tokens2.add(captureState());\n              return hasNext;\n            }\n          };\n          \n          cache.put(fieldName, tokens2);\n          return tokenStream;\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public boolean incrementToken() {\n              if (!iter.hasNext()) return false;\n              restoreState((AttributeSource.State) iter.next());\n              return true;\n            }\n          };\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["b584dd842d8dafab32b398a7252ca91e666a4422"],"8c53928df49cc0a3b0f3b9df66e6bf9ce21a654a":["cbd4f37195fa1ffd923031c58ae7b82b9959a260"],"416a0fb892148aa9a6c24611491df4b727eaab06":["a7df847add05971ad8e4209615386df687c3cbbe"],"1e4f6aaa14836889ddc3c570c5637720aa36b01b":["86fab45674bc40ecfc974abda9e8bfc19364a86a"],"b584dd842d8dafab32b398a7252ca91e666a4422":["fdc2cf9abcc3aff05c4860c448d70aa7a2999684"],"a7df847add05971ad8e4209615386df687c3cbbe":["1e4f6aaa14836889ddc3c570c5637720aa36b01b"],"e294b09f8895bed713b23a472f55d3c67917d43d":["7dcef0ee2aeb0b537bfe29114b738ac13deb4c87"],"fdc2cf9abcc3aff05c4860c448d70aa7a2999684":["8c53928df49cc0a3b0f3b9df66e6bf9ce21a654a"],"5c9841ea9df1e03b039e461a66bccbf584ef78ce":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"b92e421d87170033ef8a6105a8f26581cff1f248":["e294b09f8895bed713b23a472f55d3c67917d43d"],"7dcef0ee2aeb0b537bfe29114b738ac13deb4c87":["5c9841ea9df1e03b039e461a66bccbf584ef78ce"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9b5756469957918cac40a831acec9cf01c8c2bb3":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"377b6c04030498cf26062c99aa1d46b15118945b":["416a0fb892148aa9a6c24611491df4b727eaab06"],"86fab45674bc40ecfc974abda9e8bfc19364a86a":["b92e421d87170033ef8a6105a8f26581cff1f248"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d68e5c46e6a5ebdf4dafec4a123344092b915cc0"],"d68e5c46e6a5ebdf4dafec4a123344092b915cc0":["9b5756469957918cac40a831acec9cf01c8c2bb3"],"cbd4f37195fa1ffd923031c58ae7b82b9959a260":["377b6c04030498cf26062c99aa1d46b15118945b"]},"commit2Childs":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["9b5756469957918cac40a831acec9cf01c8c2bb3"],"8c53928df49cc0a3b0f3b9df66e6bf9ce21a654a":["fdc2cf9abcc3aff05c4860c448d70aa7a2999684"],"416a0fb892148aa9a6c24611491df4b727eaab06":["377b6c04030498cf26062c99aa1d46b15118945b"],"1e4f6aaa14836889ddc3c570c5637720aa36b01b":["a7df847add05971ad8e4209615386df687c3cbbe"],"b584dd842d8dafab32b398a7252ca91e666a4422":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"a7df847add05971ad8e4209615386df687c3cbbe":["416a0fb892148aa9a6c24611491df4b727eaab06"],"e294b09f8895bed713b23a472f55d3c67917d43d":["b92e421d87170033ef8a6105a8f26581cff1f248"],"fdc2cf9abcc3aff05c4860c448d70aa7a2999684":["b584dd842d8dafab32b398a7252ca91e666a4422"],"5c9841ea9df1e03b039e461a66bccbf584ef78ce":["7dcef0ee2aeb0b537bfe29114b738ac13deb4c87"],"7dcef0ee2aeb0b537bfe29114b738ac13deb4c87":["e294b09f8895bed713b23a472f55d3c67917d43d"],"b92e421d87170033ef8a6105a8f26581cff1f248":["86fab45674bc40ecfc974abda9e8bfc19364a86a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["5c9841ea9df1e03b039e461a66bccbf584ef78ce"],"9b5756469957918cac40a831acec9cf01c8c2bb3":["d68e5c46e6a5ebdf4dafec4a123344092b915cc0"],"377b6c04030498cf26062c99aa1d46b15118945b":["cbd4f37195fa1ffd923031c58ae7b82b9959a260"],"86fab45674bc40ecfc974abda9e8bfc19364a86a":["1e4f6aaa14836889ddc3c570c5637720aa36b01b"],"cbd4f37195fa1ffd923031c58ae7b82b9959a260":["8c53928df49cc0a3b0f3b9df66e6bf9ce21a654a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"d68e5c46e6a5ebdf4dafec4a123344092b915cc0":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}