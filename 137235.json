{"path":"solr/core/src/test/org/apache/solr/search/facet/RangeFacetCloudTest#setupCluster().mjava","commits":[{"id":"c05b634713ca09b2267477408773904d4c69dd9d","date":1530894845,"type":0,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/search/facet/RangeFacetCloudTest#setupCluster().mjava","pathOld":"/dev/null","sourceNew":"  @BeforeClass\n  public static void setupCluster() throws Exception {\n    final int numShards = TestUtil.nextInt(random(),1,5);\n    final int numReplicas = 1;\n    final int maxShardsPerNode = 1;\n    final int nodeCount = numShards * numReplicas;\n\n    configureCluster(nodeCount)\n      .addConfig(CONF, Paths.get(TEST_HOME(), \"collection1\", \"conf\"))\n      .configure();\n\n    assertEquals(0, (CollectionAdminRequest.createCollection(COLLECTION, CONF, numShards, numReplicas)\n                     .setMaxShardsPerNode(maxShardsPerNode)\n                     .setProperties(Collections.singletonMap(CoreAdminParams.CONFIG, \"solrconfig-minimal.xml\"))\n                     .process(cluster.getSolrClient())).getStatus());\n    \n    cluster.getSolrClient().setDefaultCollection(COLLECTION);\n\n    final int numDocs = atLeast(1000);\n    final int maxTermId = atLeast(TERM_VALUES_RANDOMIZER);\n    \n    // seed the TERM_MODEL Maps so we don't have null check later\n    for (int i = 0; i < NUM_RANGE_VALUES; i++) {\n      TERM_MODEL[i] = new LinkedHashMap<>();\n    }\n\n    // build our index & our models\n    for (int id = 0; id < numDocs; id++) {\n      final int rangeVal = random().nextInt(NUM_RANGE_VALUES);\n      final String termVal = \"x\" + random().nextInt(maxTermId);\n      final SolrInputDocument doc = sdoc(\"id\", \"\"+id,\n                                         INT_FIELD, \"\"+rangeVal,\n                                         STR_FIELD, termVal);\n      RANGE_MODEL[rangeVal]++;\n      TERM_MODEL[rangeVal].merge(termVal, 1, Integer::sum);\n\n      assertEquals(0, (new UpdateRequest().add(doc)).process(cluster.getSolrClient()).getStatus());\n    }\n    assertEquals(0, cluster.getSolrClient().commit().getStatus());\n    \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","date":1531589977,"type":0,"author":"Michael Braun","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/search/facet/RangeFacetCloudTest#setupCluster().mjava","pathOld":"/dev/null","sourceNew":"  @BeforeClass\n  public static void setupCluster() throws Exception {\n    final int numShards = TestUtil.nextInt(random(),1,5);\n    final int numReplicas = 1;\n    final int maxShardsPerNode = 1;\n    final int nodeCount = numShards * numReplicas;\n\n    configureCluster(nodeCount)\n      .addConfig(CONF, Paths.get(TEST_HOME(), \"collection1\", \"conf\"))\n      .configure();\n\n    assertEquals(0, (CollectionAdminRequest.createCollection(COLLECTION, CONF, numShards, numReplicas)\n                     .setMaxShardsPerNode(maxShardsPerNode)\n                     .setProperties(Collections.singletonMap(CoreAdminParams.CONFIG, \"solrconfig-minimal.xml\"))\n                     .process(cluster.getSolrClient())).getStatus());\n    \n    cluster.getSolrClient().setDefaultCollection(COLLECTION);\n\n    final int numDocs = atLeast(1000);\n    final int maxTermId = atLeast(TERM_VALUES_RANDOMIZER);\n    \n    // seed the TERM_MODEL Maps so we don't have null check later\n    for (int i = 0; i < NUM_RANGE_VALUES; i++) {\n      TERM_MODEL[i] = new LinkedHashMap<>();\n    }\n\n    // build our index & our models\n    for (int id = 0; id < numDocs; id++) {\n      final int rangeVal = random().nextInt(NUM_RANGE_VALUES);\n      final String termVal = \"x\" + random().nextInt(maxTermId);\n      final SolrInputDocument doc = sdoc(\"id\", \"\"+id,\n                                         INT_FIELD, \"\"+rangeVal,\n                                         STR_FIELD, termVal);\n      RANGE_MODEL[rangeVal]++;\n      TERM_MODEL[rangeVal].merge(termVal, 1, Integer::sum);\n\n      assertEquals(0, (new UpdateRequest().add(doc)).process(cluster.getSolrClient()).getStatus());\n    }\n    assertEquals(0, cluster.getSolrClient().commit().getStatus());\n    \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","date":1531905561,"type":0,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/search/facet/RangeFacetCloudTest#setupCluster().mjava","pathOld":"/dev/null","sourceNew":"  @BeforeClass\n  public static void setupCluster() throws Exception {\n    final int numShards = TestUtil.nextInt(random(),1,5);\n    final int numReplicas = 1;\n    final int maxShardsPerNode = 1;\n    final int nodeCount = numShards * numReplicas;\n\n    configureCluster(nodeCount)\n      .addConfig(CONF, Paths.get(TEST_HOME(), \"collection1\", \"conf\"))\n      .configure();\n\n    assertEquals(0, (CollectionAdminRequest.createCollection(COLLECTION, CONF, numShards, numReplicas)\n                     .setMaxShardsPerNode(maxShardsPerNode)\n                     .setProperties(Collections.singletonMap(CoreAdminParams.CONFIG, \"solrconfig-minimal.xml\"))\n                     .process(cluster.getSolrClient())).getStatus());\n    \n    cluster.getSolrClient().setDefaultCollection(COLLECTION);\n\n    final int numDocs = atLeast(1000);\n    final int maxTermId = atLeast(TERM_VALUES_RANDOMIZER);\n    \n    // seed the TERM_MODEL Maps so we don't have null check later\n    for (int i = 0; i < NUM_RANGE_VALUES; i++) {\n      TERM_MODEL[i] = new LinkedHashMap<>();\n    }\n\n    // build our index & our models\n    for (int id = 0; id < numDocs; id++) {\n      final int rangeVal = random().nextInt(NUM_RANGE_VALUES);\n      final String termVal = \"x\" + random().nextInt(maxTermId);\n      final SolrInputDocument doc = sdoc(\"id\", \"\"+id,\n                                         INT_FIELD, \"\"+rangeVal,\n                                         STR_FIELD, termVal);\n      RANGE_MODEL[rangeVal]++;\n      TERM_MODEL[rangeVal].merge(termVal, 1, Integer::sum);\n\n      assertEquals(0, (new UpdateRequest().add(doc)).process(cluster.getSolrClient()).getStatus());\n    }\n    assertEquals(0, cluster.getSolrClient().commit().getStatus());\n    \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad2010e2591e2d11678b59291d2ab31bd30e3ac7","date":1559150835,"type":3,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/search/facet/RangeFacetCloudTest#setupCluster().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/facet/RangeFacetCloudTest#setupCluster().mjava","sourceNew":"  @BeforeClass\n  public static void setupCluster() throws Exception {\n    final int numShards = TestUtil.nextInt(random(),1,5);\n    final int numReplicas = 1;\n    final int maxShardsPerNode = 1;\n    final int nodeCount = numShards * numReplicas;\n\n    configureCluster(nodeCount)\n      .addConfig(CONF, Paths.get(TEST_HOME(), \"collection1\", \"conf\"))\n      .configure();\n\n    assertEquals(0, (CollectionAdminRequest.createCollection(COLLECTION, CONF, numShards, numReplicas)\n                     .setMaxShardsPerNode(maxShardsPerNode)\n                     .setProperties(Collections.singletonMap(CoreAdminParams.CONFIG, \"solrconfig-minimal.xml\"))\n                     .process(cluster.getSolrClient())).getStatus());\n    \n    cluster.getSolrClient().setDefaultCollection(COLLECTION);\n\n    final int numDocs = atLeast(1000);\n    final int maxTermId = atLeast(TERM_VALUES_RANDOMIZER);\n\n    // clear the RANGE_MODEL\n    Arrays.fill(RANGE_MODEL, 0);\n    // seed the TERM_MODEL Maps so we don't have null check later\n    for (int i = 0; i < NUM_RANGE_VALUES; i++) {\n      TERM_MODEL[i] = new LinkedHashMap<>();\n    }\n\n    // build our index & our models\n    for (int id = 0; id < numDocs; id++) {\n      final int rangeVal = random().nextInt(NUM_RANGE_VALUES);\n      final String termVal = \"x\" + random().nextInt(maxTermId);\n      final SolrInputDocument doc = sdoc(\"id\", \"\"+id,\n                                         INT_FIELD, \"\"+rangeVal,\n                                         STR_FIELD, termVal);\n      RANGE_MODEL[rangeVal]++;\n      TERM_MODEL[rangeVal].merge(termVal, 1, Integer::sum);\n\n      assertEquals(0, (new UpdateRequest().add(doc)).process(cluster.getSolrClient()).getStatus());\n    }\n    assertEquals(0, cluster.getSolrClient().commit().getStatus());\n    \n  }\n\n","sourceOld":"  @BeforeClass\n  public static void setupCluster() throws Exception {\n    final int numShards = TestUtil.nextInt(random(),1,5);\n    final int numReplicas = 1;\n    final int maxShardsPerNode = 1;\n    final int nodeCount = numShards * numReplicas;\n\n    configureCluster(nodeCount)\n      .addConfig(CONF, Paths.get(TEST_HOME(), \"collection1\", \"conf\"))\n      .configure();\n\n    assertEquals(0, (CollectionAdminRequest.createCollection(COLLECTION, CONF, numShards, numReplicas)\n                     .setMaxShardsPerNode(maxShardsPerNode)\n                     .setProperties(Collections.singletonMap(CoreAdminParams.CONFIG, \"solrconfig-minimal.xml\"))\n                     .process(cluster.getSolrClient())).getStatus());\n    \n    cluster.getSolrClient().setDefaultCollection(COLLECTION);\n\n    final int numDocs = atLeast(1000);\n    final int maxTermId = atLeast(TERM_VALUES_RANDOMIZER);\n    \n    // seed the TERM_MODEL Maps so we don't have null check later\n    for (int i = 0; i < NUM_RANGE_VALUES; i++) {\n      TERM_MODEL[i] = new LinkedHashMap<>();\n    }\n\n    // build our index & our models\n    for (int id = 0; id < numDocs; id++) {\n      final int rangeVal = random().nextInt(NUM_RANGE_VALUES);\n      final String termVal = \"x\" + random().nextInt(maxTermId);\n      final SolrInputDocument doc = sdoc(\"id\", \"\"+id,\n                                         INT_FIELD, \"\"+rangeVal,\n                                         STR_FIELD, termVal);\n      RANGE_MODEL[rangeVal]++;\n      TERM_MODEL[rangeVal].merge(termVal, 1, Integer::sum);\n\n      assertEquals(0, (new UpdateRequest().add(doc)).process(cluster.getSolrClient()).getStatus());\n    }\n    assertEquals(0, cluster.getSolrClient().commit().getStatus());\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"58af973abe9f7c0faa777c1a6285fc7f72c171c5","date":1559329541,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/search/facet/RangeFacetCloudTest#setupCluster().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/facet/RangeFacetCloudTest#setupCluster().mjava","sourceNew":"  @BeforeClass\n  public static void setupCluster() throws Exception {\n    final int numShards = TestUtil.nextInt(random(),1,5);\n    final int numReplicas = 1;\n    final int maxShardsPerNode = 1;\n    final int nodeCount = numShards * numReplicas;\n\n    configureCluster(nodeCount)\n      .addConfig(CONF, Paths.get(TEST_HOME(), \"collection1\", \"conf\"))\n      .configure();\n\n    assertEquals(0, (CollectionAdminRequest.createCollection(COLLECTION, CONF, numShards, numReplicas)\n                     .setMaxShardsPerNode(maxShardsPerNode)\n                     .setProperties(Collections.singletonMap(CoreAdminParams.CONFIG, \"solrconfig-minimal.xml\"))\n                     .process(cluster.getSolrClient())).getStatus());\n    \n    cluster.getSolrClient().setDefaultCollection(COLLECTION);\n\n    final int numDocs = atLeast(1000);\n    final int maxTermId = atLeast(TERM_VALUES_RANDOMIZER);\n\n    // clear the RANGE_MODEL\n    Arrays.fill(RANGE_MODEL, 0);\n    // seed the TERM_MODEL Maps so we don't have null check later\n    for (int i = 0; i < NUM_RANGE_VALUES; i++) {\n      TERM_MODEL[i] = new LinkedHashMap<>();\n    }\n\n    // build our index & our models\n    for (int id = 0; id < numDocs; id++) {\n      final int rangeVal = random().nextInt(NUM_RANGE_VALUES);\n      final String termVal = \"x\" + random().nextInt(maxTermId);\n      final SolrInputDocument doc = sdoc(\"id\", \"\"+id,\n                                         INT_FIELD, \"\"+rangeVal,\n                                         STR_FIELD, termVal);\n      RANGE_MODEL[rangeVal]++;\n      TERM_MODEL[rangeVal].merge(termVal, 1, Integer::sum);\n\n      assertEquals(0, (new UpdateRequest().add(doc)).process(cluster.getSolrClient()).getStatus());\n    }\n    assertEquals(0, cluster.getSolrClient().commit().getStatus());\n    \n  }\n\n","sourceOld":"  @BeforeClass\n  public static void setupCluster() throws Exception {\n    final int numShards = TestUtil.nextInt(random(),1,5);\n    final int numReplicas = 1;\n    final int maxShardsPerNode = 1;\n    final int nodeCount = numShards * numReplicas;\n\n    configureCluster(nodeCount)\n      .addConfig(CONF, Paths.get(TEST_HOME(), \"collection1\", \"conf\"))\n      .configure();\n\n    assertEquals(0, (CollectionAdminRequest.createCollection(COLLECTION, CONF, numShards, numReplicas)\n                     .setMaxShardsPerNode(maxShardsPerNode)\n                     .setProperties(Collections.singletonMap(CoreAdminParams.CONFIG, \"solrconfig-minimal.xml\"))\n                     .process(cluster.getSolrClient())).getStatus());\n    \n    cluster.getSolrClient().setDefaultCollection(COLLECTION);\n\n    final int numDocs = atLeast(1000);\n    final int maxTermId = atLeast(TERM_VALUES_RANDOMIZER);\n    \n    // seed the TERM_MODEL Maps so we don't have null check later\n    for (int i = 0; i < NUM_RANGE_VALUES; i++) {\n      TERM_MODEL[i] = new LinkedHashMap<>();\n    }\n\n    // build our index & our models\n    for (int id = 0; id < numDocs; id++) {\n      final int rangeVal = random().nextInt(NUM_RANGE_VALUES);\n      final String termVal = \"x\" + random().nextInt(maxTermId);\n      final SolrInputDocument doc = sdoc(\"id\", \"\"+id,\n                                         INT_FIELD, \"\"+rangeVal,\n                                         STR_FIELD, termVal);\n      RANGE_MODEL[rangeVal]++;\n      TERM_MODEL[rangeVal].merge(termVal, 1, Integer::sum);\n\n      assertEquals(0, (new UpdateRequest().add(doc)).process(cluster.getSolrClient()).getStatus());\n    }\n    assertEquals(0, cluster.getSolrClient().commit().getStatus());\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e","date":1594223844,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/search/facet/RangeFacetCloudTest#setupCluster().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/facet/RangeFacetCloudTest#setupCluster().mjava","sourceNew":"  @BeforeClass\n  public static void setupCluster() throws Exception {\n    final int numShards = TestUtil.nextInt(random(),1,5);\n    final int numReplicas = 1;\n    final int nodeCount = numShards * numReplicas;\n\n    configureCluster(nodeCount)\n      .addConfig(CONF, Paths.get(TEST_HOME(), \"collection1\", \"conf\"))\n      .configure();\n\n    assertEquals(0, (CollectionAdminRequest.createCollection(COLLECTION, CONF, numShards, numReplicas)\n                     .setProperties(Collections.singletonMap(CoreAdminParams.CONFIG, \"solrconfig-minimal.xml\"))\n                     .process(cluster.getSolrClient())).getStatus());\n    \n    cluster.getSolrClient().setDefaultCollection(COLLECTION);\n\n    final int numDocs = atLeast(1000);\n    final int maxTermId = atLeast(TERM_VALUES_RANDOMIZER);\n\n    // clear the RANGE_MODEL\n    Arrays.fill(RANGE_MODEL, 0);\n    // seed the TERM_MODEL Maps so we don't have null check later\n    for (int i = 0; i < NUM_RANGE_VALUES; i++) {\n      TERM_MODEL[i] = new LinkedHashMap<>();\n    }\n\n    // build our index & our models\n    for (int id = 0; id < numDocs; id++) {\n      final int rangeVal = random().nextInt(NUM_RANGE_VALUES);\n      final String termVal = \"x\" + random().nextInt(maxTermId);\n      final SolrInputDocument doc = sdoc(\"id\", \"\"+id,\n                                         INT_FIELD, \"\"+rangeVal,\n                                         STR_FIELD, termVal);\n      RANGE_MODEL[rangeVal]++;\n      TERM_MODEL[rangeVal].merge(termVal, 1, Integer::sum);\n\n      assertEquals(0, (new UpdateRequest().add(doc)).process(cluster.getSolrClient()).getStatus());\n    }\n    assertEquals(0, cluster.getSolrClient().commit().getStatus());\n    \n  }\n\n","sourceOld":"  @BeforeClass\n  public static void setupCluster() throws Exception {\n    final int numShards = TestUtil.nextInt(random(),1,5);\n    final int numReplicas = 1;\n    final int maxShardsPerNode = 1;\n    final int nodeCount = numShards * numReplicas;\n\n    configureCluster(nodeCount)\n      .addConfig(CONF, Paths.get(TEST_HOME(), \"collection1\", \"conf\"))\n      .configure();\n\n    assertEquals(0, (CollectionAdminRequest.createCollection(COLLECTION, CONF, numShards, numReplicas)\n                     .setMaxShardsPerNode(maxShardsPerNode)\n                     .setProperties(Collections.singletonMap(CoreAdminParams.CONFIG, \"solrconfig-minimal.xml\"))\n                     .process(cluster.getSolrClient())).getStatus());\n    \n    cluster.getSolrClient().setDefaultCollection(COLLECTION);\n\n    final int numDocs = atLeast(1000);\n    final int maxTermId = atLeast(TERM_VALUES_RANDOMIZER);\n\n    // clear the RANGE_MODEL\n    Arrays.fill(RANGE_MODEL, 0);\n    // seed the TERM_MODEL Maps so we don't have null check later\n    for (int i = 0; i < NUM_RANGE_VALUES; i++) {\n      TERM_MODEL[i] = new LinkedHashMap<>();\n    }\n\n    // build our index & our models\n    for (int id = 0; id < numDocs; id++) {\n      final int rangeVal = random().nextInt(NUM_RANGE_VALUES);\n      final String termVal = \"x\" + random().nextInt(maxTermId);\n      final SolrInputDocument doc = sdoc(\"id\", \"\"+id,\n                                         INT_FIELD, \"\"+rangeVal,\n                                         STR_FIELD, termVal);\n      RANGE_MODEL[rangeVal]++;\n      TERM_MODEL[rangeVal].merge(termVal, 1, Integer::sum);\n\n      assertEquals(0, (new UpdateRequest().add(doc)).process(cluster.getSolrClient()).getStatus());\n    }\n    assertEquals(0, cluster.getSolrClient().commit().getStatus());\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c05b634713ca09b2267477408773904d4c69dd9d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"58af973abe9f7c0faa777c1a6285fc7f72c171c5":["c05b634713ca09b2267477408773904d4c69dd9d","ad2010e2591e2d11678b59291d2ab31bd30e3ac7"],"ad2010e2591e2d11678b59291d2ab31bd30e3ac7":["c05b634713ca09b2267477408773904d4c69dd9d"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["ad2010e2591e2d11678b59291d2ab31bd30e3ac7"],"c05b634713ca09b2267477408773904d4c69dd9d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c05b634713ca09b2267477408773904d4c69dd9d"]},"commit2Childs":{"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","c05b634713ca09b2267477408773904d4c69dd9d","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"58af973abe9f7c0faa777c1a6285fc7f72c171c5":[],"ad2010e2591e2d11678b59291d2ab31bd30e3ac7":["58af973abe9f7c0faa777c1a6285fc7f72c171c5","e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"],"c05b634713ca09b2267477408773904d4c69dd9d":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","58af973abe9f7c0faa777c1a6285fc7f72c171c5","ad2010e2591e2d11678b59291d2ab31bd30e3ac7","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":[]},"heads":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","58af973abe9f7c0faa777c1a6285fc7f72c171c5","cd5edd1f2b162a5cfa08efd17851a07373a96817","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}