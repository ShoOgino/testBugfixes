{"path":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","commits":[{"id":"138923418367b4cadabaadb48c45f03a96cfde8b","date":1342359927,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean).mjava","sourceNew":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    DocsAndPositionsEnum offsets = null;\n    \n    String lastField = null;\n    final FieldsEnum fieldsEnum = fields.iterator();\n    while(true) {\n      final String field = fieldsEnum.next();\n      if (field == null) {\n        break;\n      }\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fi = fieldInfos.fieldInfo(field);\n      if (fi == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fi.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fieldsEnum.terms();\n      if (terms == null) {\n        continue;\n      }\n      \n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs, false);\n        docsAndFreqs = termsEnum.docs(liveDocs, docsAndFreqs, true);\n        postings = termsEnum.docsAndPositions(liveDocs, postings, false);\n        offsets = termsEnum.docsAndPositions(liveDocs, offsets, true);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        final DocsEnum docsAndFreqs2;\n        final boolean hasPositions;\n        final boolean hasFreqs;\n        final boolean hasOffsets;\n        if (offsets != null) {\n          docs2 = postings = offsets;\n          docsAndFreqs2 = postings = offsets;\n          hasOffsets = true;\n          hasPositions = true;\n          hasFreqs = true;\n        } else if (postings != null) {\n          docs2 = postings;\n          docsAndFreqs2 = postings;\n          hasOffsets = false;\n          hasPositions = true;\n          hasFreqs = true;\n        } else if (docsAndFreqs != null) {\n          docs2 = docsAndFreqs;\n          docsAndFreqs2 = docsAndFreqs;\n          hasOffsets = false;\n          hasPositions = false;\n          hasFreqs = true;\n        } else {\n          docs2 = docs;\n          docsAndFreqs2 = null;\n          hasOffsets = false;\n          hasPositions = false;\n          hasFreqs = false;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docsAndFreqs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n              // NOTE: pos=-1 is allowed because of ancient bug\n              // (LUCENE-1542) whereby IndexWriter could\n              // write pos=-1 when first token's posInc is 0\n              // (separately: analyzers should not give\n              // posInc=0 to first token); also, term\n              // vectors are allowed to return pos=-1 if\n              // they indexed offset but not positions:\n              if (pos < -1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              if (postings.hasPayload()) {\n                postings.getPayload();\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = postings != null && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs, true);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, false);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings, hasOffsets);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n                // NOTE: pos=-1 is allowed because of ancient bug\n                // (LUCENE-1542) whereby IndexWriter could\n                // write pos=-1 when first token's posInc is 0\n                // (separately: analyzers should not give\n                // posInc=0 to first token); also, term\n                // vectors are allowed to return pos=-1 if\n                // they indexed offset but not positions:\n                if (pos < -1) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, false);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        // make sure TermsEnum is empty:\n        final Terms fieldTerms2 = fieldsEnum.terms();\n        if (fieldTerms2 != null && fieldTerms2.iterator(null).next() != null) {\n          throw new RuntimeException(\"Fields.terms(field=\" + field + \") returned null yet the field appears to have terms\");\n        }\n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, false);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, false);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, false);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","sourceOld":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    DocsAndPositionsEnum offsets = null;\n    \n    String lastField = null;\n    final FieldsEnum fieldsEnum = fields.iterator();\n    while(true) {\n      final String field = fieldsEnum.next();\n      if (field == null) {\n        break;\n      }\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fi = fieldInfos.fieldInfo(field);\n      if (fi == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fi.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fieldsEnum.terms();\n      if (terms == null) {\n        continue;\n      }\n      \n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs, false);\n        docsAndFreqs = termsEnum.docs(liveDocs, docsAndFreqs, true);\n        postings = termsEnum.docsAndPositions(liveDocs, postings, false);\n        offsets = termsEnum.docsAndPositions(liveDocs, offsets, true);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        final DocsEnum docsAndFreqs2;\n        final boolean hasPositions;\n        final boolean hasFreqs;\n        final boolean hasOffsets;\n        if (offsets != null) {\n          docs2 = postings = offsets;\n          docsAndFreqs2 = postings = offsets;\n          hasOffsets = true;\n          hasPositions = true;\n          hasFreqs = true;\n        } else if (postings != null) {\n          docs2 = postings;\n          docsAndFreqs2 = postings;\n          hasOffsets = false;\n          hasPositions = true;\n          hasFreqs = true;\n        } else if (docsAndFreqs != null) {\n          docs2 = docsAndFreqs;\n          docsAndFreqs2 = docsAndFreqs;\n          hasOffsets = false;\n          hasPositions = false;\n          hasFreqs = true;\n        } else {\n          docs2 = docs;\n          docsAndFreqs2 = null;\n          hasOffsets = false;\n          hasPositions = false;\n          hasFreqs = false;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docsAndFreqs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n              // NOTE: pos=-1 is allowed because of ancient bug\n              // (LUCENE-1542) whereby IndexWriter could\n              // write pos=-1 when first token's posInc is 0\n              // (separately: analyzers should not give\n              // posInc=0 to first token); also, term\n              // vectors are allowed to return pos=-1 if\n              // they indexed offset but not positions:\n              if (pos < -1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              if (postings.hasPayload()) {\n                postings.getPayload();\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                if (startOffset < 0) {\n                  throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                }\n                if (startOffset < lastOffset) {\n                  throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                }\n                if (endOffset < 0) {\n                  throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                }\n                if (endOffset < startOffset) {\n                  throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = postings != null && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs, true);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, false);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings, hasOffsets);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n                // NOTE: pos=-1 is allowed because of ancient bug\n                // (LUCENE-1542) whereby IndexWriter could\n                // write pos=-1 when first token's posInc is 0\n                // (separately: analyzers should not give\n                // posInc=0 to first token); also, term\n                // vectors are allowed to return pos=-1 if\n                // they indexed offset but not positions:\n                if (pos < -1) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, false);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        // make sure TermsEnum is empty:\n        final Terms fieldTerms2 = fieldsEnum.terms();\n        if (fieldTerms2 != null && fieldTerms2.iterator(null).next() != null) {\n          throw new RuntimeException(\"Fields.terms(field=\" + field + \") returned null yet the field appears to have terms\");\n        }\n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, false);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, false);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, false);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe33227f6805edab2036cbb80645cc4e2d1fa424","date":1342713534,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    DocsAndPositionsEnum offsets = null;\n    \n    String lastField = null;\n    final FieldsEnum fieldsEnum = fields.iterator();\n    while(true) {\n      final String field = fieldsEnum.next();\n      if (field == null) {\n        break;\n      }\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fi = fieldInfos.fieldInfo(field);\n      if (fi == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fi.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fieldsEnum.terms();\n      if (terms == null) {\n        continue;\n      }\n      \n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs, false);\n        docsAndFreqs = termsEnum.docs(liveDocs, docsAndFreqs, true);\n        postings = termsEnum.docsAndPositions(liveDocs, postings, false);\n        offsets = termsEnum.docsAndPositions(liveDocs, offsets, true);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        final DocsEnum docsAndFreqs2;\n        final boolean hasPositions;\n        final boolean hasFreqs;\n        final boolean hasOffsets;\n        if (offsets != null) {\n          docs2 = postings = offsets;\n          docsAndFreqs2 = postings = offsets;\n          hasOffsets = true;\n          hasPositions = true;\n          hasFreqs = true;\n        } else if (postings != null) {\n          docs2 = postings;\n          docsAndFreqs2 = postings;\n          hasOffsets = false;\n          hasPositions = true;\n          hasFreqs = true;\n        } else if (docsAndFreqs != null) {\n          docs2 = docsAndFreqs;\n          docsAndFreqs2 = docsAndFreqs;\n          hasOffsets = false;\n          hasPositions = false;\n          hasFreqs = true;\n        } else {\n          docs2 = docs;\n          docsAndFreqs2 = null;\n          hasOffsets = false;\n          hasPositions = false;\n          hasFreqs = false;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docsAndFreqs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n              // NOTE: pos=-1 is allowed because of ancient bug\n              // (LUCENE-1542) whereby IndexWriter could\n              // write pos=-1 when first token's posInc is 0\n              // (separately: analyzers should not give\n              // posInc=0 to first token); also, term\n              // vectors are allowed to return pos=-1 if\n              // they indexed offset but not positions:\n              if (pos < -1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              if (postings.hasPayload()) {\n                postings.getPayload();\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = postings != null && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs, true);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, false);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings, hasOffsets);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n                // NOTE: pos=-1 is allowed because of ancient bug\n                // (LUCENE-1542) whereby IndexWriter could\n                // write pos=-1 when first token's posInc is 0\n                // (separately: analyzers should not give\n                // posInc=0 to first token); also, term\n                // vectors are allowed to return pos=-1 if\n                // they indexed offset but not positions:\n                if (pos < -1) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, false);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        // make sure TermsEnum is empty:\n        final Terms fieldTerms2 = fieldsEnum.terms();\n        if (fieldTerms2 != null && fieldTerms2.iterator(null).next() != null) {\n          throw new RuntimeException(\"Fields.terms(field=\" + field + \") returned null yet the field appears to have terms\");\n        }\n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, false);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, false);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, false);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"aba371508186796cc6151d8223a5b4e16d02e26e","date":1343474871,"type":0,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    DocsAndPositionsEnum offsets = null;\n    \n    String lastField = null;\n    final FieldsEnum fieldsEnum = fields.iterator();\n    while(true) {\n      final String field = fieldsEnum.next();\n      if (field == null) {\n        break;\n      }\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fi = fieldInfos.fieldInfo(field);\n      if (fi == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fi.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fieldsEnum.terms();\n      if (terms == null) {\n        continue;\n      }\n      \n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs, false);\n        docsAndFreqs = termsEnum.docs(liveDocs, docsAndFreqs, true);\n        postings = termsEnum.docsAndPositions(liveDocs, postings, false);\n        offsets = termsEnum.docsAndPositions(liveDocs, offsets, true);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        final DocsEnum docsAndFreqs2;\n        final boolean hasPositions;\n        final boolean hasFreqs;\n        final boolean hasOffsets;\n        if (offsets != null) {\n          docs2 = postings = offsets;\n          docsAndFreqs2 = postings = offsets;\n          hasOffsets = true;\n          hasPositions = true;\n          hasFreqs = true;\n        } else if (postings != null) {\n          docs2 = postings;\n          docsAndFreqs2 = postings;\n          hasOffsets = false;\n          hasPositions = true;\n          hasFreqs = true;\n        } else if (docsAndFreqs != null) {\n          docs2 = docsAndFreqs;\n          docsAndFreqs2 = docsAndFreqs;\n          hasOffsets = false;\n          hasPositions = false;\n          hasFreqs = true;\n        } else {\n          docs2 = docs;\n          docsAndFreqs2 = null;\n          hasOffsets = false;\n          hasPositions = false;\n          hasFreqs = false;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docsAndFreqs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n              // NOTE: pos=-1 is allowed because of ancient bug\n              // (LUCENE-1542) whereby IndexWriter could\n              // write pos=-1 when first token's posInc is 0\n              // (separately: analyzers should not give\n              // posInc=0 to first token); also, term\n              // vectors are allowed to return pos=-1 if\n              // they indexed offset but not positions:\n              if (pos < -1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              if (postings.hasPayload()) {\n                postings.getPayload();\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = postings != null && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs, true);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, false);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings, hasOffsets);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n                // NOTE: pos=-1 is allowed because of ancient bug\n                // (LUCENE-1542) whereby IndexWriter could\n                // write pos=-1 when first token's posInc is 0\n                // (separately: analyzers should not give\n                // posInc=0 to first token); also, term\n                // vectors are allowed to return pos=-1 if\n                // they indexed offset but not positions:\n                if (pos < -1) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, false);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        // make sure TermsEnum is empty:\n        final Terms fieldTerms2 = fieldsEnum.terms();\n        if (fieldTerms2 != null && fieldTerms2.iterator(null).next() != null) {\n          throw new RuntimeException(\"Fields.terms(field=\" + field + \") returned null yet the field appears to have terms\");\n        }\n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, false);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, false);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, false);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"322360ac5185a8446d3e0b530b2068bef67cd3d5","date":1343669494,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","sourceNew":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    final FieldsEnum fieldsEnum = fields.iterator();\n    while(true) {\n      final String field = fieldsEnum.next();\n      if (field == null) {\n        break;\n      }\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fieldsEnum.terms();\n      if (terms == null) {\n        continue;\n      }\n      \n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs, false);\n        docsAndFreqs = termsEnum.docs(liveDocs, docsAndFreqs, true);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        final DocsEnum docsAndFreqs2;\n        final boolean hasPositions;\n        final boolean hasFreqs;\n        final boolean hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n        if (postings != null) {\n          docs2 = postings;\n          docsAndFreqs2 = postings;\n          hasPositions = true;\n          hasFreqs = true;\n        } else if (docsAndFreqs != null) {\n          docs2 = docsAndFreqs;\n          docsAndFreqs2 = docsAndFreqs;\n          hasPositions = false;\n          hasFreqs = true;\n        } else {\n          docs2 = docs;\n          docsAndFreqs2 = null;\n          hasPositions = false;\n          hasFreqs = false;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docsAndFreqs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n              // NOTE: pos=-1 is allowed because of ancient bug\n              // (LUCENE-1542) whereby IndexWriter could\n              // write pos=-1 when first token's posInc is 0\n              // (separately: analyzers should not give\n              // posInc=0 to first token); also, term\n              // vectors are allowed to return pos=-1 if\n              // they indexed offset but not positions:\n              if (pos < -1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              if (postings.hasPayload()) {\n                postings.getPayload();\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = postings != null && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs, true);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, false);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n                // NOTE: pos=-1 is allowed because of ancient bug\n                // (LUCENE-1542) whereby IndexWriter could\n                // write pos=-1 when first token's posInc is 0\n                // (separately: analyzers should not give\n                // posInc=0 to first token); also, term\n                // vectors are allowed to return pos=-1 if\n                // they indexed offset but not positions:\n                if (pos < -1) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, false);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        // make sure TermsEnum is empty:\n        final Terms fieldTerms2 = fieldsEnum.terms();\n        if (fieldTerms2 != null && fieldTerms2.iterator(null).next() != null) {\n          throw new RuntimeException(\"Fields.terms(field=\" + field + \") returned null yet the field appears to have terms\");\n        }\n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, false);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, false);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, false);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","sourceOld":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    DocsAndPositionsEnum offsets = null;\n    \n    String lastField = null;\n    final FieldsEnum fieldsEnum = fields.iterator();\n    while(true) {\n      final String field = fieldsEnum.next();\n      if (field == null) {\n        break;\n      }\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fi = fieldInfos.fieldInfo(field);\n      if (fi == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fi.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fieldsEnum.terms();\n      if (terms == null) {\n        continue;\n      }\n      \n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs, false);\n        docsAndFreqs = termsEnum.docs(liveDocs, docsAndFreqs, true);\n        postings = termsEnum.docsAndPositions(liveDocs, postings, false);\n        offsets = termsEnum.docsAndPositions(liveDocs, offsets, true);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        final DocsEnum docsAndFreqs2;\n        final boolean hasPositions;\n        final boolean hasFreqs;\n        final boolean hasOffsets;\n        if (offsets != null) {\n          docs2 = postings = offsets;\n          docsAndFreqs2 = postings = offsets;\n          hasOffsets = true;\n          hasPositions = true;\n          hasFreqs = true;\n        } else if (postings != null) {\n          docs2 = postings;\n          docsAndFreqs2 = postings;\n          hasOffsets = false;\n          hasPositions = true;\n          hasFreqs = true;\n        } else if (docsAndFreqs != null) {\n          docs2 = docsAndFreqs;\n          docsAndFreqs2 = docsAndFreqs;\n          hasOffsets = false;\n          hasPositions = false;\n          hasFreqs = true;\n        } else {\n          docs2 = docs;\n          docsAndFreqs2 = null;\n          hasOffsets = false;\n          hasPositions = false;\n          hasFreqs = false;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docsAndFreqs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n              // NOTE: pos=-1 is allowed because of ancient bug\n              // (LUCENE-1542) whereby IndexWriter could\n              // write pos=-1 when first token's posInc is 0\n              // (separately: analyzers should not give\n              // posInc=0 to first token); also, term\n              // vectors are allowed to return pos=-1 if\n              // they indexed offset but not positions:\n              if (pos < -1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              if (postings.hasPayload()) {\n                postings.getPayload();\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = postings != null && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs, true);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, false);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings, hasOffsets);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n                // NOTE: pos=-1 is allowed because of ancient bug\n                // (LUCENE-1542) whereby IndexWriter could\n                // write pos=-1 when first token's posInc is 0\n                // (separately: analyzers should not give\n                // posInc=0 to first token); also, term\n                // vectors are allowed to return pos=-1 if\n                // they indexed offset but not positions:\n                if (pos < -1) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, false);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        // make sure TermsEnum is empty:\n        final Terms fieldTerms2 = fieldsEnum.terms();\n        if (fieldTerms2 != null && fieldTerms2.iterator(null).next() != null) {\n          throw new RuntimeException(\"Fields.terms(field=\" + field + \") returned null yet the field appears to have terms\");\n        }\n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, false);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, false);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, false);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","bugFix":null,"bugIntro":["02331260bb246364779cb6f04919ca47900d01bb"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"02331260bb246364779cb6f04919ca47900d01bb","date":1343749884,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","sourceNew":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    final FieldsEnum fieldsEnum = fields.iterator();\n    while(true) {\n      final String field = fieldsEnum.next();\n      if (field == null) {\n        break;\n      }\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fieldsEnum.terms();\n      if (terms == null) {\n        continue;\n      }\n      \n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        final boolean hasPositions;\n        // if we are checking vectors, we have freqs implicitly\n        final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n        // if we are checking vectors, offsets are a free-for-all anyway\n        final boolean hasOffsets = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n        if (postings != null) {\n          docs2 = postings;\n          hasPositions = true;\n        } else {\n          docs2 = docs;\n          hasPositions = false;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n              // NOTE: pos=-1 is allowed because of ancient bug\n              // (LUCENE-1542) whereby IndexWriter could\n              // write pos=-1 when first token's posInc is 0\n              // (separately: analyzers should not give\n              // posInc=0 to first token); also, term\n              // vectors are allowed to return pos=-1 if\n              // they indexed offset but not positions:\n              if (pos < -1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              if (postings.hasPayload()) {\n                postings.getPayload();\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n                // NOTE: pos=-1 is allowed because of ancient bug\n                // (LUCENE-1542) whereby IndexWriter could\n                // write pos=-1 when first token's posInc is 0\n                // (separately: analyzers should not give\n                // posInc=0 to first token); also, term\n                // vectors are allowed to return pos=-1 if\n                // they indexed offset but not positions:\n                if (pos < -1) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        // make sure TermsEnum is empty:\n        final Terms fieldTerms2 = fieldsEnum.terms();\n        if (fieldTerms2 != null && fieldTerms2.iterator(null).next() != null) {\n          throw new RuntimeException(\"Fields.terms(field=\" + field + \") returned null yet the field appears to have terms\");\n        }\n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","sourceOld":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    final FieldsEnum fieldsEnum = fields.iterator();\n    while(true) {\n      final String field = fieldsEnum.next();\n      if (field == null) {\n        break;\n      }\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fieldsEnum.terms();\n      if (terms == null) {\n        continue;\n      }\n      \n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs, false);\n        docsAndFreqs = termsEnum.docs(liveDocs, docsAndFreqs, true);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        final DocsEnum docsAndFreqs2;\n        final boolean hasPositions;\n        final boolean hasFreqs;\n        final boolean hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n        if (postings != null) {\n          docs2 = postings;\n          docsAndFreqs2 = postings;\n          hasPositions = true;\n          hasFreqs = true;\n        } else if (docsAndFreqs != null) {\n          docs2 = docsAndFreqs;\n          docsAndFreqs2 = docsAndFreqs;\n          hasPositions = false;\n          hasFreqs = true;\n        } else {\n          docs2 = docs;\n          docsAndFreqs2 = null;\n          hasPositions = false;\n          hasFreqs = false;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docsAndFreqs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n              // NOTE: pos=-1 is allowed because of ancient bug\n              // (LUCENE-1542) whereby IndexWriter could\n              // write pos=-1 when first token's posInc is 0\n              // (separately: analyzers should not give\n              // posInc=0 to first token); also, term\n              // vectors are allowed to return pos=-1 if\n              // they indexed offset but not positions:\n              if (pos < -1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              if (postings.hasPayload()) {\n                postings.getPayload();\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = postings != null && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs, true);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, false);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n                // NOTE: pos=-1 is allowed because of ancient bug\n                // (LUCENE-1542) whereby IndexWriter could\n                // write pos=-1 when first token's posInc is 0\n                // (separately: analyzers should not give\n                // posInc=0 to first token); also, term\n                // vectors are allowed to return pos=-1 if\n                // they indexed offset but not positions:\n                if (pos < -1) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, false);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        // make sure TermsEnum is empty:\n        final Terms fieldTerms2 = fieldsEnum.terms();\n        if (fieldTerms2 != null && fieldTerms2.iterator(null).next() != null) {\n          throw new RuntimeException(\"Fields.terms(field=\" + field + \") returned null yet the field appears to have terms\");\n        }\n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, false);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, false);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, false);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","bugFix":["322360ac5185a8446d3e0b530b2068bef67cd3d5","f0b2a0f7efff91a413da6cc75c82ef07af7baba4","b13ebda0f2c83525d118f4859e46eb3bd87ced36"],"bugIntro":["5699a2da08aaf5a165f2ceefe7cf8f5c70a12efc"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","sourceNew":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    final FieldsEnum fieldsEnum = fields.iterator();\n    while(true) {\n      final String field = fieldsEnum.next();\n      if (field == null) {\n        break;\n      }\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fieldsEnum.terms();\n      if (terms == null) {\n        continue;\n      }\n      \n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        final boolean hasPositions;\n        // if we are checking vectors, we have freqs implicitly\n        final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n        // if we are checking vectors, offsets are a free-for-all anyway\n        final boolean hasOffsets = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n        if (postings != null) {\n          docs2 = postings;\n          hasPositions = true;\n        } else {\n          docs2 = docs;\n          hasPositions = false;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n              // NOTE: pos=-1 is allowed because of ancient bug\n              // (LUCENE-1542) whereby IndexWriter could\n              // write pos=-1 when first token's posInc is 0\n              // (separately: analyzers should not give\n              // posInc=0 to first token); also, term\n              // vectors are allowed to return pos=-1 if\n              // they indexed offset but not positions:\n              if (pos < -1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              if (postings.hasPayload()) {\n                postings.getPayload();\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n                // NOTE: pos=-1 is allowed because of ancient bug\n                // (LUCENE-1542) whereby IndexWriter could\n                // write pos=-1 when first token's posInc is 0\n                // (separately: analyzers should not give\n                // posInc=0 to first token); also, term\n                // vectors are allowed to return pos=-1 if\n                // they indexed offset but not positions:\n                if (pos < -1) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        // make sure TermsEnum is empty:\n        final Terms fieldTerms2 = fieldsEnum.terms();\n        if (fieldTerms2 != null && fieldTerms2.iterator(null).next() != null) {\n          throw new RuntimeException(\"Fields.terms(field=\" + field + \") returned null yet the field appears to have terms\");\n        }\n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","sourceOld":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    DocsAndPositionsEnum offsets = null;\n    \n    String lastField = null;\n    final FieldsEnum fieldsEnum = fields.iterator();\n    while(true) {\n      final String field = fieldsEnum.next();\n      if (field == null) {\n        break;\n      }\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fi = fieldInfos.fieldInfo(field);\n      if (fi == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fi.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fieldsEnum.terms();\n      if (terms == null) {\n        continue;\n      }\n      \n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs, false);\n        docsAndFreqs = termsEnum.docs(liveDocs, docsAndFreqs, true);\n        postings = termsEnum.docsAndPositions(liveDocs, postings, false);\n        offsets = termsEnum.docsAndPositions(liveDocs, offsets, true);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        final DocsEnum docsAndFreqs2;\n        final boolean hasPositions;\n        final boolean hasFreqs;\n        final boolean hasOffsets;\n        if (offsets != null) {\n          docs2 = postings = offsets;\n          docsAndFreqs2 = postings = offsets;\n          hasOffsets = true;\n          hasPositions = true;\n          hasFreqs = true;\n        } else if (postings != null) {\n          docs2 = postings;\n          docsAndFreqs2 = postings;\n          hasOffsets = false;\n          hasPositions = true;\n          hasFreqs = true;\n        } else if (docsAndFreqs != null) {\n          docs2 = docsAndFreqs;\n          docsAndFreqs2 = docsAndFreqs;\n          hasOffsets = false;\n          hasPositions = false;\n          hasFreqs = true;\n        } else {\n          docs2 = docs;\n          docsAndFreqs2 = null;\n          hasOffsets = false;\n          hasPositions = false;\n          hasFreqs = false;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docsAndFreqs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n              // NOTE: pos=-1 is allowed because of ancient bug\n              // (LUCENE-1542) whereby IndexWriter could\n              // write pos=-1 when first token's posInc is 0\n              // (separately: analyzers should not give\n              // posInc=0 to first token); also, term\n              // vectors are allowed to return pos=-1 if\n              // they indexed offset but not positions:\n              if (pos < -1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              if (postings.hasPayload()) {\n                postings.getPayload();\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = postings != null && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs, true);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, false);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings, hasOffsets);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n                // NOTE: pos=-1 is allowed because of ancient bug\n                // (LUCENE-1542) whereby IndexWriter could\n                // write pos=-1 when first token's posInc is 0\n                // (separately: analyzers should not give\n                // posInc=0 to first token); also, term\n                // vectors are allowed to return pos=-1 if\n                // they indexed offset but not positions:\n                if (pos < -1) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, false);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        // make sure TermsEnum is empty:\n        final Terms fieldTerms2 = fieldsEnum.terms();\n        if (fieldTerms2 != null && fieldTerms2.iterator(null).next() != null) {\n          throw new RuntimeException(\"Fields.terms(field=\" + field + \") returned null yet the field appears to have terms\");\n        }\n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, false);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, false);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, false);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","sourceNew":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    final FieldsEnum fieldsEnum = fields.iterator();\n    while(true) {\n      final String field = fieldsEnum.next();\n      if (field == null) {\n        break;\n      }\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fieldsEnum.terms();\n      if (terms == null) {\n        continue;\n      }\n      \n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        final boolean hasPositions;\n        // if we are checking vectors, we have freqs implicitly\n        final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n        // if we are checking vectors, offsets are a free-for-all anyway\n        final boolean hasOffsets = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n        if (postings != null) {\n          docs2 = postings;\n          hasPositions = true;\n        } else {\n          docs2 = docs;\n          hasPositions = false;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n              // NOTE: pos=-1 is allowed because of ancient bug\n              // (LUCENE-1542) whereby IndexWriter could\n              // write pos=-1 when first token's posInc is 0\n              // (separately: analyzers should not give\n              // posInc=0 to first token); also, term\n              // vectors are allowed to return pos=-1 if\n              // they indexed offset but not positions:\n              if (pos < -1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              if (postings.hasPayload()) {\n                postings.getPayload();\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n                // NOTE: pos=-1 is allowed because of ancient bug\n                // (LUCENE-1542) whereby IndexWriter could\n                // write pos=-1 when first token's posInc is 0\n                // (separately: analyzers should not give\n                // posInc=0 to first token); also, term\n                // vectors are allowed to return pos=-1 if\n                // they indexed offset but not positions:\n                if (pos < -1) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        // make sure TermsEnum is empty:\n        final Terms fieldTerms2 = fieldsEnum.terms();\n        if (fieldTerms2 != null && fieldTerms2.iterator(null).next() != null) {\n          throw new RuntimeException(\"Fields.terms(field=\" + field + \") returned null yet the field appears to have terms\");\n        }\n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","sourceOld":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    DocsAndPositionsEnum offsets = null;\n    \n    String lastField = null;\n    final FieldsEnum fieldsEnum = fields.iterator();\n    while(true) {\n      final String field = fieldsEnum.next();\n      if (field == null) {\n        break;\n      }\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fi = fieldInfos.fieldInfo(field);\n      if (fi == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fi.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fieldsEnum.terms();\n      if (terms == null) {\n        continue;\n      }\n      \n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs, false);\n        docsAndFreqs = termsEnum.docs(liveDocs, docsAndFreqs, true);\n        postings = termsEnum.docsAndPositions(liveDocs, postings, false);\n        offsets = termsEnum.docsAndPositions(liveDocs, offsets, true);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        final DocsEnum docsAndFreqs2;\n        final boolean hasPositions;\n        final boolean hasFreqs;\n        final boolean hasOffsets;\n        if (offsets != null) {\n          docs2 = postings = offsets;\n          docsAndFreqs2 = postings = offsets;\n          hasOffsets = true;\n          hasPositions = true;\n          hasFreqs = true;\n        } else if (postings != null) {\n          docs2 = postings;\n          docsAndFreqs2 = postings;\n          hasOffsets = false;\n          hasPositions = true;\n          hasFreqs = true;\n        } else if (docsAndFreqs != null) {\n          docs2 = docsAndFreqs;\n          docsAndFreqs2 = docsAndFreqs;\n          hasOffsets = false;\n          hasPositions = false;\n          hasFreqs = true;\n        } else {\n          docs2 = docs;\n          docsAndFreqs2 = null;\n          hasOffsets = false;\n          hasPositions = false;\n          hasFreqs = false;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docsAndFreqs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n              // NOTE: pos=-1 is allowed because of ancient bug\n              // (LUCENE-1542) whereby IndexWriter could\n              // write pos=-1 when first token's posInc is 0\n              // (separately: analyzers should not give\n              // posInc=0 to first token); also, term\n              // vectors are allowed to return pos=-1 if\n              // they indexed offset but not positions:\n              if (pos < -1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              if (postings.hasPayload()) {\n                postings.getPayload();\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = postings != null && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs, true);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, false);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings, hasOffsets);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n                // NOTE: pos=-1 is allowed because of ancient bug\n                // (LUCENE-1542) whereby IndexWriter could\n                // write pos=-1 when first token's posInc is 0\n                // (separately: analyzers should not give\n                // posInc=0 to first token); also, term\n                // vectors are allowed to return pos=-1 if\n                // they indexed offset but not positions:\n                if (pos < -1) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, false);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        // make sure TermsEnum is empty:\n        final Terms fieldTerms2 = fieldsEnum.terms();\n        if (fieldTerms2 != null && fieldTerms2.iterator(null).next() != null) {\n          throw new RuntimeException(\"Fields.terms(field=\" + field + \") returned null yet the field appears to have terms\");\n        }\n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, false);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, false);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, false);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5699a2da08aaf5a165f2ceefe7cf8f5c70a12efc","date":1344608180,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","sourceNew":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    final FieldsEnum fieldsEnum = fields.iterator();\n    while(true) {\n      final String field = fieldsEnum.next();\n      if (field == null) {\n        break;\n      }\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fieldsEnum.terms();\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n              // NOTE: pos=-1 is allowed because of ancient bug\n              // (LUCENE-1542) whereby IndexWriter could\n              // write pos=-1 when first token's posInc is 0\n\n              if (pos < -1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              if (postings.hasPayload()) {\n                postings.getPayload();\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n                // NOTE: pos=-1 is allowed because of ancient bug\n                // (LUCENE-1542) whereby IndexWriter could\n                // write pos=-1 when first token's posInc is 0\n                // (separately: analyzers should not give\n                // posInc=0 to first token); also, term\n                // vectors are allowed to return pos=-1 if\n                // they indexed offset but not positions:\n                if (pos < -1) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        // make sure TermsEnum is empty:\n        final Terms fieldTerms2 = fieldsEnum.terms();\n        if (fieldTerms2 != null && fieldTerms2.iterator(null).next() != null) {\n          throw new RuntimeException(\"Fields.terms(field=\" + field + \") returned null yet the field appears to have terms\");\n        }\n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","sourceOld":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    final FieldsEnum fieldsEnum = fields.iterator();\n    while(true) {\n      final String field = fieldsEnum.next();\n      if (field == null) {\n        break;\n      }\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fieldsEnum.terms();\n      if (terms == null) {\n        continue;\n      }\n      \n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        final boolean hasPositions;\n        // if we are checking vectors, we have freqs implicitly\n        final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n        // if we are checking vectors, offsets are a free-for-all anyway\n        final boolean hasOffsets = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n        if (postings != null) {\n          docs2 = postings;\n          hasPositions = true;\n        } else {\n          docs2 = docs;\n          hasPositions = false;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n              // NOTE: pos=-1 is allowed because of ancient bug\n              // (LUCENE-1542) whereby IndexWriter could\n              // write pos=-1 when first token's posInc is 0\n              // (separately: analyzers should not give\n              // posInc=0 to first token); also, term\n              // vectors are allowed to return pos=-1 if\n              // they indexed offset but not positions:\n              if (pos < -1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              if (postings.hasPayload()) {\n                postings.getPayload();\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n                // NOTE: pos=-1 is allowed because of ancient bug\n                // (LUCENE-1542) whereby IndexWriter could\n                // write pos=-1 when first token's posInc is 0\n                // (separately: analyzers should not give\n                // posInc=0 to first token); also, term\n                // vectors are allowed to return pos=-1 if\n                // they indexed offset but not positions:\n                if (pos < -1) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        // make sure TermsEnum is empty:\n        final Terms fieldTerms2 = fieldsEnum.terms();\n        if (fieldTerms2 != null && fieldTerms2.iterator(null).next() != null) {\n          throw new RuntimeException(\"Fields.terms(field=\" + field + \") returned null yet the field appears to have terms\");\n        }\n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","bugFix":["02331260bb246364779cb6f04919ca47900d01bb","b13ebda0f2c83525d118f4859e46eb3bd87ced36","814339e4b1ce2063ccbc6cacc6443a6446c7718b"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"86b9dbd07cb2512df60935f96f02fd20a04927dd","date":1344695516,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","sourceNew":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    final FieldsEnum fieldsEnum = fields.iterator();\n    while(true) {\n      final String field = fieldsEnum.next();\n      if (field == null) {\n        break;\n      }\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fieldsEnum.terms();\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n\n              if (pos < 0) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              if (postings.hasPayload()) {\n                postings.getPayload();\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n\n                if (pos < 0) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        // make sure TermsEnum is empty:\n        final Terms fieldTerms2 = fieldsEnum.terms();\n        if (fieldTerms2 != null && fieldTerms2.iterator(null).next() != null) {\n          throw new RuntimeException(\"Fields.terms(field=\" + field + \") returned null yet the field appears to have terms\");\n        }\n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","sourceOld":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    final FieldsEnum fieldsEnum = fields.iterator();\n    while(true) {\n      final String field = fieldsEnum.next();\n      if (field == null) {\n        break;\n      }\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fieldsEnum.terms();\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n              // NOTE: pos=-1 is allowed because of ancient bug\n              // (LUCENE-1542) whereby IndexWriter could\n              // write pos=-1 when first token's posInc is 0\n\n              if (pos < -1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              if (postings.hasPayload()) {\n                postings.getPayload();\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n                // NOTE: pos=-1 is allowed because of ancient bug\n                // (LUCENE-1542) whereby IndexWriter could\n                // write pos=-1 when first token's posInc is 0\n                // (separately: analyzers should not give\n                // posInc=0 to first token); also, term\n                // vectors are allowed to return pos=-1 if\n                // they indexed offset but not positions:\n                if (pos < -1) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        // make sure TermsEnum is empty:\n        final Terms fieldTerms2 = fieldsEnum.terms();\n        if (fieldTerms2 != null && fieldTerms2.iterator(null).next() != null) {\n          throw new RuntimeException(\"Fields.terms(field=\" + field + \") returned null yet the field appears to have terms\");\n        }\n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0935c850ea562932997b72c69d93e345f21d7f45","date":1344711506,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","sourceNew":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    final FieldsEnum fieldsEnum = fields.iterator();\n    while(true) {\n      final String field = fieldsEnum.next();\n      if (field == null) {\n        break;\n      }\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fieldsEnum.terms();\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n\n              if (pos < 0) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              if (postings.hasPayload()) {\n                BytesRef payload = postings.getPayload();\n                if (payload.length < 1) {\n                  throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" payload length is out of bounds \" + payload.length);\n                }\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n\n                if (pos < 0) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        // make sure TermsEnum is empty:\n        final Terms fieldTerms2 = fieldsEnum.terms();\n        if (fieldTerms2 != null && fieldTerms2.iterator(null).next() != null) {\n          throw new RuntimeException(\"Fields.terms(field=\" + field + \") returned null yet the field appears to have terms\");\n        }\n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","sourceOld":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    final FieldsEnum fieldsEnum = fields.iterator();\n    while(true) {\n      final String field = fieldsEnum.next();\n      if (field == null) {\n        break;\n      }\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fieldsEnum.terms();\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n\n              if (pos < 0) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              if (postings.hasPayload()) {\n                postings.getPayload();\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n\n                if (pos < 0) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        // make sure TermsEnum is empty:\n        final Terms fieldTerms2 = fieldsEnum.terms();\n        if (fieldTerms2 != null && fieldTerms2.iterator(null).next() != null) {\n          throw new RuntimeException(\"Fields.terms(field=\" + field + \") returned null yet the field appears to have terms\");\n        }\n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552","date":1344797146,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","sourceNew":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    final FieldsEnum fieldsEnum = fields.iterator();\n    while(true) {\n      final String field = fieldsEnum.next();\n      if (field == null) {\n        break;\n      }\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fieldsEnum.terms();\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n\n              if (pos < 0) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              BytesRef payload = postings.getPayload();\n              if (payload != null && payload.length < 1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" payload length is out of bounds \" + payload.length);\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n\n                if (pos < 0) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        // make sure TermsEnum is empty:\n        final Terms fieldTerms2 = fieldsEnum.terms();\n        if (fieldTerms2 != null && fieldTerms2.iterator(null).next() != null) {\n          throw new RuntimeException(\"Fields.terms(field=\" + field + \") returned null yet the field appears to have terms\");\n        }\n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","sourceOld":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    final FieldsEnum fieldsEnum = fields.iterator();\n    while(true) {\n      final String field = fieldsEnum.next();\n      if (field == null) {\n        break;\n      }\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fieldsEnum.terms();\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n\n              if (pos < 0) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              if (postings.hasPayload()) {\n                BytesRef payload = postings.getPayload();\n                if (payload.length < 1) {\n                  throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" payload length is out of bounds \" + payload.length);\n                }\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n\n                if (pos < 0) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        // make sure TermsEnum is empty:\n        final Terms fieldTerms2 = fieldsEnum.terms();\n        if (fieldTerms2 != null && fieldTerms2.iterator(null).next() != null) {\n          throw new RuntimeException(\"Fields.terms(field=\" + field + \") returned null yet the field appears to have terms\");\n        }\n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fb07ab105350b80ed9d63ca64b117084ed7391bc","date":1344824719,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","sourceNew":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    for (String field : fields) {\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fields.terms(field);\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n\n              if (pos < 0) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              BytesRef payload = postings.getPayload();\n              if (payload != null && payload.length < 1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" payload length is out of bounds \" + payload.length);\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n\n                if (pos < 0) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        \n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","sourceOld":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    final FieldsEnum fieldsEnum = fields.iterator();\n    while(true) {\n      final String field = fieldsEnum.next();\n      if (field == null) {\n        break;\n      }\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fieldsEnum.terms();\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n\n              if (pos < 0) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              BytesRef payload = postings.getPayload();\n              if (payload != null && payload.length < 1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" payload length is out of bounds \" + payload.length);\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n\n                if (pos < 0) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        // make sure TermsEnum is empty:\n        final Terms fieldTerms2 = fieldsEnum.terms();\n        if (fieldTerms2 != null && fieldTerms2.iterator(null).next() != null) {\n          throw new RuntimeException(\"Fields.terms(field=\" + field + \") returned null yet the field appears to have terms\");\n        }\n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c7869f64c874ebf7f317d22c00baf2b6857797a6","date":1344856617,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","sourceNew":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    for (String field : fields) {\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fields.terms(field);\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n\n              if (pos < 0) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              BytesRef payload = postings.getPayload();\n              if (payload != null && payload.length < 1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" payload length is out of bounds \" + payload.length);\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n\n                if (pos < 0) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        \n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","sourceOld":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    final FieldsEnum fieldsEnum = fields.iterator();\n    while(true) {\n      final String field = fieldsEnum.next();\n      if (field == null) {\n        break;\n      }\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fieldsEnum.terms();\n      if (terms == null) {\n        continue;\n      }\n      \n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        final boolean hasPositions;\n        // if we are checking vectors, we have freqs implicitly\n        final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n        // if we are checking vectors, offsets are a free-for-all anyway\n        final boolean hasOffsets = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n        if (postings != null) {\n          docs2 = postings;\n          hasPositions = true;\n        } else {\n          docs2 = docs;\n          hasPositions = false;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n              // NOTE: pos=-1 is allowed because of ancient bug\n              // (LUCENE-1542) whereby IndexWriter could\n              // write pos=-1 when first token's posInc is 0\n              // (separately: analyzers should not give\n              // posInc=0 to first token); also, term\n              // vectors are allowed to return pos=-1 if\n              // they indexed offset but not positions:\n              if (pos < -1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              if (postings.hasPayload()) {\n                postings.getPayload();\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n                // NOTE: pos=-1 is allowed because of ancient bug\n                // (LUCENE-1542) whereby IndexWriter could\n                // write pos=-1 when first token's posInc is 0\n                // (separately: analyzers should not give\n                // posInc=0 to first token); also, term\n                // vectors are allowed to return pos=-1 if\n                // they indexed offset but not positions:\n                if (pos < -1) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        // make sure TermsEnum is empty:\n        final Terms fieldTerms2 = fieldsEnum.terms();\n        if (fieldTerms2 != null && fieldTerms2.iterator(null).next() != null) {\n          throw new RuntimeException(\"Fields.terms(field=\" + field + \") returned null yet the field appears to have terms\");\n        }\n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","date":1344867506,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","sourceNew":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    for (String field : fields) {\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fields.terms(field);\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n\n              if (pos < 0) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              BytesRef payload = postings.getPayload();\n              if (payload != null && payload.length < 1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" payload length is out of bounds \" + payload.length);\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n\n                if (pos < 0) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        \n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","sourceOld":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    final FieldsEnum fieldsEnum = fields.iterator();\n    while(true) {\n      final String field = fieldsEnum.next();\n      if (field == null) {\n        break;\n      }\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fieldsEnum.terms();\n      if (terms == null) {\n        continue;\n      }\n      \n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        final boolean hasPositions;\n        // if we are checking vectors, we have freqs implicitly\n        final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n        // if we are checking vectors, offsets are a free-for-all anyway\n        final boolean hasOffsets = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n        if (postings != null) {\n          docs2 = postings;\n          hasPositions = true;\n        } else {\n          docs2 = docs;\n          hasPositions = false;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n              // NOTE: pos=-1 is allowed because of ancient bug\n              // (LUCENE-1542) whereby IndexWriter could\n              // write pos=-1 when first token's posInc is 0\n              // (separately: analyzers should not give\n              // posInc=0 to first token); also, term\n              // vectors are allowed to return pos=-1 if\n              // they indexed offset but not positions:\n              if (pos < -1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              if (postings.hasPayload()) {\n                postings.getPayload();\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n                // NOTE: pos=-1 is allowed because of ancient bug\n                // (LUCENE-1542) whereby IndexWriter could\n                // write pos=-1 when first token's posInc is 0\n                // (separately: analyzers should not give\n                // posInc=0 to first token); also, term\n                // vectors are allowed to return pos=-1 if\n                // they indexed offset but not positions:\n                if (pos < -1) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        // make sure TermsEnum is empty:\n        final Terms fieldTerms2 = fieldsEnum.terms();\n        if (fieldTerms2 != null && fieldTerms2.iterator(null).next() != null) {\n          throw new RuntimeException(\"Fields.terms(field=\" + field + \") returned null yet the field appears to have terms\");\n        }\n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ca5f6409bcf8211cae1732125479d212adb0acac","date":1345559953,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","sourceNew":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    for (String field : fields) {\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fields.terms(field);\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n\n              if (pos < 0) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              BytesRef payload = postings.getPayload();\n              if (payload != null && payload.length < 1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" payload length is out of bounds \" + payload.length);\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n\n                if (pos < 0) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        \n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","sourceOld":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    for (String field : fields) {\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fields.terms(field);\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n\n              if (pos < 0) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              BytesRef payload = postings.getPayload();\n              if (payload != null && payload.length < 1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" payload length is out of bounds \" + payload.length);\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n\n                if (pos < 0) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        \n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"001b25b42373b22a52f399dbf072f1224632e8e6","date":1345889167,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","sourceNew":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    for (String field : fields) {\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fields.terms(field);\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n\n              if (pos < 0) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              BytesRef payload = postings.getPayload();\n              if (payload != null && payload.length < 1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" payload length is out of bounds \" + payload.length);\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n\n                if (pos < 0) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        \n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","sourceOld":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    for (String field : fields) {\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fields.terms(field);\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n\n              if (pos < 0) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              BytesRef payload = postings.getPayload();\n              if (payload != null && payload.length < 1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" payload length is out of bounds \" + payload.length);\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n\n                if (pos < 0) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        \n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n    \n    // for most implementations, this is boring (just the sum across all fields)\n    // but codecs that don't work per-field like preflex actually implement this,\n    // but don't implement it on Terms, so the check isn't redundant.\n    long uniqueTermCountAllFields = fields.getUniqueTermCount();\n    \n    // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!\n    \n    if (uniqueTermCountAllFields == -1) {\n      throw new RuntimeException(\"invalid termCount: -1\");\n    }\n    \n    if (status.termCount != uniqueTermCountAllFields) {\n      throw new RuntimeException(\"termCount mismatch \" + uniqueTermCountAllFields + \" vs \" + (status.termCount));\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7312a5134c2e28e06b87256e466da72eadd966d9","date":1350427311,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","sourceNew":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    for (String field : fields) {\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fields.terms(field);\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.delTermCount + status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.delTermCount + status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          status.totFreq++;\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n\n              if (pos < 0) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              BytesRef payload = postings.getPayload();\n              if (payload != null && payload.length < 1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" payload length is out of bounds \" + payload.length);\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        if (docCount != 0) {\n          status.termCount++;\n        } else {\n          status.delTermCount++;\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n\n                if (pos < 0) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        \n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if ((status.delTermCount+status.termCount)-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.delTermCount + status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + (status.delTermCount + termCount) + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","sourceOld":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    for (String field : fields) {\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fields.terms(field);\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n\n              if (pos < 0) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              BytesRef payload = postings.getPayload();\n              if (payload != null && payload.length < 1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" payload length is out of bounds \" + payload.length);\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n\n                if (pos < 0) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        \n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","bugFix":["b13ebda0f2c83525d118f4859e46eb3bd87ced36"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"db4fdbf3d262768eabc027cd8321edca0cd11fa8","date":1350574784,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","sourceNew":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    for (String field : fields) {\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fields.terms(field);\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.delTermCount + status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.delTermCount + status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          status.totFreq++;\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n\n              if (pos < 0) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              BytesRef payload = postings.getPayload();\n              if (payload != null && payload.length < 1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" payload length is out of bounds \" + payload.length);\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        if (docCount != 0) {\n          status.termCount++;\n        } else {\n          status.delTermCount++;\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n\n                if (pos < 0) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        \n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if ((status.delTermCount+status.termCount)-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.delTermCount + status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + (status.delTermCount + termCount) + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","sourceOld":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    for (String field : fields) {\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fields.terms(field);\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        status.totFreq += docFreq;\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        status.termCount++;\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n\n              if (pos < 0) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              BytesRef payload = postings.getPayload();\n              if (payload != null && payload.length < 1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" payload length is out of bounds \" + payload.length);\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n\n                if (pos < 0) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        \n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if (status.termCount-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + termCount + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e99275efa2c9c9ae3bdba986218af82f2bf3dc30","date":1354658499,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean,PrintStream,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","sourceNew":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private static Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors, PrintStream infoStream, boolean verbose) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(infoStream, \"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    for (String field : fields) {\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fields.terms(field);\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.delTermCount + status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.delTermCount + status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          status.totFreq++;\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n\n              if (pos < 0) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              BytesRef payload = postings.getPayload();\n              if (payload != null && payload.length < 1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" payload length is out of bounds \" + payload.length);\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        if (docCount != 0) {\n          status.termCount++;\n        } else {\n          status.delTermCount++;\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n\n                if (pos < 0) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        \n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if ((status.delTermCount+status.termCount)-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.delTermCount + status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + (status.delTermCount + termCount) + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n\n    if (doPrint) {\n      msg(infoStream, \"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","sourceOld":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    for (String field : fields) {\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fields.terms(field);\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.delTermCount + status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.delTermCount + status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          status.totFreq++;\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n\n              if (pos < 0) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              BytesRef payload = postings.getPayload();\n              if (payload != null && payload.length < 1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" payload length is out of bounds \" + payload.length);\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        if (docCount != 0) {\n          status.termCount++;\n        } else {\n          status.delTermCount++;\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n\n                if (pos < 0) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        \n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if ((status.delTermCount+status.termCount)-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.delTermCount + status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + (status.delTermCount + termCount) + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","bugFix":null,"bugIntro":["8028ab7a24273833d53d35eb160dba5b57283cf5","8028ab7a24273833d53d35eb160dba5b57283cf5","8028ab7a24273833d53d35eb160dba5b57283cf5","8028ab7a24273833d53d35eb160dba5b57283cf5","8028ab7a24273833d53d35eb160dba5b57283cf5","6b64ceb507ba9aa71920c0bfad91032e2c03d42f","6b64ceb507ba9aa71920c0bfad91032e2c03d42f","6b64ceb507ba9aa71920c0bfad91032e2c03d42f","6b64ceb507ba9aa71920c0bfad91032e2c03d42f","6b64ceb507ba9aa71920c0bfad91032e2c03d42f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"15250ca94ba8ab3bcdd476daf6bf3f3febb92640","date":1355200097,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","sourceNew":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    for (String field : fields) {\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fields.terms(field);\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.delTermCount + status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.delTermCount + status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          status.totFreq++;\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n\n              if (pos < 0) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              BytesRef payload = postings.getPayload();\n              if (payload != null && payload.length < 1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" payload length is out of bounds \" + payload.length);\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        if (docCount != 0) {\n          status.termCount++;\n        } else {\n          status.delTermCount++;\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n\n                if (pos < 0) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, DocsEnum.FLAG_NONE);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        \n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, DocsEnum.FLAG_NONE);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if ((status.delTermCount+status.termCount)-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.delTermCount + status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + (status.delTermCount + termCount) + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, DocsEnum.FLAG_NONE);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","sourceOld":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    for (String field : fields) {\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fields.terms(field);\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.delTermCount + status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.delTermCount + status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          status.totFreq++;\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n\n              if (pos < 0) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              BytesRef payload = postings.getPayload();\n              if (payload != null && payload.length < 1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" payload length is out of bounds \" + payload.length);\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        if (docCount != 0) {\n          status.termCount++;\n        } else {\n          status.delTermCount++;\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, 0);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n\n                if (pos < 0) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, 0);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        \n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, 0);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if ((status.delTermCount+status.termCount)-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.delTermCount + status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + (status.delTermCount + termCount) + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, 0);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a240b823362f0ef40ac66b4764b0109a08b15036","date":1359059459,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","sourceNew":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    for (String field : fields) {\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fields.terms(field);\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.delTermCount + status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        checkBounds(term);\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.delTermCount + status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          status.totFreq++;\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n\n              if (pos < 0) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              BytesRef payload = postings.getPayload();\n              if (payload != null) {\n                checkBounds(payload);\n              }\n              if (payload != null && payload.length < 1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" payload length is out of bounds \" + payload.length);\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        if (docCount != 0) {\n          status.termCount++;\n        } else {\n          status.delTermCount++;\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n\n                if (pos < 0) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, DocsEnum.FLAG_NONE);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        \n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, DocsEnum.FLAG_NONE);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if ((status.delTermCount+status.termCount)-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.delTermCount + status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + (status.delTermCount + termCount) + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, DocsEnum.FLAG_NONE);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","sourceOld":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    for (String field : fields) {\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fields.terms(field);\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.delTermCount + status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.delTermCount + status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          status.totFreq++;\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n\n              if (pos < 0) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              BytesRef payload = postings.getPayload();\n              if (payload != null && payload.length < 1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" payload length is out of bounds \" + payload.length);\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        if (docCount != 0) {\n          status.termCount++;\n        } else {\n          status.delTermCount++;\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n\n                if (pos < 0) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, DocsEnum.FLAG_NONE);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        \n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, DocsEnum.FLAG_NONE);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if ((status.delTermCount+status.termCount)-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.delTermCount + status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + (status.delTermCount + termCount) + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, DocsEnum.FLAG_NONE);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":4,"author":"Robert Muir","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/CheckIndex#checkFields(Fields,Bits,int,FieldInfos,boolean,boolean).mjava","sourceNew":null,"sourceOld":"  /**\n   * checks Fields api is consistent with itself.\n   * searcher is optional, to verify with queries. Can be null.\n   */\n  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {\n    // TODO: we should probably return our own stats thing...?!\n    \n    final Status.TermIndexStatus status = new Status.TermIndexStatus();\n    int computedFieldCount = 0;\n    \n    if (fields == null) {\n      msg(\"OK [no fields/terms]\");\n      return status;\n    }\n    \n    DocsEnum docs = null;\n    DocsEnum docsAndFreqs = null;\n    DocsAndPositionsEnum postings = null;\n    \n    String lastField = null;\n    for (String field : fields) {\n      // MultiFieldsEnum relies upon this order...\n      if (lastField != null && field.compareTo(lastField) <= 0) {\n        throw new RuntimeException(\"fields out of order: lastField=\" + lastField + \" field=\" + field);\n      }\n      lastField = field;\n      \n      // check that the field is in fieldinfos, and is indexed.\n      // TODO: add a separate test to check this for different reader impls\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n      if (fieldInfo == null) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, no fieldInfos for: \" + field);\n      }\n      if (!fieldInfo.isIndexed()) {\n        throw new RuntimeException(\"fieldsEnum inconsistent with fieldInfos, isIndexed == false for: \" + field);\n      }\n      \n      // TODO: really the codec should not return a field\n      // from FieldsEnum if it has no Terms... but we do\n      // this today:\n      // assert fields.terms(field) != null;\n      computedFieldCount++;\n      \n      final Terms terms = fields.terms(field);\n      if (terms == null) {\n        continue;\n      }\n      \n      final boolean hasPositions = terms.hasPositions();\n      final boolean hasOffsets = terms.hasOffsets();\n      // term vectors cannot omit TF\n      final boolean hasFreqs = isVectors || fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n\n      final TermsEnum termsEnum = terms.iterator(null);\n      \n      boolean hasOrd = true;\n      final long termCountStart = status.delTermCount + status.termCount;\n      \n      BytesRef lastTerm = null;\n      \n      Comparator<BytesRef> termComp = terms.getComparator();\n      \n      long sumTotalTermFreq = 0;\n      long sumDocFreq = 0;\n      FixedBitSet visitedDocs = new FixedBitSet(maxDoc);\n      while(true) {\n        \n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        \n        checkBounds(term);\n        \n        // make sure terms arrive in order according to\n        // the comp\n        if (lastTerm == null) {\n          lastTerm = BytesRef.deepCopyOf(term);\n        } else {\n          if (termComp.compare(lastTerm, term) >= 0) {\n            throw new RuntimeException(\"terms out of order: lastTerm=\" + lastTerm + \" term=\" + term);\n          }\n          lastTerm.copyBytes(term);\n        }\n        \n        final int docFreq = termsEnum.docFreq();\n        if (docFreq <= 0) {\n          throw new RuntimeException(\"docfreq: \" + docFreq + \" is out of bounds\");\n        }\n        sumDocFreq += docFreq;\n        \n        docs = termsEnum.docs(liveDocs, docs);\n        postings = termsEnum.docsAndPositions(liveDocs, postings);\n        \n        if (hasOrd) {\n          long ord = -1;\n          try {\n            ord = termsEnum.ord();\n          } catch (UnsupportedOperationException uoe) {\n            hasOrd = false;\n          }\n          \n          if (hasOrd) {\n            final long ordExpected = status.delTermCount + status.termCount - termCountStart;\n            if (ord != ordExpected) {\n              throw new RuntimeException(\"ord mismatch: TermsEnum has ord=\" + ord + \" vs actual=\" + ordExpected);\n            }\n          }\n        }\n        \n        final DocsEnum docs2;\n        if (postings != null) {\n          docs2 = postings;\n        } else {\n          docs2 = docs;\n        }\n        \n        int lastDoc = -1;\n        int docCount = 0;\n        long totalTermFreq = 0;\n        while(true) {\n          final int doc = docs2.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          status.totFreq++;\n          visitedDocs.set(doc);\n          int freq = -1;\n          if (hasFreqs) {\n            freq = docs2.freq();\n            if (freq <= 0) {\n              throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": freq \" + freq + \" is out of bounds\");\n            }\n            status.totPos += freq;\n            totalTermFreq += freq;\n          }\n          docCount++;\n          \n          if (doc <= lastDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" <= lastDoc \" + lastDoc);\n          }\n          if (doc >= maxDoc) {\n            throw new RuntimeException(\"term \" + term + \": doc \" + doc + \" >= maxDoc \" + maxDoc);\n          }\n          \n          lastDoc = doc;\n          \n          int lastPos = -1;\n          int lastOffset = 0;\n          if (hasPositions) {\n            for(int j=0;j<freq;j++) {\n              final int pos = postings.nextPosition();\n\n              if (pos < 0) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" is out of bounds\");\n              }\n              if (pos < lastPos) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" < lastPos \" + lastPos);\n              }\n              lastPos = pos;\n              BytesRef payload = postings.getPayload();\n              if (payload != null) {\n                checkBounds(payload);\n              }\n              if (payload != null && payload.length < 1) {\n                throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \" payload length is out of bounds \" + payload.length);\n              }\n              if (hasOffsets) {\n                int startOffset = postings.startOffset();\n                int endOffset = postings.endOffset();\n                // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                if (!isVectors) {\n                  if (startOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                  }\n                  if (startOffset < lastOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                  }\n                  if (endOffset < 0) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                  }\n                  if (endOffset < startOffset) {\n                    throw new RuntimeException(\"term \" + term + \": doc \" + doc + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                  }\n                }\n                lastOffset = startOffset;\n              }\n            }\n          }\n        }\n        \n        if (docCount != 0) {\n          status.termCount++;\n        } else {\n          status.delTermCount++;\n        }\n        \n        final long totalTermFreq2 = termsEnum.totalTermFreq();\n        final boolean hasTotalTermFreq = hasFreqs && totalTermFreq2 != -1;\n        \n        // Re-count if there are deleted docs:\n        if (liveDocs != null) {\n          if (hasFreqs) {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docsAndFreqs);\n            docCount = 0;\n            totalTermFreq = 0;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n              totalTermFreq += docsNoDel.freq();\n            }\n          } else {\n            final DocsEnum docsNoDel = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n            docCount = 0;\n            totalTermFreq = -1;\n            while(docsNoDel.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n              visitedDocs.set(docsNoDel.docID());\n              docCount++;\n            }\n          }\n        }\n        \n        if (docCount != docFreq) {\n          throw new RuntimeException(\"term \" + term + \" docFreq=\" + docFreq + \" != tot docs w/o deletions \" + docCount);\n        }\n        if (hasTotalTermFreq) {\n          if (totalTermFreq2 <= 0) {\n            throw new RuntimeException(\"totalTermFreq: \" + totalTermFreq2 + \" is out of bounds\");\n          }\n          sumTotalTermFreq += totalTermFreq;\n          if (totalTermFreq != totalTermFreq2) {\n            throw new RuntimeException(\"term \" + term + \" totalTermFreq=\" + totalTermFreq2 + \" != recomputed totalTermFreq=\" + totalTermFreq);\n          }\n        }\n        \n        // Test skipping\n        if (hasPositions) {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            postings = termsEnum.docsAndPositions(liveDocs, postings);\n            final int docID = postings.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int freq = postings.freq();\n              if (freq <= 0) {\n                throw new RuntimeException(\"termFreq \" + freq + \" is out of bounds\");\n              }\n              int lastPosition = -1;\n              int lastOffset = 0;\n              for(int posUpto=0;posUpto<freq;posUpto++) {\n                final int pos = postings.nextPosition();\n\n                if (pos < 0) {\n                  throw new RuntimeException(\"position \" + pos + \" is out of bounds\");\n                }\n                if (pos < lastPosition) {\n                  throw new RuntimeException(\"position \" + pos + \" is < lastPosition \" + lastPosition);\n                }\n                lastPosition = pos;\n                if (hasOffsets) {\n                  int startOffset = postings.startOffset();\n                  int endOffset = postings.endOffset();\n                  // NOTE: we cannot enforce any bounds whatsoever on vectors... they were a free-for-all before?\n                  // but for offsets in the postings lists these checks are fine: they were always enforced by IndexWriter\n                  if (!isVectors) {\n                    if (startOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" is out of bounds\");\n                    }\n                    if (startOffset < lastOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": startOffset \" + startOffset + \" < lastStartOffset \" + lastOffset);\n                    }\n                    if (endOffset < 0) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" is out of bounds\");\n                    }\n                    if (endOffset < startOffset) {\n                      throw new RuntimeException(\"term \" + term + \": doc \" + docID + \": pos \" + pos + \": endOffset \" + endOffset + \" < startOffset \" + startOffset);\n                    }\n                  }\n                  lastOffset = startOffset;\n                }\n              } \n              \n              final int nextDocID = postings.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        } else {\n          for(int idx=0;idx<7;idx++) {\n            final int skipDocID = (int) (((idx+1)*(long) maxDoc)/8);\n            docs = termsEnum.docs(liveDocs, docs, DocsEnum.FLAG_NONE);\n            final int docID = docs.advance(skipDocID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            } else {\n              if (docID < skipDocID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \") returned docID=\" + docID);\n              }\n              final int nextDocID = docs.nextDoc();\n              if (nextDocID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              if (nextDocID <= docID) {\n                throw new RuntimeException(\"term \" + term + \": advance(docID=\" + skipDocID + \"), then .next() returned docID=\" + nextDocID + \" vs prev docID=\" + docID);\n              }\n            }\n          }\n        }\n      }\n      \n      final Terms fieldTerms = fields.terms(field);\n      if (fieldTerms == null) {\n        // Unusual: the FieldsEnum returned a field but\n        // the Terms for that field is null; this should\n        // only happen if it's a ghost field (field with\n        // no terms, eg there used to be terms but all\n        // docs got deleted and then merged away):\n        \n      } else {\n        if (fieldTerms instanceof BlockTreeTermsReader.FieldReader) {\n          final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();\n          assert stats != null;\n          if (status.blockTreeStats == null) {\n            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();\n          }\n          status.blockTreeStats.put(field, stats);\n        }\n        \n        if (sumTotalTermFreq != 0) {\n          final long v = fields.terms(field).getSumTotalTermFreq();\n          if (v != -1 && sumTotalTermFreq != v) {\n            throw new RuntimeException(\"sumTotalTermFreq for field \" + field + \"=\" + v + \" != recomputed sumTotalTermFreq=\" + sumTotalTermFreq);\n          }\n        }\n        \n        if (sumDocFreq != 0) {\n          final long v = fields.terms(field).getSumDocFreq();\n          if (v != -1 && sumDocFreq != v) {\n            throw new RuntimeException(\"sumDocFreq for field \" + field + \"=\" + v + \" != recomputed sumDocFreq=\" + sumDocFreq);\n          }\n        }\n        \n        if (fieldTerms != null) {\n          final int v = fieldTerms.getDocCount();\n          if (v != -1 && visitedDocs.cardinality() != v) {\n            throw new RuntimeException(\"docCount for field \" + field + \"=\" + v + \" != recomputed docCount=\" + visitedDocs.cardinality());\n          }\n        }\n        \n        // Test seek to last term:\n        if (lastTerm != null) {\n          if (termsEnum.seekCeil(lastTerm) != TermsEnum.SeekStatus.FOUND) { \n            throw new RuntimeException(\"seek to last term \" + lastTerm + \" failed\");\n          }\n          \n          int expectedDocFreq = termsEnum.docFreq();\n          DocsEnum d = termsEnum.docs(null, null, DocsEnum.FLAG_NONE);\n          int docFreq = 0;\n          while (d.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            docFreq++;\n          }\n          if (docFreq != expectedDocFreq) {\n            throw new RuntimeException(\"docFreq for last term \" + lastTerm + \"=\" + expectedDocFreq + \" != recomputed docFreq=\" + docFreq);\n          }\n        }\n        \n        // check unique term count\n        long termCount = -1;\n        \n        if ((status.delTermCount+status.termCount)-termCountStart > 0) {\n          termCount = fields.terms(field).size();\n          \n          if (termCount != -1 && termCount != status.delTermCount + status.termCount - termCountStart) {\n            throw new RuntimeException(\"termCount mismatch \" + (status.delTermCount + termCount) + \" vs \" + (status.termCount - termCountStart));\n          }\n        }\n        \n        // Test seeking by ord\n        if (hasOrd && status.termCount-termCountStart > 0) {\n          int seekCount = (int) Math.min(10000L, termCount);\n          if (seekCount > 0) {\n            BytesRef[] seekTerms = new BytesRef[seekCount];\n            \n            // Seek by ord\n            for(int i=seekCount-1;i>=0;i--) {\n              long ord = i*(termCount/seekCount);\n              termsEnum.seekExact(ord);\n              seekTerms[i] = BytesRef.deepCopyOf(termsEnum.term());\n            }\n            \n            // Seek by term\n            long totDocCount = 0;\n            for(int i=seekCount-1;i>=0;i--) {\n              if (termsEnum.seekCeil(seekTerms[i]) != TermsEnum.SeekStatus.FOUND) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              docs = termsEnum.docs(liveDocs, docs, DocsEnum.FLAG_NONE);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCount++;\n              }\n            }\n            \n            long totDocCountNoDeletes = 0;\n            long totDocFreq = 0;\n            for(int i=0;i<seekCount;i++) {\n              if (!termsEnum.seekExact(seekTerms[i], true)) {\n                throw new RuntimeException(\"seek to existing term \" + seekTerms[i] + \" failed\");\n              }\n              \n              totDocFreq += termsEnum.docFreq();\n              docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n              if (docs == null) {\n                throw new RuntimeException(\"null DocsEnum from to existing term \" + seekTerms[i]);\n              }\n              \n              while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n                totDocCountNoDeletes++;\n              }\n            }\n            \n            if (totDocCount > totDocCountNoDeletes) {\n              throw new RuntimeException(\"more postings with deletes=\" + totDocCount + \" than without=\" + totDocCountNoDeletes);\n            }\n            \n            if (totDocCountNoDeletes != totDocFreq) {\n              throw new RuntimeException(\"docfreqs=\" + totDocFreq + \" != recomputed docfreqs=\" + totDocCountNoDeletes);\n            }\n          }\n        }\n      }\n    }\n    \n    int fieldCount = fields.size();\n    \n    if (fieldCount != -1) {\n      if (fieldCount < 0) {\n        throw new RuntimeException(\"invalid fieldCount: \" + fieldCount);\n      }\n      if (fieldCount != computedFieldCount) {\n        throw new RuntimeException(\"fieldCount mismatch \" + fieldCount + \" vs recomputed field count \" + computedFieldCount);\n      }\n    }\n\n    if (doPrint) {\n      msg(\"OK [\" + status.termCount + \" terms; \" + status.totFreq + \" terms/docs pairs; \" + status.totPos + \" tokens]\");\n    }\n    \n    if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {\n      for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {\n        infoStream.println(\"      field \\\"\" + ent.getKey() + \"\\\":\");\n        infoStream.println(\"      \" + ent.getValue().toString().replace(\"\\n\", \"\\n      \"));\n      }\n    }\n    \n    return status;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"001b25b42373b22a52f399dbf072f1224632e8e6":["d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","ca5f6409bcf8211cae1732125479d212adb0acac"],"7312a5134c2e28e06b87256e466da72eadd966d9":["ca5f6409bcf8211cae1732125479d212adb0acac"],"fb07ab105350b80ed9d63ca64b117084ed7391bc":["2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552"],"e99275efa2c9c9ae3bdba986218af82f2bf3dc30":["7312a5134c2e28e06b87256e466da72eadd966d9"],"db4fdbf3d262768eabc027cd8321edca0cd11fa8":["ca5f6409bcf8211cae1732125479d212adb0acac","7312a5134c2e28e06b87256e466da72eadd966d9"],"a240b823362f0ef40ac66b4764b0109a08b15036":["15250ca94ba8ab3bcdd476daf6bf3f3febb92640"],"5699a2da08aaf5a165f2ceefe7cf8f5c70a12efc":["02331260bb246364779cb6f04919ca47900d01bb"],"ca5f6409bcf8211cae1732125479d212adb0acac":["fb07ab105350b80ed9d63ca64b117084ed7391bc"],"aba371508186796cc6151d8223a5b4e16d02e26e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","138923418367b4cadabaadb48c45f03a96cfde8b"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","fb07ab105350b80ed9d63ca64b117084ed7391bc"],"2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552":["0935c850ea562932997b72c69d93e345f21d7f45"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["a240b823362f0ef40ac66b4764b0109a08b15036","e99275efa2c9c9ae3bdba986218af82f2bf3dc30"],"15250ca94ba8ab3bcdd476daf6bf3f3febb92640":["7312a5134c2e28e06b87256e466da72eadd966d9"],"138923418367b4cadabaadb48c45f03a96cfde8b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","138923418367b4cadabaadb48c45f03a96cfde8b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["fe33227f6805edab2036cbb80645cc4e2d1fa424","02331260bb246364779cb6f04919ca47900d01bb"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["aba371508186796cc6151d8223a5b4e16d02e26e","02331260bb246364779cb6f04919ca47900d01bb"],"0935c850ea562932997b72c69d93e345f21d7f45":["86b9dbd07cb2512df60935f96f02fd20a04927dd"],"86b9dbd07cb2512df60935f96f02fd20a04927dd":["5699a2da08aaf5a165f2ceefe7cf8f5c70a12efc"],"322360ac5185a8446d3e0b530b2068bef67cd3d5":["138923418367b4cadabaadb48c45f03a96cfde8b"],"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9":["d6f074e73200c07d54f242d3880a8da5a35ff97b","fb07ab105350b80ed9d63ca64b117084ed7391bc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"02331260bb246364779cb6f04919ca47900d01bb":["322360ac5185a8446d3e0b530b2068bef67cd3d5"]},"commit2Childs":{"001b25b42373b22a52f399dbf072f1224632e8e6":[],"7312a5134c2e28e06b87256e466da72eadd966d9":["e99275efa2c9c9ae3bdba986218af82f2bf3dc30","db4fdbf3d262768eabc027cd8321edca0cd11fa8","15250ca94ba8ab3bcdd476daf6bf3f3febb92640"],"fb07ab105350b80ed9d63ca64b117084ed7391bc":["ca5f6409bcf8211cae1732125479d212adb0acac","c7869f64c874ebf7f317d22c00baf2b6857797a6","d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9"],"e99275efa2c9c9ae3bdba986218af82f2bf3dc30":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"db4fdbf3d262768eabc027cd8321edca0cd11fa8":[],"a240b823362f0ef40ac66b4764b0109a08b15036":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"5699a2da08aaf5a165f2ceefe7cf8f5c70a12efc":["86b9dbd07cb2512df60935f96f02fd20a04927dd"],"ca5f6409bcf8211cae1732125479d212adb0acac":["001b25b42373b22a52f399dbf072f1224632e8e6","7312a5134c2e28e06b87256e466da72eadd966d9","db4fdbf3d262768eabc027cd8321edca0cd11fa8"],"aba371508186796cc6151d8223a5b4e16d02e26e":["d6f074e73200c07d54f242d3880a8da5a35ff97b"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":[],"2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552":["fb07ab105350b80ed9d63ca64b117084ed7391bc"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"15250ca94ba8ab3bcdd476daf6bf3f3febb92640":["a240b823362f0ef40ac66b4764b0109a08b15036"],"138923418367b4cadabaadb48c45f03a96cfde8b":["aba371508186796cc6151d8223a5b4e16d02e26e","fe33227f6805edab2036cbb80645cc4e2d1fa424","322360ac5185a8446d3e0b530b2068bef67cd3d5"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["aba371508186796cc6151d8223a5b4e16d02e26e","138923418367b4cadabaadb48c45f03a96cfde8b","fe33227f6805edab2036cbb80645cc4e2d1fa424"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["c7869f64c874ebf7f317d22c00baf2b6857797a6"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9"],"0935c850ea562932997b72c69d93e345f21d7f45":["2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552"],"86b9dbd07cb2512df60935f96f02fd20a04927dd":["0935c850ea562932997b72c69d93e345f21d7f45"],"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9":["001b25b42373b22a52f399dbf072f1224632e8e6"],"322360ac5185a8446d3e0b530b2068bef67cd3d5":["02331260bb246364779cb6f04919ca47900d01bb"],"02331260bb246364779cb6f04919ca47900d01bb":["5699a2da08aaf5a165f2ceefe7cf8f5c70a12efc","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["001b25b42373b22a52f399dbf072f1224632e8e6","db4fdbf3d262768eabc027cd8321edca0cd11fa8","c7869f64c874ebf7f317d22c00baf2b6857797a6","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}