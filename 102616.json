{"path":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,DocumentsWriterDeleteQueue.Node[#],DocumentsWriter.FlushNotifications).mjava","commits":[{"id":"86a2e8a56b368d37ef3ba7180541fa317d6fd6c7","date":1524496660,"type":1,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,DocumentsWriterDeleteQueue.Node[#],DocumentsWriter.FlushNotifications).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,DocumentsWriterDeleteQueue.Node[#]).mjava","sourceNew":"  public long updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, DocumentsWriterDeleteQueue.Node<?> deleteNode, DocumentsWriter.FlushNotifications flushNotifications) throws IOException {\n    try {\n      testPoint(\"DocumentsWriterPerThread addDocuments start\");\n      assert hasHitAbortingException() == false: \"DWPT has hit aborting exception but is still indexing\";\n      assert deleteQueue != null;\n      docState.analyzer = analyzer;\n      if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n        infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + deleteNode + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n      }\n      int docCount = 0;\n      boolean allDocsIndexed = false;\n      try {\n\n        for (Iterable<? extends IndexableField> doc : docs) {\n          // Even on exception, the document is still added (but marked\n          // deleted), so we don't need to un-reserve at that point.\n          // Aborting exceptions will actually \"lose\" more than one\n          // document, so the counter will be \"wrong\" in that case, but\n          // it's very hard to fix (we can't easily distinguish aborting\n          // vs non-aborting exceptions):\n          reserveOneDoc();\n          docState.doc = doc;\n          docState.docID = numDocsInRAM;\n          docCount++;\n\n          boolean success = false;\n          try {\n            consumer.processDocument();\n            success = true;\n          } finally {\n            if (!success) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            }\n          }\n\n          numDocsInRAM++;\n        }\n        allDocsIndexed = true;\n\n        // Apply delTerm only after all indexing has\n        // succeeded, but apply it only to docs prior to when\n        // this batch started:\n        long seqNo;\n        if (deleteNode != null) {\n          seqNo = deleteQueue.add(deleteNode, deleteSlice);\n          assert deleteSlice.isTail(deleteNode) : \"expected the delete term as the tail item\";\n          deleteSlice.apply(pendingUpdates, numDocsInRAM - docCount);\n          return seqNo;\n        } else {\n          seqNo = deleteQueue.updateSlice(deleteSlice);\n          if (seqNo < 0) {\n            seqNo = -seqNo;\n            deleteSlice.apply(pendingUpdates, numDocsInRAM - docCount);\n          } else {\n            deleteSlice.reset();\n          }\n        }\n\n        return seqNo;\n\n      } finally {\n        if (!allDocsIndexed && !aborted) {\n          // the iterator threw an exception that is not aborting\n          // go and mark all docs from this block as deleted\n          int docID = numDocsInRAM - 1;\n          final int endDocID = docID - docCount;\n          while (docID > endDocID) {\n            deleteDocID(docID);\n            docID--;\n          }\n        }\n        docState.clear();\n      }\n    } finally {\n      maybeAbort(\"updateDocuments\", flushNotifications);\n    }\n  }\n\n","sourceOld":"  public long updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, DocumentsWriterDeleteQueue.Node<?> deleteNode) throws IOException {\n    try {\n      testPoint(\"DocumentsWriterPerThread addDocuments start\");\n      assert hasHitAbortingException() == false: \"DWPT has hit aborting exception but is still indexing\";\n      assert deleteQueue != null;\n      docState.analyzer = analyzer;\n      if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n        infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + deleteNode + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n      }\n      int docCount = 0;\n      boolean allDocsIndexed = false;\n      try {\n\n        for (Iterable<? extends IndexableField> doc : docs) {\n          // Even on exception, the document is still added (but marked\n          // deleted), so we don't need to un-reserve at that point.\n          // Aborting exceptions will actually \"lose\" more than one\n          // document, so the counter will be \"wrong\" in that case, but\n          // it's very hard to fix (we can't easily distinguish aborting\n          // vs non-aborting exceptions):\n          reserveOneDoc();\n          docState.doc = doc;\n          docState.docID = numDocsInRAM;\n          docCount++;\n\n          boolean success = false;\n          try {\n            consumer.processDocument();\n            success = true;\n          } finally {\n            if (!success) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            }\n          }\n\n          numDocsInRAM++;\n        }\n        allDocsIndexed = true;\n\n        // Apply delTerm only after all indexing has\n        // succeeded, but apply it only to docs prior to when\n        // this batch started:\n        long seqNo;\n        if (deleteNode != null) {\n          seqNo = deleteQueue.add(deleteNode, deleteSlice);\n          assert deleteSlice.isTail(deleteNode) : \"expected the delete term as the tail item\";\n          deleteSlice.apply(pendingUpdates, numDocsInRAM - docCount);\n          return seqNo;\n        } else {\n          seqNo = deleteQueue.updateSlice(deleteSlice);\n          if (seqNo < 0) {\n            seqNo = -seqNo;\n            deleteSlice.apply(pendingUpdates, numDocsInRAM - docCount);\n          } else {\n            deleteSlice.reset();\n          }\n        }\n\n        return seqNo;\n\n      } finally {\n        if (!allDocsIndexed && !aborted) {\n          // the iterator threw an exception that is not aborting\n          // go and mark all docs from this block as deleted\n          int docID = numDocsInRAM - 1;\n          final int endDocID = docID - docCount;\n          while (docID > endDocID) {\n            deleteDocID(docID);\n            docID--;\n          }\n        }\n        docState.clear();\n      }\n    } finally {\n      maybeAbort(\"updateDocuments\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"142f99d1da3d720b5094f5b47b0e57f8ef6ab03c","date":1584127995,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,DocumentsWriterDeleteQueue.Node[#],DocumentsWriter.FlushNotifications).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,DocumentsWriterDeleteQueue.Node[#],DocumentsWriter.FlushNotifications).mjava","sourceNew":"  public long updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, DocumentsWriterDeleteQueue.Node<?> deleteNode, DocumentsWriter.FlushNotifications flushNotifications) throws IOException {\n    try {\n      testPoint(\"DocumentsWriterPerThread addDocuments start\");\n      assert hasHitAbortingException() == false: \"DWPT has hit aborting exception but is still indexing\";\n      assert deleteQueue != null;\n      docState.analyzer = analyzer;\n      if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n        infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + deleteNode + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n      }\n      int docCount = 0;\n      boolean allDocsIndexed = false;\n      try {\n\n        for (Iterable<? extends IndexableField> doc : docs) {\n          // Even on exception, the document is still added (but marked\n          // deleted), so we don't need to un-reserve at that point.\n          // Aborting exceptions will actually \"lose\" more than one\n          // document, so the counter will be \"wrong\" in that case, but\n          // it's very hard to fix (we can't easily distinguish aborting\n          // vs non-aborting exceptions):\n          reserveOneDoc();\n          docState.doc = doc;\n          docState.docID = numDocsInRAM;\n          docCount++;\n\n          boolean success = false;\n          try {\n            consumer.processDocument();\n            success = true;\n          } finally {\n            if (!success) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            }\n          }\n\n          numDocsInRAM++;\n        }\n        allDocsIndexed = true;\n        return finishDocuments(deleteNode, docCount);\n      } finally {\n        if (!allDocsIndexed && !aborted) {\n          // the iterator threw an exception that is not aborting\n          // go and mark all docs from this block as deleted\n          int docID = numDocsInRAM - 1;\n          final int endDocID = docID - docCount;\n          while (docID > endDocID) {\n            deleteDocID(docID);\n            docID--;\n          }\n        }\n        docState.clear();\n      }\n    } finally {\n      maybeAbort(\"updateDocuments\", flushNotifications);\n    }\n  }\n\n","sourceOld":"  public long updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, DocumentsWriterDeleteQueue.Node<?> deleteNode, DocumentsWriter.FlushNotifications flushNotifications) throws IOException {\n    try {\n      testPoint(\"DocumentsWriterPerThread addDocuments start\");\n      assert hasHitAbortingException() == false: \"DWPT has hit aborting exception but is still indexing\";\n      assert deleteQueue != null;\n      docState.analyzer = analyzer;\n      if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n        infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + deleteNode + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n      }\n      int docCount = 0;\n      boolean allDocsIndexed = false;\n      try {\n\n        for (Iterable<? extends IndexableField> doc : docs) {\n          // Even on exception, the document is still added (but marked\n          // deleted), so we don't need to un-reserve at that point.\n          // Aborting exceptions will actually \"lose\" more than one\n          // document, so the counter will be \"wrong\" in that case, but\n          // it's very hard to fix (we can't easily distinguish aborting\n          // vs non-aborting exceptions):\n          reserveOneDoc();\n          docState.doc = doc;\n          docState.docID = numDocsInRAM;\n          docCount++;\n\n          boolean success = false;\n          try {\n            consumer.processDocument();\n            success = true;\n          } finally {\n            if (!success) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            }\n          }\n\n          numDocsInRAM++;\n        }\n        allDocsIndexed = true;\n\n        // Apply delTerm only after all indexing has\n        // succeeded, but apply it only to docs prior to when\n        // this batch started:\n        long seqNo;\n        if (deleteNode != null) {\n          seqNo = deleteQueue.add(deleteNode, deleteSlice);\n          assert deleteSlice.isTail(deleteNode) : \"expected the delete term as the tail item\";\n          deleteSlice.apply(pendingUpdates, numDocsInRAM - docCount);\n          return seqNo;\n        } else {\n          seqNo = deleteQueue.updateSlice(deleteSlice);\n          if (seqNo < 0) {\n            seqNo = -seqNo;\n            deleteSlice.apply(pendingUpdates, numDocsInRAM - docCount);\n          } else {\n            deleteSlice.reset();\n          }\n        }\n\n        return seqNo;\n\n      } finally {\n        if (!allDocsIndexed && !aborted) {\n          // the iterator threw an exception that is not aborting\n          // go and mark all docs from this block as deleted\n          int docID = numDocsInRAM - 1;\n          final int endDocID = docID - docCount;\n          while (docID > endDocID) {\n            deleteDocID(docID);\n            docID--;\n          }\n        }\n        docState.clear();\n      }\n    } finally {\n      maybeAbort(\"updateDocuments\", flushNotifications);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bc1841e9449be30dd7bcb15d6247b4eb5c83a07b","date":1584454718,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,DocumentsWriterDeleteQueue.Node[#],DocumentsWriter.FlushNotifications).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,DocumentsWriterDeleteQueue.Node[#],DocumentsWriter.FlushNotifications).mjava","sourceNew":"  public long updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, DocumentsWriterDeleteQueue.Node<?> deleteNode, DocumentsWriter.FlushNotifications flushNotifications) throws IOException {\n    try {\n      testPoint(\"DocumentsWriterPerThread addDocuments start\");\n      assert hasHitAbortingException() == false: \"DWPT has hit aborting exception but is still indexing\";\n      docState.analyzer = analyzer;\n      if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n        infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + deleteNode + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n      }\n      final int docsInRamBefore = numDocsInRAM;\n      boolean allDocsIndexed = false;\n      try {\n        for (Iterable<? extends IndexableField> doc : docs) {\n          // Even on exception, the document is still added (but marked\n          // deleted), so we don't need to un-reserve at that point.\n          // Aborting exceptions will actually \"lose\" more than one\n          // document, so the counter will be \"wrong\" in that case, but\n          // it's very hard to fix (we can't easily distinguish aborting\n          // vs non-aborting exceptions):\n          reserveOneDoc();\n          docState.doc = doc;\n          docState.docID = numDocsInRAM;\n          try {\n            consumer.processDocument();\n          } finally {\n            numDocsInRAM++; // we count the doc anyway even in the case of an exception\n          }\n        }\n        allDocsIndexed = true;\n        return finishDocuments(deleteNode, docsInRamBefore);\n      } finally {\n        if (!allDocsIndexed && !aborted) {\n          // the iterator threw an exception that is not aborting\n          // go and mark all docs from this block as deleted\n          deleteLastDocs(numDocsInRAM - docsInRamBefore);\n        }\n        docState.clear();\n      }\n    } finally {\n      maybeAbort(\"updateDocuments\", flushNotifications);\n    }\n  }\n\n","sourceOld":"  public long updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, DocumentsWriterDeleteQueue.Node<?> deleteNode, DocumentsWriter.FlushNotifications flushNotifications) throws IOException {\n    try {\n      testPoint(\"DocumentsWriterPerThread addDocuments start\");\n      assert hasHitAbortingException() == false: \"DWPT has hit aborting exception but is still indexing\";\n      assert deleteQueue != null;\n      docState.analyzer = analyzer;\n      if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n        infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + deleteNode + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n      }\n      int docCount = 0;\n      boolean allDocsIndexed = false;\n      try {\n\n        for (Iterable<? extends IndexableField> doc : docs) {\n          // Even on exception, the document is still added (but marked\n          // deleted), so we don't need to un-reserve at that point.\n          // Aborting exceptions will actually \"lose\" more than one\n          // document, so the counter will be \"wrong\" in that case, but\n          // it's very hard to fix (we can't easily distinguish aborting\n          // vs non-aborting exceptions):\n          reserveOneDoc();\n          docState.doc = doc;\n          docState.docID = numDocsInRAM;\n          docCount++;\n\n          boolean success = false;\n          try {\n            consumer.processDocument();\n            success = true;\n          } finally {\n            if (!success) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            }\n          }\n\n          numDocsInRAM++;\n        }\n        allDocsIndexed = true;\n        return finishDocuments(deleteNode, docCount);\n      } finally {\n        if (!allDocsIndexed && !aborted) {\n          // the iterator threw an exception that is not aborting\n          // go and mark all docs from this block as deleted\n          int docID = numDocsInRAM - 1;\n          final int endDocID = docID - docCount;\n          while (docID > endDocID) {\n            deleteDocID(docID);\n            docID--;\n          }\n        }\n        docState.clear();\n      }\n    } finally {\n      maybeAbort(\"updateDocuments\", flushNotifications);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2","date":1588002560,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],DocumentsWriterDeleteQueue.Node[#],DocumentsWriter.FlushNotifications).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,DocumentsWriterDeleteQueue.Node[#],DocumentsWriter.FlushNotifications).mjava","sourceNew":"  long updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, DocumentsWriterDeleteQueue.Node<?> deleteNode, DocumentsWriter.FlushNotifications flushNotifications) throws IOException {\n    try {\n      testPoint(\"DocumentsWriterPerThread addDocuments start\");\n      assert hasHitAbortingException() == false: \"DWPT has hit aborting exception but is still indexing\";\n      if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n        infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + deleteNode + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n      }\n      final int docsInRamBefore = numDocsInRAM;\n      boolean allDocsIndexed = false;\n      try {\n        for (Iterable<? extends IndexableField> doc : docs) {\n          // Even on exception, the document is still added (but marked\n          // deleted), so we don't need to un-reserve at that point.\n          // Aborting exceptions will actually \"lose\" more than one\n          // document, so the counter will be \"wrong\" in that case, but\n          // it's very hard to fix (we can't easily distinguish aborting\n          // vs non-aborting exceptions):\n          reserveOneDoc();\n          docState.doc = doc;\n          docState.docID = numDocsInRAM;\n          try {\n            consumer.processDocument();\n          } finally {\n            numDocsInRAM++; // we count the doc anyway even in the case of an exception\n          }\n        }\n        allDocsIndexed = true;\n        return finishDocuments(deleteNode, docsInRamBefore);\n      } finally {\n        if (!allDocsIndexed && !aborted) {\n          // the iterator threw an exception that is not aborting\n          // go and mark all docs from this block as deleted\n          deleteLastDocs(numDocsInRAM - docsInRamBefore);\n        }\n        docState.clear();\n      }\n    } finally {\n      maybeAbort(\"updateDocuments\", flushNotifications);\n    }\n  }\n\n","sourceOld":"  public long updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, DocumentsWriterDeleteQueue.Node<?> deleteNode, DocumentsWriter.FlushNotifications flushNotifications) throws IOException {\n    try {\n      testPoint(\"DocumentsWriterPerThread addDocuments start\");\n      assert hasHitAbortingException() == false: \"DWPT has hit aborting exception but is still indexing\";\n      docState.analyzer = analyzer;\n      if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n        infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + deleteNode + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n      }\n      final int docsInRamBefore = numDocsInRAM;\n      boolean allDocsIndexed = false;\n      try {\n        for (Iterable<? extends IndexableField> doc : docs) {\n          // Even on exception, the document is still added (but marked\n          // deleted), so we don't need to un-reserve at that point.\n          // Aborting exceptions will actually \"lose\" more than one\n          // document, so the counter will be \"wrong\" in that case, but\n          // it's very hard to fix (we can't easily distinguish aborting\n          // vs non-aborting exceptions):\n          reserveOneDoc();\n          docState.doc = doc;\n          docState.docID = numDocsInRAM;\n          try {\n            consumer.processDocument();\n          } finally {\n            numDocsInRAM++; // we count the doc anyway even in the case of an exception\n          }\n        }\n        allDocsIndexed = true;\n        return finishDocuments(deleteNode, docsInRamBefore);\n      } finally {\n        if (!allDocsIndexed && !aborted) {\n          // the iterator threw an exception that is not aborting\n          // go and mark all docs from this block as deleted\n          deleteLastDocs(numDocsInRAM - docsInRamBefore);\n        }\n        docState.clear();\n      }\n    } finally {\n      maybeAbort(\"updateDocuments\", flushNotifications);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"86a2e8a56b368d37ef3ba7180541fa317d6fd6c7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2":["bc1841e9449be30dd7bcb15d6247b4eb5c83a07b"],"142f99d1da3d720b5094f5b47b0e57f8ef6ab03c":["86a2e8a56b368d37ef3ba7180541fa317d6fd6c7"],"bc1841e9449be30dd7bcb15d6247b4eb5c83a07b":["142f99d1da3d720b5094f5b47b0e57f8ef6ab03c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2"]},"commit2Childs":{"86a2e8a56b368d37ef3ba7180541fa317d6fd6c7":["142f99d1da3d720b5094f5b47b0e57f8ef6ab03c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["86a2e8a56b368d37ef3ba7180541fa317d6fd6c7"],"8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"142f99d1da3d720b5094f5b47b0e57f8ef6ab03c":["bc1841e9449be30dd7bcb15d6247b4eb5c83a07b"],"bc1841e9449be30dd7bcb15d6247b4eb5c83a07b":["8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}