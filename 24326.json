{"path":"lucene/src/test/org/apache/lucene/index/values/TestTypePromotion#testMergeIncompatibleTypes().mjava","commits":[{"id":"ff266254aa2c0b84006f8f3088ee25337661554d","date":1318269918,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/values/TestTypePromotion#testMergeIncompatibleTypes().mjava","pathOld":"/dev/null","sourceNew":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer, new IndexDocValuesField(\"promote\"),\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2, new IndexDocValuesField(\"promote\"),\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer, new IndexDocValuesField(\"promote\"),\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we optimize to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now optimize\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir);\n    assertTrue(reader.isOptimized());\n    ReaderContext topReaderContext = reader.getTopReaderContext();\n    ReaderContext[] children = topReaderContext.children();\n    IndexDocValues docValues = children[0].reader.docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(ValueType.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["fa0f44f887719e97183771e977cfc4bfb485b766"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d14e8d18c0e3970c20354dbeeb49da11bd587fbd","date":1321041051,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/values/TestTypePromotion#testMergeIncompatibleTypes().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/values/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer, new IndexDocValuesField(\"promote\"),\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2, new IndexDocValuesField(\"promote\"),\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer, new IndexDocValuesField(\"promote\"),\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    ReaderContext topReaderContext = reader.getTopReaderContext();\n    ReaderContext[] children = topReaderContext.children();\n    IndexDocValues docValues = children[0].reader.docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(ValueType.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer, new IndexDocValuesField(\"promote\"),\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2, new IndexDocValuesField(\"promote\"),\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer, new IndexDocValuesField(\"promote\"),\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we optimize to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now optimize\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir);\n    assertTrue(reader.isOptimized());\n    ReaderContext topReaderContext = reader.getTopReaderContext();\n    ReaderContext[] children = topReaderContext.children();\n    IndexDocValues docValues = children[0].reader.docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(ValueType.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1e59c344a45b9502f40ec44f5fe4e20ed2291dbe","date":1323449025,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/values/TestTypePromotion#testMergeIncompatibleTypes().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/values/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer, new DocValuesField(\"promote\"),\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2, new DocValuesField(\"promote\"),\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer, new DocValuesField(\"promote\"),\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    ReaderContext topReaderContext = reader.getTopReaderContext();\n    ReaderContext[] children = topReaderContext.children();\n    DocValues docValues = children[0].reader.docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer, new IndexDocValuesField(\"promote\"),\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2, new IndexDocValuesField(\"promote\"),\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer, new IndexDocValuesField(\"promote\"),\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    ReaderContext topReaderContext = reader.getTopReaderContext();\n    ReaderContext[] children = topReaderContext.children();\n    IndexDocValues docValues = children[0].reader.docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(ValueType.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["fa0f44f887719e97183771e977cfc4bfb485b766"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d638301ad1cfcae567b681b893bc8781f0ee48a5","date":1323801546,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/values/TestTypePromotion#testMergeIncompatibleTypes().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/values/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer, new DocValuesField(\"promote\"),\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2, new DocValuesField(\"promote\"),\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer, new DocValuesField(\"promote\"),\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    ReaderContext topReaderContext = reader.getTopReaderContext();\n    ReaderContext[] children = topReaderContext.children();\n    DocValues docValues = children[0].reader.docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer, new IndexDocValuesField(\"promote\"),\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2, new IndexDocValuesField(\"promote\"),\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer, new IndexDocValuesField(\"promote\"),\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    ReaderContext topReaderContext = reader.getTopReaderContext();\n    ReaderContext[] children = topReaderContext.children();\n    IndexDocValues docValues = children[0].reader.docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(ValueType.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a0ae5e3ed1232483b7b8a014f175a5fe43595982","date":1324062192,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/values/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer, new DocValuesField(\"promote\"),\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2, new DocValuesField(\"promote\"),\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer, new DocValuesField(\"promote\"),\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    ReaderContext topReaderContext = reader.getTopReaderContext();\n    ReaderContext[] children = topReaderContext.children();\n    DocValues docValues = children[0].reader.docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer, new DocValuesField(\"promote\"),\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2, new DocValuesField(\"promote\"),\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer, new DocValuesField(\"promote\"),\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    ReaderContext topReaderContext = reader.getTopReaderContext();\n    ReaderContext[] children = topReaderContext.children();\n    DocValues docValues = children[0].reader.docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"ff266254aa2c0b84006f8f3088ee25337661554d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1e59c344a45b9502f40ec44f5fe4e20ed2291dbe":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["ff266254aa2c0b84006f8f3088ee25337661554d"],"a0ae5e3ed1232483b7b8a014f175a5fe43595982":["d638301ad1cfcae567b681b893bc8781f0ee48a5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0ae5e3ed1232483b7b8a014f175a5fe43595982"],"d638301ad1cfcae567b681b893bc8781f0ee48a5":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd","1e59c344a45b9502f40ec44f5fe4e20ed2291dbe"]},"commit2Childs":{"ff266254aa2c0b84006f8f3088ee25337661554d":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ff266254aa2c0b84006f8f3088ee25337661554d"],"1e59c344a45b9502f40ec44f5fe4e20ed2291dbe":["d638301ad1cfcae567b681b893bc8781f0ee48a5"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["1e59c344a45b9502f40ec44f5fe4e20ed2291dbe","d638301ad1cfcae567b681b893bc8781f0ee48a5"],"a0ae5e3ed1232483b7b8a014f175a5fe43595982":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d638301ad1cfcae567b681b893bc8781f0ee48a5":["a0ae5e3ed1232483b7b8a014f175a5fe43595982"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}