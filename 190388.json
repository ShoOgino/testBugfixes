{"path":"lucene/core/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","sourceNew":"  public void testGetScores() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, directory);\n    writer.commit();\n    IndexReader ir = writer.getReader();\n    writer.close();\n    IndexSearcher searcher = newSearcher(ir);\n    Weight fake = new TermQuery(new Term(\"fake\", \"weight\")).createWeight(searcher);\n    Scorer s = new SimpleScorer(fake);\n    ScoreCachingCollector scc = new ScoreCachingCollector(scores.length);\n    scc.setScorer(s);\n    \n    // We need to iterate on the scorer so that its doc() advances.\n    int doc;\n    while ((doc = s.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n      scc.collect(doc);\n    }\n    \n    for (int i = 0; i < scores.length; i++) {\n      assertEquals(scores[i], scc.mscores[i], 0f);\n    }\n    ir.close();\n    directory.close();\n  }\n\n","sourceOld":"  public void testGetScores() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, directory);\n    writer.commit();\n    IndexReader ir = writer.getReader();\n    writer.close();\n    IndexSearcher searcher = newSearcher(ir);\n    Weight fake = new TermQuery(new Term(\"fake\", \"weight\")).createWeight(searcher);\n    Scorer s = new SimpleScorer(fake);\n    ScoreCachingCollector scc = new ScoreCachingCollector(scores.length);\n    scc.setScorer(s);\n    \n    // We need to iterate on the scorer so that its doc() advances.\n    int doc;\n    while ((doc = s.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n      scc.collect(doc);\n    }\n    \n    for (int i = 0; i < scores.length; i++) {\n      assertEquals(scores[i], scc.mscores[i], 0f);\n    }\n    ir.close();\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","sourceNew":"  public void testGetScores() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory);\n    writer.commit();\n    IndexReader ir = writer.getReader();\n    writer.close();\n    IndexSearcher searcher = newSearcher(ir);\n    Weight fake = new TermQuery(new Term(\"fake\", \"weight\")).createWeight(searcher);\n    Scorer s = new SimpleScorer(fake);\n    ScoreCachingCollector scc = new ScoreCachingCollector(scores.length);\n    scc.setScorer(s);\n    \n    // We need to iterate on the scorer so that its doc() advances.\n    int doc;\n    while ((doc = s.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n      scc.collect(doc);\n    }\n    \n    for (int i = 0; i < scores.length; i++) {\n      assertEquals(scores[i], scc.mscores[i], 0f);\n    }\n    ir.close();\n    directory.close();\n  }\n\n","sourceOld":"  public void testGetScores() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, directory);\n    writer.commit();\n    IndexReader ir = writer.getReader();\n    writer.close();\n    IndexSearcher searcher = newSearcher(ir);\n    Weight fake = new TermQuery(new Term(\"fake\", \"weight\")).createWeight(searcher);\n    Scorer s = new SimpleScorer(fake);\n    ScoreCachingCollector scc = new ScoreCachingCollector(scores.length);\n    scc.setScorer(s);\n    \n    // We need to iterate on the scorer so that its doc() advances.\n    int doc;\n    while ((doc = s.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n      scc.collect(doc);\n    }\n    \n    for (int i = 0; i < scores.length; i++) {\n      assertEquals(scores[i], scc.mscores[i], 0f);\n    }\n    ir.close();\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","sourceNew":"  public void testGetScores() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory);\n    writer.commit();\n    IndexReader ir = writer.getReader();\n    writer.shutdown();\n    IndexSearcher searcher = newSearcher(ir);\n    Weight fake = new TermQuery(new Term(\"fake\", \"weight\")).createWeight(searcher);\n    Scorer s = new SimpleScorer(fake);\n    ScoreCachingCollector scc = new ScoreCachingCollector(scores.length);\n    scc.setScorer(s);\n    \n    // We need to iterate on the scorer so that its doc() advances.\n    int doc;\n    while ((doc = s.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n      scc.collect(doc);\n    }\n    \n    for (int i = 0; i < scores.length; i++) {\n      assertEquals(scores[i], scc.mscores[i], 0f);\n    }\n    ir.close();\n    directory.close();\n  }\n\n","sourceOld":"  public void testGetScores() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory);\n    writer.commit();\n    IndexReader ir = writer.getReader();\n    writer.close();\n    IndexSearcher searcher = newSearcher(ir);\n    Weight fake = new TermQuery(new Term(\"fake\", \"weight\")).createWeight(searcher);\n    Scorer s = new SimpleScorer(fake);\n    ScoreCachingCollector scc = new ScoreCachingCollector(scores.length);\n    scc.setScorer(s);\n    \n    // We need to iterate on the scorer so that its doc() advances.\n    int doc;\n    while ((doc = s.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n      scc.collect(doc);\n    }\n    \n    for (int i = 0; i < scores.length; i++) {\n      assertEquals(scores[i], scc.mscores[i], 0f);\n    }\n    ir.close();\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","sourceNew":"  public void testGetScores() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory);\n    writer.commit();\n    IndexReader ir = writer.getReader();\n    writer.close();\n    IndexSearcher searcher = newSearcher(ir);\n    Weight fake = new TermQuery(new Term(\"fake\", \"weight\")).createWeight(searcher);\n    Scorer s = new SimpleScorer(fake);\n    ScoreCachingCollector scc = new ScoreCachingCollector(scores.length);\n    scc.setScorer(s);\n    \n    // We need to iterate on the scorer so that its doc() advances.\n    int doc;\n    while ((doc = s.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n      scc.collect(doc);\n    }\n    \n    for (int i = 0; i < scores.length; i++) {\n      assertEquals(scores[i], scc.mscores[i], 0f);\n    }\n    ir.close();\n    directory.close();\n  }\n\n","sourceOld":"  public void testGetScores() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory);\n    writer.commit();\n    IndexReader ir = writer.getReader();\n    writer.shutdown();\n    IndexSearcher searcher = newSearcher(ir);\n    Weight fake = new TermQuery(new Term(\"fake\", \"weight\")).createWeight(searcher);\n    Scorer s = new SimpleScorer(fake);\n    ScoreCachingCollector scc = new ScoreCachingCollector(scores.length);\n    scc.setScorer(s);\n    \n    // We need to iterate on the scorer so that its doc() advances.\n    int doc;\n    while ((doc = s.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n      scc.collect(doc);\n    }\n    \n    for (int i = 0; i < scores.length; i++) {\n      assertEquals(scores[i], scc.mscores[i], 0f);\n    }\n    ir.close();\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fb17639909a369c1e64866842e5c213440acc17e","date":1423238093,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","sourceNew":"  public void testGetScores() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory);\n    writer.commit();\n    IndexReader ir = writer.getReader();\n    writer.close();\n    IndexSearcher searcher = newSearcher(ir);\n    Weight fake = new TermQuery(new Term(\"fake\", \"weight\")).createWeight(searcher, true);\n    Scorer s = new SimpleScorer(fake);\n    ScoreCachingCollector scc = new ScoreCachingCollector(scores.length);\n    scc.setScorer(s);\n    \n    // We need to iterate on the scorer so that its doc() advances.\n    int doc;\n    while ((doc = s.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n      scc.collect(doc);\n    }\n    \n    for (int i = 0; i < scores.length; i++) {\n      assertEquals(scores[i], scc.mscores[i], 0f);\n    }\n    ir.close();\n    directory.close();\n  }\n\n","sourceOld":"  public void testGetScores() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory);\n    writer.commit();\n    IndexReader ir = writer.getReader();\n    writer.close();\n    IndexSearcher searcher = newSearcher(ir);\n    Weight fake = new TermQuery(new Term(\"fake\", \"weight\")).createWeight(searcher);\n    Scorer s = new SimpleScorer(fake);\n    ScoreCachingCollector scc = new ScoreCachingCollector(scores.length);\n    scc.setScorer(s);\n    \n    // We need to iterate on the scorer so that its doc() advances.\n    int doc;\n    while ((doc = s.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n      scc.collect(doc);\n    }\n    \n    for (int i = 0; i < scores.length; i++) {\n      assertEquals(scores[i], scc.mscores[i], 0f);\n    }\n    ir.close();\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"954e59be3da8dc1b046646ad7af4b466852009d3","date":1423482367,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","sourceNew":"  public void testGetScores() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory);\n    writer.commit();\n    IndexReader ir = writer.getReader();\n    writer.close();\n    IndexSearcher searcher = newSearcher(ir);\n    Weight fake = new TermQuery(new Term(\"fake\", \"weight\")).createWeight(searcher, PostingsEnum.FLAG_FREQS);\n    Scorer s = new SimpleScorer(fake);\n    ScoreCachingCollector scc = new ScoreCachingCollector(scores.length);\n    scc.setScorer(s);\n    \n    // We need to iterate on the scorer so that its doc() advances.\n    int doc;\n    while ((doc = s.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n      scc.collect(doc);\n    }\n    \n    for (int i = 0; i < scores.length; i++) {\n      assertEquals(scores[i], scc.mscores[i], 0f);\n    }\n    ir.close();\n    directory.close();\n  }\n\n","sourceOld":"  public void testGetScores() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory);\n    writer.commit();\n    IndexReader ir = writer.getReader();\n    writer.close();\n    IndexSearcher searcher = newSearcher(ir);\n    Weight fake = new TermQuery(new Term(\"fake\", \"weight\")).createWeight(searcher, true);\n    Scorer s = new SimpleScorer(fake);\n    ScoreCachingCollector scc = new ScoreCachingCollector(scores.length);\n    scc.setScorer(s);\n    \n    // We need to iterate on the scorer so that its doc() advances.\n    int doc;\n    while ((doc = s.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n      scc.collect(doc);\n    }\n    \n    for (int i = 0; i < scores.length; i++) {\n      assertEquals(scores[i], scc.mscores[i], 0f);\n    }\n    ir.close();\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6a47d642ab24da1a811adce4bda9cc52c520ca13","date":1423483323,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","sourceNew":"  public void testGetScores() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory);\n    writer.commit();\n    IndexReader ir = writer.getReader();\n    writer.close();\n    IndexSearcher searcher = newSearcher(ir);\n    Weight fake = new TermQuery(new Term(\"fake\", \"weight\")).createWeight(searcher, true);\n    Scorer s = new SimpleScorer(fake);\n    ScoreCachingCollector scc = new ScoreCachingCollector(scores.length);\n    scc.setScorer(s);\n    \n    // We need to iterate on the scorer so that its doc() advances.\n    int doc;\n    while ((doc = s.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n      scc.collect(doc);\n    }\n    \n    for (int i = 0; i < scores.length; i++) {\n      assertEquals(scores[i], scc.mscores[i], 0f);\n    }\n    ir.close();\n    directory.close();\n  }\n\n","sourceOld":"  public void testGetScores() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory);\n    writer.commit();\n    IndexReader ir = writer.getReader();\n    writer.close();\n    IndexSearcher searcher = newSearcher(ir);\n    Weight fake = new TermQuery(new Term(\"fake\", \"weight\")).createWeight(searcher, PostingsEnum.FLAG_FREQS);\n    Scorer s = new SimpleScorer(fake);\n    ScoreCachingCollector scc = new ScoreCachingCollector(scores.length);\n    scc.setScorer(s);\n    \n    // We need to iterate on the scorer so that its doc() advances.\n    int doc;\n    while ((doc = s.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n      scc.collect(doc);\n    }\n    \n    for (int i = 0; i < scores.length; i++) {\n      assertEquals(scores[i], scc.mscores[i], 0f);\n    }\n    ir.close();\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7dd748bb245633a8195281556bb0e68a6ea97d18","date":1449755030,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","sourceNew":"  public void testGetScores() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory);\n    writer.commit();\n    IndexReader ir = writer.getReader();\n    writer.close();\n    IndexSearcher searcher = newSearcher(ir);\n    Weight fake = new TermQuery(new Term(\"fake\", \"weight\")).createWeight(searcher, true);\n    Scorer s = new SimpleScorer(fake);\n    ScoreCachingCollector scc = new ScoreCachingCollector(scores.length);\n    scc.setScorer(s);\n    \n    // We need to iterate on the scorer so that its doc() advances.\n    int doc;\n    while ((doc = s.iterator().nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n      scc.collect(doc);\n    }\n    \n    for (int i = 0; i < scores.length; i++) {\n      assertEquals(scores[i], scc.mscores[i], 0f);\n    }\n    ir.close();\n    directory.close();\n  }\n\n","sourceOld":"  public void testGetScores() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory);\n    writer.commit();\n    IndexReader ir = writer.getReader();\n    writer.close();\n    IndexSearcher searcher = newSearcher(ir);\n    Weight fake = new TermQuery(new Term(\"fake\", \"weight\")).createWeight(searcher, true);\n    Scorer s = new SimpleScorer(fake);\n    ScoreCachingCollector scc = new ScoreCachingCollector(scores.length);\n    scc.setScorer(s);\n    \n    // We need to iterate on the scorer so that its doc() advances.\n    int doc;\n    while ((doc = s.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n      scc.collect(doc);\n    }\n    \n    for (int i = 0; i < scores.length; i++) {\n      assertEquals(scores[i], scc.mscores[i], 0f);\n    }\n    ir.close();\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"02e175abd2c4c1611c5a9647486ae8ba249a94c1","date":1468327116,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","sourceNew":"  public void testGetScores() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory);\n    writer.commit();\n    IndexReader ir = writer.getReader();\n    writer.close();\n    IndexSearcher searcher = newSearcher(ir);\n    Weight fake = new TermQuery(new Term(\"fake\", \"weight\")).createWeight(searcher, true, 1f);\n    Scorer s = new SimpleScorer(fake);\n    ScoreCachingCollector scc = new ScoreCachingCollector(scores.length);\n    scc.setScorer(s);\n    \n    // We need to iterate on the scorer so that its doc() advances.\n    int doc;\n    while ((doc = s.iterator().nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n      scc.collect(doc);\n    }\n    \n    for (int i = 0; i < scores.length; i++) {\n      assertEquals(scores[i], scc.mscores[i], 0f);\n    }\n    ir.close();\n    directory.close();\n  }\n\n","sourceOld":"  public void testGetScores() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory);\n    writer.commit();\n    IndexReader ir = writer.getReader();\n    writer.close();\n    IndexSearcher searcher = newSearcher(ir);\n    Weight fake = new TermQuery(new Term(\"fake\", \"weight\")).createWeight(searcher, true);\n    Scorer s = new SimpleScorer(fake);\n    ScoreCachingCollector scc = new ScoreCachingCollector(scores.length);\n    scc.setScorer(s);\n    \n    // We need to iterate on the scorer so that its doc() advances.\n    int doc;\n    while ((doc = s.iterator().nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n      scc.collect(doc);\n    }\n    \n    for (int i = 0; i < scores.length; i++) {\n      assertEquals(scores[i], scc.mscores[i], 0f);\n    }\n    ir.close();\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","sourceNew":"  public void testGetScores() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory);\n    writer.commit();\n    IndexReader ir = writer.getReader();\n    writer.close();\n    IndexSearcher searcher = newSearcher(ir);\n    Weight fake = new TermQuery(new Term(\"fake\", \"weight\")).createWeight(searcher, true, 1f);\n    Scorer s = new SimpleScorer(fake);\n    ScoreCachingCollector scc = new ScoreCachingCollector(scores.length);\n    scc.setScorer(s);\n    \n    // We need to iterate on the scorer so that its doc() advances.\n    int doc;\n    while ((doc = s.iterator().nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n      scc.collect(doc);\n    }\n    \n    for (int i = 0; i < scores.length; i++) {\n      assertEquals(scores[i], scc.mscores[i], 0f);\n    }\n    ir.close();\n    directory.close();\n  }\n\n","sourceOld":"  public void testGetScores() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory);\n    writer.commit();\n    IndexReader ir = writer.getReader();\n    writer.close();\n    IndexSearcher searcher = newSearcher(ir);\n    Weight fake = new TermQuery(new Term(\"fake\", \"weight\")).createWeight(searcher, true);\n    Scorer s = new SimpleScorer(fake);\n    ScoreCachingCollector scc = new ScoreCachingCollector(scores.length);\n    scc.setScorer(s);\n    \n    // We need to iterate on the scorer so that its doc() advances.\n    int doc;\n    while ((doc = s.iterator().nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n      scc.collect(doc);\n    }\n    \n    for (int i = 0; i < scores.length; i++) {\n      assertEquals(scores[i], scc.mscores[i], 0f);\n    }\n    ir.close();\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9fc47cb7b4346802411bb432f501ed0673d7119e","date":1512640179,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","sourceNew":"  public void testGetScores() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory);\n    writer.commit();\n    IndexReader ir = writer.getReader();\n    writer.close();\n    IndexSearcher searcher = newSearcher(ir);\n    Weight fake = new TermQuery(new Term(\"fake\", \"weight\")).createWeight(searcher, ScoreMode.COMPLETE, 1f);\n    Scorer s = new SimpleScorer(fake);\n    ScoreCachingCollector scc = new ScoreCachingCollector(scores.length);\n    scc.setScorer(s);\n    \n    // We need to iterate on the scorer so that its doc() advances.\n    int doc;\n    while ((doc = s.iterator().nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n      scc.collect(doc);\n    }\n    \n    for (int i = 0; i < scores.length; i++) {\n      assertEquals(scores[i], scc.mscores[i], 0f);\n    }\n    ir.close();\n    directory.close();\n  }\n\n","sourceOld":"  public void testGetScores() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory);\n    writer.commit();\n    IndexReader ir = writer.getReader();\n    writer.close();\n    IndexSearcher searcher = newSearcher(ir);\n    Weight fake = new TermQuery(new Term(\"fake\", \"weight\")).createWeight(searcher, true, 1f);\n    Scorer s = new SimpleScorer(fake);\n    ScoreCachingCollector scc = new ScoreCachingCollector(scores.length);\n    scc.setScorer(s);\n    \n    // We need to iterate on the scorer so that its doc() advances.\n    int doc;\n    while ((doc = s.iterator().nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n      scc.collect(doc);\n    }\n    \n    for (int i = 0; i < scores.length; i++) {\n      assertEquals(scores[i], scc.mscores[i], 0f);\n    }\n    ir.close();\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"417142ff08fda9cf0b72d5133e63097a166c6458","date":1512729693,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer#testGetScores().mjava","sourceNew":"  public void testGetScores() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory);\n    writer.commit();\n    IndexReader ir = writer.getReader();\n    writer.close();\n    IndexSearcher searcher = newSearcher(ir);\n    Weight fake = new TermQuery(new Term(\"fake\", \"weight\")).createWeight(searcher, ScoreMode.COMPLETE, 1f);\n    Scorer s = new SimpleScorer(fake);\n    ScoreCachingCollector scc = new ScoreCachingCollector(scores.length);\n    scc.setScorer(s);\n    \n    // We need to iterate on the scorer so that its doc() advances.\n    int doc;\n    while ((doc = s.iterator().nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n      scc.collect(doc);\n    }\n    \n    for (int i = 0; i < scores.length; i++) {\n      assertEquals(scores[i], scc.mscores[i], 0f);\n    }\n    ir.close();\n    directory.close();\n  }\n\n","sourceOld":"  public void testGetScores() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory);\n    writer.commit();\n    IndexReader ir = writer.getReader();\n    writer.close();\n    IndexSearcher searcher = newSearcher(ir);\n    Weight fake = new TermQuery(new Term(\"fake\", \"weight\")).createWeight(searcher, true, 1f);\n    Scorer s = new SimpleScorer(fake);\n    ScoreCachingCollector scc = new ScoreCachingCollector(scores.length);\n    scc.setScorer(s);\n    \n    // We need to iterate on the scorer so that its doc() advances.\n    int doc;\n    while ((doc = s.iterator().nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n      scc.collect(doc);\n    }\n    \n    for (int i = 0; i < scores.length; i++) {\n      assertEquals(scores[i], scc.mscores[i], 0f);\n    }\n    ir.close();\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"6a47d642ab24da1a811adce4bda9cc52c520ca13":["954e59be3da8dc1b046646ad7af4b466852009d3"],"fb17639909a369c1e64866842e5c213440acc17e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"954e59be3da8dc1b046646ad7af4b466852009d3":["fb17639909a369c1e64866842e5c213440acc17e"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["7dd748bb245633a8195281556bb0e68a6ea97d18","02e175abd2c4c1611c5a9647486ae8ba249a94c1"],"417142ff08fda9cf0b72d5133e63097a166c6458":["02e175abd2c4c1611c5a9647486ae8ba249a94c1","9fc47cb7b4346802411bb432f501ed0673d7119e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"02e175abd2c4c1611c5a9647486ae8ba249a94c1":["7dd748bb245633a8195281556bb0e68a6ea97d18"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"9fc47cb7b4346802411bb432f501ed0673d7119e":["02e175abd2c4c1611c5a9647486ae8ba249a94c1"],"7dd748bb245633a8195281556bb0e68a6ea97d18":["6a47d642ab24da1a811adce4bda9cc52c520ca13"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["417142ff08fda9cf0b72d5133e63097a166c6458"]},"commit2Childs":{"6a47d642ab24da1a811adce4bda9cc52c520ca13":["7dd748bb245633a8195281556bb0e68a6ea97d18"],"fb17639909a369c1e64866842e5c213440acc17e":["954e59be3da8dc1b046646ad7af4b466852009d3"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"954e59be3da8dc1b046646ad7af4b466852009d3":["6a47d642ab24da1a811adce4bda9cc52c520ca13"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"417142ff08fda9cf0b72d5133e63097a166c6458":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["fb17639909a369c1e64866842e5c213440acc17e"],"02e175abd2c4c1611c5a9647486ae8ba249a94c1":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","417142ff08fda9cf0b72d5133e63097a166c6458","9fc47cb7b4346802411bb432f501ed0673d7119e"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"9fc47cb7b4346802411bb432f501ed0673d7119e":["417142ff08fda9cf0b72d5133e63097a166c6458"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"7dd748bb245633a8195281556bb0e68a6ea97d18":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","02e175abd2c4c1611c5a9647486ae8ba249a94c1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}