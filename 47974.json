{"path":"solr/core/src/java/org/apache/solr/uninverting/DocTermOrds#uninvert(LeafReader,Bits,BytesRef).mjava","commits":[{"id":"a076c3c721f685b7559308fdc2cd72d91bba67e5","date":1464168992,"type":1,"author":"Mike McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/uninverting/DocTermOrds#uninvert(LeafReader,Bits,BytesRef).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/uninverting/DocTermOrds#uninvert(LeafReader,Bits,BytesRef).mjava","sourceNew":"  /** Call this only once (if you subclass!) */\n  protected void uninvert(final LeafReader reader, Bits liveDocs, final BytesRef termPrefix) throws IOException {\n    final FieldInfo info = reader.getFieldInfos().fieldInfo(field);\n    if (checkForDocValues && info != null && info.getDocValuesType() != DocValuesType.NONE) {\n      throw new IllegalStateException(\"Type mismatch: \" + field + \" was indexed as \" + info.getDocValuesType());\n    }\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.nanoTime();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = reader.terms(field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // For our \"term index wrapper\"\n    final List<BytesRef> indexedTerms = new ArrayList<>();\n    final PagedBytes indexedTermsBytes = new PagedBytes(15);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in its byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    postingsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      visitTerm(te, termNum);\n\n      if ((termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        postingsEnum = te.postings(postingsEnum, PostingsEnum.NONE);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = postingsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.nanoTime();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n    }\n    indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n\n    long endTime = System.nanoTime();\n\n    total_time = (int) TimeUnit.MILLISECONDS.convert(endTime-startTime, TimeUnit.NANOSECONDS);\n    phase1_time = (int) TimeUnit.MILLISECONDS.convert(midPoint-startTime, TimeUnit.NANOSECONDS);\n  }\n\n","sourceOld":"  /** Call this only once (if you subclass!) */\n  protected void uninvert(final LeafReader reader, Bits liveDocs, final BytesRef termPrefix) throws IOException {\n    final FieldInfo info = reader.getFieldInfos().fieldInfo(field);\n    if (checkForDocValues && info != null && info.getDocValuesType() != DocValuesType.NONE) {\n      throw new IllegalStateException(\"Type mismatch: \" + field + \" was indexed as \" + info.getDocValuesType());\n    }\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = reader.terms(field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // For our \"term index wrapper\"\n    final List<BytesRef> indexedTerms = new ArrayList<>();\n    final PagedBytes indexedTermsBytes = new PagedBytes(15);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in its byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    postingsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      visitTerm(te, termNum);\n\n      if ((termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        postingsEnum = te.postings(postingsEnum, PostingsEnum.NONE);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = postingsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n    }\n    indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0e121d43b5a10f2df530f406f935102656e9c4e8","date":1464198131,"type":1,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/uninverting/DocTermOrds#uninvert(LeafReader,Bits,BytesRef).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/uninverting/DocTermOrds#uninvert(LeafReader,Bits,BytesRef).mjava","sourceNew":"  /** Call this only once (if you subclass!) */\n  protected void uninvert(final LeafReader reader, Bits liveDocs, final BytesRef termPrefix) throws IOException {\n    final FieldInfo info = reader.getFieldInfos().fieldInfo(field);\n    if (checkForDocValues && info != null && info.getDocValuesType() != DocValuesType.NONE) {\n      throw new IllegalStateException(\"Type mismatch: \" + field + \" was indexed as \" + info.getDocValuesType());\n    }\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.nanoTime();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = reader.terms(field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // For our \"term index wrapper\"\n    final List<BytesRef> indexedTerms = new ArrayList<>();\n    final PagedBytes indexedTermsBytes = new PagedBytes(15);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in its byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    postingsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      visitTerm(te, termNum);\n\n      if ((termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        postingsEnum = te.postings(postingsEnum, PostingsEnum.NONE);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = postingsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.nanoTime();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n    }\n    indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n\n    long endTime = System.nanoTime();\n\n    total_time = (int) TimeUnit.MILLISECONDS.convert(endTime-startTime, TimeUnit.NANOSECONDS);\n    phase1_time = (int) TimeUnit.MILLISECONDS.convert(midPoint-startTime, TimeUnit.NANOSECONDS);\n  }\n\n","sourceOld":"  /** Call this only once (if you subclass!) */\n  protected void uninvert(final LeafReader reader, Bits liveDocs, final BytesRef termPrefix) throws IOException {\n    final FieldInfo info = reader.getFieldInfos().fieldInfo(field);\n    if (checkForDocValues && info != null && info.getDocValuesType() != DocValuesType.NONE) {\n      throw new IllegalStateException(\"Type mismatch: \" + field + \" was indexed as \" + info.getDocValuesType());\n    }\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = reader.terms(field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // For our \"term index wrapper\"\n    final List<BytesRef> indexedTerms = new ArrayList<>();\n    final PagedBytes indexedTermsBytes = new PagedBytes(15);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in its byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    postingsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      visitTerm(te, termNum);\n\n      if ((termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        postingsEnum = te.postings(postingsEnum, PostingsEnum.NONE);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = postingsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n    }\n    indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83870855d82aba6819217abeff5a40779dbb28b4","date":1464291012,"type":1,"author":"Mike McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/uninverting/DocTermOrds#uninvert(LeafReader,Bits,BytesRef).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/uninverting/DocTermOrds#uninvert(LeafReader,Bits,BytesRef).mjava","sourceNew":"  /** Call this only once (if you subclass!) */\n  protected void uninvert(final LeafReader reader, Bits liveDocs, final BytesRef termPrefix) throws IOException {\n    final FieldInfo info = reader.getFieldInfos().fieldInfo(field);\n    if (checkForDocValues && info != null && info.getDocValuesType() != DocValuesType.NONE) {\n      throw new IllegalStateException(\"Type mismatch: \" + field + \" was indexed as \" + info.getDocValuesType());\n    }\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.nanoTime();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = reader.terms(field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // For our \"term index wrapper\"\n    final List<BytesRef> indexedTerms = new ArrayList<>();\n    final PagedBytes indexedTermsBytes = new PagedBytes(15);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in its byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    postingsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      visitTerm(te, termNum);\n\n      if ((termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        postingsEnum = te.postings(postingsEnum, PostingsEnum.NONE);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = postingsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.nanoTime();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n    }\n    indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n\n    long endTime = System.nanoTime();\n\n    total_time = (int) TimeUnit.MILLISECONDS.convert(endTime-startTime, TimeUnit.NANOSECONDS);\n    phase1_time = (int) TimeUnit.MILLISECONDS.convert(midPoint-startTime, TimeUnit.NANOSECONDS);\n  }\n\n","sourceOld":"  /** Call this only once (if you subclass!) */\n  protected void uninvert(final LeafReader reader, Bits liveDocs, final BytesRef termPrefix) throws IOException {\n    final FieldInfo info = reader.getFieldInfos().fieldInfo(field);\n    if (checkForDocValues && info != null && info.getDocValuesType() != DocValuesType.NONE) {\n      throw new IllegalStateException(\"Type mismatch: \" + field + \" was indexed as \" + info.getDocValuesType());\n    }\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = reader.terms(field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // For our \"term index wrapper\"\n    final List<BytesRef> indexedTerms = new ArrayList<>();\n    final PagedBytes indexedTermsBytes = new PagedBytes(15);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in its byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    postingsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      visitTerm(te, termNum);\n\n      if ((termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        postingsEnum = te.postings(postingsEnum, PostingsEnum.NONE);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = postingsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n    }\n    indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/uninverting/DocTermOrds#uninvert(LeafReader,Bits,BytesRef).mjava","pathOld":"/dev/null","sourceNew":"  /** Call this only once (if you subclass!) */\n  protected void uninvert(final LeafReader reader, Bits liveDocs, final BytesRef termPrefix) throws IOException {\n    final FieldInfo info = reader.getFieldInfos().fieldInfo(field);\n    if (checkForDocValues && info != null && info.getDocValuesType() != DocValuesType.NONE) {\n      throw new IllegalStateException(\"Type mismatch: \" + field + \" was indexed as \" + info.getDocValuesType());\n    }\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.nanoTime();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = reader.terms(field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // For our \"term index wrapper\"\n    final List<BytesRef> indexedTerms = new ArrayList<>();\n    final PagedBytes indexedTermsBytes = new PagedBytes(15);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in its byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    postingsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      visitTerm(te, termNum);\n\n      if ((termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        postingsEnum = te.postings(postingsEnum, PostingsEnum.NONE);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = postingsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.nanoTime();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n    }\n    indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n\n    long endTime = System.nanoTime();\n\n    total_time = (int) TimeUnit.MILLISECONDS.convert(endTime-startTime, TimeUnit.NANOSECONDS);\n    phase1_time = (int) TimeUnit.MILLISECONDS.convert(midPoint-startTime, TimeUnit.NANOSECONDS);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ac981db60ef979233b3438ec49ddae82e8cc4697","date":1503407558,"type":3,"author":"Toke Eskildsen","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/uninverting/DocTermOrds#uninvert(LeafReader,Bits,BytesRef).mjava","pathOld":"solr/core/src/java/org/apache/solr/uninverting/DocTermOrds#uninvert(LeafReader,Bits,BytesRef).mjava","sourceNew":"  /** Call this only once (if you subclass!) */\n  protected void uninvert(final LeafReader reader, Bits liveDocs, final BytesRef termPrefix) throws IOException {\n    final FieldInfo info = reader.getFieldInfos().fieldInfo(field);\n    if (checkForDocValues && info != null && info.getDocValuesType() != DocValuesType.NONE) {\n      throw new IllegalStateException(\"Type mismatch: \" + field + \" was indexed as \" + info.getDocValuesType());\n    }\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.nanoTime();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = reader.terms(field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // For our \"term index wrapper\"\n    final List<BytesRef> indexedTerms = new ArrayList<>();\n    final PagedBytes indexedTermsBytes = new PagedBytes(15);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in its byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    postingsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      visitTerm(te, termNum);\n\n      if ((termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        postingsEnum = te.postings(postingsEnum, PostingsEnum.NONE);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = postingsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0x80000000) != 0) {\n            // index into byte array (actually the end of the doc-specific byte[] when building)\n            int pos = val & 0x7fffffff;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = pos | 0x80000000;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = endPos | 0x80000000;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.nanoTime();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val & 0x80000000) != 0) {\n              int len = val & 0x7fffffff;\n              //System.out.println(\"    ptr pos=\" + pos);\n              //index[doc] = (pos<<8)|1; // change index to point to start of array\n              index[doc] = pos | 0x80000000; // change index to point to start of array\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                while (newlen <= pos + len) {\n                  if ((newlen<<=1) < 0) { // Double until overflow\n                    newlen = Integer.MAX_VALUE - 16; // ArrayList.MAX_ARRAY_SIZE says 8. We double that to be sure\n                    if (newlen <= pos + len) {\n                      throw new IllegalStateException(\n                          \"Too many terms (> Integer.MAX_VALUE-16) to uninvert field '\" + field + \"'\");\n                    }\n                  }\n                }\n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n    }\n    indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n\n    long endTime = System.nanoTime();\n\n    total_time = (int) TimeUnit.MILLISECONDS.convert(endTime-startTime, TimeUnit.NANOSECONDS);\n    phase1_time = (int) TimeUnit.MILLISECONDS.convert(midPoint-startTime, TimeUnit.NANOSECONDS);\n  }\n\n","sourceOld":"  /** Call this only once (if you subclass!) */\n  protected void uninvert(final LeafReader reader, Bits liveDocs, final BytesRef termPrefix) throws IOException {\n    final FieldInfo info = reader.getFieldInfos().fieldInfo(field);\n    if (checkForDocValues && info != null && info.getDocValuesType() != DocValuesType.NONE) {\n      throw new IllegalStateException(\"Type mismatch: \" + field + \" was indexed as \" + info.getDocValuesType());\n    }\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.nanoTime();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = reader.terms(field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // For our \"term index wrapper\"\n    final List<BytesRef> indexedTerms = new ArrayList<>();\n    final PagedBytes indexedTermsBytes = new PagedBytes(15);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in its byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    postingsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      visitTerm(te, termNum);\n\n      if ((termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        postingsEnum = te.postings(postingsEnum, PostingsEnum.NONE);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = postingsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.nanoTime();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n    }\n    indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n\n    long endTime = System.nanoTime();\n\n    total_time = (int) TimeUnit.MILLISECONDS.convert(endTime-startTime, TimeUnit.NANOSECONDS);\n    phase1_time = (int) TimeUnit.MILLISECONDS.convert(midPoint-startTime, TimeUnit.NANOSECONDS);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a7809d1d753b67f48b1a706e17034bf8b624ea3","date":1504366927,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/uninverting/DocTermOrds#uninvert(LeafReader,Bits,BytesRef).mjava","pathOld":"solr/core/src/java/org/apache/solr/uninverting/DocTermOrds#uninvert(LeafReader,Bits,BytesRef).mjava","sourceNew":"  /** Call this only once (if you subclass!) */\n  protected void uninvert(final LeafReader reader, Bits liveDocs, final BytesRef termPrefix) throws IOException {\n    final FieldInfo info = reader.getFieldInfos().fieldInfo(field);\n    if (checkForDocValues && info != null && info.getDocValuesType() != DocValuesType.NONE) {\n      throw new IllegalStateException(\"Type mismatch: \" + field + \" was indexed as \" + info.getDocValuesType());\n    }\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.nanoTime();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = reader.terms(field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // For our \"term index wrapper\"\n    final List<BytesRef> indexedTerms = new ArrayList<>();\n    final PagedBytes indexedTermsBytes = new PagedBytes(15);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in its byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    postingsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      visitTerm(te, termNum);\n\n      if ((termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        postingsEnum = te.postings(postingsEnum, PostingsEnum.NONE);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = postingsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0x80000000) != 0) {\n            // index into byte array (actually the end of the doc-specific byte[] when building)\n            int pos = val & 0x7fffffff;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = pos | 0x80000000;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = endPos | 0x80000000;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.nanoTime();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val & 0x80000000) != 0) {\n              int len = val & 0x7fffffff;\n              //System.out.println(\"    ptr pos=\" + pos);\n              //index[doc] = (pos<<8)|1; // change index to point to start of array\n              index[doc] = pos | 0x80000000; // change index to point to start of array\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                while (newlen <= pos + len) {\n                  if ((newlen<<=1) < 0) { // Double until overflow\n                    newlen = Integer.MAX_VALUE - 16; // ArrayList.MAX_ARRAY_SIZE says 8. We double that to be sure\n                    if (newlen <= pos + len) {\n                      throw new IllegalStateException(\n                          \"Too many terms (> Integer.MAX_VALUE-16) to uninvert field '\" + field + \"'\");\n                    }\n                  }\n                }\n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n    }\n    indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n\n    long endTime = System.nanoTime();\n\n    total_time = (int) TimeUnit.MILLISECONDS.convert(endTime-startTime, TimeUnit.NANOSECONDS);\n    phase1_time = (int) TimeUnit.MILLISECONDS.convert(midPoint-startTime, TimeUnit.NANOSECONDS);\n  }\n\n","sourceOld":"  /** Call this only once (if you subclass!) */\n  protected void uninvert(final LeafReader reader, Bits liveDocs, final BytesRef termPrefix) throws IOException {\n    final FieldInfo info = reader.getFieldInfos().fieldInfo(field);\n    if (checkForDocValues && info != null && info.getDocValuesType() != DocValuesType.NONE) {\n      throw new IllegalStateException(\"Type mismatch: \" + field + \" was indexed as \" + info.getDocValuesType());\n    }\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.nanoTime();\n    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = reader.terms(field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // For our \"term index wrapper\"\n    final List<BytesRef> indexedTerms = new ArrayList<>();\n    final PagedBytes indexedTermsBytes = new PagedBytes(15);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in its byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    postingsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      visitTerm(te, termNum);\n\n      if ((termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        postingsEnum = te.postings(postingsEnum, PostingsEnum.NONE);\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int doc = postingsEnum.nextDoc();\n          if (doc == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF ++;\n          termInstances++;\n          \n          //System.out.println(\"    docID=\" + doc);\n          // add TNUM_OFFSET to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            //System.out.println(\"      ipos=\" + ipos);\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            //System.out.println(\"      endpos=\" + endPos);\n            if (endPos <= 4) {\n              //System.out.println(\"      fits!\");\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.nanoTime();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n    }\n    indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n\n    long endTime = System.nanoTime();\n\n    total_time = (int) TimeUnit.MILLISECONDS.convert(endTime-startTime, TimeUnit.NANOSECONDS);\n    phase1_time = (int) TimeUnit.MILLISECONDS.convert(midPoint-startTime, TimeUnit.NANOSECONDS);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a076c3c721f685b7559308fdc2cd72d91bba67e5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"0e121d43b5a10f2df530f406f935102656e9c4e8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","a076c3c721f685b7559308fdc2cd72d91bba67e5"],"3a7809d1d753b67f48b1a706e17034bf8b624ea3":["0e121d43b5a10f2df530f406f935102656e9c4e8","ac981db60ef979233b3438ec49ddae82e8cc4697"],"ac981db60ef979233b3438ec49ddae82e8cc4697":["0e121d43b5a10f2df530f406f935102656e9c4e8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"83870855d82aba6819217abeff5a40779dbb28b4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","0e121d43b5a10f2df530f406f935102656e9c4e8"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","0e121d43b5a10f2df530f406f935102656e9c4e8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["ac981db60ef979233b3438ec49ddae82e8cc4697"]},"commit2Childs":{"a076c3c721f685b7559308fdc2cd72d91bba67e5":["0e121d43b5a10f2df530f406f935102656e9c4e8"],"0e121d43b5a10f2df530f406f935102656e9c4e8":["3a7809d1d753b67f48b1a706e17034bf8b624ea3","ac981db60ef979233b3438ec49ddae82e8cc4697","83870855d82aba6819217abeff5a40779dbb28b4","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"3a7809d1d753b67f48b1a706e17034bf8b624ea3":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a076c3c721f685b7559308fdc2cd72d91bba67e5","0e121d43b5a10f2df530f406f935102656e9c4e8","83870855d82aba6819217abeff5a40779dbb28b4","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"ac981db60ef979233b3438ec49ddae82e8cc4697":["3a7809d1d753b67f48b1a706e17034bf8b624ea3","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"83870855d82aba6819217abeff5a40779dbb28b4":[],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["3a7809d1d753b67f48b1a706e17034bf8b624ea3","83870855d82aba6819217abeff5a40779dbb28b4","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}