{"path":"src/java/org/apache/lucene/index/DocumentsWriterFieldData#invertField(Fieldable,Analyzer,int).mjava","commits":[{"id":"5a0af3a442be522899177e5e11384a45a6784a3f","date":1205348952,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriterFieldData#invertField(Fieldable,Analyzer,int).mjava","pathOld":"/dev/null","sourceNew":"  /* Invert one occurrence of one field in the document */\n  public void invertField(Fieldable field, Analyzer analyzer, final int maxFieldLength) throws IOException, AbortException {\n\n    if (length>0)\n      position += analyzer.getPositionIncrementGap(fieldInfo.name);\n\n    if (!field.isTokenized()) {\t\t  // un-tokenized field\n      String stringValue = field.stringValue();\n      final int valueLength = stringValue.length();\n      Token token = localToken;\n      token.clear();\n      char[] termBuffer = token.termBuffer();\n      if (termBuffer.length < valueLength)\n        termBuffer = token.resizeTermBuffer(valueLength);\n      stringValue.getChars(0, valueLength, termBuffer, 0);\n      token.setTermLength(valueLength);\n      token.setStartOffset(offset);\n      token.setEndOffset(offset + stringValue.length());\n      addPosition(token);\n      offset += stringValue.length();\n      length++;\n    } else {                                  // tokenized field\n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          threadState.stringReader.init(stringValue);\n          reader = threadState.stringReader;\n        }\n          \n        // Tokenize field and add to postingTable\n        stream = analyzer.reusableTokenStream(fieldInfo.name, reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      try {\n        offsetEnd = offset-1;\n        Token token;\n        for(;;) {\n          token = stream.next(localToken);\n          if (token == null) break;\n          position += (token.getPositionIncrement() - 1);\n          addPosition(token);\n          if (++length >= maxFieldLength) {\n            if (threadState.docWriter.infoStream != null)\n              threadState.docWriter.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n            break;\n          }\n        }\n        offset = offsetEnd+1;\n      } finally {\n        stream.close();\n      }\n    }\n\n    boost *= field.getBoost();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f51cb06175d6fae01dc608dd7ab884973354e4bf","date":1207240926,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriterFieldData#invertField(Fieldable,Analyzer,int).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriterFieldData#invertField(Fieldable,Analyzer,int).mjava","sourceNew":"  /* Invert one occurrence of one field in the document */\n  public void invertField(Fieldable field, Analyzer analyzer, final int maxFieldLength) throws IOException, AbortException {\n\n    if (length>0)\n      position += analyzer.getPositionIncrementGap(fieldInfo.name);\n\n    if (!field.isTokenized()) {\t\t  // un-tokenized field\n      String stringValue = field.stringValue();\n      final int valueLength = stringValue.length();\n      Token token = localToken;\n      token.clear();\n      char[] termBuffer = token.termBuffer();\n      if (termBuffer.length < valueLength)\n        termBuffer = token.resizeTermBuffer(valueLength);\n      stringValue.getChars(0, valueLength, termBuffer, 0);\n      token.setTermLength(valueLength);\n      token.setStartOffset(offset);\n      token.setEndOffset(offset + stringValue.length());\n      addPosition(token);\n      offset += stringValue.length();\n      length++;\n    } else {                                  // tokenized field\n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          threadState.stringReader.init(stringValue);\n          reader = threadState.stringReader;\n        }\n          \n        // Tokenize field and add to postingTable\n        stream = analyzer.reusableTokenStream(fieldInfo.name, reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      try {\n        offsetEnd = offset-1;\n        for(;;) {\n          Token token = stream.next(localToken);\n          if (token == null) break;\n          position += (token.getPositionIncrement() - 1);\n          // LUCENE-1255: don't allow negative positon\n          if (position < 0)\n            position = 0;\n          addPosition(token);\n          if (++length >= maxFieldLength) {\n            if (threadState.docWriter.infoStream != null)\n              threadState.docWriter.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n            break;\n          }\n        }\n        offset = offsetEnd+1;\n      } finally {\n        stream.close();\n      }\n    }\n\n    boost *= field.getBoost();\n  }\n\n","sourceOld":"  /* Invert one occurrence of one field in the document */\n  public void invertField(Fieldable field, Analyzer analyzer, final int maxFieldLength) throws IOException, AbortException {\n\n    if (length>0)\n      position += analyzer.getPositionIncrementGap(fieldInfo.name);\n\n    if (!field.isTokenized()) {\t\t  // un-tokenized field\n      String stringValue = field.stringValue();\n      final int valueLength = stringValue.length();\n      Token token = localToken;\n      token.clear();\n      char[] termBuffer = token.termBuffer();\n      if (termBuffer.length < valueLength)\n        termBuffer = token.resizeTermBuffer(valueLength);\n      stringValue.getChars(0, valueLength, termBuffer, 0);\n      token.setTermLength(valueLength);\n      token.setStartOffset(offset);\n      token.setEndOffset(offset + stringValue.length());\n      addPosition(token);\n      offset += stringValue.length();\n      length++;\n    } else {                                  // tokenized field\n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          threadState.stringReader.init(stringValue);\n          reader = threadState.stringReader;\n        }\n          \n        // Tokenize field and add to postingTable\n        stream = analyzer.reusableTokenStream(fieldInfo.name, reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      try {\n        offsetEnd = offset-1;\n        Token token;\n        for(;;) {\n          token = stream.next(localToken);\n          if (token == null) break;\n          position += (token.getPositionIncrement() - 1);\n          addPosition(token);\n          if (++length >= maxFieldLength) {\n            if (threadState.docWriter.infoStream != null)\n              threadState.docWriter.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n            break;\n          }\n        }\n        offset = offsetEnd+1;\n      } finally {\n        stream.close();\n      }\n    }\n\n    boost *= field.getBoost();\n  }\n\n","bugFix":null,"bugIntro":["abfdd5170b43f046dfac9dafd6e12c1a65f3018c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e7d928749bc06c6ab30af3c32b7258585a8db4c3","date":1207578634,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriterFieldData#invertField(Fieldable,Analyzer,int).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriterFieldData#invertField(Fieldable,Analyzer,int).mjava","sourceNew":"  /* Invert one occurrence of one field in the document */\n  public void invertField(Fieldable field, Analyzer analyzer, final int maxFieldLength) throws IOException, AbortException {\n\n    if (length>0)\n      position += analyzer.getPositionIncrementGap(fieldInfo.name);\n\n    if (!field.isTokenized()) {\t\t  // un-tokenized field\n      String stringValue = field.stringValue();\n      final int valueLength = stringValue.length();\n      Token token = localToken;\n      token.clear();\n      char[] termBuffer = token.termBuffer();\n      if (termBuffer.length < valueLength)\n        termBuffer = token.resizeTermBuffer(valueLength);\n      stringValue.getChars(0, valueLength, termBuffer, 0);\n      token.setTermLength(valueLength);\n      token.setStartOffset(offset);\n      token.setEndOffset(offset + stringValue.length());\n      addPosition(token);\n      offset += stringValue.length();\n      length++;\n    } else {                                  // tokenized field\n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          threadState.stringReader.init(stringValue);\n          reader = threadState.stringReader;\n        }\n          \n        // Tokenize field and add to postingTable\n        stream = analyzer.reusableTokenStream(fieldInfo.name, reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      try {\n        offsetEnd = offset-1;\n        for(;;) {\n          Token token = stream.next(localToken);\n          if (token == null) break;\n          position += (token.getPositionIncrement() - 1);\n          // LUCENE-1255: don't allow negative position\n          if (position < 0)\n            position = 0;\n          addPosition(token);\n          if (++length >= maxFieldLength) {\n            if (threadState.docWriter.infoStream != null)\n              threadState.docWriter.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n            break;\n          }\n        }\n        offset = offsetEnd+1;\n      } finally {\n        stream.close();\n      }\n    }\n\n    boost *= field.getBoost();\n  }\n\n","sourceOld":"  /* Invert one occurrence of one field in the document */\n  public void invertField(Fieldable field, Analyzer analyzer, final int maxFieldLength) throws IOException, AbortException {\n\n    if (length>0)\n      position += analyzer.getPositionIncrementGap(fieldInfo.name);\n\n    if (!field.isTokenized()) {\t\t  // un-tokenized field\n      String stringValue = field.stringValue();\n      final int valueLength = stringValue.length();\n      Token token = localToken;\n      token.clear();\n      char[] termBuffer = token.termBuffer();\n      if (termBuffer.length < valueLength)\n        termBuffer = token.resizeTermBuffer(valueLength);\n      stringValue.getChars(0, valueLength, termBuffer, 0);\n      token.setTermLength(valueLength);\n      token.setStartOffset(offset);\n      token.setEndOffset(offset + stringValue.length());\n      addPosition(token);\n      offset += stringValue.length();\n      length++;\n    } else {                                  // tokenized field\n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          threadState.stringReader.init(stringValue);\n          reader = threadState.stringReader;\n        }\n          \n        // Tokenize field and add to postingTable\n        stream = analyzer.reusableTokenStream(fieldInfo.name, reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      try {\n        offsetEnd = offset-1;\n        for(;;) {\n          Token token = stream.next(localToken);\n          if (token == null) break;\n          position += (token.getPositionIncrement() - 1);\n          // LUCENE-1255: don't allow negative positon\n          if (position < 0)\n            position = 0;\n          addPosition(token);\n          if (++length >= maxFieldLength) {\n            if (threadState.docWriter.infoStream != null)\n              threadState.docWriter.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n            break;\n          }\n        }\n        offset = offsetEnd+1;\n      } finally {\n        stream.close();\n      }\n    }\n\n    boost *= field.getBoost();\n  }\n\n","bugFix":null,"bugIntro":["abfdd5170b43f046dfac9dafd6e12c1a65f3018c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"abfdd5170b43f046dfac9dafd6e12c1a65f3018c","date":1209502915,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriterFieldData#invertField(Fieldable,Analyzer,int).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriterFieldData#invertField(Fieldable,Analyzer,int).mjava","sourceNew":"  /* Invert one occurrence of one field in the document */\n  public void invertField(Fieldable field, Analyzer analyzer, final int maxFieldLength) throws IOException, AbortException {\n\n    if (length>0)\n      position += analyzer.getPositionIncrementGap(fieldInfo.name);\n\n    if (!field.isTokenized()) {\t\t  // un-tokenized field\n      String stringValue = field.stringValue();\n      final int valueLength = stringValue.length();\n      Token token = localToken;\n      token.clear();\n      char[] termBuffer = token.termBuffer();\n      if (termBuffer.length < valueLength)\n        termBuffer = token.resizeTermBuffer(valueLength);\n      stringValue.getChars(0, valueLength, termBuffer, 0);\n      token.setTermLength(valueLength);\n      token.setStartOffset(offset);\n      token.setEndOffset(offset + stringValue.length());\n      addPosition(token);\n      offset += stringValue.length();\n      length++;\n    } else {                                  // tokenized field\n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          threadState.stringReader.init(stringValue);\n          reader = threadState.stringReader;\n        }\n          \n        // Tokenize field and add to postingTable\n        stream = analyzer.reusableTokenStream(fieldInfo.name, reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      try {\n        offsetEnd = offset-1;\n        for(;;) {\n          Token token = stream.next(localToken);\n          if (token == null) break;\n          position += (token.getPositionIncrement() - 1);\n          addPosition(token);\n          if (++length >= maxFieldLength) {\n            if (threadState.docWriter.infoStream != null)\n              threadState.docWriter.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n            break;\n          }\n        }\n        offset = offsetEnd+1;\n      } finally {\n        stream.close();\n      }\n    }\n\n    boost *= field.getBoost();\n  }\n\n","sourceOld":"  /* Invert one occurrence of one field in the document */\n  public void invertField(Fieldable field, Analyzer analyzer, final int maxFieldLength) throws IOException, AbortException {\n\n    if (length>0)\n      position += analyzer.getPositionIncrementGap(fieldInfo.name);\n\n    if (!field.isTokenized()) {\t\t  // un-tokenized field\n      String stringValue = field.stringValue();\n      final int valueLength = stringValue.length();\n      Token token = localToken;\n      token.clear();\n      char[] termBuffer = token.termBuffer();\n      if (termBuffer.length < valueLength)\n        termBuffer = token.resizeTermBuffer(valueLength);\n      stringValue.getChars(0, valueLength, termBuffer, 0);\n      token.setTermLength(valueLength);\n      token.setStartOffset(offset);\n      token.setEndOffset(offset + stringValue.length());\n      addPosition(token);\n      offset += stringValue.length();\n      length++;\n    } else {                                  // tokenized field\n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          threadState.stringReader.init(stringValue);\n          reader = threadState.stringReader;\n        }\n          \n        // Tokenize field and add to postingTable\n        stream = analyzer.reusableTokenStream(fieldInfo.name, reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      try {\n        offsetEnd = offset-1;\n        for(;;) {\n          Token token = stream.next(localToken);\n          if (token == null) break;\n          position += (token.getPositionIncrement() - 1);\n          // LUCENE-1255: don't allow negative position\n          if (position < 0)\n            position = 0;\n          addPosition(token);\n          if (++length >= maxFieldLength) {\n            if (threadState.docWriter.infoStream != null)\n              threadState.docWriter.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n            break;\n          }\n        }\n        offset = offsetEnd+1;\n      } finally {\n        stream.close();\n      }\n    }\n\n    boost *= field.getBoost();\n  }\n\n","bugFix":["f51cb06175d6fae01dc608dd7ab884973354e4bf","e7d928749bc06c6ab30af3c32b7258585a8db4c3"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5350389bf83287111f7760b9e3db3af8e3648474","date":1216372812,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"src/java/org/apache/lucene/index/DocumentsWriterFieldData#invertField(Fieldable,Analyzer,int).mjava","sourceNew":null,"sourceOld":"  /* Invert one occurrence of one field in the document */\n  public void invertField(Fieldable field, Analyzer analyzer, final int maxFieldLength) throws IOException, AbortException {\n\n    if (length>0)\n      position += analyzer.getPositionIncrementGap(fieldInfo.name);\n\n    if (!field.isTokenized()) {\t\t  // un-tokenized field\n      String stringValue = field.stringValue();\n      final int valueLength = stringValue.length();\n      Token token = localToken;\n      token.clear();\n      char[] termBuffer = token.termBuffer();\n      if (termBuffer.length < valueLength)\n        termBuffer = token.resizeTermBuffer(valueLength);\n      stringValue.getChars(0, valueLength, termBuffer, 0);\n      token.setTermLength(valueLength);\n      token.setStartOffset(offset);\n      token.setEndOffset(offset + stringValue.length());\n      addPosition(token);\n      offset += stringValue.length();\n      length++;\n    } else {                                  // tokenized field\n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          threadState.stringReader.init(stringValue);\n          reader = threadState.stringReader;\n        }\n          \n        // Tokenize field and add to postingTable\n        stream = analyzer.reusableTokenStream(fieldInfo.name, reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      try {\n        offsetEnd = offset-1;\n        for(;;) {\n          Token token = stream.next(localToken);\n          if (token == null) break;\n          position += (token.getPositionIncrement() - 1);\n          addPosition(token);\n          if (++length >= maxFieldLength) {\n            if (threadState.docWriter.infoStream != null)\n              threadState.docWriter.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n            break;\n          }\n        }\n        offset = offsetEnd+1;\n      } finally {\n        stream.close();\n      }\n    }\n\n    boost *= field.getBoost();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e7d928749bc06c6ab30af3c32b7258585a8db4c3":["f51cb06175d6fae01dc608dd7ab884973354e4bf"],"abfdd5170b43f046dfac9dafd6e12c1a65f3018c":["e7d928749bc06c6ab30af3c32b7258585a8db4c3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f51cb06175d6fae01dc608dd7ab884973354e4bf":["5a0af3a442be522899177e5e11384a45a6784a3f"],"5a0af3a442be522899177e5e11384a45a6784a3f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5350389bf83287111f7760b9e3db3af8e3648474":["abfdd5170b43f046dfac9dafd6e12c1a65f3018c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["5350389bf83287111f7760b9e3db3af8e3648474"]},"commit2Childs":{"e7d928749bc06c6ab30af3c32b7258585a8db4c3":["abfdd5170b43f046dfac9dafd6e12c1a65f3018c"],"abfdd5170b43f046dfac9dafd6e12c1a65f3018c":["5350389bf83287111f7760b9e3db3af8e3648474"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["5a0af3a442be522899177e5e11384a45a6784a3f"],"f51cb06175d6fae01dc608dd7ab884973354e4bf":["e7d928749bc06c6ab30af3c32b7258585a8db4c3"],"5a0af3a442be522899177e5e11384a45a6784a3f":["f51cb06175d6fae01dc608dd7ab884973354e4bf"],"5350389bf83287111f7760b9e3db3af8e3648474":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}