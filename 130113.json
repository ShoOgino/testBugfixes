{"path":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n        final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n        if (perField.termsHashPerField.bytesHash.size() > 0) {\n          allFields.add(perField);\n        }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.quickSort(allFields);\n\n    final FieldsConsumer consumer = state.codec.postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      FieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n        \n        // Aggregate the storePayload as seen by the same\n        // field across multiple threads\n        if (fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {\n          fieldInfo.storePayloads |= fieldWriter.hasPayloads;\n        }\n        \n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n        final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n        if (perField.termsHashPerField.bytesHash.size() > 0) {\n          allFields.add(perField);\n        }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.quickSort(allFields);\n\n    final FieldsConsumer consumer = state.codec.postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      FieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n        \n        // Aggregate the storePayload as seen by the same\n        // field across multiple threads\n        if (fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {\n          fieldInfo.storePayloads |= fieldWriter.hasPayloads;\n        }\n        \n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d08eba3d52b63561ebf936481ce73e6b6a14aa03","date":1333879759,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n        final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n        if (perField.termsHashPerField.bytesHash.size() > 0) {\n          allFields.add(perField);\n        }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.quickSort(allFields);\n\n    final InvertedFieldsConsumer consumer = state.codec.postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      InvertedFieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n        \n        // Aggregate the storePayload as seen by the same\n        // field across multiple threads\n        if (fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {\n          fieldInfo.storePayloads |= fieldWriter.hasPayloads;\n        }\n        \n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n        final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n        if (perField.termsHashPerField.bytesHash.size() > 0) {\n          allFields.add(perField);\n        }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.quickSort(allFields);\n\n    final FieldsConsumer consumer = state.codec.postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      FieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n        \n        // Aggregate the storePayload as seen by the same\n        // field across multiple threads\n        if (fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {\n          fieldInfo.storePayloads |= fieldWriter.hasPayloads;\n        }\n        \n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf","date":1333892281,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n        final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n        if (perField.termsHashPerField.bytesHash.size() > 0) {\n          allFields.add(perField);\n        }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.quickSort(allFields);\n\n    final FieldsConsumer consumer = state.codec.postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      FieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n        \n        // Aggregate the storePayload as seen by the same\n        // field across multiple threads\n        if (fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {\n          fieldInfo.storePayloads |= fieldWriter.hasPayloads;\n        }\n        \n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n        final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n        if (perField.termsHashPerField.bytesHash.size() > 0) {\n          allFields.add(perField);\n        }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.quickSort(allFields);\n\n    final InvertedFieldsConsumer consumer = state.codec.postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      InvertedFieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n        \n        // Aggregate the storePayload as seen by the same\n        // field across multiple threads\n        if (fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {\n          fieldInfo.storePayloads |= fieldWriter.hasPayloads;\n        }\n        \n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"76923f6a33f2c4bec7f584e3f251261afe7ea276","date":1337149711,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n        final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n        if (perField.termsHashPerField.bytesHash.size() > 0) {\n          allFields.add(perField);\n        }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.quickSort(allFields);\n\n    final FieldsConsumer consumer = state.codec.postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      FieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n        \n        // Aggregate the storePayload as seen by the same\n        // field across multiple threads\n        if (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {\n          if (fieldWriter.hasPayloads) {\n            fieldInfo.setStorePayloads();\n          }\n        }\n        \n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n        final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n        if (perField.termsHashPerField.bytesHash.size() > 0) {\n          allFields.add(perField);\n        }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.quickSort(allFields);\n\n    final FieldsConsumer consumer = state.codec.postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      FieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n        \n        // Aggregate the storePayload as seen by the same\n        // field across multiple threads\n        if (fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {\n          fieldInfo.storePayloads |= fieldWriter.hasPayloads;\n        }\n        \n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cd9165e54429bb5c99e75d5cb1c926cc98772456","date":1337362687,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[String,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(Map<String,TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n        final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n        if (perField.termsHashPerField.bytesHash.size() > 0) {\n          allFields.add(perField);\n        }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.quickSort(allFields);\n\n    final FieldsConsumer consumer = state.codec.postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      FieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n\n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n        final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n        if (perField.termsHashPerField.bytesHash.size() > 0) {\n          allFields.add(perField);\n        }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.quickSort(allFields);\n\n    final FieldsConsumer consumer = state.codec.postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      FieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n        \n        // Aggregate the storePayload as seen by the same\n        // field across multiple threads\n        if (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {\n          if (fieldWriter.hasPayloads) {\n            fieldInfo.setStorePayloads();\n          }\n        }\n        \n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"615ddbd81799980d0fdd95e0238e1c498b6f47b0","date":1338233290,"type":5,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[String,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(Map<String,TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n        final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n        if (perField.termsHashPerField.bytesHash.size() > 0) {\n          allFields.add(perField);\n        }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.quickSort(allFields);\n\n    final FieldsConsumer consumer = state.segmentInfo.getCodec().postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      FieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n\n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n        final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n        if (perField.termsHashPerField.bytesHash.size() > 0) {\n          allFields.add(perField);\n        }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.quickSort(allFields);\n\n    final FieldsConsumer consumer = state.codec.postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      FieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n        \n        // Aggregate the storePayload as seen by the same\n        // field across multiple threads\n        if (fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {\n          fieldInfo.storePayloads |= fieldWriter.hasPayloads;\n        }\n        \n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"76923f6a33f2c4bec7f584e3f251261afe7ea276":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf","cd9165e54429bb5c99e75d5cb1c926cc98772456"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf":["d08eba3d52b63561ebf936481ce73e6b6a14aa03"],"d08eba3d52b63561ebf936481ce73e6b6a14aa03":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd9165e54429bb5c99e75d5cb1c926cc98772456":["76923f6a33f2c4bec7f584e3f251261afe7ea276"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"]},"commit2Childs":{"76923f6a33f2c4bec7f584e3f251261afe7ea276":["cd9165e54429bb5c99e75d5cb1c926cc98772456"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["d08eba3d52b63561ebf936481ce73e6b6a14aa03"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf":["76923f6a33f2c4bec7f584e3f251261afe7ea276","615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"d08eba3d52b63561ebf936481ce73e6b6a14aa03":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf"],"cd9165e54429bb5c99e75d5cb1c926cc98772456":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}