{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/classic/TestClassicAnalyzer#testWickedLongTerm().mjava","commits":[{"id":"313c36388b6cae6118f75a1860ad0ba0af7e1344","date":1601279368,"type":1,"author":"Tomoko Uchida","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/classic/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = new ByteBuffersDirectory();\n    Analyzer analyzer = new ClassicAnalyzer();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(analyzer));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    PostingsEnum tps = MultiTerms.getTermPostingsEnum(reader,\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n    analyzer.close();\n    sa.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = new ByteBuffersDirectory();\n    Analyzer analyzer = new ClassicAnalyzer();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(analyzer));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    PostingsEnum tps = MultiTerms.getTermPostingsEnum(reader,\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n    analyzer.close();\n    sa.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"313c36388b6cae6118f75a1860ad0ba0af7e1344":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["313c36388b6cae6118f75a1860ad0ba0af7e1344"]},"commit2Childs":{"313c36388b6cae6118f75a1860ad0ba0af7e1344":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["313c36388b6cae6118f75a1860ad0ba0af7e1344"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}