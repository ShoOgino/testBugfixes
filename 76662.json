{"path":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#flush().mjava","commits":[{"id":"5e04b732c631a77cbbd25b6ce43c2a8abb1e9e69","date":1352818449,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#flush().mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#flush().mjava","sourceNew":"  private void flush() throws IOException {\n    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());\n\n    // transform end offsets into lengths\n    final int[] lengths = endOffsets;\n    for (int i = numBufferedDocs - 1; i > 0; --i) {\n      lengths[i] = endOffsets[i] - endOffsets[i - 1];\n      assert lengths[i] >= 0;\n    }\n    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths);\n\n    // compress stored fields to fieldsStream\n    compressor.compress(bufferedDocs.bytes, 0, bufferedDocs.length, fieldsStream);\n\n    // reset\n    docBase += numBufferedDocs;\n    numBufferedDocs = 0;\n    bufferedDocs.length = 0;\n  }\n\n","sourceOld":"  private void flush() throws IOException {\n    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());\n\n    // transform end offsets into lengths\n    final int[] lengths = endOffsets;\n    for (int i = numBufferedDocs - 1; i > 0; --i) {\n      lengths[i] = endOffsets[i] - endOffsets[i - 1];\n      assert lengths[i] >= 0;\n    }\n    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths);\n\n    // compress stored fields to fieldsStream\n    compressor.compress(bufferedDocs.bytes, 0, bufferedDocs.length, fieldsStream);\n\n    // reset\n    docBase += numBufferedDocs;\n    numBufferedDocs = 0;\n    bufferedDocs.length = 0;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"407687e67faf6e1f02a211ca078d8e3eed631027","date":1355157407,"type":1,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#flush().mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#flush().mjava","sourceNew":"  private void flush() throws IOException {\n    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());\n\n    // transform end offsets into lengths\n    final int[] lengths = endOffsets;\n    for (int i = numBufferedDocs - 1; i > 0; --i) {\n      lengths[i] = endOffsets[i] - endOffsets[i - 1];\n      assert lengths[i] >= 0;\n    }\n    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths);\n\n    // compress stored fields to fieldsStream\n    compressor.compress(bufferedDocs.bytes, 0, bufferedDocs.length, fieldsStream);\n\n    // reset\n    docBase += numBufferedDocs;\n    numBufferedDocs = 0;\n    bufferedDocs.length = 0;\n  }\n\n","sourceOld":"  private void flush() throws IOException {\n    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());\n\n    // transform end offsets into lengths\n    final int[] lengths = endOffsets;\n    for (int i = numBufferedDocs - 1; i > 0; --i) {\n      lengths[i] = endOffsets[i] - endOffsets[i - 1];\n      assert lengths[i] >= 0;\n    }\n    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths);\n\n    // compress stored fields to fieldsStream\n    compressor.compress(bufferedDocs.bytes, 0, bufferedDocs.length, fieldsStream);\n\n    // reset\n    docBase += numBufferedDocs;\n    numBufferedDocs = 0;\n    bufferedDocs.length = 0;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"47081d784f5fff71bb715c806c824b50901392fb","date":1378303234,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#flush().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#flush().mjava","sourceNew":"  private void flush() throws IOException {\n    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());\n\n    // transform end offsets into lengths\n    final int[] lengths = endOffsets;\n    for (int i = numBufferedDocs - 1; i > 0; --i) {\n      lengths[i] = endOffsets[i] - endOffsets[i - 1];\n      assert lengths[i] >= 0;\n    }\n    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths);\n\n    // compress stored fields to fieldsStream\n    if (bufferedDocs.length >= 2 * chunkSize) {\n      // big chunk, slice it\n      for (int compressed = 0; compressed < bufferedDocs.length; compressed += chunkSize) {\n        compressor.compress(bufferedDocs.bytes, compressed, Math.min(chunkSize, bufferedDocs.length - compressed), fieldsStream);\n      }\n    } else {\n      compressor.compress(bufferedDocs.bytes, 0, bufferedDocs.length, fieldsStream);\n    }\n\n    // reset\n    docBase += numBufferedDocs;\n    numBufferedDocs = 0;\n    bufferedDocs.length = 0;\n  }\n\n","sourceOld":"  private void flush() throws IOException {\n    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());\n\n    // transform end offsets into lengths\n    final int[] lengths = endOffsets;\n    for (int i = numBufferedDocs - 1; i > 0; --i) {\n      lengths[i] = endOffsets[i] - endOffsets[i - 1];\n      assert lengths[i] >= 0;\n    }\n    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths);\n\n    // compress stored fields to fieldsStream\n    compressor.compress(bufferedDocs.bytes, 0, bufferedDocs.length, fieldsStream);\n\n    // reset\n    docBase += numBufferedDocs;\n    numBufferedDocs = 0;\n    bufferedDocs.length = 0;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1f09f483a0844bb9dc34fb10380cb053aa96219b","date":1418894001,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#flush().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#flush().mjava","sourceNew":"  private void flush() throws IOException {\n    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());\n\n    // transform end offsets into lengths\n    final int[] lengths = endOffsets;\n    for (int i = numBufferedDocs - 1; i > 0; --i) {\n      lengths[i] = endOffsets[i] - endOffsets[i - 1];\n      assert lengths[i] >= 0;\n    }\n    final boolean sliced = bufferedDocs.length >= 2 * chunkSize;\n    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths, sliced);\n\n    // compress stored fields to fieldsStream\n    if (sliced) {\n      // big chunk, slice it\n      for (int compressed = 0; compressed < bufferedDocs.length; compressed += chunkSize) {\n        compressor.compress(bufferedDocs.bytes, compressed, Math.min(chunkSize, bufferedDocs.length - compressed), fieldsStream);\n      }\n    } else {\n      compressor.compress(bufferedDocs.bytes, 0, bufferedDocs.length, fieldsStream);\n    }\n\n    // reset\n    docBase += numBufferedDocs;\n    numBufferedDocs = 0;\n    bufferedDocs.length = 0;\n  }\n\n","sourceOld":"  private void flush() throws IOException {\n    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());\n\n    // transform end offsets into lengths\n    final int[] lengths = endOffsets;\n    for (int i = numBufferedDocs - 1; i > 0; --i) {\n      lengths[i] = endOffsets[i] - endOffsets[i - 1];\n      assert lengths[i] >= 0;\n    }\n    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths);\n\n    // compress stored fields to fieldsStream\n    if (bufferedDocs.length >= 2 * chunkSize) {\n      // big chunk, slice it\n      for (int compressed = 0; compressed < bufferedDocs.length; compressed += chunkSize) {\n        compressor.compress(bufferedDocs.bytes, compressed, Math.min(chunkSize, bufferedDocs.length - compressed), fieldsStream);\n      }\n    } else {\n      compressor.compress(bufferedDocs.bytes, 0, bufferedDocs.length, fieldsStream);\n    }\n\n    // reset\n    docBase += numBufferedDocs;\n    numBufferedDocs = 0;\n    bufferedDocs.length = 0;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bd7962f4da329a4e559727022b752c5cefaee5da","date":1421356185,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#flush().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#flush().mjava","sourceNew":"  private void flush() throws IOException {\n    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());\n\n    // transform end offsets into lengths\n    final int[] lengths = endOffsets;\n    for (int i = numBufferedDocs - 1; i > 0; --i) {\n      lengths[i] = endOffsets[i] - endOffsets[i - 1];\n      assert lengths[i] >= 0;\n    }\n    final boolean sliced = bufferedDocs.length >= 2 * chunkSize;\n    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths, sliced);\n\n    // compress stored fields to fieldsStream\n    if (sliced) {\n      // big chunk, slice it\n      for (int compressed = 0; compressed < bufferedDocs.length; compressed += chunkSize) {\n        compressor.compress(bufferedDocs.bytes, compressed, Math.min(chunkSize, bufferedDocs.length - compressed), fieldsStream);\n      }\n    } else {\n      compressor.compress(bufferedDocs.bytes, 0, bufferedDocs.length, fieldsStream);\n    }\n\n    // reset\n    docBase += numBufferedDocs;\n    numBufferedDocs = 0;\n    bufferedDocs.length = 0;\n    numChunks++;\n  }\n\n","sourceOld":"  private void flush() throws IOException {\n    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());\n\n    // transform end offsets into lengths\n    final int[] lengths = endOffsets;\n    for (int i = numBufferedDocs - 1; i > 0; --i) {\n      lengths[i] = endOffsets[i] - endOffsets[i - 1];\n      assert lengths[i] >= 0;\n    }\n    final boolean sliced = bufferedDocs.length >= 2 * chunkSize;\n    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths, sliced);\n\n    // compress stored fields to fieldsStream\n    if (sliced) {\n      // big chunk, slice it\n      for (int compressed = 0; compressed < bufferedDocs.length; compressed += chunkSize) {\n        compressor.compress(bufferedDocs.bytes, compressed, Math.min(chunkSize, bufferedDocs.length - compressed), fieldsStream);\n      }\n    } else {\n      compressor.compress(bufferedDocs.bytes, 0, bufferedDocs.length, fieldsStream);\n    }\n\n    // reset\n    docBase += numBufferedDocs;\n    numBufferedDocs = 0;\n    bufferedDocs.length = 0;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c4ad863d796f4e72a3a1ef4bacd2e19c3e9258c9","date":1481155163,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#flush().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#flush().mjava","sourceNew":"  private void flush() throws IOException {\n    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());\n\n    // transform end offsets into lengths\n    final int[] lengths = endOffsets;\n    for (int i = numBufferedDocs - 1; i > 0; --i) {\n      lengths[i] = endOffsets[i] - endOffsets[i - 1];\n      assert lengths[i] >= 0;\n    }\n    final boolean sliced = bufferedDocs.getPosition() >= 2 * chunkSize;\n    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths, sliced);\n\n    // compress stored fields to fieldsStream\n    if (sliced) {\n      // big chunk, slice it\n      for (int compressed = 0; compressed < bufferedDocs.getPosition(); compressed += chunkSize) {\n        compressor.compress(bufferedDocs.getBytes(), compressed, Math.min(chunkSize, bufferedDocs.getPosition() - compressed), fieldsStream);\n      }\n    } else {\n      compressor.compress(bufferedDocs.getBytes(), 0, bufferedDocs.getPosition(), fieldsStream);\n    }\n\n    // reset\n    docBase += numBufferedDocs;\n    numBufferedDocs = 0;\n    bufferedDocs.reset();\n    numChunks++;\n  }\n\n","sourceOld":"  private void flush() throws IOException {\n    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());\n\n    // transform end offsets into lengths\n    final int[] lengths = endOffsets;\n    for (int i = numBufferedDocs - 1; i > 0; --i) {\n      lengths[i] = endOffsets[i] - endOffsets[i - 1];\n      assert lengths[i] >= 0;\n    }\n    final boolean sliced = bufferedDocs.length >= 2 * chunkSize;\n    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths, sliced);\n\n    // compress stored fields to fieldsStream\n    if (sliced) {\n      // big chunk, slice it\n      for (int compressed = 0; compressed < bufferedDocs.length; compressed += chunkSize) {\n        compressor.compress(bufferedDocs.bytes, compressed, Math.min(chunkSize, bufferedDocs.length - compressed), fieldsStream);\n      }\n    } else {\n      compressor.compress(bufferedDocs.bytes, 0, bufferedDocs.length, fieldsStream);\n    }\n\n    // reset\n    docBase += numBufferedDocs;\n    numBufferedDocs = 0;\n    bufferedDocs.length = 0;\n    numChunks++;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9856095f7afb5a607bf5e65077615ed91273508c","date":1481837697,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#flush().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#flush().mjava","sourceNew":"  private void flush() throws IOException {\n    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());\n\n    // transform end offsets into lengths\n    final int[] lengths = endOffsets;\n    for (int i = numBufferedDocs - 1; i > 0; --i) {\n      lengths[i] = endOffsets[i] - endOffsets[i - 1];\n      assert lengths[i] >= 0;\n    }\n    final boolean sliced = bufferedDocs.getPosition() >= 2 * chunkSize;\n    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths, sliced);\n\n    // compress stored fields to fieldsStream\n    if (sliced) {\n      // big chunk, slice it\n      for (int compressed = 0; compressed < bufferedDocs.getPosition(); compressed += chunkSize) {\n        compressor.compress(bufferedDocs.getBytes(), compressed, Math.min(chunkSize, bufferedDocs.getPosition() - compressed), fieldsStream);\n      }\n    } else {\n      compressor.compress(bufferedDocs.getBytes(), 0, bufferedDocs.getPosition(), fieldsStream);\n    }\n\n    // reset\n    docBase += numBufferedDocs;\n    numBufferedDocs = 0;\n    bufferedDocs.reset();\n    numChunks++;\n  }\n\n","sourceOld":"  private void flush() throws IOException {\n    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());\n\n    // transform end offsets into lengths\n    final int[] lengths = endOffsets;\n    for (int i = numBufferedDocs - 1; i > 0; --i) {\n      lengths[i] = endOffsets[i] - endOffsets[i - 1];\n      assert lengths[i] >= 0;\n    }\n    final boolean sliced = bufferedDocs.length >= 2 * chunkSize;\n    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths, sliced);\n\n    // compress stored fields to fieldsStream\n    if (sliced) {\n      // big chunk, slice it\n      for (int compressed = 0; compressed < bufferedDocs.length; compressed += chunkSize) {\n        compressor.compress(bufferedDocs.bytes, compressed, Math.min(chunkSize, bufferedDocs.length - compressed), fieldsStream);\n      }\n    } else {\n      compressor.compress(bufferedDocs.bytes, 0, bufferedDocs.length, fieldsStream);\n    }\n\n    // reset\n    docBase += numBufferedDocs;\n    numBufferedDocs = 0;\n    bufferedDocs.length = 0;\n    numChunks++;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"409da428f28953cf35fddd5c9ff5c7e4f5439863","date":1547556145,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#flush().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#flush().mjava","sourceNew":"  private void flush() throws IOException {\n    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());\n\n    // transform end offsets into lengths\n    final int[] lengths = endOffsets;\n    for (int i = numBufferedDocs - 1; i > 0; --i) {\n      lengths[i] = endOffsets[i] - endOffsets[i - 1];\n      assert lengths[i] >= 0;\n    }\n    final boolean sliced = bufferedDocs.size() >= 2 * chunkSize;\n    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths, sliced);\n\n    // compress stored fields to fieldsStream.\n    //\n    // TODO: do we need to slice it since we already have the slices in the buffer? Perhaps\n    // we should use max-block-bits restriction on the buffer itself, then we won't have to check it here.\n    byte [] content = bufferedDocs.toArrayCopy();\n    bufferedDocs.reset();\n\n    if (sliced) {\n      // big chunk, slice it\n      for (int compressed = 0; compressed < content.length; compressed += chunkSize) {\n        compressor.compress(content, compressed, Math.min(chunkSize, content.length - compressed), fieldsStream);\n      }\n    } else {\n      compressor.compress(content, 0, content.length, fieldsStream);\n    }\n\n    // reset\n    docBase += numBufferedDocs;\n    numBufferedDocs = 0;\n    bufferedDocs.reset();\n    numChunks++;\n  }\n\n","sourceOld":"  private void flush() throws IOException {\n    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());\n\n    // transform end offsets into lengths\n    final int[] lengths = endOffsets;\n    for (int i = numBufferedDocs - 1; i > 0; --i) {\n      lengths[i] = endOffsets[i] - endOffsets[i - 1];\n      assert lengths[i] >= 0;\n    }\n    final boolean sliced = bufferedDocs.getPosition() >= 2 * chunkSize;\n    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths, sliced);\n\n    // compress stored fields to fieldsStream\n    if (sliced) {\n      // big chunk, slice it\n      for (int compressed = 0; compressed < bufferedDocs.getPosition(); compressed += chunkSize) {\n        compressor.compress(bufferedDocs.getBytes(), compressed, Math.min(chunkSize, bufferedDocs.getPosition() - compressed), fieldsStream);\n      }\n    } else {\n      compressor.compress(bufferedDocs.getBytes(), 0, bufferedDocs.getPosition(), fieldsStream);\n    }\n\n    // reset\n    docBase += numBufferedDocs;\n    numBufferedDocs = 0;\n    bufferedDocs.reset();\n    numChunks++;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"45264aed0cfa8a8a55ae1292b0e336d29cd88401","date":1600361948,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#flush().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#flush().mjava","sourceNew":"  private void flush() throws IOException {\n    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());\n\n    // transform end offsets into lengths\n    final int[] lengths = endOffsets;\n    for (int i = numBufferedDocs - 1; i > 0; --i) {\n      lengths[i] = endOffsets[i] - endOffsets[i - 1];\n      assert lengths[i] >= 0;\n    }\n    final boolean sliced = bufferedDocs.size() >= 2 * chunkSize;\n    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths, sliced);\n\n    // compress stored fields to fieldsStream.\n    //\n    // TODO: do we need to slice it since we already have the slices in the buffer? Perhaps\n    // we should use max-block-bits restriction on the buffer itself, then we won't have to check it here.\n    byte [] content = bufferedDocs.toArrayCopy();\n    bufferedDocs.reset();\n\n    if (sliced) {\n      // big chunk, slice it\n      for (int compressed = 0; compressed < content.length; compressed += chunkSize) {\n        compressor.compress(content, compressed, Math.min(chunkSize, content.length - compressed), fieldsStream);\n      }\n    } else {\n      compressor.compress(content, 0, content.length, fieldsStream);\n    }\n\n    // reset\n    docBase += numBufferedDocs;\n    numBufferedDocs = 0;\n    bufferedDocs.reset();\n  }\n\n","sourceOld":"  private void flush() throws IOException {\n    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());\n\n    // transform end offsets into lengths\n    final int[] lengths = endOffsets;\n    for (int i = numBufferedDocs - 1; i > 0; --i) {\n      lengths[i] = endOffsets[i] - endOffsets[i - 1];\n      assert lengths[i] >= 0;\n    }\n    final boolean sliced = bufferedDocs.size() >= 2 * chunkSize;\n    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths, sliced);\n\n    // compress stored fields to fieldsStream.\n    //\n    // TODO: do we need to slice it since we already have the slices in the buffer? Perhaps\n    // we should use max-block-bits restriction on the buffer itself, then we won't have to check it here.\n    byte [] content = bufferedDocs.toArrayCopy();\n    bufferedDocs.reset();\n\n    if (sliced) {\n      // big chunk, slice it\n      for (int compressed = 0; compressed < content.length; compressed += chunkSize) {\n        compressor.compress(content, compressed, Math.min(chunkSize, content.length - compressed), fieldsStream);\n      }\n    } else {\n      compressor.compress(content, 0, content.length, fieldsStream);\n    }\n\n    // reset\n    docBase += numBufferedDocs;\n    numBufferedDocs = 0;\n    bufferedDocs.reset();\n    numChunks++;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"409da428f28953cf35fddd5c9ff5c7e4f5439863":["c4ad863d796f4e72a3a1ef4bacd2e19c3e9258c9"],"5e04b732c631a77cbbd25b6ce43c2a8abb1e9e69":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"407687e67faf6e1f02a211ca078d8e3eed631027":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","5e04b732c631a77cbbd25b6ce43c2a8abb1e9e69"],"1f09f483a0844bb9dc34fb10380cb053aa96219b":["47081d784f5fff71bb715c806c824b50901392fb"],"bd7962f4da329a4e559727022b752c5cefaee5da":["1f09f483a0844bb9dc34fb10380cb053aa96219b"],"45264aed0cfa8a8a55ae1292b0e336d29cd88401":["409da428f28953cf35fddd5c9ff5c7e4f5439863"],"9856095f7afb5a607bf5e65077615ed91273508c":["bd7962f4da329a4e559727022b752c5cefaee5da","c4ad863d796f4e72a3a1ef4bacd2e19c3e9258c9"],"47081d784f5fff71bb715c806c824b50901392fb":["5e04b732c631a77cbbd25b6ce43c2a8abb1e9e69"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["45264aed0cfa8a8a55ae1292b0e336d29cd88401"],"c4ad863d796f4e72a3a1ef4bacd2e19c3e9258c9":["bd7962f4da329a4e559727022b752c5cefaee5da"]},"commit2Childs":{"409da428f28953cf35fddd5c9ff5c7e4f5439863":["45264aed0cfa8a8a55ae1292b0e336d29cd88401"],"5e04b732c631a77cbbd25b6ce43c2a8abb1e9e69":["407687e67faf6e1f02a211ca078d8e3eed631027","47081d784f5fff71bb715c806c824b50901392fb"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["5e04b732c631a77cbbd25b6ce43c2a8abb1e9e69","407687e67faf6e1f02a211ca078d8e3eed631027"],"407687e67faf6e1f02a211ca078d8e3eed631027":[],"1f09f483a0844bb9dc34fb10380cb053aa96219b":["bd7962f4da329a4e559727022b752c5cefaee5da"],"bd7962f4da329a4e559727022b752c5cefaee5da":["9856095f7afb5a607bf5e65077615ed91273508c","c4ad863d796f4e72a3a1ef4bacd2e19c3e9258c9"],"45264aed0cfa8a8a55ae1292b0e336d29cd88401":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"9856095f7afb5a607bf5e65077615ed91273508c":[],"47081d784f5fff71bb715c806c824b50901392fb":["1f09f483a0844bb9dc34fb10380cb053aa96219b"],"c4ad863d796f4e72a3a1ef4bacd2e19c3e9258c9":["409da428f28953cf35fddd5c9ff5c7e4f5439863","9856095f7afb5a607bf5e65077615ed91273508c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["407687e67faf6e1f02a211ca078d8e3eed631027","9856095f7afb5a607bf5e65077615ed91273508c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}