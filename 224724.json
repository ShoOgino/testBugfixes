{"path":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","pathOld":"src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","sourceNew":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new WhitespaceAnalyzer(TEST_VERSION_CURRENT));\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    for(int i=0;i<35;i++) {\n      if (!reader.isDeleted(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (!oldName.startsWith(\"19.\") &&\n            !oldName.startsWith(\"20.\") &&\n            !oldName.startsWith(\"21.\") &&\n            !oldName.startsWith(\"22.\")) {\n\n          if (d.getField(\"content3\") == null) {\n            final int numFields = oldName.startsWith(\"29.\") ? 7 : 5;\n            assertEquals(numFields, fields.size());\n            Field f =  d.getField(\"id\");\n            assertEquals(\"\"+i, f.stringValue());\n\n            f = d.getField(\"utf8\");\n            assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n            f =  d.getField(\"autf8\");\n            assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n        \n            f = d.getField(\"content2\");\n            assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n            f = d.getField(\"fie\\u2C77ld\");\n            assertEquals(\"field with non-ascii name\", f.stringValue());\n          }\n        }       \n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    testHits(hits, 34, searcher.getIndexReader());\n\n    if (!oldName.startsWith(\"19.\") &&\n        !oldName.startsWith(\"20.\") &&\n        !oldName.startsWith(\"21.\") &&\n        !oldName.startsWith(\"22.\")) {\n      // Test on indices >= 2.3\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n    }\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new WhitespaceAnalyzer(TEST_VERSION_CURRENT));\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    for(int i=0;i<35;i++) {\n      if (!reader.isDeleted(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (!oldName.startsWith(\"19.\") &&\n            !oldName.startsWith(\"20.\") &&\n            !oldName.startsWith(\"21.\") &&\n            !oldName.startsWith(\"22.\")) {\n\n          if (d.getField(\"content3\") == null) {\n            final int numFields = oldName.startsWith(\"29.\") ? 7 : 5;\n            assertEquals(numFields, fields.size());\n            Field f =  d.getField(\"id\");\n            assertEquals(\"\"+i, f.stringValue());\n\n            f = d.getField(\"utf8\");\n            assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n            f =  d.getField(\"autf8\");\n            assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n        \n            f = d.getField(\"content2\");\n            assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n            f = d.getField(\"fie\\u2C77ld\");\n            assertEquals(\"field with non-ascii name\", f.stringValue());\n          }\n        }       \n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    testHits(hits, 34, searcher.getIndexReader());\n\n    if (!oldName.startsWith(\"19.\") &&\n        !oldName.startsWith(\"20.\") &&\n        !oldName.startsWith(\"21.\") &&\n        !oldName.startsWith(\"22.\")) {\n      // Test on indices >= 2.3\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n    }\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"955c32f886db6f6356c9fcdea6b1f1cb4effda24","date":1270581567,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","sourceNew":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new WhitespaceAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    for(int i=0;i<35;i++) {\n      if (!reader.isDeleted(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (!oldName.startsWith(\"19.\") &&\n            !oldName.startsWith(\"20.\") &&\n            !oldName.startsWith(\"21.\") &&\n            !oldName.startsWith(\"22.\")) {\n\n          if (d.getField(\"content3\") == null) {\n            final int numFields = oldName.startsWith(\"29.\") ? 7 : 5;\n            assertEquals(numFields, fields.size());\n            Field f =  d.getField(\"id\");\n            assertEquals(\"\"+i, f.stringValue());\n\n            f = d.getField(\"utf8\");\n            assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n            f =  d.getField(\"autf8\");\n            assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n        \n            f = d.getField(\"content2\");\n            assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n            f = d.getField(\"fie\\u2C77ld\");\n            assertEquals(\"field with non-ascii name\", f.stringValue());\n          }\n        }       \n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    if (!oldName.startsWith(\"19.\") &&\n        !oldName.startsWith(\"20.\") &&\n        !oldName.startsWith(\"21.\") &&\n        !oldName.startsWith(\"22.\")) {\n      // Test on indices >= 2.3\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n    }\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new WhitespaceAnalyzer(TEST_VERSION_CURRENT));\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    for(int i=0;i<35;i++) {\n      if (!reader.isDeleted(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (!oldName.startsWith(\"19.\") &&\n            !oldName.startsWith(\"20.\") &&\n            !oldName.startsWith(\"21.\") &&\n            !oldName.startsWith(\"22.\")) {\n\n          if (d.getField(\"content3\") == null) {\n            final int numFields = oldName.startsWith(\"29.\") ? 7 : 5;\n            assertEquals(numFields, fields.size());\n            Field f =  d.getField(\"id\");\n            assertEquals(\"\"+i, f.stringValue());\n\n            f = d.getField(\"utf8\");\n            assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n            f =  d.getField(\"autf8\");\n            assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n        \n            f = d.getField(\"content2\");\n            assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n            f = d.getField(\"fie\\u2C77ld\");\n            assertEquals(\"field with non-ascii name\", f.stringValue());\n          }\n        }       \n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    testHits(hits, 34, searcher.getIndexReader());\n\n    if (!oldName.startsWith(\"19.\") &&\n        !oldName.startsWith(\"20.\") &&\n        !oldName.startsWith(\"21.\") &&\n        !oldName.startsWith(\"22.\")) {\n      // Test on indices >= 2.3\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n    }\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d572389229127c297dd1fa5ce4758e1cec41e799","date":1273610938,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","sourceNew":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    for(int i=0;i<35;i++) {\n      if (!reader.isDeleted(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (!oldName.startsWith(\"19.\") &&\n            !oldName.startsWith(\"20.\") &&\n            !oldName.startsWith(\"21.\") &&\n            !oldName.startsWith(\"22.\")) {\n\n          if (d.getField(\"content3\") == null) {\n            final int numFields = oldName.startsWith(\"29.\") ? 7 : 5;\n            assertEquals(numFields, fields.size());\n            Field f =  d.getField(\"id\");\n            assertEquals(\"\"+i, f.stringValue());\n\n            f = d.getField(\"utf8\");\n            assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n            f =  d.getField(\"autf8\");\n            assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n        \n            f = d.getField(\"content2\");\n            assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n            f = d.getField(\"fie\\u2C77ld\");\n            assertEquals(\"field with non-ascii name\", f.stringValue());\n          }\n        }       \n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    if (!oldName.startsWith(\"19.\") &&\n        !oldName.startsWith(\"20.\") &&\n        !oldName.startsWith(\"21.\") &&\n        !oldName.startsWith(\"22.\")) {\n      // Test on indices >= 2.3\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n    }\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new WhitespaceAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    for(int i=0;i<35;i++) {\n      if (!reader.isDeleted(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (!oldName.startsWith(\"19.\") &&\n            !oldName.startsWith(\"20.\") &&\n            !oldName.startsWith(\"21.\") &&\n            !oldName.startsWith(\"22.\")) {\n\n          if (d.getField(\"content3\") == null) {\n            final int numFields = oldName.startsWith(\"29.\") ? 7 : 5;\n            assertEquals(numFields, fields.size());\n            Field f =  d.getField(\"id\");\n            assertEquals(\"\"+i, f.stringValue());\n\n            f = d.getField(\"utf8\");\n            assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n            f =  d.getField(\"autf8\");\n            assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n        \n            f = d.getField(\"content2\");\n            assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n            f = d.getField(\"fie\\u2C77ld\");\n            assertEquals(\"field with non-ascii name\", f.stringValue());\n          }\n        }       \n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    if (!oldName.startsWith(\"19.\") &&\n        !oldName.startsWith(\"20.\") &&\n        !oldName.startsWith(\"21.\") &&\n        !oldName.startsWith(\"22.\")) {\n      // Test on indices >= 2.3\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n    }\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6267e1ce56c2eec111425690cd04e251b6f14952","date":1275222352,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","sourceNew":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    for(int i=0;i<35;i++) {\n      if (!reader.isDeleted(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (!oldName.startsWith(\"19.\") &&\n            !oldName.startsWith(\"20.\") &&\n            !oldName.startsWith(\"21.\") &&\n            !oldName.startsWith(\"22.\")) {\n\n          if (d.getField(\"content3\") == null) {\n            final int numFields = 5;\n            assertEquals(numFields, fields.size());\n            Field f =  d.getField(\"id\");\n            assertEquals(\"\"+i, f.stringValue());\n\n            f = d.getField(\"utf8\");\n            assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n            f =  d.getField(\"autf8\");\n            assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n        \n            f = d.getField(\"content2\");\n            assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n            f = d.getField(\"fie\\u2C77ld\");\n            assertEquals(\"field with non-ascii name\", f.stringValue());\n          }\n        }       \n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    if (!oldName.startsWith(\"19.\") &&\n        !oldName.startsWith(\"20.\") &&\n        !oldName.startsWith(\"21.\") &&\n        !oldName.startsWith(\"22.\")) {\n      // Test on indices >= 2.3\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n    }\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    for(int i=0;i<35;i++) {\n      if (!reader.isDeleted(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (!oldName.startsWith(\"19.\") &&\n            !oldName.startsWith(\"20.\") &&\n            !oldName.startsWith(\"21.\") &&\n            !oldName.startsWith(\"22.\")) {\n\n          if (d.getField(\"content3\") == null) {\n            final int numFields = oldName.startsWith(\"29.\") ? 7 : 5;\n            assertEquals(numFields, fields.size());\n            Field f =  d.getField(\"id\");\n            assertEquals(\"\"+i, f.stringValue());\n\n            f = d.getField(\"utf8\");\n            assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n            f =  d.getField(\"autf8\");\n            assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n        \n            f = d.getField(\"content2\");\n            assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n            f = d.getField(\"fie\\u2C77ld\");\n            assertEquals(\"field with non-ascii name\", f.stringValue());\n          }\n        }       \n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    if (!oldName.startsWith(\"19.\") &&\n        !oldName.startsWith(\"20.\") &&\n        !oldName.startsWith(\"21.\") &&\n        !oldName.startsWith(\"22.\")) {\n      // Test on indices >= 2.3\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n    }\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"55dab1706ec26d12463d6b7bbda7b211cb7e528d","date":1277504536,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","sourceNew":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    for(int i=0;i<35;i++) {\n      if (!reader.isDeleted(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    for(int i=0;i<35;i++) {\n      if (!reader.isDeleted(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (!oldName.startsWith(\"19.\") &&\n            !oldName.startsWith(\"20.\") &&\n            !oldName.startsWith(\"21.\") &&\n            !oldName.startsWith(\"22.\")) {\n\n          if (d.getField(\"content3\") == null) {\n            final int numFields = 5;\n            assertEquals(numFields, fields.size());\n            Field f =  d.getField(\"id\");\n            assertEquals(\"\"+i, f.stringValue());\n\n            f = d.getField(\"utf8\");\n            assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n            f =  d.getField(\"autf8\");\n            assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n        \n            f = d.getField(\"content2\");\n            assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n            f = d.getField(\"fie\\u2C77ld\");\n            assertEquals(\"field with non-ascii name\", f.stringValue());\n          }\n        }       \n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    if (!oldName.startsWith(\"19.\") &&\n        !oldName.startsWith(\"20.\") &&\n        !oldName.startsWith(\"21.\") &&\n        !oldName.startsWith(\"22.\")) {\n      // Test on indices >= 2.3\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n    }\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","sourceNew":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    for(int i=0;i<35;i++) {\n      if (!reader.isDeleted(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    for(int i=0;i<35;i++) {\n      if (!reader.isDeleted(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (!oldName.startsWith(\"19.\") &&\n            !oldName.startsWith(\"20.\") &&\n            !oldName.startsWith(\"21.\") &&\n            !oldName.startsWith(\"22.\")) {\n\n          if (d.getField(\"content3\") == null) {\n            final int numFields = 5;\n            assertEquals(numFields, fields.size());\n            Field f =  d.getField(\"id\");\n            assertEquals(\"\"+i, f.stringValue());\n\n            f = d.getField(\"utf8\");\n            assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n            f =  d.getField(\"autf8\");\n            assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n        \n            f = d.getField(\"content2\");\n            assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n            f = d.getField(\"fie\\u2C77ld\");\n            assertEquals(\"field with non-ascii name\", f.stringValue());\n          }\n        }       \n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    if (!oldName.startsWith(\"19.\") &&\n        !oldName.startsWith(\"20.\") &&\n        !oldName.startsWith(\"21.\") &&\n        !oldName.startsWith(\"22.\")) {\n      // Test on indices >= 2.3\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n      hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n      assertEquals(34, hits.length);\n    }\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"406e7055a3e99d3fa6ce49a555a51dd18b321806","date":1282520243,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","sourceNew":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    for(int i=0;i<35;i++) {\n      if (!reader.isDeleted(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":["c5c0bd3bf61809aea862d848dcf2119d3b9c38bf"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a13a126d15299d5c1e117ea99ddae6fb0fa3f209","date":1291909583,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","sourceNew":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"30ca900054c38836c7c167379e300af4dabb34c3","date":1292602599,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","sourceNew":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + dirName, tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","sourceNew":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + dirName, tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","sourceNew":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + dirName, tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    for(int i=0;i<35;i++) {\n      if (!reader.isDeleted(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8b241ea5e635d896cc0af83cd96ffd0322e0aba7","date":1294226200,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","sourceNew":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + dirName, tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + dirName, tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"70ad682703b8585f5d0a637efec044d57ec05efb","date":1294259117,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","sourceNew":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + dirName, tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + dirName, tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","sourceNew":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + dirName, tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + dirName, tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.doc(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cdad2c6b6234338031bcc1f24c001a5ad66f714","date":1296866109,"type":5,"author":"Doron Cohen","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","sourceNew":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + dirName, tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":4,"author":"Simon Willnauer","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","sourceNew":null,"sourceOld":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + dirName, tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":5,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","sourceNew":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + dirName, tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a13a126d15299d5c1e117ea99ddae6fb0fa3f209":["406e7055a3e99d3fa6ce49a555a51dd18b321806"],"70ad682703b8585f5d0a637efec044d57ec05efb":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","8b241ea5e635d896cc0af83cd96ffd0322e0aba7"],"55dab1706ec26d12463d6b7bbda7b211cb7e528d":["6267e1ce56c2eec111425690cd04e251b6f14952"],"3cdad2c6b6234338031bcc1f24c001a5ad66f714":["8b241ea5e635d896cc0af83cd96ffd0322e0aba7"],"d572389229127c297dd1fa5ce4758e1cec41e799":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["406e7055a3e99d3fa6ce49a555a51dd18b321806","30ca900054c38836c7c167379e300af4dabb34c3"],"5f4e87790277826a2aea119328600dfb07761f32":["6267e1ce56c2eec111425690cd04e251b6f14952","55dab1706ec26d12463d6b7bbda7b211cb7e528d"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["5f4e87790277826a2aea119328600dfb07761f32","30ca900054c38836c7c167379e300af4dabb34c3"],"6267e1ce56c2eec111425690cd04e251b6f14952":["d572389229127c297dd1fa5ce4758e1cec41e799"],"8b241ea5e635d896cc0af83cd96ffd0322e0aba7":["30ca900054c38836c7c167379e300af4dabb34c3"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["70ad682703b8585f5d0a637efec044d57ec05efb","3cdad2c6b6234338031bcc1f24c001a5ad66f714"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"30ca900054c38836c7c167379e300af4dabb34c3":["a13a126d15299d5c1e117ea99ddae6fb0fa3f209"],"406e7055a3e99d3fa6ce49a555a51dd18b321806":["55dab1706ec26d12463d6b7bbda7b211cb7e528d"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","8b241ea5e635d896cc0af83cd96ffd0322e0aba7"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["868da859b43505d9d2a023bfeae6dd0c795f5295","3cdad2c6b6234338031bcc1f24c001a5ad66f714"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3cdad2c6b6234338031bcc1f24c001a5ad66f714"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"a13a126d15299d5c1e117ea99ddae6fb0fa3f209":["30ca900054c38836c7c167379e300af4dabb34c3"],"70ad682703b8585f5d0a637efec044d57ec05efb":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"55dab1706ec26d12463d6b7bbda7b211cb7e528d":["5f4e87790277826a2aea119328600dfb07761f32","406e7055a3e99d3fa6ce49a555a51dd18b321806"],"3cdad2c6b6234338031bcc1f24c001a5ad66f714":["29ef99d61cda9641b6250bf9567329a6e65f901d","bde51b089eb7f86171eb3406e38a274743f9b7ac","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["70ad682703b8585f5d0a637efec044d57ec05efb"],"d572389229127c297dd1fa5ce4758e1cec41e799":["6267e1ce56c2eec111425690cd04e251b6f14952"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["d572389229127c297dd1fa5ce4758e1cec41e799"],"5f4e87790277826a2aea119328600dfb07761f32":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["868da859b43505d9d2a023bfeae6dd0c795f5295"],"6267e1ce56c2eec111425690cd04e251b6f14952":["55dab1706ec26d12463d6b7bbda7b211cb7e528d","5f4e87790277826a2aea119328600dfb07761f32"],"8b241ea5e635d896cc0af83cd96ffd0322e0aba7":["70ad682703b8585f5d0a637efec044d57ec05efb","3cdad2c6b6234338031bcc1f24c001a5ad66f714","868da859b43505d9d2a023bfeae6dd0c795f5295"],"29ef99d61cda9641b6250bf9567329a6e65f901d":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"30ca900054c38836c7c167379e300af4dabb34c3":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","8b241ea5e635d896cc0af83cd96ffd0322e0aba7"],"406e7055a3e99d3fa6ce49a555a51dd18b321806":["a13a126d15299d5c1e117ea99ddae6fb0fa3f209","ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["29ef99d61cda9641b6250bf9567329a6e65f901d","bde51b089eb7f86171eb3406e38a274743f9b7ac","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}