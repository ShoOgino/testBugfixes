{"path":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean).mjava","pathOld":"lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 4)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        case 2:\n          text = _TestUtil.randomHtmlishString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n\n      if (VERBOSE) {\n        System.out.println(\"NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(\"NOTE: BaseTokenStreamTestCase: re-run analysis\");\n        }\n        reader = new StringReader(text);\n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 4)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        case 2:\n          text = _TestUtil.randomHtmlishString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n\n      if (VERBOSE) {\n        System.out.println(\"NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(\"NOTE: BaseTokenStreamTestCase: re-run analysis\");\n        }\n        reader = new StringReader(text);\n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"47777586dd4c026834be0b2cc454d527cf8884b3","date":1330348390,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple) throws IOException {\n    checkRandomData(random, a, iterations, maxWordLength, random.nextBoolean(), simple);\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 4)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        case 2:\n          text = _TestUtil.randomHtmlishString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n\n      if (VERBOSE) {\n        System.out.println(\"NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(\"NOTE: BaseTokenStreamTestCase: re-run analysis\");\n        }\n        reader = new StringReader(text);\n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","date":1331075828,"type":3,"author":"Ryan McKinley","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple) throws IOException {\n    checkRandomData(random, a, iterations, maxWordLength, random.nextBoolean(), simple);\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 4)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        case 2:\n          text = _TestUtil.randomHtmlishString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n\n      if (VERBOSE) {\n        System.out.println(\"NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(\"NOTE: BaseTokenStreamTestCase: re-run analysis\");\n        }\n        reader = new StringReader(text);\n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"26bd77fc30a420a3e33c85e6fa6b0887eac4b029","date":1332293004,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple) throws IOException {\n    checkRandomData(random, a, iterations, maxWordLength, random.nextBoolean(), simple);\n    // now test with multiple threads\n    int numThreads = _TestUtil.nextInt(random, 4, 8);\n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new AnalysisThread(new Random(random.nextLong()), a, iterations, maxWordLength, simple);\n    }\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < threads.length; i++) {\n      try {\n        threads[i].join();\n      } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple) throws IOException {\n    checkRandomData(random, a, iterations, maxWordLength, random.nextBoolean(), simple);\n  }\n\n","bugFix":null,"bugIntro":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"888c2d6bca1edd8d9293631d6e1d188b036e0f05","date":1334076894,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple) throws IOException {\n    checkRandomData(random, a, iterations, maxWordLength, simple, true);\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple) throws IOException {\n    checkRandomData(random, a, iterations, maxWordLength, random.nextBoolean(), simple);\n    // now test with multiple threads\n    int numThreads = _TestUtil.nextInt(random, 4, 8);\n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new AnalysisThread(new Random(random.nextLong()), a, iterations, maxWordLength, simple);\n    }\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < threads.length; i++) {\n      try {\n        threads[i].join();\n      } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e","date":1334174049,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple) throws IOException {\n    checkRandomData(random, a, iterations, maxWordLength, simple, true);\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple) throws IOException {\n    checkRandomData(random, a, iterations, maxWordLength, random.nextBoolean(), simple);\n    // now test with multiple threads\n    int numThreads = _TestUtil.nextInt(random, 4, 8);\n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new AnalysisThread(new Random(random.nextLong()), a, iterations, maxWordLength, simple);\n    }\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < threads.length; i++) {\n      try {\n        threads[i].join();\n      } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n      }\n    }\n  }\n\n","bugFix":["47777586dd4c026834be0b2cc454d527cf8884b3","26bd77fc30a420a3e33c85e6fa6b0887eac4b029"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"888c2d6bca1edd8d9293631d6e1d188b036e0f05":["26bd77fc30a420a3e33c85e6fa6b0887eac4b029"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","47777586dd4c026834be0b2cc454d527cf8884b3"],"ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e":["26bd77fc30a420a3e33c85e6fa6b0887eac4b029","888c2d6bca1edd8d9293631d6e1d188b036e0f05"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"47777586dd4c026834be0b2cc454d527cf8884b3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"26bd77fc30a420a3e33c85e6fa6b0887eac4b029":["47777586dd4c026834be0b2cc454d527cf8884b3"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"]},"commit2Childs":{"888c2d6bca1edd8d9293631d6e1d188b036e0f05":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":[],"ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","47777586dd4c026834be0b2cc454d527cf8884b3"],"47777586dd4c026834be0b2cc454d527cf8884b3":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","26bd77fc30a420a3e33c85e6fa6b0887eac4b029"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"26bd77fc30a420a3e33c85e6fa6b0887eac4b029":["888c2d6bca1edd8d9293631d6e1d188b036e0f05","ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}