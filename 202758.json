{"path":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer#init(Version,int,int).mjava","commits":[{"id":"4c807c4005aae1acaf5cebc9af40883985fb89a8","date":1366974206,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer#init(Version,int,int).mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter#NGramTokenFilter(TokenStream,int,int).mjava","sourceNew":"  private void init(Version version, int minGram, int maxGram) {\n    if (!version.onOrAfter(Version.LUCENE_44)) {\n      throw new IllegalArgumentException(\"This class only works with Lucene 4.4+. To emulate the old (broken) behavior of NGramTokenizer, use Lucene43NGramTokenizer\");\n    }\n    if (minGram < 1) {\n      throw new IllegalArgumentException(\"minGram must be greater than zero\");\n    }\n    if (minGram > maxGram) {\n      throw new IllegalArgumentException(\"minGram must not be greater than maxGram\");\n    }\n    this.minGram = minGram;\n    this.maxGram = maxGram;\n    buffer = new char[maxGram + 1024];\n  }\n\n","sourceOld":"  /**\n   * Creates NGramTokenFilter with given min and max n-grams.\n   * @param input {@link TokenStream} holding the input to be tokenized\n   * @param minGram the smallest n-gram to generate\n   * @param maxGram the largest n-gram to generate\n   */\n  public NGramTokenFilter(TokenStream input, int minGram, int maxGram) {\n    super(input);\n    if (minGram < 1) {\n      throw new IllegalArgumentException(\"minGram must be greater than zero\");\n    }\n    if (minGram > maxGram) {\n      throw new IllegalArgumentException(\"minGram must not be greater than maxGram\");\n    }\n    this.minGram = minGram;\n    this.maxGram = maxGram;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704","date":1371043069,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer#init(Version,int,int,boolean).mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer#init(Version,int,int).mjava","sourceNew":"  private void init(Version version, int minGram, int maxGram, boolean edgesOnly) {\n    if (!edgesOnly && !version.onOrAfter(Version.LUCENE_44)) {\n      throw new IllegalArgumentException(\"This class only works with Lucene 4.4+. To emulate the old (broken) behavior of NGramTokenizer, use Lucene43NGramTokenizer\");\n    }\n    charUtils = version.onOrAfter(Version.LUCENE_44)\n        ? CharacterUtils.getInstance(version)\n        : CharacterUtils.getJava4Instance();\n    if (minGram < 1) {\n      throw new IllegalArgumentException(\"minGram must be greater than zero\");\n    }\n    if (minGram > maxGram) {\n      throw new IllegalArgumentException(\"minGram must not be greater than maxGram\");\n    }\n    this.minGram = minGram;\n    this.maxGram = maxGram;\n    this.edgesOnly = edgesOnly;\n    charBuffer = CharacterUtils.newCharacterBuffer(2 * maxGram + 1024); // 2 * maxGram in case all code points require 2 chars and + 1024 for buffering to not keep polling the Reader\n    buffer = new int[charBuffer.getBuffer().length];\n    // Make the term att large enough\n    termAtt.resizeBuffer(2 * maxGram);\n  }\n\n","sourceOld":"  private void init(Version version, int minGram, int maxGram) {\n    if (!version.onOrAfter(Version.LUCENE_44)) {\n      throw new IllegalArgumentException(\"This class only works with Lucene 4.4+. To emulate the old (broken) behavior of NGramTokenizer, use Lucene43NGramTokenizer\");\n    }\n    if (minGram < 1) {\n      throw new IllegalArgumentException(\"minGram must be greater than zero\");\n    }\n    if (minGram > maxGram) {\n      throw new IllegalArgumentException(\"minGram must not be greater than maxGram\");\n    }\n    this.minGram = minGram;\n    this.maxGram = maxGram;\n    buffer = new char[maxGram + 1024];\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704":["4c807c4005aae1acaf5cebc9af40883985fb89a8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4c807c4005aae1acaf5cebc9af40883985fb89a8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704"]},"commit2Childs":{"bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4c807c4005aae1acaf5cebc9af40883985fb89a8"],"4c807c4005aae1acaf5cebc9af40883985fb89a8":["bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}