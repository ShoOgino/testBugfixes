{"path":"solr/core/src/java/org/apache/solr/store/blockcache/BlockCache#store(BlockCacheKey,int,byte[],int,int).mjava","commits":[{"id":"849494cf2f3a96af5c8c84995108ddd8456fcd04","date":1372277913,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/store/blockcache/BlockCache#store(BlockCacheKey,int,byte[],int,int).mjava","pathOld":"/dev/null","sourceNew":"  public boolean store(BlockCacheKey blockCacheKey, int blockOffset,\n      byte[] data, int offset, int length) {\n    if (length + blockOffset > blockSize) {\n      throw new RuntimeException(\"Buffer size exceeded, expecting max [\"\n          + blockSize + \"] got length [\" + length + \"] with blockOffset [\"\n          + blockOffset + \"]\");\n    }\n    BlockCacheLocation location = cache.get(blockCacheKey);\n    boolean newLocation = false;\n    if (location == null) {\n      newLocation = true;\n      location = new BlockCacheLocation();\n      if (!findEmptyLocation(location)) {\n        return false;\n      }\n    }\n    if (location.isRemoved()) {\n      return false;\n    }\n    int bankId = location.getBankId();\n    int bankOffset = location.getBlock() * blockSize;\n    ByteBuffer bank = getBank(bankId);\n    bank.position(bankOffset + blockOffset);\n    bank.put(data, offset, length);\n    if (newLocation) {\n      releaseLocation(cache.put(blockCacheKey.clone(), location));\n      metrics.blockCacheSize.incrementAndGet();\n    }\n    return true;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":0,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/store/blockcache/BlockCache#store(BlockCacheKey,int,byte[],int,int).mjava","pathOld":"/dev/null","sourceNew":"  public boolean store(BlockCacheKey blockCacheKey, int blockOffset,\n      byte[] data, int offset, int length) {\n    if (length + blockOffset > blockSize) {\n      throw new RuntimeException(\"Buffer size exceeded, expecting max [\"\n          + blockSize + \"] got length [\" + length + \"] with blockOffset [\"\n          + blockOffset + \"]\");\n    }\n    BlockCacheLocation location = cache.get(blockCacheKey);\n    boolean newLocation = false;\n    if (location == null) {\n      newLocation = true;\n      location = new BlockCacheLocation();\n      if (!findEmptyLocation(location)) {\n        return false;\n      }\n    }\n    if (location.isRemoved()) {\n      return false;\n    }\n    int bankId = location.getBankId();\n    int bankOffset = location.getBlock() * blockSize;\n    ByteBuffer bank = getBank(bankId);\n    bank.position(bankOffset + blockOffset);\n    bank.put(data, offset, length);\n    if (newLocation) {\n      releaseLocation(cache.put(blockCacheKey.clone(), location));\n      metrics.blockCacheSize.incrementAndGet();\n    }\n    return true;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"da1d30d294dfaad0bb977ab36cb218612d8ffdf9","date":1428517399,"type":3,"author":"Shawn Heisey","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/store/blockcache/BlockCache#store(BlockCacheKey,int,byte[],int,int).mjava","pathOld":"solr/core/src/java/org/apache/solr/store/blockcache/BlockCache#store(BlockCacheKey,int,byte[],int,int).mjava","sourceNew":"  public boolean store(BlockCacheKey blockCacheKey, int blockOffset,\n      byte[] data, int offset, int length) {\n    if (length + blockOffset > blockSize) {\n      throw new RuntimeException(\"Buffer size exceeded, expecting max [\"\n          + blockSize + \"] got length [\" + length + \"] with blockOffset [\"\n          + blockOffset + \"]\");\n    }\n    BlockCacheLocation location = cache.getIfPresent(blockCacheKey);\n    boolean newLocation = false;\n    if (location == null) {\n      newLocation = true;\n      location = new BlockCacheLocation();\n      if (!findEmptyLocation(location)) {\n        return false;\n      }\n    }\n    if (location.isRemoved()) {\n      return false;\n    }\n    int bankId = location.getBankId();\n    int bankOffset = location.getBlock() * blockSize;\n    ByteBuffer bank = getBank(bankId);\n    bank.position(bankOffset + blockOffset);\n    bank.put(data, offset, length);\n    if (newLocation) {\n      cache.put(blockCacheKey.clone(), location);\n      metrics.blockCacheSize.incrementAndGet();\n    }\n    return true;\n  }\n\n","sourceOld":"  public boolean store(BlockCacheKey blockCacheKey, int blockOffset,\n      byte[] data, int offset, int length) {\n    if (length + blockOffset > blockSize) {\n      throw new RuntimeException(\"Buffer size exceeded, expecting max [\"\n          + blockSize + \"] got length [\" + length + \"] with blockOffset [\"\n          + blockOffset + \"]\");\n    }\n    BlockCacheLocation location = cache.get(blockCacheKey);\n    boolean newLocation = false;\n    if (location == null) {\n      newLocation = true;\n      location = new BlockCacheLocation();\n      if (!findEmptyLocation(location)) {\n        return false;\n      }\n    }\n    if (location.isRemoved()) {\n      return false;\n    }\n    int bankId = location.getBankId();\n    int bankOffset = location.getBlock() * blockSize;\n    ByteBuffer bank = getBank(bankId);\n    bank.position(bankOffset + blockOffset);\n    bank.put(data, offset, length);\n    if (newLocation) {\n      releaseLocation(cache.put(blockCacheKey.clone(), location));\n      metrics.blockCacheSize.incrementAndGet();\n    }\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8d474efcbfc117de753fe8289f43f0b3e3fbf8e5","date":1486742928,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/store/blockcache/BlockCache#store(BlockCacheKey,int,byte[],int,int).mjava","pathOld":"solr/core/src/java/org/apache/solr/store/blockcache/BlockCache#store(BlockCacheKey,int,byte[],int,int).mjava","sourceNew":"  /**\n   * This is only best-effort... it's possible for false to be returned.\n   * The blockCacheKey is cloned before it is inserted into the map, so it may be reused by clients if desired.\n   *\n   * @param blockCacheKey the key for the block\n   * @param blockOffset the offset within the block\n   * @param data source data to write to the block\n   * @param offset offset within the source data array\n   * @param length the number of bytes to write.\n   * @return true if the block was cached/updated\n   */\n  public boolean store(BlockCacheKey blockCacheKey, int blockOffset,\n      byte[] data, int offset, int length) {\n    if (length + blockOffset > blockSize) {\n      throw new RuntimeException(\"Buffer size exceeded, expecting max [\"\n          + blockSize + \"] got length [\" + length + \"] with blockOffset [\"\n          + blockOffset + \"]\");\n    }\n    BlockCacheLocation location = cache.getIfPresent(blockCacheKey);\n    boolean newLocation = false;\n    if (location == null) {\n      newLocation = true;\n      location = new BlockCacheLocation();\n      if (!findEmptyLocation(location)) {\n        // YCS: it looks like when the cache is full (a normal scenario), then two concurrent writes will result in one of them failing\n        // because no eviction is done first.  The code seems to rely on leaving just a single block empty.\n        // TODO: simplest fix would be to leave more than one block empty\n        return false;\n      }\n    }\n\n    // YCS: I think this means that the block existed, but it is in the process of being\n    // concurrently removed.  This flag is set in the releaseLocation eviction listener.\n    if (location.isRemoved()) {\n      return false;\n    }\n\n    int bankId = location.getBankId();\n    int bankOffset = location.getBlock() * blockSize;\n    ByteBuffer bank = getBank(bankId);\n    bank.position(bankOffset + blockOffset);\n    bank.put(data, offset, length);\n    if (newLocation) {\n      cache.put(blockCacheKey.clone(), location);\n      metrics.blockCacheSize.incrementAndGet();\n    }\n    return true;\n  }\n\n","sourceOld":"  public boolean store(BlockCacheKey blockCacheKey, int blockOffset,\n      byte[] data, int offset, int length) {\n    if (length + blockOffset > blockSize) {\n      throw new RuntimeException(\"Buffer size exceeded, expecting max [\"\n          + blockSize + \"] got length [\" + length + \"] with blockOffset [\"\n          + blockOffset + \"]\");\n    }\n    BlockCacheLocation location = cache.getIfPresent(blockCacheKey);\n    boolean newLocation = false;\n    if (location == null) {\n      newLocation = true;\n      location = new BlockCacheLocation();\n      if (!findEmptyLocation(location)) {\n        return false;\n      }\n    }\n    if (location.isRemoved()) {\n      return false;\n    }\n    int bankId = location.getBankId();\n    int bankOffset = location.getBlock() * blockSize;\n    ByteBuffer bank = getBank(bankId);\n    bank.position(bankOffset + blockOffset);\n    bank.put(data, offset, length);\n    if (newLocation) {\n      cache.put(blockCacheKey.clone(), location);\n      metrics.blockCacheSize.incrementAndGet();\n    }\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"46386805f467fa40cb9d5a3cab791713306548c2","date":1487170610,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/store/blockcache/BlockCache#store(BlockCacheKey,int,byte[],int,int).mjava","pathOld":"solr/core/src/java/org/apache/solr/store/blockcache/BlockCache#store(BlockCacheKey,int,byte[],int,int).mjava","sourceNew":"  /**\n   * This is only best-effort... it's possible for false to be returned, meaning the block was not able to be cached.\n   * NOTE: blocks may not currently be updated (false will be returned if the block is already cached)\n   * The blockCacheKey is cloned before it is inserted into the map, so it may be reused by clients if desired.\n   *\n   * @param blockCacheKey the key for the block\n   * @param blockOffset the offset within the block\n   * @param data source data to write to the block\n   * @param offset offset within the source data array\n   * @param length the number of bytes to write.\n   * @return true if the block was cached/updated\n   */\n  public boolean store(BlockCacheKey blockCacheKey, int blockOffset,\n      byte[] data, int offset, int length) {\n    if (length + blockOffset > blockSize) {\n      throw new RuntimeException(\"Buffer size exceeded, expecting max [\"\n          + blockSize + \"] got length [\" + length + \"] with blockOffset [\"\n          + blockOffset + \"]\");\n    }\n    BlockCacheLocation location = cache.getIfPresent(blockCacheKey);\n    if (location == null) {\n      location = new BlockCacheLocation();\n      if (!findEmptyLocation(location)) {\n        // YCS: it looks like when the cache is full (a normal scenario), then two concurrent writes will result in one of them failing\n        // because no eviction is done first.  The code seems to rely on leaving just a single block empty.\n        // TODO: simplest fix would be to leave more than one block empty\n        return false;\n      }\n    } else {\n      // If we allocated a new block, then it has never been published and is thus never in danger of being concurrently removed.\n      // On the other hand, if this is an existing block we are updating, it may concurrently be removed and reused for another\n      // purpose (and then our write may overwrite that).  This can happen even if clients never try to update existing blocks,\n      // since two clients can try to cache the same block concurrently.  Because of this, the ability to update an existing\n      // block has been removed for the time being (see SOLR-10121).\n      return false;\n    }\n\n    int bankId = location.getBankId();\n    int bankOffset = location.getBlock() * blockSize;\n    ByteBuffer bank = getBank(bankId);\n    bank.position(bankOffset + blockOffset);\n    bank.put(data, offset, length);\n\n    // make sure all modifications to the block have been completed before we publish it.\n    cache.put(blockCacheKey.clone(), location);\n    metrics.blockCacheSize.incrementAndGet();\n    return true;\n  }\n\n","sourceOld":"  /**\n   * This is only best-effort... it's possible for false to be returned.\n   * The blockCacheKey is cloned before it is inserted into the map, so it may be reused by clients if desired.\n   *\n   * @param blockCacheKey the key for the block\n   * @param blockOffset the offset within the block\n   * @param data source data to write to the block\n   * @param offset offset within the source data array\n   * @param length the number of bytes to write.\n   * @return true if the block was cached/updated\n   */\n  public boolean store(BlockCacheKey blockCacheKey, int blockOffset,\n      byte[] data, int offset, int length) {\n    if (length + blockOffset > blockSize) {\n      throw new RuntimeException(\"Buffer size exceeded, expecting max [\"\n          + blockSize + \"] got length [\" + length + \"] with blockOffset [\"\n          + blockOffset + \"]\");\n    }\n    BlockCacheLocation location = cache.getIfPresent(blockCacheKey);\n    boolean newLocation = false;\n    if (location == null) {\n      newLocation = true;\n      location = new BlockCacheLocation();\n      if (!findEmptyLocation(location)) {\n        // YCS: it looks like when the cache is full (a normal scenario), then two concurrent writes will result in one of them failing\n        // because no eviction is done first.  The code seems to rely on leaving just a single block empty.\n        // TODO: simplest fix would be to leave more than one block empty\n        return false;\n      }\n    }\n\n    // YCS: I think this means that the block existed, but it is in the process of being\n    // concurrently removed.  This flag is set in the releaseLocation eviction listener.\n    if (location.isRemoved()) {\n      return false;\n    }\n\n    int bankId = location.getBankId();\n    int bankOffset = location.getBlock() * blockSize;\n    ByteBuffer bank = getBank(bankId);\n    bank.position(bankOffset + blockOffset);\n    bank.put(data, offset, length);\n    if (newLocation) {\n      cache.put(blockCacheKey.clone(), location);\n      metrics.blockCacheSize.incrementAndGet();\n    }\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d329f927fde9f646d6fce133cbd41dd2dbe4ed54","date":1488303368,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/store/blockcache/BlockCache#store(BlockCacheKey,int,byte[],int,int).mjava","pathOld":"solr/core/src/java/org/apache/solr/store/blockcache/BlockCache#store(BlockCacheKey,int,byte[],int,int).mjava","sourceNew":"  /**\n   * This is only best-effort... it's possible for false to be returned, meaning the block was not able to be cached.\n   * NOTE: blocks may not currently be updated (false will be returned if the block is already cached)\n   * The blockCacheKey is cloned before it is inserted into the map, so it may be reused by clients if desired.\n   *\n   * @param blockCacheKey the key for the block\n   * @param blockOffset the offset within the block\n   * @param data source data to write to the block\n   * @param offset offset within the source data array\n   * @param length the number of bytes to write.\n   * @return true if the block was cached/updated\n   */\n  public boolean store(BlockCacheKey blockCacheKey, int blockOffset,\n      byte[] data, int offset, int length) {\n    if (length + blockOffset > blockSize) {\n      throw new RuntimeException(\"Buffer size exceeded, expecting max [\"\n          + blockSize + \"] got length [\" + length + \"] with blockOffset [\"\n          + blockOffset + \"]\");\n    }\n    BlockCacheLocation location = cache.getIfPresent(blockCacheKey);\n    if (location == null) {\n      location = new BlockCacheLocation();\n      if (!findEmptyLocation(location)) {\n        // YCS: it looks like when the cache is full (a normal scenario), then two concurrent writes will result in one of them failing\n        // because no eviction is done first.  The code seems to rely on leaving just a single block empty.\n        // TODO: simplest fix would be to leave more than one block empty\n        metrics.blockCacheStoreFail.incrementAndGet();\n        return false;\n      }\n    } else {\n      // If we allocated a new block, then it has never been published and is thus never in danger of being concurrently removed.\n      // On the other hand, if this is an existing block we are updating, it may concurrently be removed and reused for another\n      // purpose (and then our write may overwrite that).  This can happen even if clients never try to update existing blocks,\n      // since two clients can try to cache the same block concurrently.  Because of this, the ability to update an existing\n      // block has been removed for the time being (see SOLR-10121).\n\n      // No metrics to update: we don't count a redundant store as a store fail.\n      return false;\n    }\n\n    int bankId = location.getBankId();\n    int bankOffset = location.getBlock() * blockSize;\n    ByteBuffer bank = getBank(bankId);\n    bank.position(bankOffset + blockOffset);\n    bank.put(data, offset, length);\n\n    // make sure all modifications to the block have been completed before we publish it.\n    cache.put(blockCacheKey.clone(), location);\n    metrics.blockCacheSize.incrementAndGet();\n    return true;\n  }\n\n","sourceOld":"  /**\n   * This is only best-effort... it's possible for false to be returned, meaning the block was not able to be cached.\n   * NOTE: blocks may not currently be updated (false will be returned if the block is already cached)\n   * The blockCacheKey is cloned before it is inserted into the map, so it may be reused by clients if desired.\n   *\n   * @param blockCacheKey the key for the block\n   * @param blockOffset the offset within the block\n   * @param data source data to write to the block\n   * @param offset offset within the source data array\n   * @param length the number of bytes to write.\n   * @return true if the block was cached/updated\n   */\n  public boolean store(BlockCacheKey blockCacheKey, int blockOffset,\n      byte[] data, int offset, int length) {\n    if (length + blockOffset > blockSize) {\n      throw new RuntimeException(\"Buffer size exceeded, expecting max [\"\n          + blockSize + \"] got length [\" + length + \"] with blockOffset [\"\n          + blockOffset + \"]\");\n    }\n    BlockCacheLocation location = cache.getIfPresent(blockCacheKey);\n    if (location == null) {\n      location = new BlockCacheLocation();\n      if (!findEmptyLocation(location)) {\n        // YCS: it looks like when the cache is full (a normal scenario), then two concurrent writes will result in one of them failing\n        // because no eviction is done first.  The code seems to rely on leaving just a single block empty.\n        // TODO: simplest fix would be to leave more than one block empty\n        return false;\n      }\n    } else {\n      // If we allocated a new block, then it has never been published and is thus never in danger of being concurrently removed.\n      // On the other hand, if this is an existing block we are updating, it may concurrently be removed and reused for another\n      // purpose (and then our write may overwrite that).  This can happen even if clients never try to update existing blocks,\n      // since two clients can try to cache the same block concurrently.  Because of this, the ability to update an existing\n      // block has been removed for the time being (see SOLR-10121).\n      return false;\n    }\n\n    int bankId = location.getBankId();\n    int bankOffset = location.getBlock() * blockSize;\n    ByteBuffer bank = getBank(bankId);\n    bank.position(bankOffset + blockOffset);\n    bank.put(data, offset, length);\n\n    // make sure all modifications to the block have been completed before we publish it.\n    cache.put(blockCacheKey.clone(), location);\n    metrics.blockCacheSize.incrementAndGet();\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"849494cf2f3a96af5c8c84995108ddd8456fcd04":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"8d474efcbfc117de753fe8289f43f0b3e3fbf8e5":["da1d30d294dfaad0bb977ab36cb218612d8ffdf9"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","849494cf2f3a96af5c8c84995108ddd8456fcd04"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"46386805f467fa40cb9d5a3cab791713306548c2":["8d474efcbfc117de753fe8289f43f0b3e3fbf8e5"],"d329f927fde9f646d6fce133cbd41dd2dbe4ed54":["46386805f467fa40cb9d5a3cab791713306548c2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d329f927fde9f646d6fce133cbd41dd2dbe4ed54"],"da1d30d294dfaad0bb977ab36cb218612d8ffdf9":["849494cf2f3a96af5c8c84995108ddd8456fcd04"]},"commit2Childs":{"849494cf2f3a96af5c8c84995108ddd8456fcd04":["37a0f60745e53927c4c876cfe5b5a58170f0646c","da1d30d294dfaad0bb977ab36cb218612d8ffdf9"],"8d474efcbfc117de753fe8289f43f0b3e3fbf8e5":["46386805f467fa40cb9d5a3cab791713306548c2"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["849494cf2f3a96af5c8c84995108ddd8456fcd04","37a0f60745e53927c4c876cfe5b5a58170f0646c"],"46386805f467fa40cb9d5a3cab791713306548c2":["d329f927fde9f646d6fce133cbd41dd2dbe4ed54"],"d329f927fde9f646d6fce133cbd41dd2dbe4ed54":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"da1d30d294dfaad0bb977ab36cb218612d8ffdf9":["8d474efcbfc117de753fe8289f43f0b3e3fbf8e5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["37a0f60745e53927c4c876cfe5b5a58170f0646c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}