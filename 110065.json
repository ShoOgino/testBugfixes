{"path":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#doTest().mjava","commits":[{"id":"19389fe47925b510b2811e2b385a75f7ad19dcca","date":1393903127,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#doTest().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrServer> cloudServers = new ArrayList<CloudSolrServer>();\n    List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrServer server = new CloudSolrServer(zkServer.getZkAddress());\n      server.setDefaultCollection(ACOLLECTION + i);\n      cloudServers.add(server);\n      StopableIndexingThread indexThread = new StopableIndexingThread(null, server, \"1\", true, docCount);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StopableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrServer server : cloudServers) {\n      server.commit();\n      collectionsCount += server.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n    \n    for (CloudSolrServer server : cloudServers) {\n      server.shutdown();\n    }\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = ((SolrDispatchFilter) jetty.getDispatchFilter()\n          .getFilter()).getCores();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            assertTrue(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know its a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["f44b69a495e6f5985e0fbb62cee955ea6d85d3cc"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"96ea64d994d340044e0d57aeb6a5871539d10ca5","date":1394225445,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#doTest().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrServer> cloudServers = new ArrayList<CloudSolrServer>();\n    List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrServer server = new CloudSolrServer(zkServer.getZkAddress());\n      server.setDefaultCollection(ACOLLECTION + i);\n      cloudServers.add(server);\n      StopableIndexingThread indexThread = new StopableIndexingThread(null, server, \"1\", true, docCount);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StopableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrServer server : cloudServers) {\n      server.commit();\n      collectionsCount += server.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n    \n    for (CloudSolrServer server : cloudServers) {\n      server.shutdown();\n    }\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = ((SolrDispatchFilter) jetty.getDispatchFilter()\n          .getFilter()).getCores();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            assertTrue(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know its a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrServer> cloudServers = new ArrayList<>();\n    List<StopableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrServer server = new CloudSolrServer(zkServer.getZkAddress());\n      server.setDefaultCollection(ACOLLECTION + i);\n      cloudServers.add(server);\n      StopableIndexingThread indexThread = new StopableIndexingThread(null, server, \"1\", true, docCount);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StopableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrServer server : cloudServers) {\n      server.commit();\n      collectionsCount += server.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n    \n    for (CloudSolrServer server : cloudServers) {\n      server.shutdown();\n    }\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = ((SolrDispatchFilter) jetty.getDispatchFilter()\n          .getFilter()).getCores();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            assertTrue(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know its a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrServer> cloudServers = new ArrayList<CloudSolrServer>();\n    List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrServer server = new CloudSolrServer(zkServer.getZkAddress());\n      server.setDefaultCollection(ACOLLECTION + i);\n      cloudServers.add(server);\n      StopableIndexingThread indexThread = new StopableIndexingThread(null, server, \"1\", true, docCount);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StopableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrServer server : cloudServers) {\n      server.commit();\n      collectionsCount += server.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n    \n    for (CloudSolrServer server : cloudServers) {\n      server.shutdown();\n    }\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = ((SolrDispatchFilter) jetty.getDispatchFilter()\n          .getFilter()).getCores();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            assertTrue(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know its a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f44b69a495e6f5985e0fbb62cee955ea6d85d3cc","date":1408916870,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrServer> cloudServers = new ArrayList<>();\n    List<StopableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrServer server = new CloudSolrServer(zkServer.getZkAddress());\n      server.setDefaultCollection(ACOLLECTION + i);\n      cloudServers.add(server);\n      StopableIndexingThread indexThread = new StopableIndexingThread(null, server, \"1\", true, docCount);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StopableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrServer server : cloudServers) {\n      server.commit();\n      collectionsCount += server.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n    \n    for (CloudSolrServer server : cloudServers) {\n      server.shutdown();\n    }\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = ((SolrDispatchFilter) jetty.getDispatchFilter()\n          .getFilter()).getCores();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know its a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrServer> cloudServers = new ArrayList<>();\n    List<StopableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrServer server = new CloudSolrServer(zkServer.getZkAddress());\n      server.setDefaultCollection(ACOLLECTION + i);\n      cloudServers.add(server);\n      StopableIndexingThread indexThread = new StopableIndexingThread(null, server, \"1\", true, docCount);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StopableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrServer server : cloudServers) {\n      server.commit();\n      collectionsCount += server.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n    \n    for (CloudSolrServer server : cloudServers) {\n      server.shutdown();\n    }\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = ((SolrDispatchFilter) jetty.getDispatchFilter()\n          .getFilter()).getCores();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            assertTrue(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know its a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":["19389fe47925b510b2811e2b385a75f7ad19dcca"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1","date":1419400138,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrServer> cloudServers = new ArrayList<>();\n    List<StopableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrServer server = new CloudSolrServer(zkServer.getZkAddress());\n      server.setDefaultCollection(ACOLLECTION + i);\n      cloudServers.add(server);\n      StopableIndexingThread indexThread = new StopableIndexingThread(null, server, \"1\", true, docCount);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StopableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrServer server : cloudServers) {\n      server.commit();\n      collectionsCount += server.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n    \n    for (CloudSolrServer server : cloudServers) {\n      server.shutdown();\n    }\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = ((SolrDispatchFilter) jetty.getDispatchFilter()\n          .getFilter()).getCores();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrServer> cloudServers = new ArrayList<>();\n    List<StopableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrServer server = new CloudSolrServer(zkServer.getZkAddress());\n      server.setDefaultCollection(ACOLLECTION + i);\n      cloudServers.add(server);\n      StopableIndexingThread indexThread = new StopableIndexingThread(null, server, \"1\", true, docCount);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StopableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrServer server : cloudServers) {\n      server.commit();\n      collectionsCount += server.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n    \n    for (CloudSolrServer server : cloudServers) {\n      server.shutdown();\n    }\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = ((SolrDispatchFilter) jetty.getDispatchFilter()\n          .getFilter()).getCores();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know its a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bafca15d8e408346a67f4282ad1143b88023893b","date":1420034748,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StopableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = new CloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StopableIndexingThread indexThread = new StopableIndexingThread(null, client, \"1\", true, docCount);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StopableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n    \n    for (CloudSolrClient client : cloudClients) {\n      client.shutdown();\n    }\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = ((SolrDispatchFilter) jetty.getDispatchFilter()\n          .getFilter()).getCores();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrServer> cloudServers = new ArrayList<>();\n    List<StopableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrServer server = new CloudSolrServer(zkServer.getZkAddress());\n      server.setDefaultCollection(ACOLLECTION + i);\n      cloudServers.add(server);\n      StopableIndexingThread indexThread = new StopableIndexingThread(null, server, \"1\", true, docCount);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StopableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrServer server : cloudServers) {\n      server.commit();\n      collectionsCount += server.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n    \n    for (CloudSolrServer server : cloudServers) {\n      server.shutdown();\n    }\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = ((SolrDispatchFilter) jetty.getDispatchFilter()\n          .getFilter()).getCores();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["6c94d2661bc1c14426980ec7882e951fdcff08d0"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"abb23fcc2461782ab204e61213240feb77d355aa","date":1422029612,"type":5,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#doTest().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StopableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = new CloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StopableIndexingThread indexThread = new StopableIndexingThread(null, client, \"1\", true, docCount);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StopableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n    \n    for (CloudSolrClient client : cloudClients) {\n      client.shutdown();\n    }\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = ((SolrDispatchFilter) jetty.getDispatchFilter()\n          .getFilter()).getCores();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StopableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = new CloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StopableIndexingThread indexThread = new StopableIndexingThread(null, client, \"1\", true, docCount);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StopableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n    \n    for (CloudSolrClient client : cloudClients) {\n      client.shutdown();\n    }\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = ((SolrDispatchFilter) jetty.getDispatchFilter()\n          .getFilter()).getCores();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"19389fe47925b510b2811e2b385a75f7ad19dcca":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["19389fe47925b510b2811e2b385a75f7ad19dcca"],"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["f44b69a495e6f5985e0fbb62cee955ea6d85d3cc"],"f44b69a495e6f5985e0fbb62cee955ea6d85d3cc":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","19389fe47925b510b2811e2b385a75f7ad19dcca"],"abb23fcc2461782ab204e61213240feb77d355aa":["bafca15d8e408346a67f4282ad1143b88023893b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"bafca15d8e408346a67f4282ad1143b88023893b":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["abb23fcc2461782ab204e61213240feb77d355aa"]},"commit2Childs":{"19389fe47925b510b2811e2b385a75f7ad19dcca":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","96ea64d994d340044e0d57aeb6a5871539d10ca5"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["f44b69a495e6f5985e0fbb62cee955ea6d85d3cc"],"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["bafca15d8e408346a67f4282ad1143b88023893b"],"f44b69a495e6f5985e0fbb62cee955ea6d85d3cc":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":[],"abb23fcc2461782ab204e61213240feb77d355aa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["19389fe47925b510b2811e2b385a75f7ad19dcca","96ea64d994d340044e0d57aeb6a5871539d10ca5"],"bafca15d8e408346a67f4282ad1143b88023893b":["abb23fcc2461782ab204e61213240feb77d355aa"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["96ea64d994d340044e0d57aeb6a5871539d10ca5","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}