{"path":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","commits":[{"id":"2b2e7536fb06d1abad6c7543a0657bdad5242c5e","date":1341417762,"type":1,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/TestRealTimeGet#testStressLuceneNRT().mjava","sourceNew":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<Thread>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setIndexed(true);\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setIndexed(false);\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(Version.LUCENE_40, new WhitespaceAnalyzer(Version.LUCENE_40)));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<Integer,DocInfo>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                Document doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(1000);  // number of query operations to perform in total - crank up if\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<Thread>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setIndexed(true);\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setIndexed(false);\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(Version.LUCENE_40, new WhitespaceAnalyzer(Version.LUCENE_40)));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(TestRealTimeGet.this) {\n                    newCommittedModel = new HashMap<Integer,DocInfo>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                   newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(TestRealTimeGet.this) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(TestRealTimeGet.this) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(TestRealTimeGet.this) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                Document doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe33227f6805edab2036cbb80645cc4e2d1fa424","date":1342713534,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<Thread>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setIndexed(true);\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setIndexed(false);\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(Version.LUCENE_40, new WhitespaceAnalyzer(Version.LUCENE_40)));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<Integer,DocInfo>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                Document doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a7e4907084808af8fdb14b9809e6dceaccf6867b","date":1343473006,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","sourceNew":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<Thread>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setIndexed(true);\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setIndexed(false);\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(Version.LUCENE_40, new WhitespaceAnalyzer(Version.LUCENE_40)));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<Integer,DocInfo>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<Thread>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setIndexed(true);\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setIndexed(false);\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(Version.LUCENE_40, new WhitespaceAnalyzer(Version.LUCENE_40)));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<Integer,DocInfo>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                Document doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1d028314cced5858683a1bb4741423d0f934257b","date":1346596535,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","sourceNew":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<Thread>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setIndexed(true);\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setIndexed(false);\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(Version.LUCENE_40, new WhitespaceAnalyzer(Version.LUCENE_40)));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<Integer,DocInfo>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<Thread>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setIndexed(true);\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setIndexed(false);\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(Version.LUCENE_40, new WhitespaceAnalyzer(Version.LUCENE_40)));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<Integer,DocInfo>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                Document doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0bf41419d452997826ec5f17684993377be77f49","date":1386629618,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","sourceNew":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<Thread>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setIndexed(true);\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setIndexed(false);\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<Integer,DocInfo>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<Thread>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setIndexed(true);\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setIndexed(false);\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(Version.LUCENE_40, new WhitespaceAnalyzer(Version.LUCENE_40)));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<Integer,DocInfo>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":["7a3554ff15950ad0e3bcbb4e4e2ddb45b0b0f27e"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"74f45af4339b0daf7a95c820ab88c1aea74fbce0","date":1387475327,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","sourceNew":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<Thread>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setIndexed(true);\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setIndexed(false);\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<Integer,DocInfo>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<Thread>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setIndexed(true);\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setIndexed(false);\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(Version.LUCENE_40, new WhitespaceAnalyzer(Version.LUCENE_40)));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<Integer,DocInfo>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","sourceNew":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setIndexed(true);\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setIndexed(false);\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<Thread>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setIndexed(true);\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setIndexed(false);\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<Integer,DocInfo>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","sourceNew":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setIndexed(true);\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setIndexed(false);\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.shutdown();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setIndexed(true);\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setIndexed(false);\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","sourceNew":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setIndexed(true);\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setIndexed(false);\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.shutdown();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setIndexed(true);\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setIndexed(false);\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.shutdown();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","sourceNew":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setIndexed(true);\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setIndexed(false);\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setIndexed(true);\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setIndexed(false);\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.shutdown();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"19e497fe4da591a79332da97681b8017d9c61165","date":1409030374,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","sourceNew":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setIndexed(true);\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setIndexed(false);\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new WhitespaceAnalyzer()));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setIndexed(true);\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setIndexed(false);\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3184874f7f3aca850248483485b4995343066875","date":1413876758,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","sourceNew":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new WhitespaceAnalyzer()));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setIndexed(true);\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setIndexed(false);\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new WhitespaceAnalyzer()));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a22eafe3f72a4c2945eaad9547e6c78816978f4","date":1413956657,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","sourceNew":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new WhitespaceAnalyzer()));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setIndexed(true);\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setIndexed(false);\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new WhitespaceAnalyzer()));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2bb2842e561df4e8e9ad89010605fc86ac265465","date":1414768208,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","sourceNew":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(IndexOptions.DOCS);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new WhitespaceAnalyzer()));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new WhitespaceAnalyzer()));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1","date":1419400138,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","sourceNew":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(IndexOptions.DOCS);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new WhitespaceAnalyzer()));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for its tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or its tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(IndexOptions.DOCS);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new WhitespaceAnalyzer()));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for it's tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","date":1453060490,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","sourceNew":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(IndexOptions.DOCS);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new WhitespaceAnalyzer()));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for its tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or its tombstone\n                Document doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(IndexOptions.DOCS);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new WhitespaceAnalyzer()));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for its tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or its tombstone\n                StoredDocument doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f90ee54b41e0e3187a4fedafcf2bd6d947befd31","date":1453479126,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","sourceNew":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(IndexOptions.DOCS);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new WhitespaceAnalyzer()));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for its tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or its tombstone\n                Document doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(IndexOptions.DOCS);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new WhitespaceAnalyzer()));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w, true);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for its tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or its tombstone\n                Document doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6554f36a4636755009195a7840518bf6b4f03d6c","date":1481906808,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","sourceNew":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(IndexOptions.DOCS);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new WhitespaceAnalyzer()));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for its tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or its tombstone\n                Document doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(IndexOptions.DOCS);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new WhitespaceAnalyzer()));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for its tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or its tombstone\n                Document doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","date":1482251961,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","sourceNew":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(IndexOptions.DOCS);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new WhitespaceAnalyzer()));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for its tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or its tombstone\n                Document doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(IndexOptions.DOCS);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new WhitespaceAnalyzer()));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for its tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or its tombstone\n                Document doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5de502b5478255493125e7e801411ba17a6682ec","date":1490974101,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","sourceNew":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(IndexOptions.DOCS);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new WhitespaceAnalyzer()));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(FIELD, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(FIELD, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(FIELD, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for its tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or its tombstone\n                Document doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(FIELD));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(IndexOptions.DOCS);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new WhitespaceAnalyzer()));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for its tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or its tombstone\n                Document doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6f20fd35e3055a0c5b387df0b986a68d65d86441","date":1491045405,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/TestStressLucene#testStressLuceneNRT().mjava","sourceNew":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(IndexOptions.DOCS);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new WhitespaceAnalyzer()));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(FIELD, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(FIELD, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(FIELD, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for its tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or its tombstone\n                Document doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(FIELD));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testStressLuceneNRT() throws Exception {\n    final int commitPercent = 5 + random().nextInt(20);\n    final int softCommitPercent = 30+random().nextInt(75); // what percent of the commits are soft\n    final int deletePercent = 4+random().nextInt(25);\n    final int deleteByQueryPercent = 1+random().nextInt(5);\n    final int ndocs = 5 + (random().nextBoolean() ? random().nextInt(25) : random().nextInt(200));\n    int nWriteThreads = 5 + random().nextInt(25);\n\n    final int maxConcurrentCommits = nWriteThreads;\n\n    final AtomicLong operations = new AtomicLong(100000);  // number of query operations to perform in total\n    int nReadThreads = 5 + random().nextInt(25);\n    final boolean tombstones = random().nextBoolean();\n    final boolean syncCommits = random().nextBoolean();\n\n    verbose(\"commitPercent=\", commitPercent);\n    verbose(\"softCommitPercent=\",softCommitPercent);\n    verbose(\"deletePercent=\",deletePercent);\n    verbose(\"deleteByQueryPercent=\", deleteByQueryPercent);\n    verbose(\"ndocs=\", ndocs);\n    verbose(\"nWriteThreads=\", nWriteThreads);\n    verbose(\"nReadThreads=\", nReadThreads);\n    verbose(\"maxConcurrentCommits=\", maxConcurrentCommits);\n    verbose(\"operations=\", operations);\n    verbose(\"tombstones=\", tombstones);\n    verbose(\"syncCommits=\", syncCommits);\n\n    initModel(ndocs);\n\n    final AtomicInteger numCommitting = new AtomicInteger();\n\n    List<Thread> threads = new ArrayList<>();\n\n\n    final FieldType idFt = new FieldType();\n    idFt.setStored(true);\n    idFt.setOmitNorms(true);\n    idFt.setTokenized(false);\n    idFt.setIndexOptions(IndexOptions.DOCS);\n\n    final FieldType ft2 = new FieldType();\n    ft2.setStored(true);\n\n\n    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but\n    // a hard commit in progress does not stop a soft commit.\n    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;\n    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;\n\n\n    // RAMDirectory dir = new RAMDirectory();\n    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new WhitespaceAnalyzer()));\n\n    Directory dir = newDirectory();\n\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    writer.setDoRandomForceMergeAssert(false);\n\n    // writer.commit();\n    // reader = IndexReader.open(dir);\n    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged\n    // to only opening at the last commit point.\n    reader = DirectoryReader.open(writer.w);\n\n    for (int i=0; i<nWriteThreads; i++) {\n      Thread thread = new Thread(\"WRITER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.get() > 0) {\n              int oper = rand.nextInt(100);\n\n              if (oper < commitPercent) {\n                if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {\n                  Map<Integer,DocInfo> newCommittedModel;\n                  long version;\n                  DirectoryReader oldReader;\n\n                  boolean softCommit = rand.nextInt(100) < softCommitPercent;\n\n                  if (!softCommit) {\n                    // only allow one hard commit to proceed at once\n                    if (hardCommitLock != null) hardCommitLock.lock();\n                    verbose(\"hardCommit start\");\n\n                    writer.commit();\n                  }\n\n                  if (reopenLock != null) reopenLock.lock();\n\n                  synchronized(globalLock) {\n                    newCommittedModel = new HashMap<>(model);  // take a snapshot\n                    version = snapshotCount++;\n                    oldReader = reader;\n                    oldReader.incRef();  // increment the reference since we will use this for reopening\n                  }\n\n                  if (!softCommit) {\n                    // must commit after taking a snapshot of the model\n                    // writer.commit();\n                  }\n\n                  verbose(\"reopen start using\", oldReader);\n\n                  DirectoryReader newReader;\n                  if (softCommit) {\n                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);\n                  } else {\n                    // will only open to last commit\n                    newReader = DirectoryReader.openIfChanged(oldReader);\n                  }\n\n\n                  if (newReader == null) {\n                    oldReader.incRef();\n                    newReader = oldReader;\n                  }\n                  oldReader.decRef();\n\n                  verbose(\"reopen result\", newReader);\n\n                  synchronized(globalLock) {\n                    assert newReader.getRefCount() > 0;\n                    assert reader.getRefCount() > 0;\n\n                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)\n                    if (newReader.getVersion() > reader.getVersion()) {\n                      reader.decRef();\n                      reader = newReader;\n\n                      // install this snapshot only if it's newer than the current one\n                      if (version >= committedModelClock) {\n                        committedModel = newCommittedModel;\n                        committedModelClock = version;\n                      }\n\n                    } else {\n                      // close if unused\n                      newReader.decRef();\n                    }\n\n                  }\n\n                  if (reopenLock != null) reopenLock.unlock();\n\n                  if (!softCommit) {\n                    if (hardCommitLock != null) hardCommitLock.unlock();\n                  }\n\n                }\n                numCommitting.decrementAndGet();\n                continue;\n              }\n\n\n              int id = rand.nextInt(ndocs);\n              Object sync = syncArr[id];\n\n              // set the lastId before we actually change it sometimes to try and\n              // uncover more race conditions between writing and reading\n              boolean before = rand.nextBoolean();\n              if (before) {\n                lastId = id;\n              }\n\n              // We can't concurrently update the same document and retain our invariants of increasing values\n              // since we can't guarantee what order the updates will be executed.\n              synchronized (sync) {\n                DocInfo info = model.get(id);\n                long val = info.val;\n                long nextVal = Math.abs(val)+1;\n\n                if (oper < commitPercent + deletePercent) {\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleting id\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new Term(\"id\",Integer.toString(id)));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleting id\",id,\"val=\",nextVal,\"DONE\");\n\n                } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n                  //assertU(\"<delete><query>id:\" + id + \"</query></delete>\");\n\n                  // add tombstone first\n                  if (tombstones) {\n                    Document d = new Document();\n                    d.add(new Field(\"id\",\"-\"+Integer.toString(id), idFt));\n                    d.add(new Field(field, Long.toString(nextVal), ft2));\n                    verbose(\"adding tombstone for id\",id,\"val=\",nextVal);\n                    writer.updateDocument(new Term(\"id\", \"-\"+Integer.toString(id)), d);\n                  }\n\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal);\n                  writer.deleteDocuments(new TermQuery(new Term(\"id\", Integer.toString(id))));\n                  model.put(id, new DocInfo(0,-nextVal));\n                  verbose(\"deleteByQuery\",id,\"val=\",nextVal,\"DONE\");\n                } else {\n                  // model.put(id, nextVal);   // uncomment this and this test should fail.\n\n                  // assertU(adoc(\"id\",Integer.toString(id), field, Long.toString(nextVal)));\n                  Document d = new Document();\n                  d.add(new Field(\"id\",Integer.toString(id), idFt));\n                  d.add(new Field(field, Long.toString(nextVal), ft2));\n                  verbose(\"adding id\",id,\"val=\",nextVal);\n                  writer.updateDocument(new Term(\"id\", Integer.toString(id)), d);\n                  if (tombstones) {\n                    // remove tombstone after new addition (this should be optional?)\n                    verbose(\"deleting tombstone for id\",id);\n                    writer.deleteDocuments(new Term(\"id\",\"-\"+Integer.toString(id)));\n                    verbose(\"deleting tombstone for id\",id,\"DONE\");\n                  }\n\n                  model.put(id, new DocInfo(0,nextVal));\n                  verbose(\"adding id\",id,\"val=\",nextVal,\"DONE\");\n                }\n              }\n\n              if (!before) {\n                lastId = id;\n              }\n            }\n          } catch (Exception  ex) {\n            throw new RuntimeException(ex);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (int i=0; i<nReadThreads; i++) {\n      Thread thread = new Thread(\"READER\"+i) {\n        Random rand = new Random(random().nextInt());\n\n        @Override\n        public void run() {\n          try {\n            while (operations.decrementAndGet() >= 0) {\n              // bias toward a recently changed doc\n              int id = rand.nextInt(100) < 25 ? lastId : rand.nextInt(ndocs);\n\n              // when indexing, we update the index, then the model\n              // so when querying, we should first check the model, and then the index\n\n              DocInfo info;\n              synchronized(globalLock) {\n                info = committedModel.get(id);\n              }\n              long val = info.val;\n\n              IndexReader r;\n              synchronized(globalLock) {\n                r = reader;\n                r.incRef();\n              }\n\n              int docid = getFirstMatch(r, new Term(\"id\",Integer.toString(id)));\n\n              if (docid < 0 && tombstones) {\n                // if we couldn't find the doc, look for its tombstone\n                docid = getFirstMatch(r, new Term(\"id\",\"-\"+Integer.toString(id)));\n                if (docid < 0) {\n                  if (val == -1L) {\n                    // expected... no doc was added yet\n                    r.decRef();\n                    continue;\n                  }\n                  verbose(\"ERROR: Couldn't find a doc  or tombstone for id\", id, \"using reader\",r,\"expected value\",val);\n                  fail(\"No documents or tombstones found for id \" + id + \", expected at least \" + val);\n                }\n              }\n\n              if (docid < 0 && !tombstones) {\n                // nothing to do - we can't tell anything from a deleted doc without tombstones\n              } else {\n                if (docid < 0) {\n                  verbose(\"ERROR: Couldn't find a doc for id\", id, \"using reader\",r);\n                }\n                assertTrue(docid >= 0);   // we should have found the document, or its tombstone\n                Document doc = r.document(docid);\n                long foundVal = Long.parseLong(doc.get(field));\n                if (foundVal < Math.abs(val)) {\n                  verbose(\"ERROR: id\",id,\"model_val=\",val,\" foundVal=\",foundVal,\"reader=\",reader);\n                }\n                assertTrue(foundVal >= Math.abs(val));\n              }\n\n              r.decRef();\n            }\n          } catch (Throwable e) {\n            operations.set(-1L);\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      threads.add(thread);\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["0bf41419d452997826ec5f17684993377be77f49"],"5de502b5478255493125e7e801411ba17a6682ec":["6554f36a4636755009195a7840518bf6b4f03d6c"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"6554f36a4636755009195a7840518bf6b4f03d6c":["f90ee54b41e0e3187a4fedafcf2bd6d947befd31"],"6f20fd35e3055a0c5b387df0b986a68d65d86441":["6554f36a4636755009195a7840518bf6b4f03d6c"],"f90ee54b41e0e3187a4fedafcf2bd6d947befd31":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"2bb2842e561df4e8e9ad89010605fc86ac265465":["3184874f7f3aca850248483485b4995343066875"],"3184874f7f3aca850248483485b4995343066875":["19e497fe4da591a79332da97681b8017d9c61165"],"19e497fe4da591a79332da97681b8017d9c61165":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["2bb2842e561df4e8e9ad89010605fc86ac265465"],"1d028314cced5858683a1bb4741423d0f934257b":["2b2e7536fb06d1abad6c7543a0657bdad5242c5e","a7e4907084808af8fdb14b9809e6dceaccf6867b"],"0bf41419d452997826ec5f17684993377be77f49":["1d028314cced5858683a1bb4741423d0f934257b"],"0a22eafe3f72a4c2945eaad9547e6c78816978f4":["19e497fe4da591a79332da97681b8017d9c61165","3184874f7f3aca850248483485b4995343066875"],"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf":["f90ee54b41e0e3187a4fedafcf2bd6d947befd31","6554f36a4636755009195a7840518bf6b4f03d6c"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","2b2e7536fb06d1abad6c7543a0657bdad5242c5e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"2b2e7536fb06d1abad6c7543a0657bdad5242c5e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":["1d028314cced5858683a1bb4741423d0f934257b","0bf41419d452997826ec5f17684993377be77f49"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"a7e4907084808af8fdb14b9809e6dceaccf6867b":["2b2e7536fb06d1abad6c7543a0657bdad5242c5e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["5de502b5478255493125e7e801411ba17a6682ec"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"5de502b5478255493125e7e801411ba17a6682ec":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"6554f36a4636755009195a7840518bf6b4f03d6c":["5de502b5478255493125e7e801411ba17a6682ec","6f20fd35e3055a0c5b387df0b986a68d65d86441","5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf"],"6f20fd35e3055a0c5b387df0b986a68d65d86441":[],"f90ee54b41e0e3187a4fedafcf2bd6d947befd31":["6554f36a4636755009195a7840518bf6b4f03d6c","5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf"],"2bb2842e561df4e8e9ad89010605fc86ac265465":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"],"3184874f7f3aca850248483485b4995343066875":["2bb2842e561df4e8e9ad89010605fc86ac265465","0a22eafe3f72a4c2945eaad9547e6c78816978f4"],"19e497fe4da591a79332da97681b8017d9c61165":["3184874f7f3aca850248483485b4995343066875","0a22eafe3f72a4c2945eaad9547e6c78816978f4"],"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"1d028314cced5858683a1bb4741423d0f934257b":["0bf41419d452997826ec5f17684993377be77f49","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"0bf41419d452997826ec5f17684993377be77f49":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"0a22eafe3f72a4c2945eaad9547e6c78816978f4":[],"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf":[],"fe33227f6805edab2036cbb80645cc4e2d1fa424":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["fe33227f6805edab2036cbb80645cc4e2d1fa424","2b2e7536fb06d1abad6c7543a0657bdad5242c5e"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["19e497fe4da591a79332da97681b8017d9c61165"],"2b2e7536fb06d1abad6c7543a0657bdad5242c5e":["1d028314cced5858683a1bb4741423d0f934257b","fe33227f6805edab2036cbb80645cc4e2d1fa424","a7e4907084808af8fdb14b9809e6dceaccf6867b"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":[],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"a7e4907084808af8fdb14b9809e6dceaccf6867b":["1d028314cced5858683a1bb4741423d0f934257b"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["f90ee54b41e0e3187a4fedafcf2bd6d947befd31"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["6f20fd35e3055a0c5b387df0b986a68d65d86441","0a22eafe3f72a4c2945eaad9547e6c78816978f4","5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","fe33227f6805edab2036cbb80645cc4e2d1fa424","74f45af4339b0daf7a95c820ab88c1aea74fbce0","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}