{"path":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms();\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      DocsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, true);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms();\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      DocsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, true);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","date":1338430031,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms();\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      DocsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, true);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms();\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      DocsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, true);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"02331260bb246364779cb6f04919ca47900d01bb","date":1343749884,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms();\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      DocsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, DocsEnum.FLAG_FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms();\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      DocsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, true);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms();\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      DocsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, DocsEnum.FLAG_FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms();\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      DocsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, true);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms();\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      DocsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, DocsEnum.FLAG_FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms();\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      DocsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, true);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fb07ab105350b80ed9d63ca64b117084ed7391bc","date":1344824719,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    Fields fields = MultiFields.getFields(reader);\n\n    for (String fieldName : fields) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms(fieldName);\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      DocsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, DocsEnum.FLAG_FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms();\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      DocsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, DocsEnum.FLAG_FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c7869f64c874ebf7f317d22c00baf2b6857797a6","date":1344856617,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    Fields fields = MultiFields.getFields(reader);\n\n    for (String fieldName : fields) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms(fieldName);\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      DocsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, DocsEnum.FLAG_FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms();\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      DocsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, DocsEnum.FLAG_FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","date":1344867506,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    Fields fields = MultiFields.getFields(reader);\n\n    for (String fieldName : fields) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms(fieldName);\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      DocsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, DocsEnum.FLAG_FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms();\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      DocsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, DocsEnum.FLAG_FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"19275ba31e621f6da1b83bf13af75233876fd3d4","date":1374846698,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    Fields fields = MultiFields.getFields(reader);\n\n    for (String fieldName : fields) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms(fieldName);\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      DocsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, DocsEnum.FLAG_FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    Fields fields = MultiFields.getFields(reader);\n\n    for (String fieldName : fields) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms(fieldName);\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      DocsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, DocsEnum.FLAG_FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","date":1376366778,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    Fields fields = MultiFields.getFields(reader);\n\n    for (String fieldName : fields) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms(fieldName);\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      DocsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, DocsEnum.FLAG_FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    Fields fields = MultiFields.getFields(reader);\n\n    for (String fieldName : fields) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms(fieldName);\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      DocsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, DocsEnum.FLAG_FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    Fields fields = MultiFields.getFields(reader);\n\n    for (String fieldName : fields) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms(fieldName);\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      DocsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, DocsEnum.FLAG_FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    Fields fields = MultiFields.getFields(reader);\n\n    for (String fieldName : fields) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms(fieldName);\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      DocsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, DocsEnum.FLAG_FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    Fields fields = MultiFields.getFields(reader);\n\n    for (String fieldName : fields) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms(fieldName);\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      PostingsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, PostingsEnum.FLAG_FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    Fields fields = MultiFields.getFields(reader);\n\n    for (String fieldName : fields) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms(fieldName);\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      DocsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, DocsEnum.FLAG_FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e73063b92d958076ef4ae8beb5f493e8ccdcecb4","date":1424177215,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    Fields fields = MultiFields.getFields(reader);\n\n    for (String fieldName : fields) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms(fieldName);\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      PostingsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, PostingsEnum.FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    Fields fields = MultiFields.getFields(reader);\n\n    for (String fieldName : fields) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms(fieldName);\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      PostingsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, PostingsEnum.FLAG_FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","date":1428522487,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    Fields fields = MultiFields.getFields(reader);\n\n    for (String fieldName : fields) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms(fieldName);\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator();\n      PostingsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, PostingsEnum.FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    Fields fields = MultiFields.getFields(reader);\n\n    for (String fieldName : fields) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms(fieldName);\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator(null);\n      PostingsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, PostingsEnum.FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    Fields fields = MultiFields.getFields(reader);\n\n    for (String fieldName : fields) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms(fieldName);\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator();\n      PostingsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = TestUtil.docs(random(), termsEnum, docs, PostingsEnum.FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    Fields fields = MultiFields.getFields(reader);\n\n    for (String fieldName : fields) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms(fieldName);\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator();\n      PostingsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, PostingsEnum.FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9ddad75c20492a8ce36565647b4eaf7b6f3fd1f7","date":1538404752,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    Collection<String> fields = MultiFields.getIndexedFields(reader);\n\n    for (String fieldName : fields) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = MultiFields.getTerms(reader, fieldName);\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator();\n      PostingsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = TestUtil.docs(random(), termsEnum, docs, PostingsEnum.FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    Fields fields = MultiFields.getFields(reader);\n\n    for (String fieldName : fields) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = fields.terms(fieldName);\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator();\n      PostingsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = TestUtil.docs(random(), termsEnum, docs, PostingsEnum.FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"04e775de416dd2d8067b10db1c8af975a1d5017e","date":1539906554,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    Collection<String> fields = FieldInfos.getIndexedFields(reader);\n\n    for (String fieldName : fields) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = MultiTerms.getTerms(reader, fieldName);\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator();\n      PostingsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = TestUtil.docs(random(), termsEnum, docs, PostingsEnum.FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = DirectoryReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    Collection<String> fields = MultiFields.getIndexedFields(reader);\n\n    for (String fieldName : fields) {\n      if (fieldName.equals(DocMaker.ID_FIELD) || fieldName.equals(DocMaker.DATE_MSEC_FIELD) || fieldName.equals(DocMaker.TIME_SEC_FIELD)) {\n        continue;\n      }\n      Terms terms = MultiFields.getTerms(reader, fieldName);\n      if (terms == null) {\n        continue;\n      }\n      TermsEnum termsEnum = terms.iterator();\n      PostingsEnum docs = null;\n      while(termsEnum.next() != null) {\n        docs = TestUtil.docs(random(), termsEnum, docs, PostingsEnum.FREQS);\n        while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"6613659748fe4411a7dcf85266e55db1f95f7315":["19275ba31e621f6da1b83bf13af75233876fd3d4"],"fb07ab105350b80ed9d63ca64b117084ed7391bc":["02331260bb246364779cb6f04919ca47900d01bb"],"9ddad75c20492a8ce36565647b4eaf7b6f3fd1f7":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"19275ba31e621f6da1b83bf13af75233876fd3d4":["fb07ab105350b80ed9d63ca64b117084ed7391bc"],"51f5280f31484820499077f41fcdfe92d527d9dc":["6613659748fe4411a7dcf85266e55db1f95f7315"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["51f5280f31484820499077f41fcdfe92d527d9dc"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","fb07ab105350b80ed9d63ca64b117084ed7391bc"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","02331260bb246364779cb6f04919ca47900d01bb"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["fb07ab105350b80ed9d63ca64b117084ed7391bc"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","02331260bb246364779cb6f04919ca47900d01bb"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["9ddad75c20492a8ce36565647b4eaf7b6f3fd1f7"],"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9":["d6f074e73200c07d54f242d3880a8da5a35ff97b","fb07ab105350b80ed9d63ca64b117084ed7391bc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"02331260bb246364779cb6f04919ca47900d01bb":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"]},"commit2Childs":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"6613659748fe4411a7dcf85266e55db1f95f7315":["51f5280f31484820499077f41fcdfe92d527d9dc"],"fb07ab105350b80ed9d63ca64b117084ed7391bc":["19275ba31e621f6da1b83bf13af75233876fd3d4","c7869f64c874ebf7f317d22c00baf2b6857797a6","8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9"],"19275ba31e621f6da1b83bf13af75233876fd3d4":["6613659748fe4411a7dcf85266e55db1f95f7315"],"9ddad75c20492a8ce36565647b4eaf7b6f3fd1f7":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"51f5280f31484820499077f41fcdfe92d527d9dc":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["9ddad75c20492a8ce36565647b4eaf7b6f3fd1f7"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":[],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","02331260bb246364779cb6f04919ca47900d01bb"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["c7869f64c874ebf7f317d22c00baf2b6857797a6"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":[],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9":[],"02331260bb246364779cb6f04919ca47900d01bb":["fb07ab105350b80ed9d63ca64b117084ed7391bc","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c7869f64c874ebf7f317d22c00baf2b6857797a6","8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}