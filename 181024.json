{"path":"lucene/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","commits":[{"id":"60ba444201d2570214b6fcf1d15600dc1a01f548","date":1313868045,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","pathOld":"/dev/null","sourceNew":"  public void setUp() throws Exception {\n    super.setUp();\n    // we generate aweful regexps: good for testing.\n    // but for preflex codec, the test can be very slow, so use less iterations.\n    numIterations = CodecProvider.getDefault().getFieldCodec(\"field\").equals(\"PreFlex\") ? 10 * RANDOM_MULTIPLIER : atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = DaciukMihovAutomatonBuilder.build(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","sourceNew":"  public void setUp() throws Exception {\n    super.setUp();\n    // we generate aweful regexps: good for testing.\n    // but for preflex codec, the test can be very slow, so use less iterations.\n    numIterations = CodecProvider.getDefault().getFieldCodec(\"field\").equals(\"PreFlex\") ? 10 * RANDOM_MULTIPLIER : atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", StringField.TYPE_STORED);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = DaciukMihovAutomatonBuilder.build(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  public void setUp() throws Exception {\n    super.setUp();\n    // we generate aweful regexps: good for testing.\n    // but for preflex codec, the test can be very slow, so use less iterations.\n    numIterations = CodecProvider.getDefault().getFieldCodec(\"field\").equals(\"PreFlex\") ? 10 * RANDOM_MULTIPLIER : atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = DaciukMihovAutomatonBuilder.build(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","sourceNew":"  public void setUp() throws Exception {\n    super.setUp();\n    // we generate aweful regexps: good for testing.\n    // but for preflex codec, the test can be very slow, so use less iterations.\n    numIterations = Codec.getDefault().getName().equals(\"Lucene3x\") ? 10 * RANDOM_MULTIPLIER : atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", StringField.TYPE_STORED);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = DaciukMihovAutomatonBuilder.build(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  public void setUp() throws Exception {\n    super.setUp();\n    // we generate aweful regexps: good for testing.\n    // but for preflex codec, the test can be very slow, so use less iterations.\n    numIterations = CodecProvider.getDefault().getFieldCodec(\"field\").equals(\"PreFlex\") ? 10 * RANDOM_MULTIPLIER : atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", StringField.TYPE_STORED);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = DaciukMihovAutomatonBuilder.build(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","sourceNew":"  public void setUp() throws Exception {\n    super.setUp();\n    // we generate aweful regexps: good for testing.\n    // but for preflex codec, the test can be very slow, so use less iterations.\n    numIterations = Codec.getDefault().getName().equals(\"Lucene3x\") ? 10 * RANDOM_MULTIPLIER : atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", StringField.TYPE_STORED);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = DaciukMihovAutomatonBuilder.build(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  public void setUp() throws Exception {\n    super.setUp();\n    // we generate aweful regexps: good for testing.\n    // but for preflex codec, the test can be very slow, so use less iterations.\n    numIterations = Codec.getDefault().getName().equals(\"Lucene3x\") ? 10 * RANDOM_MULTIPLIER : atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", StringField.TYPE_STORED);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = DaciukMihovAutomatonBuilder.build(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"60ba444201d2570214b6fcf1d15600dc1a01f548":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"7b91922b55d15444d554721b352861d028eb8278":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["7b91922b55d15444d554721b352861d028eb8278"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["60ba444201d2570214b6fcf1d15600dc1a01f548"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"60ba444201d2570214b6fcf1d15600dc1a01f548":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"7b91922b55d15444d554721b352861d028eb8278":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["60ba444201d2570214b6fcf1d15600dc1a01f548"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["7b91922b55d15444d554721b352861d028eb8278"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}