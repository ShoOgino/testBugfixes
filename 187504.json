{"path":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","commits":[{"id":"ec58fb7921964848d01bea54f8ec4a2ac813eaeb","date":1295476876,"type":0,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    final Set<BytesRef> tokens = new HashSet<BytesRef>();\n    final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n    final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        final BytesRef bytes = new BytesRef();\n        bytesAtt.toBytesRef(bytes);\n        tokens.add(bytes);\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n    return tokens;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["782ed6a4b4ba50ec19734fc8db4e570ee193d627","782ed6a4b4ba50ec19734fc8db4e570ee193d627","782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e79a6d080bdd5b2a8f56342cf571b5476de04180","date":1295638686,"type":0,"author":"Michael Busch","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    final Set<BytesRef> tokens = new HashSet<BytesRef>();\n    final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n    final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        final BytesRef bytes = new BytesRef();\n        bytesAtt.toBytesRef(bytes);\n        tokens.add(bytes);\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n    return tokens;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    final Set<BytesRef> tokens = new HashSet<BytesRef>();\n    final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n    final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        final BytesRef bytes = new BytesRef();\n        bytesAtt.toBytesRef(bytes);\n        tokens.add(bytes);\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n    return tokens;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3d07f1ae3b58102f36f3393c397d78ba4e547a4","date":1300715535,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","pathOld":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","sourceNew":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    final Set<BytesRef> tokens = new HashSet<BytesRef>();\n    final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n    final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    final BytesRef bytes = bytesAtt.getBytesRef();\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(new BytesRef(bytes));\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n    return tokens;\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    final Set<BytesRef> tokens = new HashSet<BytesRef>();\n    final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n    final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        final BytesRef bytes = new BytesRef();\n        bytesAtt.toBytesRef(bytes);\n        tokens.add(bytes);\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d619839baa8ce5503e496b94a9e42ad6f079293f","date":1301309428,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","pathOld":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","sourceNew":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    final Set<BytesRef> tokens = new HashSet<BytesRef>();\n    final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n    final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    final BytesRef bytes = bytesAtt.getBytesRef();\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(new BytesRef(bytes));\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n    return tokens;\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    final Set<BytesRef> tokens = new HashSet<BytesRef>();\n    final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n    final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        final BytesRef bytes = new BytesRef();\n        bytesAtt.toBytesRef(bytes);\n        tokens.add(bytes);\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0ef0193974807e4bddf5432a6b0287fe4d6c9df","date":1301476645,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","pathOld":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","sourceNew":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    final Set<BytesRef> tokens = new HashSet<BytesRef>();\n    final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n    final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    final BytesRef bytes = bytesAtt.getBytesRef();\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(new BytesRef(bytes));\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n    return tokens;\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    final Set<BytesRef> tokens = new HashSet<BytesRef>();\n    final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n    final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        final BytesRef bytes = new BytesRef();\n        bytesAtt.toBytesRef(bytes);\n        tokens.add(bytes);\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","pathOld":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","sourceNew":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    final Set<BytesRef> tokens = new HashSet<BytesRef>();\n    final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n    final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    final BytesRef bytes = bytesAtt.getBytesRef();\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(new BytesRef(bytes));\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n    return tokens;\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    final Set<BytesRef> tokens = new HashSet<BytesRef>();\n    final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n    final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    final BytesRef bytes = bytesAtt.getBytesRef();\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(new BytesRef(bytes));\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","pathOld":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","sourceNew":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    final Set<BytesRef> tokens = new HashSet<BytesRef>();\n    final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n    final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    final BytesRef bytes = bytesAtt.getBytesRef();\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(new BytesRef(bytes));\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n    return tokens;\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    final Set<BytesRef> tokens = new HashSet<BytesRef>();\n    final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n    final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    final BytesRef bytes = bytesAtt.getBytesRef();\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(new BytesRef(bytes));\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":5,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","pathOld":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","sourceNew":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    final Set<BytesRef> tokens = new HashSet<BytesRef>();\n    final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n    final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    final BytesRef bytes = bytesAtt.getBytesRef();\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(new BytesRef(bytes));\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n    return tokens;\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    final Set<BytesRef> tokens = new HashSet<BytesRef>();\n    final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n    final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    final BytesRef bytes = bytesAtt.getBytesRef();\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(new BytesRef(bytes));\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["e79a6d080bdd5b2a8f56342cf571b5476de04180","b3d07f1ae3b58102f36f3393c397d78ba4e547a4"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["b3d07f1ae3b58102f36f3393c397d78ba4e547a4"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["29ef99d61cda9641b6250bf9567329a6e65f901d","b3d07f1ae3b58102f36f3393c397d78ba4e547a4"],"c26f00b574427b55127e869b935845554afde1fa":["b3d07f1ae3b58102f36f3393c397d78ba4e547a4","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","ec58fb7921964848d01bea54f8ec4a2ac813eaeb"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a258fbb26824fd104ed795e5d9033d2d040049ee":["b3d07f1ae3b58102f36f3393c397d78ba4e547a4"],"b3d07f1ae3b58102f36f3393c397d78ba4e547a4":["ec58fb7921964848d01bea54f8ec4a2ac813eaeb"],"ec58fb7921964848d01bea54f8ec4a2ac813eaeb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"e79a6d080bdd5b2a8f56342cf571b5476de04180":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","ec58fb7921964848d01bea54f8ec4a2ac813eaeb"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c26f00b574427b55127e869b935845554afde1fa"]},"commit2Childs":{"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":[],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"d619839baa8ce5503e496b94a9e42ad6f079293f":[],"29ef99d61cda9641b6250bf9567329a6e65f901d":["d619839baa8ce5503e496b94a9e42ad6f079293f"],"c26f00b574427b55127e869b935845554afde1fa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["29ef99d61cda9641b6250bf9567329a6e65f901d","ec58fb7921964848d01bea54f8ec4a2ac813eaeb","e79a6d080bdd5b2a8f56342cf571b5476de04180"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"b3d07f1ae3b58102f36f3393c397d78ba4e547a4":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df","c903c3d15906a3da96b8c0c2fb704491005fdbdb","d619839baa8ce5503e496b94a9e42ad6f079293f","c26f00b574427b55127e869b935845554afde1fa","a258fbb26824fd104ed795e5d9033d2d040049ee"],"ec58fb7921964848d01bea54f8ec4a2ac813eaeb":["29ef99d61cda9641b6250bf9567329a6e65f901d","b3d07f1ae3b58102f36f3393c397d78ba4e547a4","e79a6d080bdd5b2a8f56342cf571b5476de04180"],"e79a6d080bdd5b2a8f56342cf571b5476de04180":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df","d619839baa8ce5503e496b94a9e42ad6f079293f","a258fbb26824fd104ed795e5d9033d2d040049ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}