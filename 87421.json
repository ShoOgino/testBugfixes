{"path":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","commits":[{"id":"1786be6a11f9cf5e48ce84869d1bb71e9c02f966","date":1448381196,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","pathOld":"/dev/null","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      leafBlockDocIDs[leafCount] = reader.docIDBase + reader.docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount++, lastPackedValue, reader.state.scratchPackedValue);\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i]);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["b6d206ce7675894027133736953dbb79a81351c5"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b6d206ce7675894027133736953dbb79a81351c5","date":1450463132,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      leafBlockDocIDs[leafCount] = reader.docIDBase + reader.docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount++, lastPackedValue, reader.state.scratchPackedValue);\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i]);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      leafBlockDocIDs[leafCount] = reader.docIDBase + reader.docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount++, lastPackedValue, reader.state.scratchPackedValue);\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i]);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","bugFix":["1786be6a11f9cf5e48ce84869d1bb71e9c02f966"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ecf331f9d7bdd234863d2df2bb5c1f019979422f","date":1452250335,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      leafBlockDocIDs[leafCount] = reader.docIDBase + reader.docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount++, lastPackedValue, reader.state.scratchPackedValue);\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i]);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      leafBlockDocIDs[leafCount] = reader.docIDBase + reader.docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount++, lastPackedValue, reader.state.scratchPackedValue);\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i]);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"879e8cf5ab626b9bf29f1ef603e3a28601fcb1a7","date":1456959208,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      leafBlockDocIDs[leafCount] = reader.docIDBase + reader.docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue);\n      valueCount++;\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i]);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      leafBlockDocIDs[leafCount] = reader.docIDBase + reader.docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount++, lastPackedValue, reader.state.scratchPackedValue);\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i]);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"251c5b33f0a2c8988550b63c78ed22b0e84524e5","date":1456961997,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docIDBase + reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue);\n      valueCount++;\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i]);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      leafBlockDocIDs[leafCount] = reader.docIDBase + reader.docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue);\n      valueCount++;\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i]);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cf1a614098b46c9c22afebd7b898ae4d1d2fc273","date":1457088850,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docIDBase + reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue);\n      valueCount++;\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i]);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      leafBlockDocIDs[leafCount] = reader.docIDBase + reader.docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount++, lastPackedValue, reader.state.scratchPackedValue);\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i]);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9a5a0f27d9486cd33de88627ed3d2ff8dc5074ca","date":1457777566,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docIDBase + reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i]);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docIDBase + reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue);\n      valueCount++;\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i]);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"16ffb58ba57f805651a528311c104f104d9f4573","date":1457861471,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docIDBase + reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue, 0);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i], 0);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docIDBase + reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i]);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b9028cf27fe30db95667505bb92ecaee8fa3aef7","date":1457861734,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docIDBase + reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue, 0);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i], 0);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docIDBase + reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i]);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ceaef6cfc68c8ab22a684192e469a8280f9e6e70","date":1462354657,"type":5,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue, 0);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i], 0);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docIDBase + reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue, 0);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i], 0);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3d33e731a93d4b57e662ff094f64f94a745422d4","date":1463128289,"type":5,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue, 0);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i], 0);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docIDBase + reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue, 0);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i], 0);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0ad30c6a479e764150a3316e57263319775f1df2","date":1463395403,"type":5,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue, 0);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i], 0);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docIDBase + reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue, 0);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i], 0);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":5,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue, 0);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i], 0);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docIDBase + reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue, 0);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i], 0);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","sourceNew":null,"sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docIDBase + reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue, 0);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i], 0);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"1786be6a11f9cf5e48ce84869d1bb71e9c02f966":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"ecf331f9d7bdd234863d2df2bb5c1f019979422f":["b6d206ce7675894027133736953dbb79a81351c5"],"0ad30c6a479e764150a3316e57263319775f1df2":["b9028cf27fe30db95667505bb92ecaee8fa3aef7","3d33e731a93d4b57e662ff094f64f94a745422d4"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["b9028cf27fe30db95667505bb92ecaee8fa3aef7","d470c8182e92b264680e34081b75e70a9f2b3c89"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["b9028cf27fe30db95667505bb92ecaee8fa3aef7","0ad30c6a479e764150a3316e57263319775f1df2"],"251c5b33f0a2c8988550b63c78ed22b0e84524e5":["879e8cf5ab626b9bf29f1ef603e3a28601fcb1a7"],"16ffb58ba57f805651a528311c104f104d9f4573":["9a5a0f27d9486cd33de88627ed3d2ff8dc5074ca"],"b9028cf27fe30db95667505bb92ecaee8fa3aef7":["9a5a0f27d9486cd33de88627ed3d2ff8dc5074ca","16ffb58ba57f805651a528311c104f104d9f4573"],"ceaef6cfc68c8ab22a684192e469a8280f9e6e70":["b9028cf27fe30db95667505bb92ecaee8fa3aef7"],"9a5a0f27d9486cd33de88627ed3d2ff8dc5074ca":["cf1a614098b46c9c22afebd7b898ae4d1d2fc273"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"879e8cf5ab626b9bf29f1ef603e3a28601fcb1a7":["ecf331f9d7bdd234863d2df2bb5c1f019979422f"],"cf1a614098b46c9c22afebd7b898ae4d1d2fc273":["ecf331f9d7bdd234863d2df2bb5c1f019979422f","251c5b33f0a2c8988550b63c78ed22b0e84524e5"],"b6d206ce7675894027133736953dbb79a81351c5":["1786be6a11f9cf5e48ce84869d1bb71e9c02f966"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["b9028cf27fe30db95667505bb92ecaee8fa3aef7","ceaef6cfc68c8ab22a684192e469a8280f9e6e70"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d470c8182e92b264680e34081b75e70a9f2b3c89"]},"commit2Childs":{"1786be6a11f9cf5e48ce84869d1bb71e9c02f966":["b6d206ce7675894027133736953dbb79a81351c5"],"ecf331f9d7bdd234863d2df2bb5c1f019979422f":["879e8cf5ab626b9bf29f1ef603e3a28601fcb1a7","cf1a614098b46c9c22afebd7b898ae4d1d2fc273"],"0ad30c6a479e764150a3316e57263319775f1df2":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"d470c8182e92b264680e34081b75e70a9f2b3c89":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"251c5b33f0a2c8988550b63c78ed22b0e84524e5":["cf1a614098b46c9c22afebd7b898ae4d1d2fc273"],"16ffb58ba57f805651a528311c104f104d9f4573":["b9028cf27fe30db95667505bb92ecaee8fa3aef7"],"b9028cf27fe30db95667505bb92ecaee8fa3aef7":["0ad30c6a479e764150a3316e57263319775f1df2","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d470c8182e92b264680e34081b75e70a9f2b3c89","ceaef6cfc68c8ab22a684192e469a8280f9e6e70","3d33e731a93d4b57e662ff094f64f94a745422d4"],"ceaef6cfc68c8ab22a684192e469a8280f9e6e70":["3d33e731a93d4b57e662ff094f64f94a745422d4"],"9a5a0f27d9486cd33de88627ed3d2ff8dc5074ca":["16ffb58ba57f805651a528311c104f104d9f4573","b9028cf27fe30db95667505bb92ecaee8fa3aef7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1786be6a11f9cf5e48ce84869d1bb71e9c02f966"],"879e8cf5ab626b9bf29f1ef603e3a28601fcb1a7":["251c5b33f0a2c8988550b63c78ed22b0e84524e5"],"cf1a614098b46c9c22afebd7b898ae4d1d2fc273":["9a5a0f27d9486cd33de88627ed3d2ff8dc5074ca"],"b6d206ce7675894027133736953dbb79a81351c5":["ecf331f9d7bdd234863d2df2bb5c1f019979422f"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["0ad30c6a479e764150a3316e57263319775f1df2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}