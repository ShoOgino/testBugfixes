{"path":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(DocumentsWriter).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(DocumentsWriter).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(DocumentsWriter).mjava","sourceNew":"  private void innerPurge(DocumentsWriter writer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        try {\n          /*\n           * if we bock on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since ther could\n           * be a ticket still in the queue. \n           */\n          head.publish(writer);\n        } finally {\n          synchronized (this) {\n            // finally remove the publised ticket from the queue\n            final FlushTicket poll = queue.poll();\n            ticketCount.decrementAndGet();\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n  }\n\n","sourceOld":"  private void innerPurge(DocumentsWriter writer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        try {\n          /*\n           * if we bock on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since ther could\n           * be a ticket still in the queue. \n           */\n          head.publish(writer);\n        } finally {\n          synchronized (this) {\n            // finally remove the publised ticket from the queue\n            final FlushTicket poll = queue.poll();\n            ticketCount.decrementAndGet();\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d9773480aa9e800d0a232ab6ccac265e874b0c51","date":1349461188,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(DocumentsWriter).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(DocumentsWriter).mjava","sourceNew":"  private void innerPurge(DocumentsWriter writer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        try {\n          /*\n           * if we block on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since ther could\n           * be a ticket still in the queue. \n           */\n          head.publish(writer);\n        } finally {\n          synchronized (this) {\n            // finally remove the published ticket from the queue\n            final FlushTicket poll = queue.poll();\n            ticketCount.decrementAndGet();\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n  }\n\n","sourceOld":"  private void innerPurge(DocumentsWriter writer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        try {\n          /*\n           * if we bock on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since ther could\n           * be a ticket still in the queue. \n           */\n          head.publish(writer);\n        } finally {\n          synchronized (this) {\n            // finally remove the publised ticket from the queue\n            final FlushTicket poll = queue.poll();\n            ticketCount.decrementAndGet();\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7af110b00ea8df9429309d83e38e0533d82e144f","date":1376924768,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(IndexWriter).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(DocumentsWriter).mjava","sourceNew":"  private int innerPurge(IndexWriter writer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    int numPurged = 0;\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        numPurged++;\n        try {\n          /*\n           * if we block on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since ther could\n           * be a ticket still in the queue. \n           */\n          head.publish(writer);\n          \n        } finally {\n          synchronized (this) {\n            // finally remove the published ticket from the queue\n            final FlushTicket poll = queue.poll();\n            ticketCount.decrementAndGet();\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n    return numPurged;\n  }\n\n","sourceOld":"  private void innerPurge(DocumentsWriter writer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        try {\n          /*\n           * if we block on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since ther could\n           * be a ticket still in the queue. \n           */\n          head.publish(writer);\n        } finally {\n          synchronized (this) {\n            // finally remove the published ticket from the queue\n            final FlushTicket poll = queue.poll();\n            ticketCount.decrementAndGet();\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"31d4861802ca404d78ca1d15f4550eec415b9199","date":1376947894,"type":5,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(IndexWriter).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(DocumentsWriter).mjava","sourceNew":"  private int innerPurge(IndexWriter writer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    int numPurged = 0;\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        numPurged++;\n        try {\n          /*\n           * if we block on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since ther could\n           * be a ticket still in the queue. \n           */\n          head.publish(writer);\n          \n        } finally {\n          synchronized (this) {\n            // finally remove the published ticket from the queue\n            final FlushTicket poll = queue.poll();\n            ticketCount.decrementAndGet();\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n    return numPurged;\n  }\n\n","sourceOld":"  private void innerPurge(DocumentsWriter writer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        try {\n          /*\n           * if we block on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since ther could\n           * be a ticket still in the queue. \n           */\n          head.publish(writer);\n        } finally {\n          synchronized (this) {\n            // finally remove the published ticket from the queue\n            final FlushTicket poll = queue.poll();\n            ticketCount.decrementAndGet();\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":4,"author":"Han Jiang","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(DocumentsWriter).mjava","sourceNew":null,"sourceOld":"  private void innerPurge(DocumentsWriter writer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        try {\n          /*\n           * if we block on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since ther could\n           * be a ticket still in the queue. \n           */\n          head.publish(writer);\n        } finally {\n          synchronized (this) {\n            // finally remove the published ticket from the queue\n            final FlushTicket poll = queue.poll();\n            ticketCount.decrementAndGet();\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d9773480aa9e800d0a232ab6ccac265e874b0c51":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"7af110b00ea8df9429309d83e38e0533d82e144f":["d9773480aa9e800d0a232ab6ccac265e874b0c51"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["d9773480aa9e800d0a232ab6ccac265e874b0c51","7af110b00ea8df9429309d83e38e0533d82e144f"],"31d4861802ca404d78ca1d15f4550eec415b9199":["d9773480aa9e800d0a232ab6ccac265e874b0c51","7af110b00ea8df9429309d83e38e0533d82e144f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["7af110b00ea8df9429309d83e38e0533d82e144f"]},"commit2Childs":{"d9773480aa9e800d0a232ab6ccac265e874b0c51":["7af110b00ea8df9429309d83e38e0533d82e144f","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","31d4861802ca404d78ca1d15f4550eec415b9199"],"7af110b00ea8df9429309d83e38e0533d82e144f":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","31d4861802ca404d78ca1d15f4550eec415b9199","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["d9773480aa9e800d0a232ab6ccac265e874b0c51"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"31d4861802ca404d78ca1d15f4550eec415b9199":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","31d4861802ca404d78ca1d15f4550eec415b9199","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}