{"path":"lucene/analysis/nori/src/tools/java/org/apache/lucene/analysis/ko/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","commits":[{"id":"8493925b2e70246f0961df584c01a8c2e61ee52f","date":1523611602,"type":0,"author":"Jim Ferenczi","isMerge":false,"pathNew":"lucene/analysis/nori/src/tools/java/org/apache/lucene/analysis/ko/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"/dev/null","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 12) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n\n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          lines.add(normalizedEntry);\n        } else {\n          lines.add(entry);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, Comparator.comparing(left -> left[0]));\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n\n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n\n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.setLength(token.length());\n        for (int i = 0; i < token.length(); i++) {\n          scratch.setIntAt(i, (int) token.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n\n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["12136ebc45a29f27a6ab47b007873d0f630f8d11"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5c6453827f947004a68ad9db7418781e9df2f660","date":1523626811,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/analysis/nori/src/tools/java/org/apache/lucene/analysis/ko/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"/dev/null","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 12) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n\n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          lines.add(normalizedEntry);\n        } else {\n          lines.add(entry);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, Comparator.comparing(left -> left[0]));\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n\n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n\n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.setLength(token.length());\n        for (int i = 0; i < token.length(); i++) {\n          scratch.setIntAt(i, (int) token.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n\n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"12136ebc45a29f27a6ab47b007873d0f630f8d11","date":1540542517,"type":3,"author":"Jim Ferenczi","isMerge":false,"pathNew":"lucene/analysis/nori/src/tools/java/org/apache/lucene/analysis/ko/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/nori/src/tools/java/org/apache/lucene/analysis/ko/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 12) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n\n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          lines.add(normalizedEntry);\n        } else {\n          lines.add(entry);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, Comparator.comparing(left -> left[0]));\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      String surfaceForm = entry[0].trim();\n      if (surfaceForm.isEmpty()) {\n        continue;\n      }\n      int next = dictionary.put(entry);\n\n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n\n      if (!surfaceForm.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = surfaceForm;\n        scratch.grow(surfaceForm.length());\n        scratch.setLength(surfaceForm.length());\n        for (int i = 0; i < surfaceForm.length(); i++) {\n          scratch.setIntAt(i, (int) surfaceForm.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n\n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 12) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n\n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          lines.add(normalizedEntry);\n        } else {\n          lines.add(entry);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, Comparator.comparing(left -> left[0]));\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n\n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n\n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.setLength(token.length());\n        for (int i = 0; i < token.length(); i++) {\n          scratch.setIntAt(i, (int) token.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n\n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":["8493925b2e70246f0961df584c01a8c2e61ee52f"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae67e1f82a53594208ca929f382ee861dad3d7a8","date":1557134375,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/analysis/nori/src/tools/java/org/apache/lucene/analysis/ko/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/nori/src/tools/java/org/apache/lucene/analysis/ko/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 12) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n\n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          lines.add(normalizedEntry);\n        } else {\n          lines.add(entry);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, Comparator.comparing(left -> left[0]));\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15, false);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      String surfaceForm = entry[0].trim();\n      if (surfaceForm.isEmpty()) {\n        continue;\n      }\n      int next = dictionary.put(entry);\n\n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n\n      if (!surfaceForm.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = surfaceForm;\n        scratch.grow(surfaceForm.length());\n        scratch.setLength(surfaceForm.length());\n        for (int i = 0; i < surfaceForm.length(); i++) {\n          scratch.setIntAt(i, (int) surfaceForm.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n\n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 12) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n\n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          lines.add(normalizedEntry);\n        } else {\n          lines.add(entry);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, Comparator.comparing(left -> left[0]));\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      String surfaceForm = entry[0].trim();\n      if (surfaceForm.isEmpty()) {\n        continue;\n      }\n      int next = dictionary.put(entry);\n\n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n\n      if (!surfaceForm.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = surfaceForm;\n        scratch.grow(surfaceForm.length());\n        scratch.setLength(surfaceForm.length());\n        for (int i = 0; i < surfaceForm.length(); i++) {\n          scratch.setIntAt(i, (int) surfaceForm.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n\n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"59a1204a92958bea883656169000a87a6c55c2d0","date":1562106073,"type":3,"author":"Michael Sokolov","isMerge":false,"pathNew":"lucene/analysis/nori/src/tools/java/org/apache/lucene/analysis/ko/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/nori/src/tools/java/org/apache/lucene/analysis/ko/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 12) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n\n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          lines.add(normalizedEntry);\n        } else {\n          lines.add(entry);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, Comparator.comparing(left -> left[0]));\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      String surfaceForm = entry[0].trim();\n      if (surfaceForm.isEmpty()) {\n        continue;\n      }\n      int next = dictionary.put(entry);\n\n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n\n      if (!surfaceForm.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = surfaceForm;\n        scratch.grow(surfaceForm.length());\n        scratch.setLength(surfaceForm.length());\n        for (int i = 0; i < surfaceForm.length(); i++) {\n          scratch.setIntAt(i, (int) surfaceForm.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n\n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 12) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n\n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          lines.add(normalizedEntry);\n        } else {\n          lines.add(entry);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, Comparator.comparing(left -> left[0]));\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15, false);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      String surfaceForm = entry[0].trim();\n      if (surfaceForm.isEmpty()) {\n        continue;\n      }\n      int next = dictionary.put(entry);\n\n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n\n      if (!surfaceForm.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = surfaceForm;\n        scratch.grow(surfaceForm.length());\n        scratch.setLength(surfaceForm.length());\n        for (int i = 0; i < surfaceForm.length(); i++) {\n          scratch.setIntAt(i, (int) surfaceForm.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n\n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c149f6975379ebb860e93139126a8aabf8e2b66d","date":1562857174,"type":3,"author":"Namgyu Kim","isMerge":false,"pathNew":"lucene/analysis/nori/src/tools/java/org/apache/lucene/analysis/ko/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/nori/src/tools/java/org/apache/lucene/analysis/ko/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 12) {\n          throw new IllegalArgumentException(\"Entry in CSV is not valid (12 field values expected): \" + line);\n        }\n\n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          lines.add(normalizedEntry);\n        } else {\n          lines.add(entry);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, Comparator.comparing(left -> left[0]));\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      String surfaceForm = entry[0].trim();\n      if (surfaceForm.isEmpty()) {\n        continue;\n      }\n      int next = dictionary.put(entry);\n\n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n\n      if (!surfaceForm.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = surfaceForm;\n        scratch.grow(surfaceForm.length());\n        scratch.setLength(surfaceForm.length());\n        for (int i = 0; i < surfaceForm.length(); i++) {\n          scratch.setIntAt(i, (int) surfaceForm.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n\n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 12) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n\n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          lines.add(normalizedEntry);\n        } else {\n          lines.add(entry);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, Comparator.comparing(left -> left[0]));\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      String surfaceForm = entry[0].trim();\n      if (surfaceForm.isEmpty()) {\n        continue;\n      }\n      int next = dictionary.put(entry);\n\n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n\n      if (!surfaceForm.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = surfaceForm;\n        scratch.grow(surfaceForm.length());\n        scratch.setLength(surfaceForm.length());\n        for (int i = 0; i < surfaceForm.length(); i++) {\n          scratch.setIntAt(i, (int) surfaceForm.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n\n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7b61fb1b05654ba49bbbfa4f574bab65e0b2bcfa","date":1563808081,"type":3,"author":"Namgyu Kim","isMerge":false,"pathNew":"lucene/analysis/nori/src/tools/java/org/apache/lucene/analysis/ko/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/nori/src/tools/java/org/apache/lucene/analysis/ko/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 12) {\n          throw new IllegalArgumentException(\"Entry in CSV is not valid (12 field values expected): \" + line);\n        }\n\n        // NFKC normalize dictionary entry\n        if (normalForm != null) {\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = Normalizer.normalize(entry[i], normalForm);\n          }\n          lines.add(normalizedEntry);\n        } else {\n          lines.add(entry);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, Comparator.comparing(left -> left[0]));\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      String surfaceForm = entry[0].trim();\n      if (surfaceForm.isEmpty()) {\n        continue;\n      }\n      int next = dictionary.put(entry);\n\n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n\n      if (!surfaceForm.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = surfaceForm;\n        scratch.grow(surfaceForm.length());\n        scratch.setLength(surfaceForm.length());\n        for (int i = 0; i < surfaceForm.length(); i++) {\n          scratch.setIntAt(i, (int) surfaceForm.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n\n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 12) {\n          throw new IllegalArgumentException(\"Entry in CSV is not valid (12 field values expected): \" + line);\n        }\n\n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          lines.add(normalizedEntry);\n        } else {\n          lines.add(entry);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, Comparator.comparing(left -> left[0]));\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      String surfaceForm = entry[0].trim();\n      if (surfaceForm.isEmpty()) {\n        continue;\n      }\n      int next = dictionary.put(entry);\n\n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n\n      if (!surfaceForm.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = surfaceForm;\n        scratch.grow(surfaceForm.length());\n        scratch.setLength(surfaceForm.length());\n        for (int i = 0; i < surfaceForm.length(); i++) {\n          scratch.setIntAt(i, (int) surfaceForm.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n\n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6977470bb0ac9af26b57d86129e64237ec970957","date":1563892586,"type":3,"author":"Namgyu Kim","isMerge":false,"pathNew":"lucene/analysis/nori/src/tools/java/org/apache/lucene/analysis/ko/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/nori/src/tools/java/org/apache/lucene/analysis/ko/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 12) {\n          throw new IllegalArgumentException(\"Entry in CSV is not valid (12 field values expected): \" + line);\n        }\n\n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          lines.add(normalizedEntry);\n        } else {\n          lines.add(entry);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, Comparator.comparing(left -> left[0]));\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      String surfaceForm = entry[0].trim();\n      if (surfaceForm.isEmpty()) {\n        continue;\n      }\n      int next = dictionary.put(entry);\n\n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n\n      if (!surfaceForm.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = surfaceForm;\n        scratch.grow(surfaceForm.length());\n        scratch.setLength(surfaceForm.length());\n        for (int i = 0; i < surfaceForm.length(); i++) {\n          scratch.setIntAt(i, (int) surfaceForm.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n\n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 12) {\n          throw new IllegalArgumentException(\"Entry in CSV is not valid (12 field values expected): \" + line);\n        }\n\n        // NFKC normalize dictionary entry\n        if (normalForm != null) {\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = Normalizer.normalize(entry[i], normalForm);\n          }\n          lines.add(normalizedEntry);\n        } else {\n          lines.add(entry);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, Comparator.comparing(left -> left[0]));\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      String surfaceForm = entry[0].trim();\n      if (surfaceForm.isEmpty()) {\n        continue;\n      }\n      int next = dictionary.put(entry);\n\n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n\n      if (!surfaceForm.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = surfaceForm;\n        scratch.grow(surfaceForm.length());\n        scratch.setLength(surfaceForm.length());\n        for (int i = 0; i < surfaceForm.length(); i++) {\n          scratch.setIntAt(i, (int) surfaceForm.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n\n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bd2b2593da5ccd4727ac03d74d56133de4a697c2","date":1563893970,"type":3,"author":"Namgyu Kim","isMerge":false,"pathNew":"lucene/analysis/nori/src/tools/java/org/apache/lucene/analysis/ko/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/nori/src/tools/java/org/apache/lucene/analysis/ko/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 12) {\n          throw new IllegalArgumentException(\"Entry in CSV is not valid (12 field values expected): \" + line);\n        }\n\n        // NFKC normalize dictionary entry\n        if (normalForm != null) {\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = Normalizer.normalize(entry[i], normalForm);\n          }\n          lines.add(normalizedEntry);\n        } else {\n          lines.add(entry);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, Comparator.comparing(left -> left[0]));\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      String surfaceForm = entry[0].trim();\n      if (surfaceForm.isEmpty()) {\n        continue;\n      }\n      int next = dictionary.put(entry);\n\n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n\n      if (!surfaceForm.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = surfaceForm;\n        scratch.grow(surfaceForm.length());\n        scratch.setLength(surfaceForm.length());\n        for (int i = 0; i < surfaceForm.length(); i++) {\n          scratch.setIntAt(i, (int) surfaceForm.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n\n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 12) {\n          throw new IllegalArgumentException(\"Entry in CSV is not valid (12 field values expected): \" + line);\n        }\n\n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          lines.add(normalizedEntry);\n        } else {\n          lines.add(entry);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, Comparator.comparing(left -> left[0]));\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      String surfaceForm = entry[0].trim();\n      if (surfaceForm.isEmpty()) {\n        continue;\n      }\n      int next = dictionary.put(entry);\n\n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n\n      if (!surfaceForm.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = surfaceForm;\n        scratch.grow(surfaceForm.length());\n        scratch.setLength(surfaceForm.length());\n        for (int i = 0; i < surfaceForm.length(); i++) {\n          scratch.setIntAt(i, (int) surfaceForm.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n\n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e3010cab237afb0b81c042f263115756e3cc6d67","date":1564503244,"type":5,"author":"Namgyu Kim","isMerge":false,"pathNew":"lucene/analysis/nori/src/java/org/apache/lucene/analysis/ko/util/TokenInfoDictionaryBuilder#buildDictionary(List[Path]).mjava","pathOld":"lucene/analysis/nori/src/tools/java/org/apache/lucene/analysis/ko/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  private TokenInfoDictionaryWriter buildDictionary(List<Path> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    // all lines in the file\n    List<String[]> lines = new ArrayList<>(400000);\n    for (Path path : csvFiles) {\n      try (BufferedReader reader = Files.newBufferedReader(path, Charset.forName(encoding))) {\n        String line;\n        while ((line = reader.readLine()) != null) {\n          String[] entry = CSVUtil.parse(line);\n\n          if (entry.length < 12) {\n            throw new IllegalArgumentException(\"Entry in CSV is not valid (12 field values expected): \" + line);\n          }\n\n          // NFKC normalize dictionary entry\n          if (normalForm != null) {\n            String[] normalizedEntry = new String[entry.length];\n            for (int i = 0; i < entry.length; i++) {\n              normalizedEntry[i] = Normalizer.normalize(entry[i], normalForm);\n            }\n            lines.add(normalizedEntry);\n          } else {\n            lines.add(entry);\n          }\n        }\n      }\n    }\n    \n    // sort by term: we sorted the files already and use a stable sort.\n    lines.sort(Comparator.comparing(left -> left[0]));\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build token info dictionary\n    for (String[] entry : lines) {\n      String surfaceForm = entry[0].trim();\n      if (surfaceForm.isEmpty()) {\n        continue;\n      }\n      int next = dictionary.put(entry);\n\n      if(next == offset) {\n        throw new IllegalStateException(\"Failed to process line: \" + Arrays.toString(entry));\n      }\n\n      if (!surfaceForm.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = surfaceForm;\n        scratch.grow(surfaceForm.length());\n        scratch.setLength(surfaceForm.length());\n        for (int i = 0; i < surfaceForm.length(); i++) {\n          scratch.setIntAt(i, (int) surfaceForm.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int) ord, offset);\n      offset = next;\n    }\n    dictionary.setFST(fstBuilder.finish());\n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 12) {\n          throw new IllegalArgumentException(\"Entry in CSV is not valid (12 field values expected): \" + line);\n        }\n\n        // NFKC normalize dictionary entry\n        if (normalForm != null) {\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = Normalizer.normalize(entry[i], normalForm);\n          }\n          lines.add(normalizedEntry);\n        } else {\n          lines.add(entry);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, Comparator.comparing(left -> left[0]));\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      String surfaceForm = entry[0].trim();\n      if (surfaceForm.isEmpty()) {\n        continue;\n      }\n      int next = dictionary.put(entry);\n\n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n\n      if (!surfaceForm.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = surfaceForm;\n        scratch.grow(surfaceForm.length());\n        scratch.setLength(surfaceForm.length());\n        for (int i = 0; i < surfaceForm.length(); i++) {\n          scratch.setIntAt(i, (int) surfaceForm.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n\n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f8061ddd97f3352007d927dae445884a6f3d857b","date":1564988276,"type":5,"author":"Atri Sharma","isMerge":true,"pathNew":"lucene/analysis/nori/src/java/org/apache/lucene/analysis/ko/util/TokenInfoDictionaryBuilder#buildDictionary(List[Path]).mjava","pathOld":"lucene/analysis/nori/src/tools/java/org/apache/lucene/analysis/ko/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  private TokenInfoDictionaryWriter buildDictionary(List<Path> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    // all lines in the file\n    List<String[]> lines = new ArrayList<>(400000);\n    for (Path path : csvFiles) {\n      try (BufferedReader reader = Files.newBufferedReader(path, Charset.forName(encoding))) {\n        String line;\n        while ((line = reader.readLine()) != null) {\n          String[] entry = CSVUtil.parse(line);\n\n          if (entry.length < 12) {\n            throw new IllegalArgumentException(\"Entry in CSV is not valid (12 field values expected): \" + line);\n          }\n\n          // NFKC normalize dictionary entry\n          if (normalForm != null) {\n            String[] normalizedEntry = new String[entry.length];\n            for (int i = 0; i < entry.length; i++) {\n              normalizedEntry[i] = Normalizer.normalize(entry[i], normalForm);\n            }\n            lines.add(normalizedEntry);\n          } else {\n            lines.add(entry);\n          }\n        }\n      }\n    }\n    \n    // sort by term: we sorted the files already and use a stable sort.\n    lines.sort(Comparator.comparing(left -> left[0]));\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build token info dictionary\n    for (String[] entry : lines) {\n      String surfaceForm = entry[0].trim();\n      if (surfaceForm.isEmpty()) {\n        continue;\n      }\n      int next = dictionary.put(entry);\n\n      if(next == offset) {\n        throw new IllegalStateException(\"Failed to process line: \" + Arrays.toString(entry));\n      }\n\n      if (!surfaceForm.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = surfaceForm;\n        scratch.grow(surfaceForm.length());\n        scratch.setLength(surfaceForm.length());\n        for (int i = 0; i < surfaceForm.length(); i++) {\n          scratch.setIntAt(i, (int) surfaceForm.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int) ord, offset);\n      offset = next;\n    }\n    dictionary.setFST(fstBuilder.finish());\n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 12) {\n          throw new IllegalArgumentException(\"Entry in CSV is not valid (12 field values expected): \" + line);\n        }\n\n        // NFKC normalize dictionary entry\n        if (normalForm != null) {\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = Normalizer.normalize(entry[i], normalForm);\n          }\n          lines.add(normalizedEntry);\n        } else {\n          lines.add(entry);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, Comparator.comparing(left -> left[0]));\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      String surfaceForm = entry[0].trim();\n      if (surfaceForm.isEmpty()) {\n        continue;\n      }\n      int next = dictionary.put(entry);\n\n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n\n      if (!surfaceForm.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = surfaceForm;\n        scratch.grow(surfaceForm.length());\n        scratch.setLength(surfaceForm.length());\n        for (int i = 0; i < surfaceForm.length(); i++) {\n          scratch.setIntAt(i, (int) surfaceForm.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n\n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7b61fb1b05654ba49bbbfa4f574bab65e0b2bcfa":["c149f6975379ebb860e93139126a8aabf8e2b66d"],"6977470bb0ac9af26b57d86129e64237ec970957":["7b61fb1b05654ba49bbbfa4f574bab65e0b2bcfa"],"12136ebc45a29f27a6ab47b007873d0f630f8d11":["5c6453827f947004a68ad9db7418781e9df2f660"],"8493925b2e70246f0961df584c01a8c2e61ee52f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"ae67e1f82a53594208ca929f382ee861dad3d7a8":["12136ebc45a29f27a6ab47b007873d0f630f8d11"],"f8061ddd97f3352007d927dae445884a6f3d857b":["bd2b2593da5ccd4727ac03d74d56133de4a697c2","e3010cab237afb0b81c042f263115756e3cc6d67"],"59a1204a92958bea883656169000a87a6c55c2d0":["ae67e1f82a53594208ca929f382ee861dad3d7a8"],"c149f6975379ebb860e93139126a8aabf8e2b66d":["59a1204a92958bea883656169000a87a6c55c2d0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"bd2b2593da5ccd4727ac03d74d56133de4a697c2":["6977470bb0ac9af26b57d86129e64237ec970957"],"e3010cab237afb0b81c042f263115756e3cc6d67":["bd2b2593da5ccd4727ac03d74d56133de4a697c2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e3010cab237afb0b81c042f263115756e3cc6d67"],"5c6453827f947004a68ad9db7418781e9df2f660":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","8493925b2e70246f0961df584c01a8c2e61ee52f"]},"commit2Childs":{"7b61fb1b05654ba49bbbfa4f574bab65e0b2bcfa":["6977470bb0ac9af26b57d86129e64237ec970957"],"6977470bb0ac9af26b57d86129e64237ec970957":["bd2b2593da5ccd4727ac03d74d56133de4a697c2"],"12136ebc45a29f27a6ab47b007873d0f630f8d11":["ae67e1f82a53594208ca929f382ee861dad3d7a8"],"8493925b2e70246f0961df584c01a8c2e61ee52f":["5c6453827f947004a68ad9db7418781e9df2f660"],"ae67e1f82a53594208ca929f382ee861dad3d7a8":["59a1204a92958bea883656169000a87a6c55c2d0"],"f8061ddd97f3352007d927dae445884a6f3d857b":[],"59a1204a92958bea883656169000a87a6c55c2d0":["c149f6975379ebb860e93139126a8aabf8e2b66d"],"c149f6975379ebb860e93139126a8aabf8e2b66d":["7b61fb1b05654ba49bbbfa4f574bab65e0b2bcfa"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["8493925b2e70246f0961df584c01a8c2e61ee52f","5c6453827f947004a68ad9db7418781e9df2f660"],"bd2b2593da5ccd4727ac03d74d56133de4a697c2":["f8061ddd97f3352007d927dae445884a6f3d857b","e3010cab237afb0b81c042f263115756e3cc6d67"],"e3010cab237afb0b81c042f263115756e3cc6d67":["f8061ddd97f3352007d927dae445884a6f3d857b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5c6453827f947004a68ad9db7418781e9df2f660":["12136ebc45a29f27a6ab47b007873d0f630f8d11"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["f8061ddd97f3352007d927dae445884a6f3d857b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}