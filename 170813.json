{"path":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","commits":[{"id":"e91746d568843981a68fb96dcecc7f383eb55701","date":1347520198,"type":0,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","pathOld":"/dev/null","sourceNew":"  private String[] tokenizeDoc(String doc) throws IOException {\n    Collection<String> result = new LinkedList<String>();\n    TokenStream tokenStream = analyzer.tokenStream(textFieldName, new StringReader(doc));\n    CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n    tokenStream.reset();\n    while (tokenStream.incrementToken()) {\n      result.add(charTermAttribute.toString());\n    }\n    tokenStream.end();\n    tokenStream.close();\n    return result.toArray(new String[result.size()]);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["c83d6c4335f31cae14f625a222bc842f20073dcd","782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c83d6c4335f31cae14f625a222bc842f20073dcd","date":1373306148,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","sourceNew":"  private String[] tokenizeDoc(String doc) throws IOException {\n    Collection<String> result = new LinkedList<String>();\n    TokenStream tokenStream = analyzer.tokenStream(textFieldName, doc);\n    CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n    tokenStream.reset();\n    while (tokenStream.incrementToken()) {\n      result.add(charTermAttribute.toString());\n    }\n    tokenStream.end();\n    tokenStream.close();\n    return result.toArray(new String[result.size()]);\n  }\n\n","sourceOld":"  private String[] tokenizeDoc(String doc) throws IOException {\n    Collection<String> result = new LinkedList<String>();\n    TokenStream tokenStream = analyzer.tokenStream(textFieldName, new StringReader(doc));\n    CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n    tokenStream.reset();\n    while (tokenStream.incrementToken()) {\n      result.add(charTermAttribute.toString());\n    }\n    tokenStream.end();\n    tokenStream.close();\n    return result.toArray(new String[result.size()]);\n  }\n\n","bugFix":["e91746d568843981a68fb96dcecc7f383eb55701"],"bugIntro":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","sourceNew":"  private String[] tokenizeDoc(String doc) throws IOException {\n    Collection<String> result = new LinkedList<String>();\n    TokenStream tokenStream = analyzer.tokenStream(textFieldName, doc);\n    CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n    tokenStream.reset();\n    while (tokenStream.incrementToken()) {\n      result.add(charTermAttribute.toString());\n    }\n    tokenStream.end();\n    tokenStream.close();\n    return result.toArray(new String[result.size()]);\n  }\n\n","sourceOld":"  private String[] tokenizeDoc(String doc) throws IOException {\n    Collection<String> result = new LinkedList<String>();\n    TokenStream tokenStream = analyzer.tokenStream(textFieldName, new StringReader(doc));\n    CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n    tokenStream.reset();\n    while (tokenStream.incrementToken()) {\n      result.add(charTermAttribute.toString());\n    }\n    tokenStream.end();\n    tokenStream.close();\n    return result.toArray(new String[result.size()]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"782ed6a4b4ba50ec19734fc8db4e570ee193d627","date":1381127065,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","sourceNew":"  private String[] tokenizeDoc(String doc) throws IOException {\n    Collection<String> result = new LinkedList<String>();\n    try (TokenStream tokenStream = analyzer.tokenStream(textFieldName, doc)) {\n      CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        result.add(charTermAttribute.toString());\n      }\n      tokenStream.end();\n    }\n    return result.toArray(new String[result.size()]);\n  }\n\n","sourceOld":"  private String[] tokenizeDoc(String doc) throws IOException {\n    Collection<String> result = new LinkedList<String>();\n    TokenStream tokenStream = analyzer.tokenStream(textFieldName, doc);\n    CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n    tokenStream.reset();\n    while (tokenStream.incrementToken()) {\n      result.add(charTermAttribute.toString());\n    }\n    tokenStream.end();\n    tokenStream.close();\n    return result.toArray(new String[result.size()]);\n  }\n\n","bugFix":["e91746d568843981a68fb96dcecc7f383eb55701","c83d6c4335f31cae14f625a222bc842f20073dcd"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a1f508b269e97eeeb33e0d21c851eceb57bfd1eb","date":1383406909,"type":3,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","sourceNew":"  private String[] tokenizeDoc(String doc) throws IOException {\n    Collection<String> result = new LinkedList<String>();\n    for (String textFieldName : textFieldNames) {\n      try (TokenStream tokenStream = analyzer.tokenStream(textFieldName, doc)) {\n        CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n        tokenStream.reset();\n        while (tokenStream.incrementToken()) {\n          result.add(charTermAttribute.toString());\n        }\n        tokenStream.end();\n      }\n    }\n    return result.toArray(new String[result.size()]);\n  }\n\n","sourceOld":"  private String[] tokenizeDoc(String doc) throws IOException {\n    Collection<String> result = new LinkedList<String>();\n    try (TokenStream tokenStream = analyzer.tokenStream(textFieldName, doc)) {\n      CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        result.add(charTermAttribute.toString());\n      }\n      tokenStream.end();\n    }\n    return result.toArray(new String[result.size()]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","sourceNew":"  private String[] tokenizeDoc(String doc) throws IOException {\n    Collection<String> result = new LinkedList<>();\n    for (String textFieldName : textFieldNames) {\n      try (TokenStream tokenStream = analyzer.tokenStream(textFieldName, doc)) {\n        CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n        tokenStream.reset();\n        while (tokenStream.incrementToken()) {\n          result.add(charTermAttribute.toString());\n        }\n        tokenStream.end();\n      }\n    }\n    return result.toArray(new String[result.size()]);\n  }\n\n","sourceOld":"  private String[] tokenizeDoc(String doc) throws IOException {\n    Collection<String> result = new LinkedList<String>();\n    for (String textFieldName : textFieldNames) {\n      try (TokenStream tokenStream = analyzer.tokenStream(textFieldName, doc)) {\n        CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n        tokenStream.reset();\n        while (tokenStream.incrementToken()) {\n          result.add(charTermAttribute.toString());\n        }\n        tokenStream.end();\n      }\n    }\n    return result.toArray(new String[result.size()]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7533828cbcc5f498a44cca5d9bce92692663778","date":1408525002,"type":3,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","sourceNew":"  protected String[] tokenizeDoc(String doc) throws IOException {\n    Collection<String> result = new LinkedList<>();\n    for (String textFieldName : textFieldNames) {\n      try (TokenStream tokenStream = analyzer.tokenStream(textFieldName, doc)) {\n        CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n        tokenStream.reset();\n        while (tokenStream.incrementToken()) {\n          result.add(charTermAttribute.toString());\n        }\n        tokenStream.end();\n      }\n    }\n    return result.toArray(new String[result.size()]);\n  }\n\n","sourceOld":"  private String[] tokenizeDoc(String doc) throws IOException {\n    Collection<String> result = new LinkedList<>();\n    for (String textFieldName : textFieldNames) {\n      try (TokenStream tokenStream = analyzer.tokenStream(textFieldName, doc)) {\n        CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n        tokenStream.reset();\n        while (tokenStream.incrementToken()) {\n          result.add(charTermAttribute.toString());\n        }\n        tokenStream.end();\n      }\n    }\n    return result.toArray(new String[result.size()]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"be961b1729b3dfa66883980eadaa4ca4732820e1","date":1408694535,"type":3,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","sourceNew":"  /**\n   * tokenize a <code>String</code> on this classifier's text fields and analyzer\n   *\n   * @param doc the <code>String</code> representing an input text (to be classified)\n   * @return\n   * @throws IOException\n   */\n  protected String[] tokenizeDoc(String doc) throws IOException {\n    Collection<String> result = new LinkedList<>();\n    for (String textFieldName : textFieldNames) {\n      try (TokenStream tokenStream = analyzer.tokenStream(textFieldName, doc)) {\n        CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n        tokenStream.reset();\n        while (tokenStream.incrementToken()) {\n          result.add(charTermAttribute.toString());\n        }\n        tokenStream.end();\n      }\n    }\n    return result.toArray(new String[result.size()]);\n  }\n\n","sourceOld":"  protected String[] tokenizeDoc(String doc) throws IOException {\n    Collection<String> result = new LinkedList<>();\n    for (String textFieldName : textFieldNames) {\n      try (TokenStream tokenStream = analyzer.tokenStream(textFieldName, doc)) {\n        CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n        tokenStream.reset();\n        while (tokenStream.incrementToken()) {\n          result.add(charTermAttribute.toString());\n        }\n        tokenStream.end();\n      }\n    }\n    return result.toArray(new String[result.size()]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a100f4e9288c563fb03c083266f03fcd1ba0c585","date":1408742209,"type":3,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","sourceNew":"  /**\n   * tokenize a <code>String</code> on this classifier's text fields and analyzer\n   *\n   * @param doc the <code>String</code> representing an input text (to be classified)\n   * @return a <code>String</code> array of the resulting tokens\n   * @throws IOException\n   */\n  protected String[] tokenizeDoc(String doc) throws IOException {\n    Collection<String> result = new LinkedList<>();\n    for (String textFieldName : textFieldNames) {\n      try (TokenStream tokenStream = analyzer.tokenStream(textFieldName, doc)) {\n        CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n        tokenStream.reset();\n        while (tokenStream.incrementToken()) {\n          result.add(charTermAttribute.toString());\n        }\n        tokenStream.end();\n      }\n    }\n    return result.toArray(new String[result.size()]);\n  }\n\n","sourceOld":"  /**\n   * tokenize a <code>String</code> on this classifier's text fields and analyzer\n   *\n   * @param doc the <code>String</code> representing an input text (to be classified)\n   * @return\n   * @throws IOException\n   */\n  protected String[] tokenizeDoc(String doc) throws IOException {\n    Collection<String> result = new LinkedList<>();\n    for (String textFieldName : textFieldNames) {\n      try (TokenStream tokenStream = analyzer.tokenStream(textFieldName, doc)) {\n        CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n        tokenStream.reset();\n        while (tokenStream.incrementToken()) {\n          result.add(charTermAttribute.toString());\n        }\n        tokenStream.end();\n      }\n    }\n    return result.toArray(new String[result.size()]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"50f877e16e8f57c5e72b1cf7791c798d68899b4e","date":1408795676,"type":3,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","sourceNew":"  /**\n   * tokenize a <code>String</code> on this classifier's text fields and analyzer\n   *\n   * @param doc the <code>String</code> representing an input text (to be classified)\n   * @return a <code>String</code> array of the resulting tokens\n   * @throws IOException if tokenization fails\n   */\n  protected String[] tokenizeDoc(String doc) throws IOException {\n    Collection<String> result = new LinkedList<>();\n    for (String textFieldName : textFieldNames) {\n      try (TokenStream tokenStream = analyzer.tokenStream(textFieldName, doc)) {\n        CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n        tokenStream.reset();\n        while (tokenStream.incrementToken()) {\n          result.add(charTermAttribute.toString());\n        }\n        tokenStream.end();\n      }\n    }\n    return result.toArray(new String[result.size()]);\n  }\n\n","sourceOld":"  /**\n   * tokenize a <code>String</code> on this classifier's text fields and analyzer\n   *\n   * @param doc the <code>String</code> representing an input text (to be classified)\n   * @return a <code>String</code> array of the resulting tokens\n   * @throws IOException\n   */\n  protected String[] tokenizeDoc(String doc) throws IOException {\n    Collection<String> result = new LinkedList<>();\n    for (String textFieldName : textFieldNames) {\n      try (TokenStream tokenStream = analyzer.tokenStream(textFieldName, doc)) {\n        CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n        tokenStream.reset();\n        while (tokenStream.incrementToken()) {\n          result.add(charTermAttribute.toString());\n        }\n        tokenStream.end();\n      }\n    }\n    return result.toArray(new String[result.size()]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9ba9b7b122d927a15ff4837e3d72876c609fef1b","date":1441232513,"type":3,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","sourceNew":"  /**\n   * tokenize a <code>String</code> on this classifier's text fields and analyzer\n   *\n   * @param doc the <code>String</code> representing an input text (to be classified)\n   * @return a <code>String</code> array of the resulting tokens\n   * @throws IOException if tokenization fails\n   */\n  protected String[] tokenizeDoc(String doc) throws IOException {\n    Collection<String> result = new LinkedList<>();\n    for (String textFieldName : textFieldNames) {\n      try (TokenStream tokenStream = analyzer.tokenStream(textFieldName, doc)) {\n        CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n        tokenStream.reset();\n        while (tokenStream.incrementToken()) {\n          result.add(charTermAttribute.toString());\n        }\n        tokenStream.end();\n        tokenStream.close();\n      }\n    }\n    return result.toArray(new String[result.size()]);\n  }\n\n","sourceOld":"  /**\n   * tokenize a <code>String</code> on this classifier's text fields and analyzer\n   *\n   * @param doc the <code>String</code> representing an input text (to be classified)\n   * @return a <code>String</code> array of the resulting tokens\n   * @throws IOException if tokenization fails\n   */\n  protected String[] tokenizeDoc(String doc) throws IOException {\n    Collection<String> result = new LinkedList<>();\n    for (String textFieldName : textFieldNames) {\n      try (TokenStream tokenStream = analyzer.tokenStream(textFieldName, doc)) {\n        CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n        tokenStream.reset();\n        while (tokenStream.incrementToken()) {\n          result.add(charTermAttribute.toString());\n        }\n        tokenStream.end();\n      }\n    }\n    return result.toArray(new String[result.size()]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f71444b947a2941c0e12d40225501190333449f9","date":1445162893,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","sourceNew":"  /**\n   * tokenize a <code>String</code> on this classifier's text fields and analyzer\n   *\n   * @param doc the <code>String</code> representing an input text (to be classified)\n   * @return a <code>String</code> array of the resulting tokens\n   * @throws IOException if tokenization fails\n   */\n  protected String[] tokenizeDoc(String doc) throws IOException {\n    Collection<String> result = new LinkedList<>();\n    for (String textFieldName : textFieldNames) {\n      try (TokenStream tokenStream = analyzer.tokenStream(textFieldName, doc)) {\n        CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n        tokenStream.reset();\n        while (tokenStream.incrementToken()) {\n          result.add(charTermAttribute.toString());\n        }\n        tokenStream.end();\n      }\n    }\n    return result.toArray(new String[result.size()]);\n  }\n\n","sourceOld":"  /**\n   * tokenize a <code>String</code> on this classifier's text fields and analyzer\n   *\n   * @param doc the <code>String</code> representing an input text (to be classified)\n   * @return a <code>String</code> array of the resulting tokens\n   * @throws IOException if tokenization fails\n   */\n  protected String[] tokenizeDoc(String doc) throws IOException {\n    Collection<String> result = new LinkedList<>();\n    for (String textFieldName : textFieldNames) {\n      try (TokenStream tokenStream = analyzer.tokenStream(textFieldName, doc)) {\n        CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n        tokenStream.reset();\n        while (tokenStream.incrementToken()) {\n          result.add(charTermAttribute.toString());\n        }\n        tokenStream.end();\n        tokenStream.close();\n      }\n    }\n    return result.toArray(new String[result.size()]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"134a24d0cb66520908d88384f1a559875704ed25","date":1445326601,"type":5,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenize(String).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier#tokenizeDoc(String).mjava","sourceNew":"  /**\n   * tokenize a <code>String</code> on this classifier's text fields and analyzer\n   *\n   * @param text the <code>String</code> representing an input text (to be classified)\n   * @return a <code>String</code> array of the resulting tokens\n   * @throws IOException if tokenization fails\n   */\n  protected String[] tokenize(String text) throws IOException {\n    Collection<String> result = new LinkedList<>();\n    for (String textFieldName : textFieldNames) {\n      try (TokenStream tokenStream = analyzer.tokenStream(textFieldName, text)) {\n        CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n        tokenStream.reset();\n        while (tokenStream.incrementToken()) {\n          result.add(charTermAttribute.toString());\n        }\n        tokenStream.end();\n      }\n    }\n    return result.toArray(new String[result.size()]);\n  }\n\n","sourceOld":"  /**\n   * tokenize a <code>String</code> on this classifier's text fields and analyzer\n   *\n   * @param doc the <code>String</code> representing an input text (to be classified)\n   * @return a <code>String</code> array of the resulting tokens\n   * @throws IOException if tokenization fails\n   */\n  protected String[] tokenizeDoc(String doc) throws IOException {\n    Collection<String> result = new LinkedList<>();\n    for (String textFieldName : textFieldNames) {\n      try (TokenStream tokenStream = analyzer.tokenStream(textFieldName, doc)) {\n        CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\n        tokenStream.reset();\n        while (tokenStream.incrementToken()) {\n          result.add(charTermAttribute.toString());\n        }\n        tokenStream.end();\n      }\n    }\n    return result.toArray(new String[result.size()]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["a1f508b269e97eeeb33e0d21c851eceb57bfd1eb"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["e91746d568843981a68fb96dcecc7f383eb55701","c83d6c4335f31cae14f625a222bc842f20073dcd"],"be961b1729b3dfa66883980eadaa4ca4732820e1":["e7533828cbcc5f498a44cca5d9bce92692663778"],"782ed6a4b4ba50ec19734fc8db4e570ee193d627":["c83d6c4335f31cae14f625a222bc842f20073dcd"],"50f877e16e8f57c5e72b1cf7791c798d68899b4e":["a100f4e9288c563fb03c083266f03fcd1ba0c585"],"134a24d0cb66520908d88384f1a559875704ed25":["f71444b947a2941c0e12d40225501190333449f9"],"e91746d568843981a68fb96dcecc7f383eb55701":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"9ba9b7b122d927a15ff4837e3d72876c609fef1b":["50f877e16e8f57c5e72b1cf7791c798d68899b4e"],"e7533828cbcc5f498a44cca5d9bce92692663778":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"c83d6c4335f31cae14f625a222bc842f20073dcd":["e91746d568843981a68fb96dcecc7f383eb55701"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a1f508b269e97eeeb33e0d21c851eceb57bfd1eb":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"a100f4e9288c563fb03c083266f03fcd1ba0c585":["be961b1729b3dfa66883980eadaa4ca4732820e1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["134a24d0cb66520908d88384f1a559875704ed25"],"f71444b947a2941c0e12d40225501190333449f9":["9ba9b7b122d927a15ff4837e3d72876c609fef1b"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["e7533828cbcc5f498a44cca5d9bce92692663778"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"be961b1729b3dfa66883980eadaa4ca4732820e1":["a100f4e9288c563fb03c083266f03fcd1ba0c585"],"782ed6a4b4ba50ec19734fc8db4e570ee193d627":["a1f508b269e97eeeb33e0d21c851eceb57bfd1eb"],"50f877e16e8f57c5e72b1cf7791c798d68899b4e":["9ba9b7b122d927a15ff4837e3d72876c609fef1b"],"e91746d568843981a68fb96dcecc7f383eb55701":["37a0f60745e53927c4c876cfe5b5a58170f0646c","c83d6c4335f31cae14f625a222bc842f20073dcd"],"134a24d0cb66520908d88384f1a559875704ed25":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"9ba9b7b122d927a15ff4837e3d72876c609fef1b":["f71444b947a2941c0e12d40225501190333449f9"],"c83d6c4335f31cae14f625a222bc842f20073dcd":["37a0f60745e53927c4c876cfe5b5a58170f0646c","782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"e7533828cbcc5f498a44cca5d9bce92692663778":["be961b1729b3dfa66883980eadaa4ca4732820e1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["e91746d568843981a68fb96dcecc7f383eb55701"],"a1f508b269e97eeeb33e0d21c851eceb57bfd1eb":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"a100f4e9288c563fb03c083266f03fcd1ba0c585":["50f877e16e8f57c5e72b1cf7791c798d68899b4e"],"f71444b947a2941c0e12d40225501190333449f9":["134a24d0cb66520908d88384f1a559875704ed25"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["37a0f60745e53927c4c876cfe5b5a58170f0646c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}