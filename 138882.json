{"path":"src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldSelector,FieldsWriter,IndexReader,FieldsReader).mjava","commits":[{"id":"d736930237c54e1516a9e3bae803c92ff19ec4e5","date":1245789156,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldSelector,FieldsWriter,IndexReader,FieldsReader).mjava","pathOld":"/dev/null","sourceNew":"  private int copyFieldsWithDeletions(final FieldSelector fieldSelectorMerge,\n                                      final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (reader.isDeleted(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (reader.isDeleted(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (reader.isDeleted(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j, fieldSelectorMerge);\n        fieldsWriter.addDocument(doc);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["406e7055a3e99d3fa6ce49a555a51dd18b321806","4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e0c804f7aa477229414a7e12882af490c241f64d","date":1254963299,"type":5,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldSelector,FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (reader.isDeleted(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (reader.isDeleted(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (reader.isDeleted(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsWithDeletions(final FieldSelector fieldSelectorMerge,\n                                      final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (reader.isDeleted(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (reader.isDeleted(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (reader.isDeleted(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j, fieldSelectorMerge);\n        fieldsWriter.addDocument(doc);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d736930237c54e1516a9e3bae803c92ff19ec4e5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"e0c804f7aa477229414a7e12882af490c241f64d":["d736930237c54e1516a9e3bae803c92ff19ec4e5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e0c804f7aa477229414a7e12882af490c241f64d"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d736930237c54e1516a9e3bae803c92ff19ec4e5"],"d736930237c54e1516a9e3bae803c92ff19ec4e5":["e0c804f7aa477229414a7e12882af490c241f64d"],"e0c804f7aa477229414a7e12882af490c241f64d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}