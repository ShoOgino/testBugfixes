{"path":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","commits":[{"id":"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5","date":1379624229,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","pathOld":"/dev/null","sourceNew":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segDeletes.terms;\n      List<Term> deleteTerms = new ArrayList<Term>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          docsEnum = termsEnum.docs(null, docsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          while (true) {\n            int doc = docsEnum.nextDoc();\n            if (doc == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe883bfde8eb1e5e07c1304e980a2e86028379f9","date":1380021018,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","sourceNew":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segDeletes.terms;\n      List<Term> deleteTerms = new ArrayList<Term>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          docsEnum = termsEnum.docs(null, docsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          while (true) {\n            int doc = docsEnum.nextDoc();\n            if (doc == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segDeletes.terms;\n      List<Term> deleteTerms = new ArrayList<Term>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          docsEnum = termsEnum.docs(null, docsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          while (true) {\n            int doc = docsEnum.nextDoc();\n            if (doc == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0","date":1383367127,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","sourceNew":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<Term>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          docsEnum = termsEnum.docs(null, docsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          while (true) {\n            int doc = docsEnum.nextDoc();\n            if (doc == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segDeletes.terms;\n      List<Term> deleteTerms = new ArrayList<Term>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          docsEnum = termsEnum.docs(null, docsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          while (true) {\n            int doc = docsEnum.nextDoc();\n            if (doc == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","sourceNew":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          docsEnum = termsEnum.docs(null, docsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          while (true) {\n            int doc = docsEnum.nextDoc();\n            if (doc == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<Term>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          docsEnum = termsEnum.docs(null, docsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          while (true) {\n            int doc = docsEnum.nextDoc();\n            if (doc == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a16b40feb4e6e0d55c1716733bde48296bedd20","date":1400540388,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","sourceNew":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n    System.out.println(\"applyDeletes segUpdates=\" + state.segUpdates);\n\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          docsEnum = termsEnum.docs(null, docsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          while (true) {\n            int doc = docsEnum.nextDoc();\n            if (doc == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          docsEnum = termsEnum.docs(null, docsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          while (true) {\n            int doc = docsEnum.nextDoc();\n            if (doc == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0efcf27cd5ca23def8376b4c321970c14dd71623","date":1400662679,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","sourceNew":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          docsEnum = termsEnum.docs(null, docsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          while (true) {\n            int doc = docsEnum.nextDoc();\n            if (doc == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n    System.out.println(\"applyDeletes segUpdates=\" + state.segUpdates);\n\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          docsEnum = termsEnum.docs(null, docsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          while (true) {\n            int doc = docsEnum.nextDoc();\n            if (doc == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4ae62bdfdfc2a17d4df98e6004938c8b0eed0a20","date":1400712483,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","sourceNew":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          docsEnum = termsEnum.docs(null, docsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          while (true) {\n            int doc = docsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          docsEnum = termsEnum.docs(null, docsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          while (true) {\n            int doc = docsEnum.nextDoc();\n            if (doc == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9017ec91c7e47796f2938c5f5705089cb048c4ae","date":1400795272,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","sourceNew":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          docsEnum = termsEnum.docs(null, docsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          assert delDocLimit < DocsEnum.NO_MORE_DOCS;\n          while (true) {\n            int doc = docsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          docsEnum = termsEnum.docs(null, docsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          while (true) {\n            int doc = docsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4d637064d608752565d4f9f41b2497dfdfdde50e","date":1400798123,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","sourceNew":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          docsEnum = termsEnum.docs(null, docsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          assert delDocLimit < DocsEnum.NO_MORE_DOCS;\n          while (true) {\n            int doc = docsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          docsEnum = termsEnum.docs(null, docsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          while (true) {\n            int doc = docsEnum.nextDoc();\n            if (doc == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","sourceNew":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      PostingsEnum postingsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          postingsEnum = termsEnum.postings(null, postingsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          assert delDocLimit < PostingsEnum.NO_MORE_DOCS;\n          while (true) {\n            int doc = postingsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      DocsEnum docsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          docsEnum = termsEnum.docs(null, docsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          assert delDocLimit < DocsEnum.NO_MORE_DOCS;\n          while (true) {\n            int doc = docsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0267c69e2456a3477a1ad785723f2135da3117e","date":1425317087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","sourceNew":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      PostingsEnum postingsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          postingsEnum = termsEnum.postings(null, postingsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          assert delDocLimit < PostingsEnum.NO_MORE_DOCS;\n          while (true) {\n            int doc = postingsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.maxDoc());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      PostingsEnum postingsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          postingsEnum = termsEnum.postings(null, postingsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          assert delDocLimit < PostingsEnum.NO_MORE_DOCS;\n          while (true) {\n            int doc = postingsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b06445ae1731e049327712db0454e5643ca9b7fe","date":1425329139,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","sourceNew":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      PostingsEnum postingsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          postingsEnum = termsEnum.postings(null, postingsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          assert delDocLimit < PostingsEnum.NO_MORE_DOCS;\n          while (true) {\n            int doc = postingsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.maxDoc());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      PostingsEnum postingsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          postingsEnum = termsEnum.postings(null, postingsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          assert delDocLimit < PostingsEnum.NO_MORE_DOCS;\n          while (true) {\n            int doc = postingsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","sourceNew":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      PostingsEnum postingsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          postingsEnum = termsEnum.postings(null, postingsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          assert delDocLimit < PostingsEnum.NO_MORE_DOCS;\n          while (true) {\n            int doc = postingsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.maxDoc());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      PostingsEnum postingsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          postingsEnum = termsEnum.postings(null, postingsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          assert delDocLimit < PostingsEnum.NO_MORE_DOCS;\n          while (true) {\n            int doc = postingsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","date":1428522487,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","sourceNew":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      PostingsEnum postingsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          postingsEnum = termsEnum.postings(null, postingsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          assert delDocLimit < PostingsEnum.NO_MORE_DOCS;\n          while (true) {\n            int doc = postingsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.maxDoc());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      PostingsEnum postingsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator(termsEnum);\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          postingsEnum = termsEnum.postings(null, postingsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          assert delDocLimit < PostingsEnum.NO_MORE_DOCS;\n          while (true) {\n            int doc = postingsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.maxDoc());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","sourceNew":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      PostingsEnum postingsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          postingsEnum = termsEnum.postings(postingsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          assert delDocLimit < PostingsEnum.NO_MORE_DOCS;\n          while (true) {\n            int doc = postingsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.maxDoc());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      PostingsEnum postingsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          postingsEnum = termsEnum.postings(null, postingsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          assert delDocLimit < PostingsEnum.NO_MORE_DOCS;\n          while (true) {\n            int doc = postingsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.maxDoc());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f4363cd33f6eff7fb4753574a441e2d18c1022a4","date":1498067235,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","sourceNew":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.deleteTerms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.deleteTerms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      PostingsEnum postingsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          postingsEnum = termsEnum.postings(postingsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          assert delDocLimit < PostingsEnum.NO_MORE_DOCS;\n          while (true) {\n            int doc = postingsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.maxDoc());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      PostingsEnum postingsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          postingsEnum = termsEnum.postings(postingsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          assert delDocLimit < PostingsEnum.NO_MORE_DOCS;\n          while (true) {\n            int doc = postingsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.maxDoc());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","sourceNew":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.deleteTerms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.deleteTerms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      PostingsEnum postingsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          postingsEnum = termsEnum.postings(postingsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          assert delDocLimit < PostingsEnum.NO_MORE_DOCS;\n          while (true) {\n            int doc = postingsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.maxDoc());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      PostingsEnum postingsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          postingsEnum = termsEnum.postings(postingsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          assert delDocLimit < PostingsEnum.NO_MORE_DOCS;\n          while (true) {\n            int doc = postingsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.maxDoc());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","sourceNew":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.deleteTerms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.deleteTerms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      PostingsEnum postingsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          postingsEnum = termsEnum.postings(postingsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          assert delDocLimit < PostingsEnum.NO_MORE_DOCS;\n          while (true) {\n            int doc = postingsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.maxDoc());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.terms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      PostingsEnum postingsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          postingsEnum = termsEnum.postings(postingsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          assert delDocLimit < PostingsEnum.NO_MORE_DOCS;\n          while (true) {\n            int doc = postingsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.maxDoc());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fb8a3dfca2d8b39c20bdfc87eb7171c06ea9400d","date":1525873214,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","sourceNew":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.deleteTerms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.deleteTerms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      PostingsEnum postingsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          postingsEnum = termsEnum.postings(postingsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          assert delDocLimit < PostingsEnum.NO_MORE_DOCS;\n          while (true) {\n            int doc = postingsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = new FixedBitSet(state.segmentInfo.maxDoc());\n                state.liveDocs.set(0, state.segmentInfo.maxDoc());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.deleteTerms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.deleteTerms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      PostingsEnum postingsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          postingsEnum = termsEnum.postings(postingsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          assert delDocLimit < PostingsEnum.NO_MORE_DOCS;\n          while (true) {\n            int doc = postingsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.maxDoc());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83b6ce113ec151d7bf9175578d92d5320f91ab2e","date":1544711434,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#applyDeletes(SegmentWriteState,Fields).mjava","sourceNew":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.deleteTerms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.deleteTerms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      FrozenBufferedUpdates.TermDocsIterator iterator = new FrozenBufferedUpdates.TermDocsIterator(fields, true);\n      for(Term deleteTerm : deleteTerms) {\n        DocIdSetIterator postings = iterator.nextTerm(deleteTerm.field(), deleteTerm.bytes());\n        if (postings != null ) {\n          int delDocLimit = segDeletes.get(deleteTerm);\n          assert delDocLimit < PostingsEnum.NO_MORE_DOCS;\n          int doc;\n          while ((doc = postings.nextDoc()) < delDocLimit) {\n            if (state.liveDocs == null) {\n              state.liveDocs = new FixedBitSet(state.segmentInfo.maxDoc());\n              state.liveDocs.set(0, state.segmentInfo.maxDoc());\n            }\n            if (state.liveDocs.get(doc)) {\n              state.delCountOnFlush++;\n              state.liveDocs.clear(doc);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void applyDeletes(SegmentWriteState state, Fields fields) throws IOException {\n    // Process any pending Term deletes for this newly\n    // flushed segment:\n    if (state.segUpdates != null && state.segUpdates.deleteTerms.size() > 0) {\n      Map<Term,Integer> segDeletes = state.segUpdates.deleteTerms;\n      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());\n      Collections.sort(deleteTerms);\n      String lastField = null;\n      TermsEnum termsEnum = null;\n      PostingsEnum postingsEnum = null;\n      for(Term deleteTerm : deleteTerms) {\n        if (deleteTerm.field().equals(lastField) == false) {\n          lastField = deleteTerm.field();\n          Terms terms = fields.terms(lastField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null && termsEnum.seekExact(deleteTerm.bytes())) {\n          postingsEnum = termsEnum.postings(postingsEnum, 0);\n          int delDocLimit = segDeletes.get(deleteTerm);\n          assert delDocLimit < PostingsEnum.NO_MORE_DOCS;\n          while (true) {\n            int doc = postingsEnum.nextDoc();\n            if (doc < delDocLimit) {\n              if (state.liveDocs == null) {\n                state.liveDocs = new FixedBitSet(state.segmentInfo.maxDoc());\n                state.liveDocs.set(0, state.segmentInfo.maxDoc());\n              }\n              if (state.liveDocs.get(doc)) {\n                state.delCountOnFlush++;\n                state.liveDocs.clear(doc);\n              }\n            } else {\n              break;\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["b0267c69e2456a3477a1ad785723f2135da3117e"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"4ae62bdfdfc2a17d4df98e6004938c8b0eed0a20":["0efcf27cd5ca23def8376b4c321970c14dd71623"],"0a16b40feb4e6e0d55c1716733bde48296bedd20":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["fe883bfde8eb1e5e07c1304e980a2e86028379f9"],"9017ec91c7e47796f2938c5f5705089cb048c4ae":["4ae62bdfdfc2a17d4df98e6004938c8b0eed0a20"],"0efcf27cd5ca23def8376b4c321970c14dd71623":["0a16b40feb4e6e0d55c1716733bde48296bedd20"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"b0267c69e2456a3477a1ad785723f2135da3117e":["51f5280f31484820499077f41fcdfe92d527d9dc"],"fe883bfde8eb1e5e07c1304e980a2e86028379f9":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5"],"51f5280f31484820499077f41fcdfe92d527d9dc":["4d637064d608752565d4f9f41b2497dfdfdde50e"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["0f4464508ee83288c8c4585b533f9faaa93aa314","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"28288370235ed02234a64753cdbf0c6ec096304a":["0f4464508ee83288c8c4585b533f9faaa93aa314","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"83b6ce113ec151d7bf9175578d92d5320f91ab2e":["fb8a3dfca2d8b39c20bdfc87eb7171c06ea9400d"],"4d637064d608752565d4f9f41b2497dfdfdde50e":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","9017ec91c7e47796f2938c5f5705089cb048c4ae"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["51f5280f31484820499077f41fcdfe92d527d9dc","b0267c69e2456a3477a1ad785723f2135da3117e"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"b06445ae1731e049327712db0454e5643ca9b7fe":["51f5280f31484820499077f41fcdfe92d527d9dc","b0267c69e2456a3477a1ad785723f2135da3117e"],"fb8a3dfca2d8b39c20bdfc87eb7171c06ea9400d":["28288370235ed02234a64753cdbf0c6ec096304a"],"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["83b6ce113ec151d7bf9175578d92d5320f91ab2e"]},"commit2Childs":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["0a16b40feb4e6e0d55c1716733bde48296bedd20","4d637064d608752565d4f9f41b2497dfdfdde50e"],"4ae62bdfdfc2a17d4df98e6004938c8b0eed0a20":["9017ec91c7e47796f2938c5f5705089cb048c4ae"],"0a16b40feb4e6e0d55c1716733bde48296bedd20":["0efcf27cd5ca23def8376b4c321970c14dd71623"],"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"0efcf27cd5ca23def8376b4c321970c14dd71623":["4ae62bdfdfc2a17d4df98e6004938c8b0eed0a20"],"9017ec91c7e47796f2938c5f5705089cb048c4ae":["4d637064d608752565d4f9f41b2497dfdfdde50e"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"b0267c69e2456a3477a1ad785723f2135da3117e":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe"],"fe883bfde8eb1e5e07c1304e980a2e86028379f9":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"51f5280f31484820499077f41fcdfe92d527d9dc":["b0267c69e2456a3477a1ad785723f2135da3117e","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":[],"28288370235ed02234a64753cdbf0c6ec096304a":["fb8a3dfca2d8b39c20bdfc87eb7171c06ea9400d"],"4d637064d608752565d4f9f41b2497dfdfdde50e":["51f5280f31484820499077f41fcdfe92d527d9dc"],"83b6ce113ec151d7bf9175578d92d5320f91ab2e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"0f4464508ee83288c8c4585b533f9faaa93aa314":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"b06445ae1731e049327712db0454e5643ca9b7fe":[],"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5":["fe883bfde8eb1e5e07c1304e980a2e86028379f9"],"fb8a3dfca2d8b39c20bdfc87eb7171c06ea9400d":["83b6ce113ec151d7bf9175578d92d5320f91ab2e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}