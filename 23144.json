{"path":"contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordTokenizer#processNextSentence().mjava","commits":[{"id":"05ff0cc6e864c7d71a48579f2acfca4f58943568","date":1242295762,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordTokenizer#processNextSentence().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * 当当前的句子分词并索引完毕时，需要读取下一个句子Token， 本函数负责调用上一层的SentenceTokenizer去加载下一个句子， 并将其分词，\n   * 将分词结果保存成Token放在tokenBuffer中\n   * \n   * @return 读取并处理下一个句子成功与否，如果没有成功，说明文件处理完毕，后面没有Token了\n   * @throws IOException\n   */\n  private boolean processNextSentence() throws IOException {\n    sentenceToken = in.next(sentenceToken);\n    if (sentenceToken == null)\n      return false;\n    tokenBuffer = wordSegmenter.segmentSentence(sentenceToken, 1);\n    tokenIter = tokenBuffer.iterator();\n    return tokenBuffer != null && tokenIter.hasNext();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"be5ef2f970a6c1141562b06dd26ed04c3dc29d70","date":1246444343,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordTokenizer#processNextSentence().mjava","pathOld":"contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordTokenizer#processNextSentence().mjava","sourceNew":"  /**\n   * Process the next input sentence, placing tokens into tokenBuffer\n   * \n   * @return true if more tokens were placed into tokenBuffer.\n   * @throws IOException\n   */\n  private boolean processNextSentence() throws IOException {\n    sentenceToken = in.next(sentenceToken);\n    if (sentenceToken == null)\n      return false;\n    tokenBuffer = wordSegmenter.segmentSentence(sentenceToken);\n    tokenIter = tokenBuffer.iterator();\n    return tokenBuffer != null && tokenIter.hasNext();\n  }\n\n","sourceOld":"  /**\n   * 当当前的句子分词并索引完毕时，需要读取下一个句子Token， 本函数负责调用上一层的SentenceTokenizer去加载下一个句子， 并将其分词，\n   * 将分词结果保存成Token放在tokenBuffer中\n   * \n   * @return 读取并处理下一个句子成功与否，如果没有成功，说明文件处理完毕，后面没有Token了\n   * @throws IOException\n   */\n  private boolean processNextSentence() throws IOException {\n    sentenceToken = in.next(sentenceToken);\n    if (sentenceToken == null)\n      return false;\n    tokenBuffer = wordSegmenter.segmentSentence(sentenceToken, 1);\n    tokenIter = tokenBuffer.iterator();\n    return tokenBuffer != null && tokenIter.hasNext();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dd745d580729e528151b58aeda87ef82f1b95c9b","date":1248369082,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordTokenFilter#processNextSentence(Token).mjava","pathOld":"contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordTokenizer#processNextSentence().mjava","sourceNew":"  /**\n   * Process the next input sentence, placing tokens into tokenBuffer\n   * \n   * @param reusableSentenceToken input sentence\n   * @return true if more tokens were placed into tokenBuffer.\n   * @throws IOException\n   */\n  private boolean processNextSentence(final Token reusableSentenceToken) throws IOException {\n    if (reusableSentenceToken == null)\n      return false;\n    tokenBuffer = wordSegmenter.segmentSentence(reusableSentenceToken);\n    tokenIter = tokenBuffer.iterator();\n    return tokenBuffer != null && tokenIter.hasNext();\n  }\n\n","sourceOld":"  /**\n   * Process the next input sentence, placing tokens into tokenBuffer\n   * \n   * @return true if more tokens were placed into tokenBuffer.\n   * @throws IOException\n   */\n  private boolean processNextSentence() throws IOException {\n    sentenceToken = in.next(sentenceToken);\n    if (sentenceToken == null)\n      return false;\n    tokenBuffer = wordSegmenter.segmentSentence(sentenceToken);\n    tokenIter = tokenBuffer.iterator();\n    return tokenBuffer != null && tokenIter.hasNext();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"be5ef2f970a6c1141562b06dd26ed04c3dc29d70":["05ff0cc6e864c7d71a48579f2acfca4f58943568"],"dd745d580729e528151b58aeda87ef82f1b95c9b":["be5ef2f970a6c1141562b06dd26ed04c3dc29d70"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"05ff0cc6e864c7d71a48579f2acfca4f58943568":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["dd745d580729e528151b58aeda87ef82f1b95c9b"]},"commit2Childs":{"be5ef2f970a6c1141562b06dd26ed04c3dc29d70":["dd745d580729e528151b58aeda87ef82f1b95c9b"],"dd745d580729e528151b58aeda87ef82f1b95c9b":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["05ff0cc6e864c7d71a48579f2acfca4f58943568"],"05ff0cc6e864c7d71a48579f2acfca4f58943568":["be5ef2f970a6c1141562b06dd26ed04c3dc29d70"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}