{"path":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","commits":[{"id":"833a7987bc1c94455fde83e3311f72bddedcfb93","date":1279951470,"type":1,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean,boolean).mjava","sourceNew":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized final boolean doFlush(boolean flushDeletes) throws CorruptIndexException, IOException {\n    return docWriter.flushAllThreads(flushDeletes);\n    // nocommit\n//    try {\n//      try {\n//        return doFlushInternal(flushDocStores, flushDeletes);\n//      } finally {\n//        docWriter.balanceRAM();\n//      }\n//    } finally {\n//      docWriter.clearFlushPending();\n//    }\n  }\n\n","sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized final boolean doFlush(boolean flushDocStores, boolean flushDeletes) throws CorruptIndexException, IOException {\n    return docWriter.flushAllThreads(flushDocStores, flushDeletes);\n    // nocommit\n//    try {\n//      try {\n//        return doFlushInternal(flushDocStores, flushDeletes);\n//      } finally {\n//        docWriter.balanceRAM();\n//      }\n//    } finally {\n//      docWriter.clearFlushPending();\n//    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5ef87af8c7bd0f8429622b83aa74202383f2e757","date":1280262785,"type":4,"author":"Michael Busch","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","sourceNew":null,"sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized final boolean doFlush(boolean flushDeletes) throws CorruptIndexException, IOException {\n    return docWriter.flushAllThreads(flushDeletes);\n    // nocommit\n//    try {\n//      try {\n//        return doFlushInternal(flushDocStores, flushDeletes);\n//      } finally {\n//        docWriter.balanceRAM();\n//      }\n//    } finally {\n//      docWriter.clearFlushPending();\n//    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5","date":1292695408,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean,boolean).mjava","sourceNew":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n    \n      final SegmentInfo newSegment = docWriter.flush(this, deleter, mergePolicy, segmentInfos);\n      if (newSegment != null) {\n        setDiagnostics(newSegment, \"flush\");\n        segmentInfos.add(newSegment);\n        checkpoint();\n      }\n\n      if (!applyAllDeletes) {\n        // If deletes alone are consuming > 1/2 our RAM\n        // buffer, force them all to apply now. This is to\n        // prevent too-frequent flushing of a long tail of\n        // tiny segments:\n        if (flushControl.getFlushDeletes() ||\n            (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n             bufferedDeletes.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n          applyAllDeletes = true;\n          if (infoStream != null) {\n            message(\"force apply deletes bytesUsed=\" + bufferedDeletes.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n          }\n        }\n      }\n\n      if (applyAllDeletes) {\n        if (infoStream != null) {\n          message(\"apply all deletes during flush\");\n        }\n        flushDeletesCount.incrementAndGet();\n        if (bufferedDeletes.applyDeletes(readerPool, segmentInfos, segmentInfos)) {\n          checkpoint();\n        }\n        flushControl.clearDeletes();\n      } else if (infoStream != null) {\n        message(\"don't apply deletes now delTermCount=\" + bufferedDeletes.numTerms() + \" bytesUsed=\" + bufferedDeletes.bytesUsed());\n      }\n\n      doAfterFlush();\n      flushCount.incrementAndGet();\n\n      success = true;\n\n      return newSegment != null;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized final boolean doFlush(boolean closeDocStores, boolean applyAllDeletes) throws CorruptIndexException, IOException {\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes + \" closeDocStores=\" + closeDocStores);\n        message(\"  index before flush \" + segString());\n      }\n    \n      final SegmentInfo newSegment = docWriter.flush(this, closeDocStores, deleter, mergePolicy, segmentInfos);\n      if (newSegment != null) {\n        setDiagnostics(newSegment, \"flush\");\n        segmentInfos.add(newSegment);\n        checkpoint();\n      }\n\n      if (!applyAllDeletes) {\n        // If deletes alone are consuming > 1/2 our RAM\n        // buffer, force them all to apply now. This is to\n        // prevent too-frequent flushing of a long tail of\n        // tiny segments:\n        if (flushControl.getFlushDeletes() ||\n            (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n             bufferedDeletes.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n          applyAllDeletes = true;\n          if (infoStream != null) {\n            message(\"force apply deletes bytesUsed=\" + bufferedDeletes.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n          }\n        }\n      }\n\n      if (applyAllDeletes) {\n        if (infoStream != null) {\n          message(\"apply all deletes during flush\");\n        }\n        flushDeletesCount.incrementAndGet();\n        if (bufferedDeletes.applyDeletes(readerPool, segmentInfos, segmentInfos)) {\n          checkpoint();\n        }\n        flushControl.clearDeletes();\n      } else if (infoStream != null) {\n        message(\"don't apply deletes now delTermCount=\" + bufferedDeletes.numTerms() + \" bytesUsed=\" + bufferedDeletes.bytesUsed());\n      }\n\n      doAfterFlush();\n      flushCount.incrementAndGet();\n\n      success = true;\n\n      return newSegment != null;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success) {\n        if (infoStream != null) {\n          message(\"hit exception during flush\");\n        }\n        if (docWriter != null) {\n          final Collection<String> files = docWriter.abortedFiles();\n          if (files != null) {\n            deleter.deleteNewFiles(files);\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"/dev/null","sourceNew":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n    \n      final SegmentInfo newSegment = docWriter.flush(this, deleter, mergePolicy, segmentInfos);\n      if (newSegment != null) {\n        setDiagnostics(newSegment, \"flush\");\n        segmentInfos.add(newSegment);\n        checkpoint();\n      }\n\n      if (!applyAllDeletes) {\n        // If deletes alone are consuming > 1/2 our RAM\n        // buffer, force them all to apply now. This is to\n        // prevent too-frequent flushing of a long tail of\n        // tiny segments:\n        if (flushControl.getFlushDeletes() ||\n            (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n             bufferedDeletes.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n          applyAllDeletes = true;\n          if (infoStream != null) {\n            message(\"force apply deletes bytesUsed=\" + bufferedDeletes.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n          }\n        }\n      }\n\n      if (applyAllDeletes) {\n        if (infoStream != null) {\n          message(\"apply all deletes during flush\");\n        }\n        flushDeletesCount.incrementAndGet();\n        if (bufferedDeletes.applyDeletes(readerPool, segmentInfos, segmentInfos)) {\n          checkpoint();\n        }\n        flushControl.clearDeletes();\n      } else if (infoStream != null) {\n        message(\"don't apply deletes now delTermCount=\" + bufferedDeletes.numTerms() + \" bytesUsed=\" + bufferedDeletes.bytesUsed());\n      }\n\n      doAfterFlush();\n      flushCount.incrementAndGet();\n\n      success = true;\n\n      return newSegment != null;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":0,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"/dev/null","sourceNew":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n\n      boolean maybeMerge = docWriter.flushAllThreads(applyAllDeletes);\n\n      synchronized(this) {\n\n        if (!applyAllDeletes) {\n          // If deletes alone are consuming > 1/2 our RAM\n          // buffer, force them all to apply now. This is to\n          // prevent too-frequent flushing of a long tail of\n          // tiny segments:\n          if (flushControl.getFlushDeletes() ||\n              (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n               bufferedDeletes.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n            applyAllDeletes = true;\n            if (infoStream != null) {\n              message(\"force apply deletes bytesUsed=\" + bufferedDeletes.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n            }\n          }\n        }\n\n        if (applyAllDeletes) {\n          if (infoStream != null) {\n            message(\"apply all deletes during flush\");\n          }\n          flushDeletesCount.incrementAndGet();\n          if (bufferedDeletes.applyDeletes(readerPool, segmentInfos, segmentInfos)) {\n            checkpoint();\n          }\n          flushControl.clearDeletes();\n        } else if (infoStream != null) {\n          message(\"don't apply deletes now delTermCount=\" + bufferedDeletes.numTerms() + \" bytesUsed=\" + bufferedDeletes.bytesUsed());\n        }\n\n        doAfterFlush();\n        flushCount.incrementAndGet();\n\n        success = true;\n\n        return maybeMerge;\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["4d3e8520fd031bab31fd0e4d480e55958bc45efe","7d45e9e2ad7f57776540627c78f5e22e469ccdc1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3161e3ffcf20c09a22504a589d4d9bd273e11e33","date":1295142360,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","sourceNew":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n\n      boolean maybeMerge = docWriter.flushAllThreads(applyAllDeletes);\n\n      synchronized(this) {\n        if (!applyAllDeletes) {\n          // If deletes alone are consuming > 1/2 our RAM\n          // buffer, force them all to apply now. This is to\n          // prevent too-frequent flushing of a long tail of\n          // tiny segments:\n          if (flushControl.getFlushDeletes() ||\n              (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n               bufferedDeletes.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n            applyAllDeletes = true;\n            if (infoStream != null) {\n              message(\"force apply deletes bytesUsed=\" + bufferedDeletes.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n            }\n          }\n        }\n\n        if (applyAllDeletes) {\n          if (infoStream != null) {\n            message(\"apply all deletes during flush\");\n          }\n          flushDeletesCount.incrementAndGet();\n          if (bufferedDeletes.applyDeletes(readerPool, segmentInfos, segmentInfos)) {\n            checkpoint();\n          }\n          flushControl.clearDeletes();\n        } else if (infoStream != null) {\n          message(\"don't apply deletes now delTermCount=\" + bufferedDeletes.numTerms() + \" bytesUsed=\" + bufferedDeletes.bytesUsed());\n        }\n\n        doAfterFlush();\n        flushCount.incrementAndGet();\n\n        success = true;\n\n        return maybeMerge;\n\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n\n      boolean maybeMerge = docWriter.flushAllThreads(applyAllDeletes);\n\n      synchronized(this) {\n\n        if (!applyAllDeletes) {\n          // If deletes alone are consuming > 1/2 our RAM\n          // buffer, force them all to apply now. This is to\n          // prevent too-frequent flushing of a long tail of\n          // tiny segments:\n          if (flushControl.getFlushDeletes() ||\n              (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n               bufferedDeletes.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n            applyAllDeletes = true;\n            if (infoStream != null) {\n              message(\"force apply deletes bytesUsed=\" + bufferedDeletes.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n            }\n          }\n        }\n\n        if (applyAllDeletes) {\n          if (infoStream != null) {\n            message(\"apply all deletes during flush\");\n          }\n          flushDeletesCount.incrementAndGet();\n          if (bufferedDeletes.applyDeletes(readerPool, segmentInfos, segmentInfos)) {\n            checkpoint();\n          }\n          flushControl.clearDeletes();\n        } else if (infoStream != null) {\n          message(\"don't apply deletes now delTermCount=\" + bufferedDeletes.numTerms() + \" bytesUsed=\" + bufferedDeletes.bytesUsed());\n        }\n\n        doAfterFlush();\n        flushCount.incrementAndGet();\n\n        success = true;\n\n        return maybeMerge;\n      }\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c19f985e36a65cc969e8e564fe337a0d41512075","date":1296330536,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","sourceNew":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n    \n      final SegmentInfo newSegment = docWriter.flush(this, deleter, mergePolicy, segmentInfos);\n      if (newSegment != null) {\n        setDiagnostics(newSegment, \"flush\");\n        segmentInfos.add(newSegment);\n        checkpoint();\n      }\n\n      if (!applyAllDeletes) {\n        // If deletes alone are consuming > 1/2 our RAM\n        // buffer, force them all to apply now. This is to\n        // prevent too-frequent flushing of a long tail of\n        // tiny segments:\n        if (flushControl.getFlushDeletes() ||\n            (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n             bufferedDeletesStream.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n          applyAllDeletes = true;\n          if (infoStream != null) {\n            message(\"force apply deletes bytesUsed=\" + bufferedDeletesStream.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n          }\n        }\n      }\n\n      if (applyAllDeletes) {\n        if (infoStream != null) {\n          message(\"apply all deletes during flush\");\n        }\n        flushDeletesCount.incrementAndGet();\n        final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, segmentInfos);\n        if (result.anyDeletes) {\n          checkpoint();\n        }\n        bufferedDeletesStream.prune(segmentInfos);\n        assert !bufferedDeletesStream.any();\n        flushControl.clearDeletes();\n      } else if (infoStream != null) {\n        message(\"don't apply deletes now delTermCount=\" + bufferedDeletesStream.numTerms() + \" bytesUsed=\" + bufferedDeletesStream.bytesUsed());\n      }\n\n      doAfterFlush();\n      flushCount.incrementAndGet();\n\n      success = true;\n\n      return newSegment != null;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n    \n      final SegmentInfo newSegment = docWriter.flush(this, deleter, mergePolicy, segmentInfos);\n      if (newSegment != null) {\n        setDiagnostics(newSegment, \"flush\");\n        segmentInfos.add(newSegment);\n        checkpoint();\n      }\n\n      if (!applyAllDeletes) {\n        // If deletes alone are consuming > 1/2 our RAM\n        // buffer, force them all to apply now. This is to\n        // prevent too-frequent flushing of a long tail of\n        // tiny segments:\n        if (flushControl.getFlushDeletes() ||\n            (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n             bufferedDeletes.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n          applyAllDeletes = true;\n          if (infoStream != null) {\n            message(\"force apply deletes bytesUsed=\" + bufferedDeletes.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n          }\n        }\n      }\n\n      if (applyAllDeletes) {\n        if (infoStream != null) {\n          message(\"apply all deletes during flush\");\n        }\n        flushDeletesCount.incrementAndGet();\n        if (bufferedDeletes.applyDeletes(readerPool, segmentInfos, segmentInfos)) {\n          checkpoint();\n        }\n        flushControl.clearDeletes();\n      } else if (infoStream != null) {\n        message(\"don't apply deletes now delTermCount=\" + bufferedDeletes.numTerms() + \" bytesUsed=\" + bufferedDeletes.bytesUsed());\n      }\n\n      doAfterFlush();\n      flushCount.incrementAndGet();\n\n      success = true;\n\n      return newSegment != null;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","sourceNew":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n    \n      final SegmentInfo newSegment = docWriter.flush(this, deleter, mergePolicy, segmentInfos);\n      if (newSegment != null) {\n        setDiagnostics(newSegment, \"flush\");\n        segmentInfos.add(newSegment);\n        checkpoint();\n      }\n\n      if (!applyAllDeletes) {\n        // If deletes alone are consuming > 1/2 our RAM\n        // buffer, force them all to apply now. This is to\n        // prevent too-frequent flushing of a long tail of\n        // tiny segments:\n        if (flushControl.getFlushDeletes() ||\n            (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n             bufferedDeletesStream.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n          applyAllDeletes = true;\n          if (infoStream != null) {\n            message(\"force apply deletes bytesUsed=\" + bufferedDeletesStream.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n          }\n        }\n      }\n\n      if (applyAllDeletes) {\n        if (infoStream != null) {\n          message(\"apply all deletes during flush\");\n        }\n        flushDeletesCount.incrementAndGet();\n        final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, segmentInfos);\n        if (result.anyDeletes) {\n          checkpoint();\n        }\n        bufferedDeletesStream.prune(segmentInfos);\n        assert !bufferedDeletesStream.any();\n        flushControl.clearDeletes();\n      } else if (infoStream != null) {\n        message(\"don't apply deletes now delTermCount=\" + bufferedDeletesStream.numTerms() + \" bytesUsed=\" + bufferedDeletesStream.bytesUsed());\n      }\n\n      doAfterFlush();\n      flushCount.incrementAndGet();\n\n      success = true;\n\n      return newSegment != null;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n    \n      final SegmentInfo newSegment = docWriter.flush(this, deleter, mergePolicy, segmentInfos);\n      if (newSegment != null) {\n        setDiagnostics(newSegment, \"flush\");\n        segmentInfos.add(newSegment);\n        checkpoint();\n      }\n\n      if (!applyAllDeletes) {\n        // If deletes alone are consuming > 1/2 our RAM\n        // buffer, force them all to apply now. This is to\n        // prevent too-frequent flushing of a long tail of\n        // tiny segments:\n        if (flushControl.getFlushDeletes() ||\n            (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n             bufferedDeletes.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n          applyAllDeletes = true;\n          if (infoStream != null) {\n            message(\"force apply deletes bytesUsed=\" + bufferedDeletes.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n          }\n        }\n      }\n\n      if (applyAllDeletes) {\n        if (infoStream != null) {\n          message(\"apply all deletes during flush\");\n        }\n        flushDeletesCount.incrementAndGet();\n        if (bufferedDeletes.applyDeletes(readerPool, segmentInfos, segmentInfos)) {\n          checkpoint();\n        }\n        flushControl.clearDeletes();\n      } else if (infoStream != null) {\n        message(\"don't apply deletes now delTermCount=\" + bufferedDeletes.numTerms() + \" bytesUsed=\" + bufferedDeletes.bytesUsed());\n      }\n\n      doAfterFlush();\n      flushCount.incrementAndGet();\n\n      success = true;\n\n      return newSegment != null;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3","date":1297940445,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","sourceNew":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n    \n      final SegmentInfo newSegment = docWriter.flush(this, deleter, mergePolicy, segmentInfos);\n      if (newSegment != null) {\n        setDiagnostics(newSegment, \"flush\");\n        segmentInfos.add(newSegment);\n        checkpoint();\n      }\n\n      if (!applyAllDeletes) {\n        // If deletes alone are consuming > 1/2 our RAM\n        // buffer, force them all to apply now. This is to\n        // prevent too-frequent flushing of a long tail of\n        // tiny segments:\n        if (flushControl.getFlushDeletes() ||\n            (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n             bufferedDeletesStream.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n          applyAllDeletes = true;\n          if (infoStream != null) {\n            message(\"force apply deletes bytesUsed=\" + bufferedDeletesStream.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n          }\n        }\n      }\n\n      if (applyAllDeletes) {\n        if (infoStream != null) {\n          message(\"apply all deletes during flush\");\n        }\n        flushDeletesCount.incrementAndGet();\n        final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, segmentInfos);\n        if (result.anyDeletes) {\n          checkpoint();\n        }\n        if (!keepFullyDeletedSegments && result.allDeleted != null) {\n          if (infoStream != null) {\n            message(\"drop 100% deleted segments: \" + result.allDeleted);\n          }\n          for(SegmentInfo info : result.allDeleted) {\n            // If a merge has already registered for this\n            // segment, we leave it in the readerPool; the\n            // merge will skip merging it and will then drop\n            // it once it's done:\n            if (!mergingSegments.contains(info)) {\n              segmentInfos.remove(info);\n              if (readerPool != null) {\n                readerPool.drop(info);\n              }\n            }\n          }\n          checkpoint();\n        }\n        bufferedDeletesStream.prune(segmentInfos);\n        assert !bufferedDeletesStream.any();\n        flushControl.clearDeletes();\n      } else if (infoStream != null) {\n        message(\"don't apply deletes now delTermCount=\" + bufferedDeletesStream.numTerms() + \" bytesUsed=\" + bufferedDeletesStream.bytesUsed());\n      }\n\n      doAfterFlush();\n      flushCount.incrementAndGet();\n\n      success = true;\n\n      return newSegment != null;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n    \n      final SegmentInfo newSegment = docWriter.flush(this, deleter, mergePolicy, segmentInfos);\n      if (newSegment != null) {\n        setDiagnostics(newSegment, \"flush\");\n        segmentInfos.add(newSegment);\n        checkpoint();\n      }\n\n      if (!applyAllDeletes) {\n        // If deletes alone are consuming > 1/2 our RAM\n        // buffer, force them all to apply now. This is to\n        // prevent too-frequent flushing of a long tail of\n        // tiny segments:\n        if (flushControl.getFlushDeletes() ||\n            (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n             bufferedDeletesStream.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n          applyAllDeletes = true;\n          if (infoStream != null) {\n            message(\"force apply deletes bytesUsed=\" + bufferedDeletesStream.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n          }\n        }\n      }\n\n      if (applyAllDeletes) {\n        if (infoStream != null) {\n          message(\"apply all deletes during flush\");\n        }\n        flushDeletesCount.incrementAndGet();\n        final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, segmentInfos);\n        if (result.anyDeletes) {\n          checkpoint();\n        }\n        bufferedDeletesStream.prune(segmentInfos);\n        assert !bufferedDeletesStream.any();\n        flushControl.clearDeletes();\n      } else if (infoStream != null) {\n        message(\"don't apply deletes now delTermCount=\" + bufferedDeletesStream.numTerms() + \" bytesUsed=\" + bufferedDeletesStream.bytesUsed());\n      }\n\n      doAfterFlush();\n      flushCount.incrementAndGet();\n\n      success = true;\n\n      return newSegment != null;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f1bdbf92da222965b46c0a942c3857ba56e5c638","date":1298297608,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","sourceNew":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n    \n      final SegmentInfo newSegment = docWriter.flush(this, deleter, mergePolicy, segmentInfos);\n      if (newSegment != null) {\n        setDiagnostics(newSegment, \"flush\");\n        segmentInfos.add(newSegment);\n        checkpoint();\n      }\n\n      if (!applyAllDeletes) {\n        // If deletes alone are consuming > 1/2 our RAM\n        // buffer, force them all to apply now. This is to\n        // prevent too-frequent flushing of a long tail of\n        // tiny segments:\n        if (flushControl.getFlushDeletes() ||\n            (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n             bufferedDeletesStream.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n          applyAllDeletes = true;\n          if (infoStream != null) {\n            message(\"force apply deletes bytesUsed=\" + bufferedDeletesStream.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n          }\n        }\n      }\n\n      if (applyAllDeletes) {\n        if (infoStream != null) {\n          message(\"apply all deletes during flush\");\n        }\n        flushDeletesCount.incrementAndGet();\n        final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, segmentInfos);\n        if (result.anyDeletes) {\n          checkpoint();\n        }\n        if (!keepFullyDeletedSegments && result.allDeleted != null) {\n          if (infoStream != null) {\n            message(\"drop 100% deleted segments: \" + result.allDeleted);\n          }\n          for(SegmentInfo info : result.allDeleted) {\n            // If a merge has already registered for this\n            // segment, we leave it in the readerPool; the\n            // merge will skip merging it and will then drop\n            // it once it's done:\n            if (!mergingSegments.contains(info)) {\n              segmentInfos.remove(info);\n              if (readerPool != null) {\n                readerPool.drop(info);\n              }\n            }\n          }\n          checkpoint();\n        }\n        bufferedDeletesStream.prune(segmentInfos);\n        assert !bufferedDeletesStream.any();\n        flushControl.clearDeletes();\n      } else if (infoStream != null) {\n        message(\"don't apply deletes now delTermCount=\" + bufferedDeletesStream.numTerms() + \" bytesUsed=\" + bufferedDeletesStream.bytesUsed());\n      }\n\n      doAfterFlush();\n      flushCount.incrementAndGet();\n\n      success = true;\n\n      return newSegment != null;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n    \n      final SegmentInfo newSegment = docWriter.flush(this, deleter, mergePolicy, segmentInfos);\n      if (newSegment != null) {\n        setDiagnostics(newSegment, \"flush\");\n        segmentInfos.add(newSegment);\n        checkpoint();\n      }\n\n      if (!applyAllDeletes) {\n        // If deletes alone are consuming > 1/2 our RAM\n        // buffer, force them all to apply now. This is to\n        // prevent too-frequent flushing of a long tail of\n        // tiny segments:\n        if (flushControl.getFlushDeletes() ||\n            (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n             bufferedDeletesStream.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n          applyAllDeletes = true;\n          if (infoStream != null) {\n            message(\"force apply deletes bytesUsed=\" + bufferedDeletesStream.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n          }\n        }\n      }\n\n      if (applyAllDeletes) {\n        if (infoStream != null) {\n          message(\"apply all deletes during flush\");\n        }\n        flushDeletesCount.incrementAndGet();\n        final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, segmentInfos);\n        if (result.anyDeletes) {\n          checkpoint();\n        }\n        bufferedDeletesStream.prune(segmentInfos);\n        assert !bufferedDeletesStream.any();\n        flushControl.clearDeletes();\n      } else if (infoStream != null) {\n        message(\"don't apply deletes now delTermCount=\" + bufferedDeletesStream.numTerms() + \" bytesUsed=\" + bufferedDeletesStream.bytesUsed());\n      }\n\n      doAfterFlush();\n      flushCount.incrementAndGet();\n\n      success = true;\n\n      return newSegment != null;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","sourceNew":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n\n      boolean maybeMerge = docWriter.flushAllThreads(applyAllDeletes);\n\n      synchronized(this) {\n        if (!applyAllDeletes) {\n          // If deletes alone are consuming > 1/2 our RAM\n          // buffer, force them all to apply now. This is to\n          // prevent too-frequent flushing of a long tail of\n          // tiny segments:\n          if (flushControl.getFlushDeletes() ||\n              (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n               bufferedDeletesStream.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n            applyAllDeletes = true;\n            if (infoStream != null) {\n              message(\"force apply deletes bytesUsed=\" + bufferedDeletesStream.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n            }\n          }\n        }\n\n        if (applyAllDeletes) {\n          if (infoStream != null) {\n            message(\"apply all deletes during flush\");\n          }\n          flushDeletesCount.incrementAndGet();\n          final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, segmentInfos);\n          if (result.anyDeletes) {\n            checkpoint();\n          }\n          if (!keepFullyDeletedSegments && result.allDeleted != null) {\n            if (infoStream != null) {\n              message(\"drop 100% deleted segments: \" + result.allDeleted);\n            }\n            for(SegmentInfo info : result.allDeleted) {\n              // If a merge has already registered for this\n              // segment, we leave it in the readerPool; the\n              // merge will skip merging it and will then drop\n              // it once it's done:\n              if (!mergingSegments.contains(info)) {\n                segmentInfos.remove(info);\n                if (readerPool != null) {\n                  readerPool.drop(info);\n                }\n              }\n            }\n            checkpoint();\n          }\n          bufferedDeletesStream.prune(segmentInfos);\n          assert !bufferedDeletesStream.any();\n\n          flushControl.clearDeletes();\n        } else if (infoStream != null) {\n          message(\"don't apply deletes now delTermCount=\" + bufferedDeletesStream.numTerms() + \" bytesUsed=\" + bufferedDeletesStream.bytesUsed());\n        }\n\n        doAfterFlush();\n        flushCount.incrementAndGet();\n\n        success = true;\n\n        return maybeMerge;\n\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n\n      boolean maybeMerge = docWriter.flushAllThreads(applyAllDeletes);\n\n      synchronized(this) {\n        if (!applyAllDeletes) {\n          // If deletes alone are consuming > 1/2 our RAM\n          // buffer, force them all to apply now. This is to\n          // prevent too-frequent flushing of a long tail of\n          // tiny segments:\n          if (flushControl.getFlushDeletes() ||\n              (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n               bufferedDeletes.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n            applyAllDeletes = true;\n            if (infoStream != null) {\n              message(\"force apply deletes bytesUsed=\" + bufferedDeletes.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n            }\n          }\n        }\n\n        if (applyAllDeletes) {\n          if (infoStream != null) {\n            message(\"apply all deletes during flush\");\n          }\n          flushDeletesCount.incrementAndGet();\n          if (bufferedDeletes.applyDeletes(readerPool, segmentInfos, segmentInfos)) {\n            checkpoint();\n          }\n          flushControl.clearDeletes();\n        } else if (infoStream != null) {\n          message(\"don't apply deletes now delTermCount=\" + bufferedDeletes.numTerms() + \" bytesUsed=\" + bufferedDeletes.bytesUsed());\n        }\n\n        doAfterFlush();\n        flushCount.incrementAndGet();\n\n        success = true;\n\n        return maybeMerge;\n\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["7d45e9e2ad7f57776540627c78f5e22e469ccdc1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d2ed1b9b7b46829fe3199afe9a8bc203f201b175","date":1301491807,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","sourceNew":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n    boolean success = false;\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n\n      boolean maybeMerge = docWriter.flushAllThreads(applyAllDeletes);\n\n      synchronized(this) {\n        if (!applyAllDeletes) {\n          // If deletes alone are consuming > 1/2 our RAM\n          // buffer, force them all to apply now. This is to\n          // prevent too-frequent flushing of a long tail of\n          // tiny segments:\n          if ((config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n               bufferedDeletesStream.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n            applyAllDeletes = true;\n            if (infoStream != null) {\n              message(\"force apply deletes bytesUsed=\" + bufferedDeletesStream.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n            }\n          }\n        }\n\n        if (applyAllDeletes) {\n          if (infoStream != null) {\n            message(\"apply all deletes during flush\");\n          }\n          applyAllDeletes();\n        } else if (infoStream != null) {\n          message(\"don't apply deletes now delTermCount=\" + bufferedDeletesStream.numTerms() + \" bytesUsed=\" + bufferedDeletesStream.bytesUsed());\n        }\n\n        doAfterFlush();\n        if (!maybeMerge) {\n          // flushCount is incremented in flushAllThreads\n          flushCount.incrementAndGet();\n        }\n\n        success = true;\n\n        return maybeMerge;\n\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n\n      boolean maybeMerge = docWriter.flushAllThreads(applyAllDeletes);\n\n      synchronized(this) {\n        if (!applyAllDeletes) {\n          // If deletes alone are consuming > 1/2 our RAM\n          // buffer, force them all to apply now. This is to\n          // prevent too-frequent flushing of a long tail of\n          // tiny segments:\n          if (flushControl.getFlushDeletes() ||\n              (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n               bufferedDeletesStream.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n            applyAllDeletes = true;\n            if (infoStream != null) {\n              message(\"force apply deletes bytesUsed=\" + bufferedDeletesStream.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n            }\n          }\n        }\n\n        if (applyAllDeletes) {\n          if (infoStream != null) {\n            message(\"apply all deletes during flush\");\n          }\n          flushDeletesCount.incrementAndGet();\n          final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, segmentInfos);\n          if (result.anyDeletes) {\n            checkpoint();\n          }\n          if (!keepFullyDeletedSegments && result.allDeleted != null) {\n            if (infoStream != null) {\n              message(\"drop 100% deleted segments: \" + result.allDeleted);\n            }\n            for(SegmentInfo info : result.allDeleted) {\n              // If a merge has already registered for this\n              // segment, we leave it in the readerPool; the\n              // merge will skip merging it and will then drop\n              // it once it's done:\n              if (!mergingSegments.contains(info)) {\n                segmentInfos.remove(info);\n                if (readerPool != null) {\n                  readerPool.drop(info);\n                }\n              }\n            }\n            checkpoint();\n          }\n          bufferedDeletesStream.prune(segmentInfos);\n          assert !bufferedDeletesStream.any();\n\n          flushControl.clearDeletes();\n        } else if (infoStream != null) {\n          message(\"don't apply deletes now delTermCount=\" + bufferedDeletesStream.numTerms() + \" bytesUsed=\" + bufferedDeletesStream.bytesUsed());\n        }\n\n        doAfterFlush();\n        flushCount.incrementAndGet();\n\n        success = true;\n\n        return maybeMerge;\n\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["e4f3b0a30c9d521b86f768348f832af93505b4eb","7d45e9e2ad7f57776540627c78f5e22e469ccdc1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7d45e9e2ad7f57776540627c78f5e22e469ccdc1","date":1302784878,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","sourceNew":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n    assert testPoint(\"startDoFlush\");\n    boolean success = false;\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n      final boolean maybeMerge;\n      \n      synchronized (fullFlushLock) {\n        try {\n          maybeMerge = docWriter.flushAllThreads(applyAllDeletes);\n          success = true;\n        } finally {\n          docWriter.finishFullFlush(success);\n        }\n      }\n      success = false;\n      synchronized(this) {\n        maybeApplyDeletes(applyAllDeletes);\n        doAfterFlush();\n        if (!maybeMerge) {\n          // flushCount is incremented in flushAllThreads\n          flushCount.incrementAndGet();\n        }\n        success = true;\n        return maybeMerge;\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n    boolean success = false;\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n\n      boolean maybeMerge = docWriter.flushAllThreads(applyAllDeletes);\n\n      synchronized(this) {\n        if (!applyAllDeletes) {\n          // If deletes alone are consuming > 1/2 our RAM\n          // buffer, force them all to apply now. This is to\n          // prevent too-frequent flushing of a long tail of\n          // tiny segments:\n          if ((config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n               bufferedDeletesStream.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n            applyAllDeletes = true;\n            if (infoStream != null) {\n              message(\"force apply deletes bytesUsed=\" + bufferedDeletesStream.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n            }\n          }\n        }\n\n        if (applyAllDeletes) {\n          if (infoStream != null) {\n            message(\"apply all deletes during flush\");\n          }\n          applyAllDeletes();\n        } else if (infoStream != null) {\n          message(\"don't apply deletes now delTermCount=\" + bufferedDeletesStream.numTerms() + \" bytesUsed=\" + bufferedDeletesStream.bytesUsed());\n        }\n\n        doAfterFlush();\n        if (!maybeMerge) {\n          // flushCount is incremented in flushAllThreads\n          flushCount.incrementAndGet();\n        }\n\n        success = true;\n\n        return maybeMerge;\n\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","bugFix":["44fcbde6fb2ac44ee3b45e013e54a42911e689ff","bde51b089eb7f86171eb3406e38a274743f9b7ac","d2ed1b9b7b46829fe3199afe9a8bc203f201b175","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"bugIntro":["e4f3b0a30c9d521b86f768348f832af93505b4eb","a1c3710e755dc60691ac0d7fe23c9fcba0537bdc"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f6f4cae61e16730201371ab7e9912721c19324e7","date":1303199575,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","sourceNew":"  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n    assert testPoint(\"startDoFlush\");\n    boolean success = false;\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n      final boolean maybeMerge;\n      \n      synchronized (fullFlushLock) {\n        try {\n          maybeMerge = docWriter.flushAllThreads(applyAllDeletes);\n          success = true;\n        } finally {\n          docWriter.finishFullFlush(success);\n        }\n      }\n      success = false;\n      synchronized(this) {\n        maybeApplyDeletes(applyAllDeletes);\n        doAfterFlush();\n        if (!maybeMerge) {\n          // flushCount is incremented in flushAllThreads\n          flushCount.incrementAndGet();\n        }\n        success = true;\n        return maybeMerge;\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n    assert testPoint(\"startDoFlush\");\n    boolean success = false;\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n      final boolean maybeMerge;\n      \n      synchronized (fullFlushLock) {\n        try {\n          maybeMerge = docWriter.flushAllThreads(applyAllDeletes);\n          success = true;\n        } finally {\n          docWriter.finishFullFlush(success);\n        }\n      }\n      success = false;\n      synchronized(this) {\n        maybeApplyDeletes(applyAllDeletes);\n        doAfterFlush();\n        if (!maybeMerge) {\n          // flushCount is incremented in flushAllThreads\n          flushCount.incrementAndGet();\n        }\n        success = true;\n        return maybeMerge;\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fcef5771aee556e6c886946095ae4485a392526b","date":1304005192,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","sourceNew":"  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n    assert testPoint(\"startDoFlush\");\n    boolean success = false;\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n      final boolean maybeMerge;\n      \n      synchronized (fullFlushLock) {\n        try {\n          maybeMerge = docWriter.flushAllThreads();\n          success = true;\n        } finally {\n          docWriter.finishFullFlush(success);\n        }\n      }\n      success = false;\n      synchronized(this) {\n        maybeApplyDeletes(applyAllDeletes);\n        doAfterFlush();\n        if (!maybeMerge) {\n          // flushCount is incremented in flushAllThreads\n          flushCount.incrementAndGet();\n        }\n        success = true;\n        return maybeMerge;\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","sourceOld":"  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n    assert testPoint(\"startDoFlush\");\n    boolean success = false;\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n      final boolean maybeMerge;\n      \n      synchronized (fullFlushLock) {\n        try {\n          maybeMerge = docWriter.flushAllThreads(applyAllDeletes);\n          success = true;\n        } finally {\n          docWriter.finishFullFlush(success);\n        }\n      }\n      success = false;\n      synchronized(this) {\n        maybeApplyDeletes(applyAllDeletes);\n        doAfterFlush();\n        if (!maybeMerge) {\n          // flushCount is incremented in flushAllThreads\n          flushCount.incrementAndGet();\n        }\n        success = true;\n        return maybeMerge;\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1b1414bb9669ffe06a89e46b889729f2e2588081","date":1304006610,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","sourceNew":"  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n    assert testPoint(\"startDoFlush\");\n    boolean success = false;\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n      final boolean anySegmentFlushed;\n      \n      synchronized (fullFlushLock) {\n        try {\n          anySegmentFlushed = docWriter.flushAllThreads();\n          success = true;\n        } finally {\n          docWriter.finishFullFlush(success);\n        }\n      }\n      success = false;\n      synchronized(this) {\n        maybeApplyDeletes(applyAllDeletes);\n        doAfterFlush();\n        if (!anySegmentFlushed) {\n          // flushCount is incremented in flushAllThreads\n          flushCount.incrementAndGet();\n        }\n        success = true;\n        return anySegmentFlushed;\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","sourceOld":"  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n    assert testPoint(\"startDoFlush\");\n    boolean success = false;\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n      final boolean maybeMerge;\n      \n      synchronized (fullFlushLock) {\n        try {\n          maybeMerge = docWriter.flushAllThreads();\n          success = true;\n        } finally {\n          docWriter.finishFullFlush(success);\n        }\n      }\n      success = false;\n      synchronized(this) {\n        maybeApplyDeletes(applyAllDeletes);\n        doAfterFlush();\n        if (!maybeMerge) {\n          // flushCount is incremented in flushAllThreads\n          flushCount.incrementAndGet();\n        }\n        success = true;\n        return maybeMerge;\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["e4f3b0a30c9d521b86f768348f832af93505b4eb"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","sourceNew":"  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n    assert testPoint(\"startDoFlush\");\n    boolean success = false;\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n      final boolean anySegmentFlushed;\n      \n      synchronized (fullFlushLock) {\n        try {\n          anySegmentFlushed = docWriter.flushAllThreads();\n          success = true;\n        } finally {\n          docWriter.finishFullFlush(success);\n        }\n      }\n      success = false;\n      synchronized(this) {\n        maybeApplyDeletes(applyAllDeletes);\n        doAfterFlush();\n        if (!anySegmentFlushed) {\n          // flushCount is incremented in flushAllThreads\n          flushCount.incrementAndGet();\n        }\n        success = true;\n        return anySegmentFlushed;\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n    \n      final SegmentInfo newSegment = docWriter.flush(this, deleter, mergePolicy, segmentInfos);\n      if (newSegment != null) {\n        setDiagnostics(newSegment, \"flush\");\n        segmentInfos.add(newSegment);\n        checkpoint();\n      }\n\n      if (!applyAllDeletes) {\n        // If deletes alone are consuming > 1/2 our RAM\n        // buffer, force them all to apply now. This is to\n        // prevent too-frequent flushing of a long tail of\n        // tiny segments:\n        if (flushControl.getFlushDeletes() ||\n            (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n             bufferedDeletesStream.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n          applyAllDeletes = true;\n          if (infoStream != null) {\n            message(\"force apply deletes bytesUsed=\" + bufferedDeletesStream.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n          }\n        }\n      }\n\n      if (applyAllDeletes) {\n        if (infoStream != null) {\n          message(\"apply all deletes during flush\");\n        }\n        flushDeletesCount.incrementAndGet();\n        final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, segmentInfos);\n        if (result.anyDeletes) {\n          checkpoint();\n        }\n        if (!keepFullyDeletedSegments && result.allDeleted != null) {\n          if (infoStream != null) {\n            message(\"drop 100% deleted segments: \" + result.allDeleted);\n          }\n          for(SegmentInfo info : result.allDeleted) {\n            // If a merge has already registered for this\n            // segment, we leave it in the readerPool; the\n            // merge will skip merging it and will then drop\n            // it once it's done:\n            if (!mergingSegments.contains(info)) {\n              segmentInfos.remove(info);\n              if (readerPool != null) {\n                readerPool.drop(info);\n              }\n            }\n          }\n          checkpoint();\n        }\n        bufferedDeletesStream.prune(segmentInfos);\n        assert !bufferedDeletesStream.any();\n        flushControl.clearDeletes();\n      } else if (infoStream != null) {\n        message(\"don't apply deletes now delTermCount=\" + bufferedDeletesStream.numTerms() + \" bytesUsed=\" + bufferedDeletesStream.bytesUsed());\n      }\n\n      doAfterFlush();\n      flushCount.incrementAndGet();\n\n      success = true;\n\n      return newSegment != null;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","sourceNew":"  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n    assert testPoint(\"startDoFlush\");\n    boolean success = false;\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n      final boolean anySegmentFlushed;\n      \n      synchronized (fullFlushLock) {\n        try {\n          anySegmentFlushed = docWriter.flushAllThreads();\n          success = true;\n        } finally {\n          docWriter.finishFullFlush(success);\n        }\n      }\n      success = false;\n      synchronized(this) {\n        maybeApplyDeletes(applyAllDeletes);\n        doAfterFlush();\n        if (!anySegmentFlushed) {\n          // flushCount is incremented in flushAllThreads\n          flushCount.incrementAndGet();\n        }\n        success = true;\n        return anySegmentFlushed;\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n    \n      final SegmentInfo newSegment = docWriter.flush(this, deleter, mergePolicy, segmentInfos);\n      if (newSegment != null) {\n        setDiagnostics(newSegment, \"flush\");\n        segmentInfos.add(newSegment);\n        checkpoint();\n      }\n\n      if (!applyAllDeletes) {\n        // If deletes alone are consuming > 1/2 our RAM\n        // buffer, force them all to apply now. This is to\n        // prevent too-frequent flushing of a long tail of\n        // tiny segments:\n        if (flushControl.getFlushDeletes() ||\n            (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n             bufferedDeletesStream.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n          applyAllDeletes = true;\n          if (infoStream != null) {\n            message(\"force apply deletes bytesUsed=\" + bufferedDeletesStream.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n          }\n        }\n      }\n\n      if (applyAllDeletes) {\n        if (infoStream != null) {\n          message(\"apply all deletes during flush\");\n        }\n        flushDeletesCount.incrementAndGet();\n        final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, segmentInfos);\n        if (result.anyDeletes) {\n          checkpoint();\n        }\n        if (!keepFullyDeletedSegments && result.allDeleted != null) {\n          if (infoStream != null) {\n            message(\"drop 100% deleted segments: \" + result.allDeleted);\n          }\n          for(SegmentInfo info : result.allDeleted) {\n            // If a merge has already registered for this\n            // segment, we leave it in the readerPool; the\n            // merge will skip merging it and will then drop\n            // it once it's done:\n            if (!mergingSegments.contains(info)) {\n              segmentInfos.remove(info);\n              if (readerPool != null) {\n                readerPool.drop(info);\n              }\n            }\n          }\n          checkpoint();\n        }\n        bufferedDeletesStream.prune(segmentInfos);\n        assert !bufferedDeletesStream.any();\n        flushControl.clearDeletes();\n      } else if (infoStream != null) {\n        message(\"don't apply deletes now delTermCount=\" + bufferedDeletesStream.numTerms() + \" bytesUsed=\" + bufferedDeletesStream.bytesUsed());\n      }\n\n      doAfterFlush();\n      flushCount.incrementAndGet();\n\n      success = true;\n\n      return newSegment != null;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","sourceNew":"  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n    assert testPoint(\"startDoFlush\");\n    boolean success = false;\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n      final boolean anySegmentFlushed;\n      \n      synchronized (fullFlushLock) {\n        try {\n          anySegmentFlushed = docWriter.flushAllThreads();\n          success = true;\n        } finally {\n          docWriter.finishFullFlush(success);\n        }\n      }\n      success = false;\n      synchronized(this) {\n        maybeApplyDeletes(applyAllDeletes);\n        doAfterFlush();\n        if (!anySegmentFlushed) {\n          // flushCount is incremented in flushAllThreads\n          flushCount.incrementAndGet();\n        }\n        success = true;\n        return anySegmentFlushed;\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","sourceOld":"  // TODO: this method should not have to be entirely\n  // synchronized, ie, merges should be allowed to commit\n  // even while a flush is happening\n  private synchronized boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n\n    assert testPoint(\"startDoFlush\");\n\n    // We may be flushing because it was triggered by doc\n    // count, del count, ram usage (in which case flush\n    // pending is already set), or we may be flushing\n    // due to external event eg getReader or commit is\n    // called (in which case we now set it, and this will\n    // pause all threads):\n    flushControl.setFlushPendingNoWait(\"explicit flush\");\n\n    boolean success = false;\n\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n    \n      final SegmentInfo newSegment = docWriter.flush(this, deleter, mergePolicy, segmentInfos);\n      if (newSegment != null) {\n        setDiagnostics(newSegment, \"flush\");\n        segmentInfos.add(newSegment);\n        checkpoint();\n      }\n\n      if (!applyAllDeletes) {\n        // If deletes alone are consuming > 1/2 our RAM\n        // buffer, force them all to apply now. This is to\n        // prevent too-frequent flushing of a long tail of\n        // tiny segments:\n        if (flushControl.getFlushDeletes() ||\n            (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n             bufferedDeletesStream.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {\n          applyAllDeletes = true;\n          if (infoStream != null) {\n            message(\"force apply deletes bytesUsed=\" + bufferedDeletesStream.bytesUsed() + \" vs ramBuffer=\" + (1024*1024*config.getRAMBufferSizeMB()));\n          }\n        }\n      }\n\n      if (applyAllDeletes) {\n        if (infoStream != null) {\n          message(\"apply all deletes during flush\");\n        }\n        flushDeletesCount.incrementAndGet();\n        final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, segmentInfos);\n        if (result.anyDeletes) {\n          checkpoint();\n        }\n        if (!keepFullyDeletedSegments && result.allDeleted != null) {\n          if (infoStream != null) {\n            message(\"drop 100% deleted segments: \" + result.allDeleted);\n          }\n          for(SegmentInfo info : result.allDeleted) {\n            // If a merge has already registered for this\n            // segment, we leave it in the readerPool; the\n            // merge will skip merging it and will then drop\n            // it once it's done:\n            if (!mergingSegments.contains(info)) {\n              segmentInfos.remove(info);\n              if (readerPool != null) {\n                readerPool.drop(info);\n              }\n            }\n          }\n          checkpoint();\n        }\n        bufferedDeletesStream.prune(segmentInfos);\n        assert !bufferedDeletesStream.any();\n        flushControl.clearDeletes();\n      } else if (infoStream != null) {\n        message(\"don't apply deletes now delTermCount=\" + bufferedDeletesStream.numTerms() + \" bytesUsed=\" + bufferedDeletesStream.bytesUsed());\n      }\n\n      doAfterFlush();\n      flushCount.incrementAndGet();\n\n      success = true;\n\n      return newSegment != null;\n\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      flushControl.clearFlushPending();\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a1c3710e755dc60691ac0d7fe23c9fcba0537bdc","date":1320437768,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","sourceNew":"  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n    assert testPoint(\"startDoFlush\");\n    boolean success = false;\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n      final boolean anySegmentFlushed;\n      \n      synchronized (fullFlushLock) {\n    \tboolean flushSuccess = false;\n        try {\n          anySegmentFlushed = docWriter.flushAllThreads();\n          flushSuccess = true;\n        } finally {\n          docWriter.finishFullFlush(flushSuccess);\n        }\n      }\n      synchronized(this) {\n        maybeApplyDeletes(applyAllDeletes);\n        doAfterFlush();\n        if (!anySegmentFlushed) {\n          // flushCount is incremented in flushAllThreads\n          flushCount.incrementAndGet();\n        }\n        success = true;\n        return anySegmentFlushed;\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","sourceOld":"  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n    assert testPoint(\"startDoFlush\");\n    boolean success = false;\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n      final boolean anySegmentFlushed;\n      \n      synchronized (fullFlushLock) {\n        try {\n          anySegmentFlushed = docWriter.flushAllThreads();\n          success = true;\n        } finally {\n          docWriter.finishFullFlush(success);\n        }\n      }\n      success = false;\n      synchronized(this) {\n        maybeApplyDeletes(applyAllDeletes);\n        doAfterFlush();\n        if (!anySegmentFlushed) {\n          // flushCount is incremented in flushAllThreads\n          flushCount.incrementAndGet();\n        }\n        success = true;\n        return anySegmentFlushed;\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","bugFix":["7d45e9e2ad7f57776540627c78f5e22e469ccdc1"],"bugIntro":["901d103ab7c2eeae92b111fc91bb1b00580a3fd7"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","sourceNew":"  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n    assert testPoint(\"startDoFlush\");\n    boolean success = false;\n    try {\n\n      if (infoStream != null) {\n        infoStream.message(\"IW\", \"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        infoStream.message(\"IW\", \"  index before flush \" + segString());\n      }\n      final boolean anySegmentFlushed;\n      \n      synchronized (fullFlushLock) {\n    \tboolean flushSuccess = false;\n        try {\n          anySegmentFlushed = docWriter.flushAllThreads();\n          flushSuccess = true;\n        } finally {\n          docWriter.finishFullFlush(flushSuccess);\n        }\n      }\n      synchronized(this) {\n        maybeApplyDeletes(applyAllDeletes);\n        doAfterFlush();\n        if (!anySegmentFlushed) {\n          // flushCount is incremented in flushAllThreads\n          flushCount.incrementAndGet();\n        }\n        success = true;\n        return anySegmentFlushed;\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      if (!success && infoStream != null)\n        infoStream.message(\"IW\", \"hit exception during flush\");\n    }\n  }\n\n","sourceOld":"  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n    assert testPoint(\"startDoFlush\");\n    boolean success = false;\n    try {\n\n      if (infoStream != null) {\n        message(\"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        message(\"  index before flush \" + segString());\n      }\n      final boolean anySegmentFlushed;\n      \n      synchronized (fullFlushLock) {\n    \tboolean flushSuccess = false;\n        try {\n          anySegmentFlushed = docWriter.flushAllThreads();\n          flushSuccess = true;\n        } finally {\n          docWriter.finishFullFlush(flushSuccess);\n        }\n      }\n      synchronized(this) {\n        maybeApplyDeletes(applyAllDeletes);\n        doAfterFlush();\n        if (!anySegmentFlushed) {\n          // flushCount is incremented in flushAllThreads\n          flushCount.incrementAndGet();\n        }\n        success = true;\n        return anySegmentFlushed;\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      if (!success && infoStream != null)\n        message(\"hit exception during flush\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"58c6bbc222f074c844e736e6fb23647e3db9cfe3","date":1322743940,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","sourceNew":"  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n    assert testPoint(\"startDoFlush\");\n    boolean success = false;\n    try {\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        infoStream.message(\"IW\", \"  index before flush \" + segString());\n      }\n      final boolean anySegmentFlushed;\n      \n      synchronized (fullFlushLock) {\n    \tboolean flushSuccess = false;\n        try {\n          anySegmentFlushed = docWriter.flushAllThreads();\n          flushSuccess = true;\n        } finally {\n          docWriter.finishFullFlush(flushSuccess);\n        }\n      }\n      synchronized(this) {\n        maybeApplyDeletes(applyAllDeletes);\n        doAfterFlush();\n        if (!anySegmentFlushed) {\n          // flushCount is incremented in flushAllThreads\n          flushCount.incrementAndGet();\n        }\n        success = true;\n        return anySegmentFlushed;\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      if (!success)\n        infoStream.message(\"IW\", \"hit exception during flush\");\n    }\n  }\n\n","sourceOld":"  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n    assert testPoint(\"startDoFlush\");\n    boolean success = false;\n    try {\n\n      if (infoStream != null) {\n        infoStream.message(\"IW\", \"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        infoStream.message(\"IW\", \"  index before flush \" + segString());\n      }\n      final boolean anySegmentFlushed;\n      \n      synchronized (fullFlushLock) {\n    \tboolean flushSuccess = false;\n        try {\n          anySegmentFlushed = docWriter.flushAllThreads();\n          flushSuccess = true;\n        } finally {\n          docWriter.finishFullFlush(flushSuccess);\n        }\n      }\n      synchronized(this) {\n        maybeApplyDeletes(applyAllDeletes);\n        doAfterFlush();\n        if (!anySegmentFlushed) {\n          // flushCount is incremented in flushAllThreads\n          flushCount.incrementAndGet();\n        }\n        success = true;\n        return anySegmentFlushed;\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      if (!success && infoStream != null)\n        infoStream.message(\"IW\", \"hit exception during flush\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c5df35ab57c223ea11aec64b53bf611904f3dced","date":1323640545,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","sourceNew":"  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n    assert testPoint(\"startDoFlush\");\n    boolean success = false;\n    try {\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        infoStream.message(\"IW\", \"  index before flush \" + segString());\n      }\n      final boolean anySegmentFlushed;\n      \n      synchronized (fullFlushLock) {\n    \tboolean flushSuccess = false;\n        try {\n          anySegmentFlushed = docWriter.flushAllThreads();\n          flushSuccess = true;\n        } finally {\n          docWriter.finishFullFlush(flushSuccess);\n        }\n      }\n      synchronized(this) {\n        maybeApplyDeletes(applyAllDeletes);\n        doAfterFlush();\n        if (!anySegmentFlushed) {\n          // flushCount is incremented in flushAllThreads\n          flushCount.incrementAndGet();\n        }\n        success = true;\n        return anySegmentFlushed;\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"hit exception during flush\");\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n    assert testPoint(\"startDoFlush\");\n    boolean success = false;\n    try {\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        infoStream.message(\"IW\", \"  index before flush \" + segString());\n      }\n      final boolean anySegmentFlushed;\n      \n      synchronized (fullFlushLock) {\n    \tboolean flushSuccess = false;\n        try {\n          anySegmentFlushed = docWriter.flushAllThreads();\n          flushSuccess = true;\n        } finally {\n          docWriter.finishFullFlush(flushSuccess);\n        }\n      }\n      synchronized(this) {\n        maybeApplyDeletes(applyAllDeletes);\n        doAfterFlush();\n        if (!anySegmentFlushed) {\n          // flushCount is incremented in flushAllThreads\n          flushCount.incrementAndGet();\n        }\n        success = true;\n        return anySegmentFlushed;\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      if (!success)\n        infoStream.message(\"IW\", \"hit exception during flush\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","date":1323720782,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","sourceNew":"  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n    assert testPoint(\"startDoFlush\");\n    boolean success = false;\n    try {\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        infoStream.message(\"IW\", \"  index before flush \" + segString());\n      }\n      final boolean anySegmentFlushed;\n      \n      synchronized (fullFlushLock) {\n    \tboolean flushSuccess = false;\n        try {\n          anySegmentFlushed = docWriter.flushAllThreads();\n          flushSuccess = true;\n        } finally {\n          docWriter.finishFullFlush(flushSuccess);\n        }\n      }\n      synchronized(this) {\n        maybeApplyDeletes(applyAllDeletes);\n        doAfterFlush();\n        if (!anySegmentFlushed) {\n          // flushCount is incremented in flushAllThreads\n          flushCount.incrementAndGet();\n        }\n        success = true;\n        return anySegmentFlushed;\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"hit exception during flush\");\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n    assert testPoint(\"startDoFlush\");\n    boolean success = false;\n    try {\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        infoStream.message(\"IW\", \"  index before flush \" + segString());\n      }\n      final boolean anySegmentFlushed;\n      \n      synchronized (fullFlushLock) {\n    \tboolean flushSuccess = false;\n        try {\n          anySegmentFlushed = docWriter.flushAllThreads();\n          flushSuccess = true;\n        } finally {\n          docWriter.finishFullFlush(flushSuccess);\n        }\n      }\n      synchronized(this) {\n        maybeApplyDeletes(applyAllDeletes);\n        doAfterFlush();\n        if (!anySegmentFlushed) {\n          // flushCount is incremented in flushAllThreads\n          flushCount.incrementAndGet();\n        }\n        success = true;\n        return anySegmentFlushed;\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      if (!success)\n        infoStream.message(\"IW\", \"hit exception during flush\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#doFlush(boolean).mjava","sourceNew":"  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n    assert testPoint(\"startDoFlush\");\n    boolean success = false;\n    try {\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        infoStream.message(\"IW\", \"  index before flush \" + segString());\n      }\n      final boolean anySegmentFlushed;\n      \n      synchronized (fullFlushLock) {\n    \tboolean flushSuccess = false;\n        try {\n          anySegmentFlushed = docWriter.flushAllThreads();\n          flushSuccess = true;\n        } finally {\n          docWriter.finishFullFlush(flushSuccess);\n        }\n      }\n      synchronized(this) {\n        maybeApplyDeletes(applyAllDeletes);\n        doAfterFlush();\n        if (!anySegmentFlushed) {\n          // flushCount is incremented in flushAllThreads\n          flushCount.incrementAndGet();\n        }\n        success = true;\n        return anySegmentFlushed;\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"hit exception during flush\");\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot flush\");\n    }\n\n    doBeforeFlush();\n    assert testPoint(\"startDoFlush\");\n    boolean success = false;\n    try {\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"  start flush: applyAllDeletes=\" + applyAllDeletes);\n        infoStream.message(\"IW\", \"  index before flush \" + segString());\n      }\n      final boolean anySegmentFlushed;\n      \n      synchronized (fullFlushLock) {\n    \tboolean flushSuccess = false;\n        try {\n          anySegmentFlushed = docWriter.flushAllThreads();\n          flushSuccess = true;\n        } finally {\n          docWriter.finishFullFlush(flushSuccess);\n        }\n      }\n      synchronized(this) {\n        maybeApplyDeletes(applyAllDeletes);\n        doAfterFlush();\n        if (!anySegmentFlushed) {\n          // flushCount is incremented in flushAllThreads\n          flushCount.incrementAndGet();\n        }\n        success = true;\n        return anySegmentFlushed;\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"doFlush\");\n      // never hit\n      return false;\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"hit exception during flush\");\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"c5df35ab57c223ea11aec64b53bf611904f3dced":["58c6bbc222f074c844e736e6fb23647e3db9cfe3"],"58c6bbc222f074c844e736e6fb23647e3db9cfe3":["06584e6e98d592b34e1329b384182f368d2025e8"],"f1bdbf92da222965b46c0a942c3857ba56e5c638":["29ef99d61cda9641b6250bf9567329a6e65f901d","e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3"],"1b1414bb9669ffe06a89e46b889729f2e2588081":["fcef5771aee556e6c886946095ae4485a392526b"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["c5df35ab57c223ea11aec64b53bf611904f3dced"],"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":["58c6bbc222f074c844e736e6fb23647e3db9cfe3","c5df35ab57c223ea11aec64b53bf611904f3dced"],"fcef5771aee556e6c886946095ae4485a392526b":["f6f4cae61e16730201371ab7e9912721c19324e7"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["5ef87af8c7bd0f8429622b83aa74202383f2e757","4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"f6f4cae61e16730201371ab7e9912721c19324e7":["7d45e9e2ad7f57776540627c78f5e22e469ccdc1"],"c19f985e36a65cc969e8e564fe337a0d41512075":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3":["c19f985e36a65cc969e8e564fe337a0d41512075"],"d2ed1b9b7b46829fe3199afe9a8bc203f201b175":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","c19f985e36a65cc969e8e564fe337a0d41512075"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5ef87af8c7bd0f8429622b83aa74202383f2e757":["833a7987bc1c94455fde83e3311f72bddedcfb93"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["3161e3ffcf20c09a22504a589d4d9bd273e11e33","e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3"],"833a7987bc1c94455fde83e3311f72bddedcfb93":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"06584e6e98d592b34e1329b384182f368d2025e8":["a1c3710e755dc60691ac0d7fe23c9fcba0537bdc"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["f1bdbf92da222965b46c0a942c3857ba56e5c638","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3","1b1414bb9669ffe06a89e46b889729f2e2588081"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"7d45e9e2ad7f57776540627c78f5e22e469ccdc1":["d2ed1b9b7b46829fe3199afe9a8bc203f201b175"],"3161e3ffcf20c09a22504a589d4d9bd273e11e33":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"a3776dccca01c11e7046323cfad46a3b4a471233":["e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"a1c3710e755dc60691ac0d7fe23c9fcba0537bdc":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"c5df35ab57c223ea11aec64b53bf611904f3dced":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","93ccd971aca7fb61b7f1b946e44714cfc80bfc7c"],"58c6bbc222f074c844e736e6fb23647e3db9cfe3":["c5df35ab57c223ea11aec64b53bf611904f3dced","93ccd971aca7fb61b7f1b946e44714cfc80bfc7c"],"f1bdbf92da222965b46c0a942c3857ba56e5c638":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"1b1414bb9669ffe06a89e46b889729f2e2588081":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":[],"fcef5771aee556e6c886946095ae4485a392526b":["1b1414bb9669ffe06a89e46b889729f2e2588081"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["3161e3ffcf20c09a22504a589d4d9bd273e11e33"],"f6f4cae61e16730201371ab7e9912721c19324e7":["fcef5771aee556e6c886946095ae4485a392526b"],"c19f985e36a65cc969e8e564fe337a0d41512075":["e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3","29ef99d61cda9641b6250bf9567329a6e65f901d"],"e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3":["f1bdbf92da222965b46c0a942c3857ba56e5c638","bde51b089eb7f86171eb3406e38a274743f9b7ac","b3e06be49006ecac364d39d12b9c9f74882f9b9f","a3776dccca01c11e7046323cfad46a3b4a471233"],"d2ed1b9b7b46829fe3199afe9a8bc203f201b175":["7d45e9e2ad7f57776540627c78f5e22e469ccdc1"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["f1bdbf92da222965b46c0a942c3857ba56e5c638"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5","833a7987bc1c94455fde83e3311f72bddedcfb93","ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"5ef87af8c7bd0f8429622b83aa74202383f2e757":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","c19f985e36a65cc969e8e564fe337a0d41512075","ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["d2ed1b9b7b46829fe3199afe9a8bc203f201b175"],"833a7987bc1c94455fde83e3311f72bddedcfb93":["5ef87af8c7bd0f8429622b83aa74202383f2e757"],"06584e6e98d592b34e1329b384182f368d2025e8":["58c6bbc222f074c844e736e6fb23647e3db9cfe3"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233","a1c3710e755dc60691ac0d7fe23c9fcba0537bdc"],"7d45e9e2ad7f57776540627c78f5e22e469ccdc1":["f6f4cae61e16730201371ab7e9912721c19324e7"],"3161e3ffcf20c09a22504a589d4d9bd273e11e33":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"a3776dccca01c11e7046323cfad46a3b4a471233":[],"a1c3710e755dc60691ac0d7fe23c9fcba0537bdc":["06584e6e98d592b34e1329b384182f368d2025e8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}