{"path":"solr/core/src/java/org/apache/solr/cloud/ZkController#ZkController(CoreContainer,String,int,int,String,String,String,String,boolean,int,int,CurrentCoreDescriptorProvider).mjava","commits":[{"id":"849494cf2f3a96af5c8c84995108ddd8456fcd04","date":1372277913,"type":1,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ZkController(CoreContainer,String,int,int,String,String,String,String,boolean,int,int,CurrentCoreDescriptorProvider).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ZkController(CoreContainer,String,int,int,String,String,String,String,int,int,CurrentCoreDescriptorProvider).mjava","sourceNew":"  public ZkController(final CoreContainer cc, String zkServerAddress, int zkClientTimeout, int zkClientConnectTimeout, String localHost, String locaHostPort,\n      String localHostContext, String leaderVoteWait, boolean genericCoreNodeNames, int distribUpdateConnTimeout, int distribUpdateSoTimeout, final CurrentCoreDescriptorProvider registerOnReconnect) throws InterruptedException,\n      TimeoutException, IOException {\n    if (cc == null) throw new IllegalArgumentException(\"CoreContainer cannot be null.\");\n    this.cc = cc;\n    this.genericCoreNodeNames = genericCoreNodeNames;\n    // be forgiving and strip this off leading/trailing slashes\n    // this allows us to support users specifying hostContext=\"/\" in \n    // solr.xml to indicate the root context, instead of hostContext=\"\" \n    // which means the default of \"solr\"\n    localHostContext = trimLeadingAndTrailingSlashes(localHostContext);\n    \n    updateShardHandler = new UpdateShardHandler(distribUpdateConnTimeout, distribUpdateSoTimeout);\n    \n    this.zkServerAddress = zkServerAddress;\n    this.localHostPort = locaHostPort;\n    this.localHostContext = localHostContext;\n    this.localHost = getHostAddress(localHost);\n    this.baseURL = this.localHost + \":\" + this.localHostPort + \n      (this.localHostContext.isEmpty() ? \"\" : (\"/\" + this.localHostContext));\n\n    this.hostName = getHostNameFromAddress(this.localHost);\n    this.nodeName = generateNodeName(this.hostName, \n                                     this.localHostPort, \n                                     this.localHostContext);\n\n    this.leaderVoteWait = leaderVoteWait;\n    this.clientTimeout = zkClientTimeout;\n    zkClient = new SolrZkClient(zkServerAddress, zkClientTimeout, zkClientConnectTimeout,\n        // on reconnect, reload cloud info\n        new OnReconnect() {\n\n          @Override\n          public void command() {\n            try {\n              markAllAsNotLeader(registerOnReconnect);\n              \n              // this is troublesome - we dont want to kill anything the old leader accepted\n              // though I guess sync will likely get those updates back? But only if\n              // he is involved in the sync, and he certainly may not be\n            //  ExecutorUtil.shutdownAndAwaitTermination(cc.getCmdDistribExecutor());\n              // we need to create all of our lost watches\n              \n              // seems we dont need to do this again...\n              //Overseer.createClientNodes(zkClient, getNodeName());\n              ShardHandler shardHandler;\n              String adminPath;\n              shardHandler = cc.getShardHandlerFactory().getShardHandler();\n              adminPath = cc.getAdminPath();\n\n              cc.cancelCoreRecoveries();\n              \n              registerAllCoresAsDown(registerOnReconnect, false);\n\n              ZkController.this.overseer = new Overseer(shardHandler, adminPath, zkStateReader);\n              ElectionContext context = new OverseerElectionContext(zkClient, overseer, getNodeName());\n              overseerElector.joinElection(context, true);\n              zkStateReader.createClusterStateWatchersAndUpdate();\n              \n              // we have to register as live first to pick up docs in the buffer\n              createEphemeralLiveNode();\n              \n              List<CoreDescriptor> descriptors = registerOnReconnect.getCurrentDescriptors();\n              // re register all descriptors\n              if (descriptors  != null) {\n                for (CoreDescriptor descriptor : descriptors) {\n                  // TODO: we need to think carefully about what happens when it was\n                  // a leader that was expired - as well as what to do about leaders/overseers\n                  // with connection loss\n                  try {\n                    register(descriptor.getName(), descriptor, true, true);\n                  } catch (Throwable t) {\n                    SolrException.log(log, \"Error registering SolrCore\", t);\n                  }\n                }\n              }\n  \n            } catch (InterruptedException e) {\n              // Restore the interrupted status\n              Thread.currentThread().interrupt();\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            }\n\n          }\n\n \n        });\n    \n    this.overseerJobQueue = Overseer.getInQueue(zkClient);\n    this.overseerCollectionQueue = Overseer.getCollectionQueue(zkClient);\n    cmdExecutor = new ZkCmdExecutor(zkClientTimeout);\n    leaderElector = new LeaderElector(zkClient);\n    zkStateReader = new ZkStateReader(zkClient);\n    \n    init(registerOnReconnect);\n  }\n\n","sourceOld":"  public ZkController(final CoreContainer cc, String zkServerAddress, int zkClientTimeout, int zkClientConnectTimeout, String localHost, String locaHostPort,\n      String localHostContext, String leaderVoteWait, int distribUpdateConnTimeout, int distribUpdateSoTimeout, final CurrentCoreDescriptorProvider registerOnReconnect) throws InterruptedException,\n      TimeoutException, IOException {\n    if (cc == null) throw new IllegalArgumentException(\"CoreContainer cannot be null.\");\n    this.cc = cc;\n\n    // be forgiving and strip this off leading/trailing slashes\n    // this allows us to support users specifying hostContext=\"/\" in \n    // solr.xml to indicate the root context, instead of hostContext=\"\" \n    // which means the default of \"solr\"\n    localHostContext = trimLeadingAndTrailingSlashes(localHostContext);\n    \n    updateShardHandler = new UpdateShardHandler(distribUpdateConnTimeout, distribUpdateSoTimeout);\n    \n    this.zkServerAddress = zkServerAddress;\n    this.localHostPort = locaHostPort;\n    this.localHostContext = localHostContext;\n    this.localHost = getHostAddress(localHost);\n    this.baseURL = this.localHost + \":\" + this.localHostPort + \n      (this.localHostContext.isEmpty() ? \"\" : (\"/\" + this.localHostContext));\n\n    this.hostName = getHostNameFromAddress(this.localHost);\n    this.nodeName = generateNodeName(this.hostName, \n                                     this.localHostPort, \n                                     this.localHostContext);\n\n    this.leaderVoteWait = leaderVoteWait;\n    this.clientTimeout = zkClientTimeout;\n    zkClient = new SolrZkClient(zkServerAddress, zkClientTimeout, zkClientConnectTimeout,\n        // on reconnect, reload cloud info\n        new OnReconnect() {\n\n          @Override\n          public void command() {\n            try {\n              markAllAsNotLeader(registerOnReconnect);\n              \n              // this is troublesome - we dont want to kill anything the old leader accepted\n              // though I guess sync will likely get those updates back? But only if\n              // he is involved in the sync, and he certainly may not be\n            //  ExecutorUtil.shutdownAndAwaitTermination(cc.getCmdDistribExecutor());\n              // we need to create all of our lost watches\n              \n              // seems we dont need to do this again...\n              //Overseer.createClientNodes(zkClient, getNodeName());\n              ShardHandler shardHandler;\n              String adminPath;\n              shardHandler = cc.getShardHandlerFactory().getShardHandler();\n              adminPath = cc.getAdminPath();\n\n              cc.cancelCoreRecoveries();\n              \n              registerAllCoresAsDown(registerOnReconnect, false);\n\n              ZkController.this.overseer = new Overseer(shardHandler, adminPath, zkStateReader);\n              ElectionContext context = new OverseerElectionContext(zkClient, overseer, getNodeName());\n              overseerElector.joinElection(context, true);\n              zkStateReader.createClusterStateWatchersAndUpdate();\n              \n              // we have to register as live first to pick up docs in the buffer\n              createEphemeralLiveNode();\n              \n              List<CoreDescriptor> descriptors = registerOnReconnect.getCurrentDescriptors();\n              // re register all descriptors\n              if (descriptors  != null) {\n                for (CoreDescriptor descriptor : descriptors) {\n                  // TODO: we need to think carefully about what happens when it was\n                  // a leader that was expired - as well as what to do about leaders/overseers\n                  // with connection loss\n                  try {\n                    register(descriptor.getName(), descriptor, true, true);\n                  } catch (Throwable t) {\n                    SolrException.log(log, \"Error registering SolrCore\", t);\n                  }\n                }\n              }\n  \n            } catch (InterruptedException e) {\n              // Restore the interrupted status\n              Thread.currentThread().interrupt();\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            }\n\n          }\n\n \n        });\n    \n    this.overseerJobQueue = Overseer.getInQueue(zkClient);\n    this.overseerCollectionQueue = Overseer.getCollectionQueue(zkClient);\n    cmdExecutor = new ZkCmdExecutor(zkClientTimeout);\n    leaderElector = new LeaderElector(zkClient);\n    zkStateReader = new ZkStateReader(zkClient);\n    \n    init(registerOnReconnect);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":0,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ZkController(CoreContainer,String,int,int,String,String,String,String,boolean,int,int,CurrentCoreDescriptorProvider).mjava","pathOld":"/dev/null","sourceNew":"  public ZkController(final CoreContainer cc, String zkServerAddress, int zkClientTimeout, int zkClientConnectTimeout, String localHost, String locaHostPort,\n      String localHostContext, String leaderVoteWait, boolean genericCoreNodeNames, int distribUpdateConnTimeout, int distribUpdateSoTimeout, final CurrentCoreDescriptorProvider registerOnReconnect) throws InterruptedException,\n      TimeoutException, IOException {\n    if (cc == null) throw new IllegalArgumentException(\"CoreContainer cannot be null.\");\n    this.cc = cc;\n    this.genericCoreNodeNames = genericCoreNodeNames;\n    // be forgiving and strip this off leading/trailing slashes\n    // this allows us to support users specifying hostContext=\"/\" in \n    // solr.xml to indicate the root context, instead of hostContext=\"\" \n    // which means the default of \"solr\"\n    localHostContext = trimLeadingAndTrailingSlashes(localHostContext);\n    \n    updateShardHandler = new UpdateShardHandler(distribUpdateConnTimeout, distribUpdateSoTimeout);\n    \n    this.zkServerAddress = zkServerAddress;\n    this.localHostPort = locaHostPort;\n    this.localHostContext = localHostContext;\n    this.localHost = getHostAddress(localHost);\n    this.baseURL = this.localHost + \":\" + this.localHostPort + \n      (this.localHostContext.isEmpty() ? \"\" : (\"/\" + this.localHostContext));\n\n    this.hostName = getHostNameFromAddress(this.localHost);\n    this.nodeName = generateNodeName(this.hostName, \n                                     this.localHostPort, \n                                     this.localHostContext);\n\n    this.leaderVoteWait = leaderVoteWait;\n    this.clientTimeout = zkClientTimeout;\n    zkClient = new SolrZkClient(zkServerAddress, zkClientTimeout, zkClientConnectTimeout,\n        // on reconnect, reload cloud info\n        new OnReconnect() {\n\n          @Override\n          public void command() {\n            try {\n              markAllAsNotLeader(registerOnReconnect);\n              \n              // this is troublesome - we dont want to kill anything the old leader accepted\n              // though I guess sync will likely get those updates back? But only if\n              // he is involved in the sync, and he certainly may not be\n            //  ExecutorUtil.shutdownAndAwaitTermination(cc.getCmdDistribExecutor());\n              // we need to create all of our lost watches\n              \n              // seems we dont need to do this again...\n              //Overseer.createClientNodes(zkClient, getNodeName());\n              ShardHandler shardHandler;\n              String adminPath;\n              shardHandler = cc.getShardHandlerFactory().getShardHandler();\n              adminPath = cc.getAdminPath();\n\n              cc.cancelCoreRecoveries();\n              \n              registerAllCoresAsDown(registerOnReconnect, false);\n\n              ZkController.this.overseer = new Overseer(shardHandler, adminPath, zkStateReader);\n              ElectionContext context = new OverseerElectionContext(zkClient, overseer, getNodeName());\n              overseerElector.joinElection(context, true);\n              zkStateReader.createClusterStateWatchersAndUpdate();\n              \n              // we have to register as live first to pick up docs in the buffer\n              createEphemeralLiveNode();\n              \n              List<CoreDescriptor> descriptors = registerOnReconnect.getCurrentDescriptors();\n              // re register all descriptors\n              if (descriptors  != null) {\n                for (CoreDescriptor descriptor : descriptors) {\n                  // TODO: we need to think carefully about what happens when it was\n                  // a leader that was expired - as well as what to do about leaders/overseers\n                  // with connection loss\n                  try {\n                    register(descriptor.getName(), descriptor, true, true);\n                  } catch (Throwable t) {\n                    SolrException.log(log, \"Error registering SolrCore\", t);\n                  }\n                }\n              }\n  \n            } catch (InterruptedException e) {\n              // Restore the interrupted status\n              Thread.currentThread().interrupt();\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            }\n\n          }\n\n \n        });\n    \n    this.overseerJobQueue = Overseer.getInQueue(zkClient);\n    this.overseerCollectionQueue = Overseer.getCollectionQueue(zkClient);\n    cmdExecutor = new ZkCmdExecutor(zkClientTimeout);\n    leaderElector = new LeaderElector(zkClient);\n    zkStateReader = new ZkStateReader(zkClient);\n    \n    init(registerOnReconnect);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"92a4da96826f502cf1a56a096929b37ce73e523a","date":1374584011,"type":5,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ZkController(CoreContainer,String,int,int,String,String,String,int,boolean,int,int,CurrentCoreDescriptorProvider).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ZkController(CoreContainer,String,int,int,String,String,String,String,boolean,int,int,CurrentCoreDescriptorProvider).mjava","sourceNew":"  public ZkController(final CoreContainer cc, String zkServerAddress, int zkClientTimeout, int zkClientConnectTimeout, String localHost, String locaHostPort,\n      String localHostContext, int leaderVoteWait, boolean genericCoreNodeNames, int distribUpdateConnTimeout, int distribUpdateSoTimeout, final CurrentCoreDescriptorProvider registerOnReconnect) throws InterruptedException,\n      TimeoutException, IOException {\n    if (cc == null) throw new IllegalArgumentException(\"CoreContainer cannot be null.\");\n    this.cc = cc;\n    this.genericCoreNodeNames = genericCoreNodeNames;\n    // be forgiving and strip this off leading/trailing slashes\n    // this allows us to support users specifying hostContext=\"/\" in \n    // solr.xml to indicate the root context, instead of hostContext=\"\" \n    // which means the default of \"solr\"\n    localHostContext = trimLeadingAndTrailingSlashes(localHostContext);\n    \n    updateShardHandler = new UpdateShardHandler(distribUpdateConnTimeout, distribUpdateSoTimeout);\n    \n    this.zkServerAddress = zkServerAddress;\n    this.localHostPort = locaHostPort;\n    this.localHostContext = localHostContext;\n    this.localHost = getHostAddress(localHost);\n    this.baseURL = this.localHost + \":\" + this.localHostPort + \n      (this.localHostContext.isEmpty() ? \"\" : (\"/\" + this.localHostContext));\n\n    this.hostName = getHostNameFromAddress(this.localHost);\n    this.nodeName = generateNodeName(this.hostName, \n                                     this.localHostPort, \n                                     this.localHostContext);\n\n    this.leaderVoteWait = leaderVoteWait;\n    this.clientTimeout = zkClientTimeout;\n    zkClient = new SolrZkClient(zkServerAddress, zkClientTimeout, zkClientConnectTimeout,\n        // on reconnect, reload cloud info\n        new OnReconnect() {\n\n          @Override\n          public void command() {\n            try {\n              markAllAsNotLeader(registerOnReconnect);\n              \n              // this is troublesome - we dont want to kill anything the old leader accepted\n              // though I guess sync will likely get those updates back? But only if\n              // he is involved in the sync, and he certainly may not be\n            //  ExecutorUtil.shutdownAndAwaitTermination(cc.getCmdDistribExecutor());\n              // we need to create all of our lost watches\n              \n              // seems we dont need to do this again...\n              //Overseer.createClientNodes(zkClient, getNodeName());\n              ShardHandler shardHandler;\n              String adminPath;\n              shardHandler = cc.getShardHandlerFactory().getShardHandler();\n              adminPath = cc.getAdminPath();\n\n              cc.cancelCoreRecoveries();\n              \n              registerAllCoresAsDown(registerOnReconnect, false);\n\n              ZkController.this.overseer = new Overseer(shardHandler, adminPath, zkStateReader);\n              ElectionContext context = new OverseerElectionContext(zkClient, overseer, getNodeName());\n              overseerElector.joinElection(context, true);\n              zkStateReader.createClusterStateWatchersAndUpdate();\n              \n              // we have to register as live first to pick up docs in the buffer\n              createEphemeralLiveNode();\n              \n              List<CoreDescriptor> descriptors = registerOnReconnect.getCurrentDescriptors();\n              // re register all descriptors\n              if (descriptors  != null) {\n                for (CoreDescriptor descriptor : descriptors) {\n                  // TODO: we need to think carefully about what happens when it was\n                  // a leader that was expired - as well as what to do about leaders/overseers\n                  // with connection loss\n                  try {\n                    register(descriptor.getName(), descriptor, true, true);\n                  } catch (Throwable t) {\n                    SolrException.log(log, \"Error registering SolrCore\", t);\n                  }\n                }\n              }\n  \n            } catch (InterruptedException e) {\n              // Restore the interrupted status\n              Thread.currentThread().interrupt();\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            }\n\n          }\n\n \n        });\n    \n    this.overseerJobQueue = Overseer.getInQueue(zkClient);\n    this.overseerCollectionQueue = Overseer.getCollectionQueue(zkClient);\n    cmdExecutor = new ZkCmdExecutor(zkClientTimeout);\n    leaderElector = new LeaderElector(zkClient);\n    zkStateReader = new ZkStateReader(zkClient);\n    \n    init(registerOnReconnect);\n  }\n\n","sourceOld":"  public ZkController(final CoreContainer cc, String zkServerAddress, int zkClientTimeout, int zkClientConnectTimeout, String localHost, String locaHostPort,\n      String localHostContext, String leaderVoteWait, boolean genericCoreNodeNames, int distribUpdateConnTimeout, int distribUpdateSoTimeout, final CurrentCoreDescriptorProvider registerOnReconnect) throws InterruptedException,\n      TimeoutException, IOException {\n    if (cc == null) throw new IllegalArgumentException(\"CoreContainer cannot be null.\");\n    this.cc = cc;\n    this.genericCoreNodeNames = genericCoreNodeNames;\n    // be forgiving and strip this off leading/trailing slashes\n    // this allows us to support users specifying hostContext=\"/\" in \n    // solr.xml to indicate the root context, instead of hostContext=\"\" \n    // which means the default of \"solr\"\n    localHostContext = trimLeadingAndTrailingSlashes(localHostContext);\n    \n    updateShardHandler = new UpdateShardHandler(distribUpdateConnTimeout, distribUpdateSoTimeout);\n    \n    this.zkServerAddress = zkServerAddress;\n    this.localHostPort = locaHostPort;\n    this.localHostContext = localHostContext;\n    this.localHost = getHostAddress(localHost);\n    this.baseURL = this.localHost + \":\" + this.localHostPort + \n      (this.localHostContext.isEmpty() ? \"\" : (\"/\" + this.localHostContext));\n\n    this.hostName = getHostNameFromAddress(this.localHost);\n    this.nodeName = generateNodeName(this.hostName, \n                                     this.localHostPort, \n                                     this.localHostContext);\n\n    this.leaderVoteWait = leaderVoteWait;\n    this.clientTimeout = zkClientTimeout;\n    zkClient = new SolrZkClient(zkServerAddress, zkClientTimeout, zkClientConnectTimeout,\n        // on reconnect, reload cloud info\n        new OnReconnect() {\n\n          @Override\n          public void command() {\n            try {\n              markAllAsNotLeader(registerOnReconnect);\n              \n              // this is troublesome - we dont want to kill anything the old leader accepted\n              // though I guess sync will likely get those updates back? But only if\n              // he is involved in the sync, and he certainly may not be\n            //  ExecutorUtil.shutdownAndAwaitTermination(cc.getCmdDistribExecutor());\n              // we need to create all of our lost watches\n              \n              // seems we dont need to do this again...\n              //Overseer.createClientNodes(zkClient, getNodeName());\n              ShardHandler shardHandler;\n              String adminPath;\n              shardHandler = cc.getShardHandlerFactory().getShardHandler();\n              adminPath = cc.getAdminPath();\n\n              cc.cancelCoreRecoveries();\n              \n              registerAllCoresAsDown(registerOnReconnect, false);\n\n              ZkController.this.overseer = new Overseer(shardHandler, adminPath, zkStateReader);\n              ElectionContext context = new OverseerElectionContext(zkClient, overseer, getNodeName());\n              overseerElector.joinElection(context, true);\n              zkStateReader.createClusterStateWatchersAndUpdate();\n              \n              // we have to register as live first to pick up docs in the buffer\n              createEphemeralLiveNode();\n              \n              List<CoreDescriptor> descriptors = registerOnReconnect.getCurrentDescriptors();\n              // re register all descriptors\n              if (descriptors  != null) {\n                for (CoreDescriptor descriptor : descriptors) {\n                  // TODO: we need to think carefully about what happens when it was\n                  // a leader that was expired - as well as what to do about leaders/overseers\n                  // with connection loss\n                  try {\n                    register(descriptor.getName(), descriptor, true, true);\n                  } catch (Throwable t) {\n                    SolrException.log(log, \"Error registering SolrCore\", t);\n                  }\n                }\n              }\n  \n            } catch (InterruptedException e) {\n              // Restore the interrupted status\n              Thread.currentThread().interrupt();\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            }\n\n          }\n\n \n        });\n    \n    this.overseerJobQueue = Overseer.getInQueue(zkClient);\n    this.overseerCollectionQueue = Overseer.getCollectionQueue(zkClient);\n    cmdExecutor = new ZkCmdExecutor(zkClientTimeout);\n    leaderElector = new LeaderElector(zkClient);\n    zkStateReader = new ZkStateReader(zkClient);\n    \n    init(registerOnReconnect);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","date":1376375609,"type":5,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ZkController(CoreContainer,String,int,int,String,String,String,int,boolean,int,int,CurrentCoreDescriptorProvider).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ZkController(CoreContainer,String,int,int,String,String,String,String,boolean,int,int,CurrentCoreDescriptorProvider).mjava","sourceNew":"  public ZkController(final CoreContainer cc, String zkServerAddress, int zkClientTimeout, int zkClientConnectTimeout, String localHost, String locaHostPort,\n      String localHostContext, int leaderVoteWait, boolean genericCoreNodeNames, int distribUpdateConnTimeout, int distribUpdateSoTimeout, final CurrentCoreDescriptorProvider registerOnReconnect) throws InterruptedException,\n      TimeoutException, IOException {\n    if (cc == null) throw new IllegalArgumentException(\"CoreContainer cannot be null.\");\n    this.cc = cc;\n    this.genericCoreNodeNames = genericCoreNodeNames;\n    // be forgiving and strip this off leading/trailing slashes\n    // this allows us to support users specifying hostContext=\"/\" in \n    // solr.xml to indicate the root context, instead of hostContext=\"\" \n    // which means the default of \"solr\"\n    localHostContext = trimLeadingAndTrailingSlashes(localHostContext);\n    \n    updateShardHandler = new UpdateShardHandler(distribUpdateConnTimeout, distribUpdateSoTimeout);\n    \n    this.zkServerAddress = zkServerAddress;\n    this.localHostPort = locaHostPort;\n    this.localHostContext = localHostContext;\n    this.localHost = getHostAddress(localHost);\n    this.baseURL = this.localHost + \":\" + this.localHostPort + \n      (this.localHostContext.isEmpty() ? \"\" : (\"/\" + this.localHostContext));\n\n    this.hostName = getHostNameFromAddress(this.localHost);\n    this.nodeName = generateNodeName(this.hostName, \n                                     this.localHostPort, \n                                     this.localHostContext);\n\n    this.leaderVoteWait = leaderVoteWait;\n    this.clientTimeout = zkClientTimeout;\n    zkClient = new SolrZkClient(zkServerAddress, zkClientTimeout, zkClientConnectTimeout,\n        // on reconnect, reload cloud info\n        new OnReconnect() {\n\n          @Override\n          public void command() {\n            try {\n              markAllAsNotLeader(registerOnReconnect);\n              \n              // this is troublesome - we dont want to kill anything the old leader accepted\n              // though I guess sync will likely get those updates back? But only if\n              // he is involved in the sync, and he certainly may not be\n            //  ExecutorUtil.shutdownAndAwaitTermination(cc.getCmdDistribExecutor());\n              // we need to create all of our lost watches\n              \n              // seems we dont need to do this again...\n              //Overseer.createClientNodes(zkClient, getNodeName());\n              ShardHandler shardHandler;\n              String adminPath;\n              shardHandler = cc.getShardHandlerFactory().getShardHandler();\n              adminPath = cc.getAdminPath();\n\n              cc.cancelCoreRecoveries();\n              \n              registerAllCoresAsDown(registerOnReconnect, false);\n\n              ZkController.this.overseer = new Overseer(shardHandler, adminPath, zkStateReader);\n              ElectionContext context = new OverseerElectionContext(zkClient, overseer, getNodeName());\n              overseerElector.joinElection(context, true);\n              zkStateReader.createClusterStateWatchersAndUpdate();\n              \n              // we have to register as live first to pick up docs in the buffer\n              createEphemeralLiveNode();\n              \n              List<CoreDescriptor> descriptors = registerOnReconnect.getCurrentDescriptors();\n              // re register all descriptors\n              if (descriptors  != null) {\n                for (CoreDescriptor descriptor : descriptors) {\n                  // TODO: we need to think carefully about what happens when it was\n                  // a leader that was expired - as well as what to do about leaders/overseers\n                  // with connection loss\n                  try {\n                    register(descriptor.getName(), descriptor, true, true);\n                  } catch (Throwable t) {\n                    SolrException.log(log, \"Error registering SolrCore\", t);\n                  }\n                }\n              }\n  \n            } catch (InterruptedException e) {\n              // Restore the interrupted status\n              Thread.currentThread().interrupt();\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            }\n\n          }\n\n \n        });\n    \n    this.overseerJobQueue = Overseer.getInQueue(zkClient);\n    this.overseerCollectionQueue = Overseer.getCollectionQueue(zkClient);\n    cmdExecutor = new ZkCmdExecutor(zkClientTimeout);\n    leaderElector = new LeaderElector(zkClient);\n    zkStateReader = new ZkStateReader(zkClient);\n    \n    init(registerOnReconnect);\n  }\n\n","sourceOld":"  public ZkController(final CoreContainer cc, String zkServerAddress, int zkClientTimeout, int zkClientConnectTimeout, String localHost, String locaHostPort,\n      String localHostContext, String leaderVoteWait, boolean genericCoreNodeNames, int distribUpdateConnTimeout, int distribUpdateSoTimeout, final CurrentCoreDescriptorProvider registerOnReconnect) throws InterruptedException,\n      TimeoutException, IOException {\n    if (cc == null) throw new IllegalArgumentException(\"CoreContainer cannot be null.\");\n    this.cc = cc;\n    this.genericCoreNodeNames = genericCoreNodeNames;\n    // be forgiving and strip this off leading/trailing slashes\n    // this allows us to support users specifying hostContext=\"/\" in \n    // solr.xml to indicate the root context, instead of hostContext=\"\" \n    // which means the default of \"solr\"\n    localHostContext = trimLeadingAndTrailingSlashes(localHostContext);\n    \n    updateShardHandler = new UpdateShardHandler(distribUpdateConnTimeout, distribUpdateSoTimeout);\n    \n    this.zkServerAddress = zkServerAddress;\n    this.localHostPort = locaHostPort;\n    this.localHostContext = localHostContext;\n    this.localHost = getHostAddress(localHost);\n    this.baseURL = this.localHost + \":\" + this.localHostPort + \n      (this.localHostContext.isEmpty() ? \"\" : (\"/\" + this.localHostContext));\n\n    this.hostName = getHostNameFromAddress(this.localHost);\n    this.nodeName = generateNodeName(this.hostName, \n                                     this.localHostPort, \n                                     this.localHostContext);\n\n    this.leaderVoteWait = leaderVoteWait;\n    this.clientTimeout = zkClientTimeout;\n    zkClient = new SolrZkClient(zkServerAddress, zkClientTimeout, zkClientConnectTimeout,\n        // on reconnect, reload cloud info\n        new OnReconnect() {\n\n          @Override\n          public void command() {\n            try {\n              markAllAsNotLeader(registerOnReconnect);\n              \n              // this is troublesome - we dont want to kill anything the old leader accepted\n              // though I guess sync will likely get those updates back? But only if\n              // he is involved in the sync, and he certainly may not be\n            //  ExecutorUtil.shutdownAndAwaitTermination(cc.getCmdDistribExecutor());\n              // we need to create all of our lost watches\n              \n              // seems we dont need to do this again...\n              //Overseer.createClientNodes(zkClient, getNodeName());\n              ShardHandler shardHandler;\n              String adminPath;\n              shardHandler = cc.getShardHandlerFactory().getShardHandler();\n              adminPath = cc.getAdminPath();\n\n              cc.cancelCoreRecoveries();\n              \n              registerAllCoresAsDown(registerOnReconnect, false);\n\n              ZkController.this.overseer = new Overseer(shardHandler, adminPath, zkStateReader);\n              ElectionContext context = new OverseerElectionContext(zkClient, overseer, getNodeName());\n              overseerElector.joinElection(context, true);\n              zkStateReader.createClusterStateWatchersAndUpdate();\n              \n              // we have to register as live first to pick up docs in the buffer\n              createEphemeralLiveNode();\n              \n              List<CoreDescriptor> descriptors = registerOnReconnect.getCurrentDescriptors();\n              // re register all descriptors\n              if (descriptors  != null) {\n                for (CoreDescriptor descriptor : descriptors) {\n                  // TODO: we need to think carefully about what happens when it was\n                  // a leader that was expired - as well as what to do about leaders/overseers\n                  // with connection loss\n                  try {\n                    register(descriptor.getName(), descriptor, true, true);\n                  } catch (Throwable t) {\n                    SolrException.log(log, \"Error registering SolrCore\", t);\n                  }\n                }\n              }\n  \n            } catch (InterruptedException e) {\n              // Restore the interrupted status\n              Thread.currentThread().interrupt();\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            }\n\n          }\n\n \n        });\n    \n    this.overseerJobQueue = Overseer.getInQueue(zkClient);\n    this.overseerCollectionQueue = Overseer.getCollectionQueue(zkClient);\n    cmdExecutor = new ZkCmdExecutor(zkClientTimeout);\n    leaderElector = new LeaderElector(zkClient);\n    zkStateReader = new ZkStateReader(zkClient);\n    \n    init(registerOnReconnect);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"849494cf2f3a96af5c8c84995108ddd8456fcd04":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec":["37a0f60745e53927c4c876cfe5b5a58170f0646c","92a4da96826f502cf1a56a096929b37ce73e523a"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","849494cf2f3a96af5c8c84995108ddd8456fcd04"],"92a4da96826f502cf1a56a096929b37ce73e523a":["849494cf2f3a96af5c8c84995108ddd8456fcd04"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["92a4da96826f502cf1a56a096929b37ce73e523a"]},"commit2Childs":{"849494cf2f3a96af5c8c84995108ddd8456fcd04":["37a0f60745e53927c4c876cfe5b5a58170f0646c","92a4da96826f502cf1a56a096929b37ce73e523a"],"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec":[],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["716d18f3a9b0993bc679d7fa7abdc9bfb03411ec"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["849494cf2f3a96af5c8c84995108ddd8456fcd04","37a0f60745e53927c4c876cfe5b5a58170f0646c"],"92a4da96826f502cf1a56a096929b37ce73e523a":["716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}