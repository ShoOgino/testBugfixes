{"path":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","commits":[{"id":"98a8a68e6714cb8742c790308b9f5180d63417d4","date":1272554039,"type":0,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"/dev/null","sourceNew":"    void countTerms() throws IOException {\n      FieldCache.StringIndex si = FieldCache.DEFAULT.getStringIndex(reader, fieldName);\n      final String[] terms = this.terms = si.lookup;\n      final int[] termNum = this.ords = si.order;\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        startTermIndex = Arrays.binarySearch(terms,prefix,nullStrComparator);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n        // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n        // TODO: switch to binarySearch version that takes start/end in Java6\n        endTermIndex = Arrays.binarySearch(terms,prefix+\"\\uffff\\uffff\\uffff\\uffff\",nullStrComparator);\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=terms.length;\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(reader);\n        DocIdSetIterator iter = idSet.iterator();\n\n        if (startTermIndex==0 && endTermIndex==terms.length) {\n          // specialized version when collecting counts for all terms\n          int doc;\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[termNum[doc]]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          int doc;\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = termNum[doc];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"be20f9fed1d3edcb1c84abcc39df87a90fab22df","date":1275590285,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        startTermIndex = si.binarySearchLookup(new BytesRef(prefix), tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n        // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n        // TODO: switch to binarySearch version that takes start/end in Java6\n        endTermIndex = si.binarySearchLookup(new BytesRef(prefix+\"\\uffff\\uffff\\uffff\\uffff\"), tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(reader);\n        DocIdSetIterator iter = idSet.iterator();\n\n        if (startTermIndex==0 && endTermIndex==si.numOrd()) {\n          // specialized version when collecting counts for all terms\n          int doc;\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[si.getOrd(doc)]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          int doc;\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = si.getOrd(doc);\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      FieldCache.StringIndex si = FieldCache.DEFAULT.getStringIndex(reader, fieldName);\n      final String[] terms = this.terms = si.lookup;\n      final int[] termNum = this.ords = si.order;\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        startTermIndex = Arrays.binarySearch(terms,prefix,nullStrComparator);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n        // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n        // TODO: switch to binarySearch version that takes start/end in Java6\n        endTermIndex = Arrays.binarySearch(terms,prefix+\"\\uffff\\uffff\\uffff\\uffff\",nullStrComparator);\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=terms.length;\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(reader);\n        DocIdSetIterator iter = idSet.iterator();\n\n        if (startTermIndex==0 && endTermIndex==terms.length) {\n          // specialized version when collecting counts for all terms\n          int doc;\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[termNum[doc]]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          int doc;\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = termNum[doc];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c51c66468c7c8fd86e3d4162f5be31654d33e54c","date":1276980321,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        startTermIndex = si.binarySearchLookup(new BytesRef(prefix), tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n        // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n        // TODO: switch to binarySearch version that takes start/end in Java6\n        endTermIndex = si.binarySearchLookup(new BytesRef(prefix+\"\\uffff\\uffff\\uffff\\uffff\"), tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(reader);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        startTermIndex = si.binarySearchLookup(new BytesRef(prefix), tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n        // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n        // TODO: switch to binarySearch version that takes start/end in Java6\n        endTermIndex = si.binarySearchLookup(new BytesRef(prefix+\"\\uffff\\uffff\\uffff\\uffff\"), tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(reader);\n        DocIdSetIterator iter = idSet.iterator();\n\n        if (startTermIndex==0 && endTermIndex==si.numOrd()) {\n          // specialized version when collecting counts for all terms\n          int doc;\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[si.getOrd(doc)]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          int doc;\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = si.getOrd(doc);\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        startTermIndex = si.binarySearchLookup(new BytesRef(prefix), tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n        // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n        // TODO: switch to binarySearch version that takes start/end in Java6\n        endTermIndex = si.binarySearchLookup(new BytesRef(prefix+\"\\uffff\\uffff\\uffff\\uffff\"), tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(reader);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        startTermIndex = si.binarySearchLookup(new BytesRef(prefix), tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n        // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n        // TODO: switch to binarySearch version that takes start/end in Java6\n        endTermIndex = si.binarySearchLookup(new BytesRef(prefix+\"\\uffff\\uffff\\uffff\\uffff\"), tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(reader);\n        DocIdSetIterator iter = idSet.iterator();\n\n        if (startTermIndex==0 && endTermIndex==si.numOrd()) {\n          // specialized version when collecting counts for all terms\n          int doc;\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[si.getOrd(doc)]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          int doc;\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = si.getOrd(doc);\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"627ce218a5a68018115c2deb6559b41e3665b8ab","date":1284500689,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(ByteUtils.bigTerm);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(reader);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        startTermIndex = si.binarySearchLookup(new BytesRef(prefix), tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n        // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n        // TODO: switch to binarySearch version that takes start/end in Java6\n        endTermIndex = si.binarySearchLookup(new BytesRef(prefix+\"\\uffff\\uffff\\uffff\\uffff\"), tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(reader);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50","7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50","7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(ByteUtils.bigTerm);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(reader);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        startTermIndex = si.binarySearchLookup(new BytesRef(prefix), tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n        // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n        // TODO: switch to binarySearch version that takes start/end in Java6\n        endTermIndex = si.binarySearchLookup(new BytesRef(prefix+\"\\uffff\\uffff\\uffff\\uffff\"), tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(reader);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2dadf0f3286a34a0fee6e788ffce88624bf2984e","date":1294260428,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(info.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(ByteUtils.bigTerm);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(info);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(ByteUtils.bigTerm);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(reader);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a10b98ef1ef4bf9e38d2e07a9e425a916afa8705","date":1294747166,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(ByteUtils.bigTerm);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(info.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(ByteUtils.bigTerm);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(info);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(ByteUtils.bigTerm);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(ByteUtils.bigTerm);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(reader);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(ByteUtils.bigTerm);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(ByteUtils.bigTerm);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(reader);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a1b3a24d5d9b47345473ff564f5cc127a7b526b4","date":1306277076,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(ByteUtils.bigTerm);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","date":1306767085,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(ByteUtils.bigTerm);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2e10cb22a8bdb44339e282925a29182bb2f3174d","date":1306841137,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(ByteUtils.bigTerm);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":5,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"627ce218a5a68018115c2deb6559b41e3665b8ab":["c51c66468c7c8fd86e3d4162f5be31654d33e54c"],"be20f9fed1d3edcb1c84abcc39df87a90fab22df":["98a8a68e6714cb8742c790308b9f5180d63417d4"],"c51c66468c7c8fd86e3d4162f5be31654d33e54c":["be20f9fed1d3edcb1c84abcc39df87a90fab22df"],"c26f00b574427b55127e869b935845554afde1fa":["a1b3a24d5d9b47345473ff564f5cc127a7b526b4","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"5f4e87790277826a2aea119328600dfb07761f32":["be20f9fed1d3edcb1c84abcc39df87a90fab22df","c51c66468c7c8fd86e3d4162f5be31654d33e54c"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["5f4e87790277826a2aea119328600dfb07761f32","627ce218a5a68018115c2deb6559b41e3665b8ab"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["a10b98ef1ef4bf9e38d2e07a9e425a916afa8705","a1b3a24d5d9b47345473ff564f5cc127a7b526b4"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["627ce218a5a68018115c2deb6559b41e3665b8ab","a10b98ef1ef4bf9e38d2e07a9e425a916afa8705"],"2dadf0f3286a34a0fee6e788ffce88624bf2984e":["627ce218a5a68018115c2deb6559b41e3665b8ab"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"98a8a68e6714cb8742c790308b9f5180d63417d4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["a1b3a24d5d9b47345473ff564f5cc127a7b526b4"],"a1b3a24d5d9b47345473ff564f5cc127a7b526b4":["a10b98ef1ef4bf9e38d2e07a9e425a916afa8705"],"a10b98ef1ef4bf9e38d2e07a9e425a916afa8705":["2dadf0f3286a34a0fee6e788ffce88624bf2984e"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","a10b98ef1ef4bf9e38d2e07a9e425a916afa8705"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c26f00b574427b55127e869b935845554afde1fa"],"2e10cb22a8bdb44339e282925a29182bb2f3174d":["29ef99d61cda9641b6250bf9567329a6e65f901d","a1b3a24d5d9b47345473ff564f5cc127a7b526b4"]},"commit2Childs":{"627ce218a5a68018115c2deb6559b41e3665b8ab":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","29ef99d61cda9641b6250bf9567329a6e65f901d","2dadf0f3286a34a0fee6e788ffce88624bf2984e"],"be20f9fed1d3edcb1c84abcc39df87a90fab22df":["c51c66468c7c8fd86e3d4162f5be31654d33e54c","5f4e87790277826a2aea119328600dfb07761f32"],"c51c66468c7c8fd86e3d4162f5be31654d33e54c":["627ce218a5a68018115c2deb6559b41e3665b8ab","5f4e87790277826a2aea119328600dfb07761f32"],"c26f00b574427b55127e869b935845554afde1fa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5f4e87790277826a2aea119328600dfb07761f32":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["868da859b43505d9d2a023bfeae6dd0c795f5295"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["2e10cb22a8bdb44339e282925a29182bb2f3174d"],"2dadf0f3286a34a0fee6e788ffce88624bf2984e":["a10b98ef1ef4bf9e38d2e07a9e425a916afa8705"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["98a8a68e6714cb8742c790308b9f5180d63417d4"],"98a8a68e6714cb8742c790308b9f5180d63417d4":["be20f9fed1d3edcb1c84abcc39df87a90fab22df"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"a1b3a24d5d9b47345473ff564f5cc127a7b526b4":["c26f00b574427b55127e869b935845554afde1fa","5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","a258fbb26824fd104ed795e5d9033d2d040049ee","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"a10b98ef1ef4bf9e38d2e07a9e425a916afa8705":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","29ef99d61cda9641b6250bf9567329a6e65f901d","a1b3a24d5d9b47345473ff564f5cc127a7b526b4","868da859b43505d9d2a023bfeae6dd0c795f5295"],"868da859b43505d9d2a023bfeae6dd0c795f5295":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"2e10cb22a8bdb44339e282925a29182bb2f3174d":[]},"heads":["a258fbb26824fd104ed795e5d9033d2d040049ee","868da859b43505d9d2a023bfeae6dd0c795f5295","cd5edd1f2b162a5cfa08efd17851a07373a96817","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}