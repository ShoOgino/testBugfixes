{"path":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextBKDWriter#build(int,int,BKDRadixSelector.PathSlice,IndexOutput,BKDRadixSelector,byte[],byte[],byte[],long[]).mjava","commits":[{"id":"a69ebf290ab26d026cc224e517e0d93d931ac87b","date":1549869083,"type":1,"author":"iverase","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextBKDWriter#build(int,int,BKDRadixSelector.PathSlice,IndexOutput,BKDRadixSelector,byte[],byte[],byte[],long[]).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextBKDWriter#build(int,int,PointWriter,IndexOutput,BKDRadixSelector,byte[],byte[],byte[],long[]).mjava","sourceNew":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to. */\n  private void build(int nodeID, int leafNodeOffset,\n                     BKDRadixSelector.PathSlice points,\n                     IndexOutput out,\n                     BKDRadixSelector radixSelector,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs) throws IOException {\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n\n      HeapPointWriter heapSource;\n      if (points.writer instanceof HeapPointWriter == false) {\n        // Adversarial cases can cause this, e.g. very lopsided data, all equal points, such that we started\n        // offline, but then kept splitting only in one dimension, and so never had to rewrite into heap writer\n        heapSource  = switchToHeap(points.writer);\n      } else {\n        heapSource = (HeapPointWriter) points.writer;\n      }\n\n      int from = Math.toIntExact(points.start);\n      int to = Math.toIntExact(points.start + points.count);\n\n      //we store common prefix on scratch1\n      computeCommonPrefixLength(heapSource, scratch1);\n\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n      FixedBitSet[] usedBytes = new FixedBitSet[numDataDims];\n      for (int dim = 0; dim < numDataDims; ++dim) {\n        if (commonPrefixLengths[dim] < bytesPerDim) {\n          usedBytes[dim] = new FixedBitSet(256);\n        }\n      }\n      //Find the dimension to compress\n      for (int dim = 0; dim < numDataDims; dim++) {\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int offset = dim * bytesPerDim;\n          for (int i = 0; i < heapSource.count(); ++i) {\n            heapSource.getPackedValueSlice(i, scratchBytesRef1);\n            int bucket = scratchBytesRef1.bytes[scratchBytesRef1.offset + offset + prefix] & 0xff;\n            usedBytes[dim].set(bucket);\n          }\n          int cardinality = usedBytes[dim].cardinality();\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      // sort the chosen dimension\n      sortHeapPointWriter(heapSource, from, to, sortedDim, commonPrefixLengths[sortedDim]);\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = to - from;\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, from, count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          heapSource.getPackedValueSlice(from + i, scratch);\n          return scratch;\n        }\n      };\n      assert valuesInOrderAndBounds(count, sortedDim, minPackedValue, maxPackedValue, packedValues,\n          heapSource.docIDs, from);\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numIndexDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      assert nodeID < splitPackedValues.length : \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = points.count / 2;\n      long leftCount = points.count - rightCount;\n\n      int commonPrefixLen = FutureArrays.mismatch(minPackedValue, splitDim * bytesPerDim,\n          splitDim * bytesPerDim + bytesPerDim, maxPackedValue, splitDim * bytesPerDim,\n          splitDim * bytesPerDim + bytesPerDim);\n      if (commonPrefixLen == -1) {\n        commonPrefixLen = bytesPerDim;\n      }\n\n      BKDRadixSelector.PathSlice[] pathSlices = new BKDRadixSelector.PathSlice[2];\n\n      byte[] splitValue =  radixSelector.select(points, pathSlices, points.start, points.start + points.count,  points.start + leftCount, splitDim, commonPrefixLen);\n\n      int address = nodeID * (1 + bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      byte[] minSplitPackedValue = new byte[packedIndexBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedIndexBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedIndexBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedIndexBytesLength);\n\n      System.arraycopy(splitValue, 0, minSplitPackedValue, splitDim * bytesPerDim, bytesPerDim);\n      System.arraycopy(splitValue, 0, maxSplitPackedValue, splitDim * bytesPerDim, bytesPerDim);\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, pathSlices[0], out, radixSelector,\n            minPackedValue, maxSplitPackedValue, splitPackedValues, leafBlockFPs);\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, pathSlices[1], out, radixSelector,\n            minSplitPackedValue, maxPackedValue, splitPackedValues, leafBlockFPs);\n    }\n  }\n\n","sourceOld":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to. */\n  private void build(int nodeID, int leafNodeOffset,\n                     PointWriter data,\n                     IndexOutput out,\n                     BKDRadixSelector radixSelector,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs) throws IOException {\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n\n      if (data instanceof HeapPointWriter == false) {\n        // Adversarial cases can cause this, e.g. very lopsided data, all equal points, such that we started\n        // offline, but then kept splitting only in one dimension, and so never had to rewrite into heap writer\n        data = switchToHeap(data);\n      }\n\n      // We ensured that maxPointsSortInHeap was >= maxPointsInLeafNode, so we better be in heap at this point:\n      HeapPointWriter heapSource = (HeapPointWriter) data;\n\n      //we store common prefix on scratch1\n      computeCommonPrefixLength(heapSource, scratch1);\n\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n      FixedBitSet[] usedBytes = new FixedBitSet[numDataDims];\n      for (int dim = 0; dim < numDataDims; ++dim) {\n        if (commonPrefixLengths[dim] < bytesPerDim) {\n          usedBytes[dim] = new FixedBitSet(256);\n        }\n      }\n      //Find the dimension to compress\n      for (int dim = 0; dim < numDataDims; dim++) {\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int offset = dim * bytesPerDim;\n          for (int i = 0; i < heapSource.count(); ++i) {\n            heapSource.getPackedValueSlice(i, scratchBytesRef1);\n            int bucket = scratchBytesRef1.bytes[scratchBytesRef1.offset + offset + prefix] & 0xff;\n            usedBytes[dim].set(bucket);\n          }\n          int cardinality = usedBytes[dim].cardinality();\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      sortHeapPointWriter(heapSource, sortedDim);\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = Math.toIntExact(heapSource.count());\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, 0, count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          heapSource.getPackedValueSlice(i, scratch);\n          return scratch;\n        }\n      };\n      assert valuesInOrderAndBounds(count, sortedDim, minPackedValue, maxPackedValue, packedValues,\n          heapSource.docIDs, Math.toIntExact(0));\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numIndexDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n\n      assert nodeID < splitPackedValues.length : \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = data.count() / 2;\n      long leftCount = data.count() - rightCount;\n\n      PointWriter leftPointWriter;\n      PointWriter rightPointWriter;\n      byte[] splitValue;\n\n      try (PointWriter leftPointWriter2 = getPointWriter(leftCount, \"left\" + splitDim);\n           PointWriter rightPointWriter2 = getPointWriter(rightCount, \"right\" + splitDim)) {\n        splitValue = radixSelector.select(data, leftPointWriter2, rightPointWriter2, 0, data.count(),  leftCount, splitDim);\n        leftPointWriter = leftPointWriter2;\n        rightPointWriter = rightPointWriter2;\n      } catch (Throwable t) {\n        throw verifyChecksum(t, data);\n      }\n\n      int address = nodeID * (1 + bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      byte[] minSplitPackedValue = new byte[packedIndexBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedIndexBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedIndexBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedIndexBytesLength);\n\n      System.arraycopy(splitValue, 0, minSplitPackedValue, splitDim * bytesPerDim, bytesPerDim);\n      System.arraycopy(splitValue, 0, maxSplitPackedValue, splitDim * bytesPerDim, bytesPerDim);\n\n\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, leftPointWriter, out, radixSelector,\n            minPackedValue, maxSplitPackedValue, splitPackedValues, leafBlockFPs);\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, rightPointWriter, out, radixSelector,\n            minSplitPackedValue, maxPackedValue, splitPackedValues, leafBlockFPs);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c2344a1c769566d8c85cffcacc5e55153fa54b86","date":1550661298,"type":3,"author":"iverase","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextBKDWriter#build(int,int,BKDRadixSelector.PathSlice,IndexOutput,BKDRadixSelector,byte[],byte[],byte[],long[]).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextBKDWriter#build(int,int,BKDRadixSelector.PathSlice,IndexOutput,BKDRadixSelector,byte[],byte[],byte[],long[]).mjava","sourceNew":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to. */\n  private void build(int nodeID, int leafNodeOffset,\n                     BKDRadixSelector.PathSlice points,\n                     IndexOutput out,\n                     BKDRadixSelector radixSelector,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs) throws IOException {\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n\n      HeapPointWriter heapSource;\n      if (points.writer instanceof HeapPointWriter == false) {\n        // Adversarial cases can cause this, e.g. very lopsided data, all equal points, such that we started\n        // offline, but then kept splitting only in one dimension, and so never had to rewrite into heap writer\n        heapSource  = switchToHeap(points.writer);\n      } else {\n        heapSource = (HeapPointWriter) points.writer;\n      }\n\n      int from = Math.toIntExact(points.start);\n      int to = Math.toIntExact(points.start + points.count);\n\n      //we store common prefix on scratch1\n      computeCommonPrefixLength(heapSource, scratch1);\n\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n      FixedBitSet[] usedBytes = new FixedBitSet[numDataDims];\n      for (int dim = 0; dim < numDataDims; ++dim) {\n        if (commonPrefixLengths[dim] < bytesPerDim) {\n          usedBytes[dim] = new FixedBitSet(256);\n        }\n      }\n      //Find the dimension to compress\n      for (int dim = 0; dim < numDataDims; dim++) {\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int offset = dim * bytesPerDim;\n          for (int i = 0; i < heapSource.count(); ++i) {\n            PointValue value = heapSource.getPackedValueSlice(i);\n            BytesRef packedValue = value.packedValue();\n            int bucket = packedValue.bytes[packedValue.offset + offset + prefix] & 0xff;\n            usedBytes[dim].set(bucket);\n          }\n          int cardinality = usedBytes[dim].cardinality();\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      // sort the chosen dimension\n      radixSelector.heapRadixSort(heapSource, from, to, sortedDim, commonPrefixLengths[sortedDim]);\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = to - from;\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, from, count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          PointValue value = heapSource.getPackedValueSlice(from + i);\n          return value.packedValue();\n        }\n      };\n      assert valuesInOrderAndBounds(count, sortedDim, minPackedValue, maxPackedValue, packedValues,\n          heapSource.docIDs, from);\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numIndexDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      assert nodeID < splitPackedValues.length : \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = points.count / 2;\n      long leftCount = points.count - rightCount;\n\n      int commonPrefixLen = FutureArrays.mismatch(minPackedValue, splitDim * bytesPerDim,\n          splitDim * bytesPerDim + bytesPerDim, maxPackedValue, splitDim * bytesPerDim,\n          splitDim * bytesPerDim + bytesPerDim);\n      if (commonPrefixLen == -1) {\n        commonPrefixLen = bytesPerDim;\n      }\n\n      BKDRadixSelector.PathSlice[] pathSlices = new BKDRadixSelector.PathSlice[2];\n\n      byte[] splitValue =  radixSelector.select(points, pathSlices, points.start, points.start + points.count,  points.start + leftCount, splitDim, commonPrefixLen);\n\n      int address = nodeID * (1 + bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      byte[] minSplitPackedValue = new byte[packedIndexBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedIndexBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedIndexBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedIndexBytesLength);\n\n      System.arraycopy(splitValue, 0, minSplitPackedValue, splitDim * bytesPerDim, bytesPerDim);\n      System.arraycopy(splitValue, 0, maxSplitPackedValue, splitDim * bytesPerDim, bytesPerDim);\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, pathSlices[0], out, radixSelector,\n            minPackedValue, maxSplitPackedValue, splitPackedValues, leafBlockFPs);\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, pathSlices[1], out, radixSelector,\n            minSplitPackedValue, maxPackedValue, splitPackedValues, leafBlockFPs);\n    }\n  }\n\n","sourceOld":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to. */\n  private void build(int nodeID, int leafNodeOffset,\n                     BKDRadixSelector.PathSlice points,\n                     IndexOutput out,\n                     BKDRadixSelector radixSelector,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs) throws IOException {\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n\n      HeapPointWriter heapSource;\n      if (points.writer instanceof HeapPointWriter == false) {\n        // Adversarial cases can cause this, e.g. very lopsided data, all equal points, such that we started\n        // offline, but then kept splitting only in one dimension, and so never had to rewrite into heap writer\n        heapSource  = switchToHeap(points.writer);\n      } else {\n        heapSource = (HeapPointWriter) points.writer;\n      }\n\n      int from = Math.toIntExact(points.start);\n      int to = Math.toIntExact(points.start + points.count);\n\n      //we store common prefix on scratch1\n      computeCommonPrefixLength(heapSource, scratch1);\n\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n      FixedBitSet[] usedBytes = new FixedBitSet[numDataDims];\n      for (int dim = 0; dim < numDataDims; ++dim) {\n        if (commonPrefixLengths[dim] < bytesPerDim) {\n          usedBytes[dim] = new FixedBitSet(256);\n        }\n      }\n      //Find the dimension to compress\n      for (int dim = 0; dim < numDataDims; dim++) {\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int offset = dim * bytesPerDim;\n          for (int i = 0; i < heapSource.count(); ++i) {\n            heapSource.getPackedValueSlice(i, scratchBytesRef1);\n            int bucket = scratchBytesRef1.bytes[scratchBytesRef1.offset + offset + prefix] & 0xff;\n            usedBytes[dim].set(bucket);\n          }\n          int cardinality = usedBytes[dim].cardinality();\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      // sort the chosen dimension\n      sortHeapPointWriter(heapSource, from, to, sortedDim, commonPrefixLengths[sortedDim]);\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = to - from;\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, from, count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          heapSource.getPackedValueSlice(from + i, scratch);\n          return scratch;\n        }\n      };\n      assert valuesInOrderAndBounds(count, sortedDim, minPackedValue, maxPackedValue, packedValues,\n          heapSource.docIDs, from);\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numIndexDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      assert nodeID < splitPackedValues.length : \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = points.count / 2;\n      long leftCount = points.count - rightCount;\n\n      int commonPrefixLen = FutureArrays.mismatch(minPackedValue, splitDim * bytesPerDim,\n          splitDim * bytesPerDim + bytesPerDim, maxPackedValue, splitDim * bytesPerDim,\n          splitDim * bytesPerDim + bytesPerDim);\n      if (commonPrefixLen == -1) {\n        commonPrefixLen = bytesPerDim;\n      }\n\n      BKDRadixSelector.PathSlice[] pathSlices = new BKDRadixSelector.PathSlice[2];\n\n      byte[] splitValue =  radixSelector.select(points, pathSlices, points.start, points.start + points.count,  points.start + leftCount, splitDim, commonPrefixLen);\n\n      int address = nodeID * (1 + bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      byte[] minSplitPackedValue = new byte[packedIndexBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedIndexBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedIndexBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedIndexBytesLength);\n\n      System.arraycopy(splitValue, 0, minSplitPackedValue, splitDim * bytesPerDim, bytesPerDim);\n      System.arraycopy(splitValue, 0, maxSplitPackedValue, splitDim * bytesPerDim, bytesPerDim);\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, pathSlices[0], out, radixSelector,\n            minPackedValue, maxSplitPackedValue, splitPackedValues, leafBlockFPs);\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, pathSlices[1], out, radixSelector,\n            minSplitPackedValue, maxPackedValue, splitPackedValues, leafBlockFPs);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"76a51551f05a6c96a115b5a656837ecc8fd0b1ff","date":1551422476,"type":3,"author":"iverase","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextBKDWriter#build(int,int,BKDRadixSelector.PathSlice,IndexOutput,BKDRadixSelector,byte[],byte[],byte[],long[]).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextBKDWriter#build(int,int,BKDRadixSelector.PathSlice,IndexOutput,BKDRadixSelector,byte[],byte[],byte[],long[]).mjava","sourceNew":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to. */\n  private void build(int nodeID, int leafNodeOffset,\n                     BKDRadixSelector.PathSlice points,\n                     IndexOutput out,\n                     BKDRadixSelector radixSelector,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs) throws IOException {\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n      HeapPointWriter heapSource;\n      if (points.writer instanceof HeapPointWriter == false) {\n        // Adversarial cases can cause this, e.g. merging big segments with most of the points deleted\n        heapSource  = switchToHeap(points.writer);\n      } else {\n        heapSource = (HeapPointWriter) points.writer;\n      }\n\n      int from = Math.toIntExact(points.start);\n      int to = Math.toIntExact(points.start + points.count);\n\n      //we store common prefix on scratch1\n      computeCommonPrefixLength(heapSource, scratch1);\n\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n      FixedBitSet[] usedBytes = new FixedBitSet[numDataDims];\n      for (int dim = 0; dim < numDataDims; ++dim) {\n        if (commonPrefixLengths[dim] < bytesPerDim) {\n          usedBytes[dim] = new FixedBitSet(256);\n        }\n      }\n      //Find the dimension to compress\n      for (int dim = 0; dim < numDataDims; dim++) {\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int offset = dim * bytesPerDim;\n          for (int i = 0; i < heapSource.count(); ++i) {\n            PointValue value = heapSource.getPackedValueSlice(i);\n            BytesRef packedValue = value.packedValue();\n            int bucket = packedValue.bytes[packedValue.offset + offset + prefix] & 0xff;\n            usedBytes[dim].set(bucket);\n          }\n          int cardinality = usedBytes[dim].cardinality();\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      // sort the chosen dimension\n      radixSelector.heapRadixSort(heapSource, from, to, sortedDim, commonPrefixLengths[sortedDim]);\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = to - from;\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, from, count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          PointValue value = heapSource.getPackedValueSlice(from + i);\n          return value.packedValue();\n        }\n      };\n      assert valuesInOrderAndBounds(count, sortedDim, minPackedValue, maxPackedValue, packedValues,\n          heapSource.docIDs, from);\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numIndexDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      assert nodeID < splitPackedValues.length : \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = points.count / 2;\n      long leftCount = points.count - rightCount;\n\n      int commonPrefixLen = FutureArrays.mismatch(minPackedValue, splitDim * bytesPerDim,\n          splitDim * bytesPerDim + bytesPerDim, maxPackedValue, splitDim * bytesPerDim,\n          splitDim * bytesPerDim + bytesPerDim);\n      if (commonPrefixLen == -1) {\n        commonPrefixLen = bytesPerDim;\n      }\n\n      BKDRadixSelector.PathSlice[] pathSlices = new BKDRadixSelector.PathSlice[2];\n\n      byte[] splitValue =  radixSelector.select(points, pathSlices, points.start, points.start + points.count,  points.start + leftCount, splitDim, commonPrefixLen);\n\n      int address = nodeID * (1 + bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      byte[] minSplitPackedValue = new byte[packedIndexBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedIndexBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedIndexBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedIndexBytesLength);\n\n      System.arraycopy(splitValue, 0, minSplitPackedValue, splitDim * bytesPerDim, bytesPerDim);\n      System.arraycopy(splitValue, 0, maxSplitPackedValue, splitDim * bytesPerDim, bytesPerDim);\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, pathSlices[0], out, radixSelector,\n            minPackedValue, maxSplitPackedValue, splitPackedValues, leafBlockFPs);\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, pathSlices[1], out, radixSelector,\n            minSplitPackedValue, maxPackedValue, splitPackedValues, leafBlockFPs);\n    }\n  }\n\n","sourceOld":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to. */\n  private void build(int nodeID, int leafNodeOffset,\n                     BKDRadixSelector.PathSlice points,\n                     IndexOutput out,\n                     BKDRadixSelector radixSelector,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs) throws IOException {\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n\n      HeapPointWriter heapSource;\n      if (points.writer instanceof HeapPointWriter == false) {\n        // Adversarial cases can cause this, e.g. very lopsided data, all equal points, such that we started\n        // offline, but then kept splitting only in one dimension, and so never had to rewrite into heap writer\n        heapSource  = switchToHeap(points.writer);\n      } else {\n        heapSource = (HeapPointWriter) points.writer;\n      }\n\n      int from = Math.toIntExact(points.start);\n      int to = Math.toIntExact(points.start + points.count);\n\n      //we store common prefix on scratch1\n      computeCommonPrefixLength(heapSource, scratch1);\n\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n      FixedBitSet[] usedBytes = new FixedBitSet[numDataDims];\n      for (int dim = 0; dim < numDataDims; ++dim) {\n        if (commonPrefixLengths[dim] < bytesPerDim) {\n          usedBytes[dim] = new FixedBitSet(256);\n        }\n      }\n      //Find the dimension to compress\n      for (int dim = 0; dim < numDataDims; dim++) {\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int offset = dim * bytesPerDim;\n          for (int i = 0; i < heapSource.count(); ++i) {\n            PointValue value = heapSource.getPackedValueSlice(i);\n            BytesRef packedValue = value.packedValue();\n            int bucket = packedValue.bytes[packedValue.offset + offset + prefix] & 0xff;\n            usedBytes[dim].set(bucket);\n          }\n          int cardinality = usedBytes[dim].cardinality();\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      // sort the chosen dimension\n      radixSelector.heapRadixSort(heapSource, from, to, sortedDim, commonPrefixLengths[sortedDim]);\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = to - from;\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, from, count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          PointValue value = heapSource.getPackedValueSlice(from + i);\n          return value.packedValue();\n        }\n      };\n      assert valuesInOrderAndBounds(count, sortedDim, minPackedValue, maxPackedValue, packedValues,\n          heapSource.docIDs, from);\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numIndexDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      assert nodeID < splitPackedValues.length : \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = points.count / 2;\n      long leftCount = points.count - rightCount;\n\n      int commonPrefixLen = FutureArrays.mismatch(minPackedValue, splitDim * bytesPerDim,\n          splitDim * bytesPerDim + bytesPerDim, maxPackedValue, splitDim * bytesPerDim,\n          splitDim * bytesPerDim + bytesPerDim);\n      if (commonPrefixLen == -1) {\n        commonPrefixLen = bytesPerDim;\n      }\n\n      BKDRadixSelector.PathSlice[] pathSlices = new BKDRadixSelector.PathSlice[2];\n\n      byte[] splitValue =  radixSelector.select(points, pathSlices, points.start, points.start + points.count,  points.start + leftCount, splitDim, commonPrefixLen);\n\n      int address = nodeID * (1 + bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      byte[] minSplitPackedValue = new byte[packedIndexBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedIndexBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedIndexBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedIndexBytesLength);\n\n      System.arraycopy(splitValue, 0, minSplitPackedValue, splitDim * bytesPerDim, bytesPerDim);\n      System.arraycopy(splitValue, 0, maxSplitPackedValue, splitDim * bytesPerDim, bytesPerDim);\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, pathSlices[0], out, radixSelector,\n            minPackedValue, maxSplitPackedValue, splitPackedValues, leafBlockFPs);\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, pathSlices[1], out, radixSelector,\n            minSplitPackedValue, maxPackedValue, splitPackedValues, leafBlockFPs);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b88a121b875f9ae2ac50f85cf46dcb680f126357","date":1555416009,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextBKDWriter#build(int,int,BKDRadixSelector.PathSlice,IndexOutput,BKDRadixSelector,byte[],byte[],byte[],long[]).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextBKDWriter#build(int,int,BKDRadixSelector.PathSlice,IndexOutput,BKDRadixSelector,byte[],byte[],byte[],long[]).mjava","sourceNew":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to. */\n  private void build(int nodeID, int leafNodeOffset,\n                     BKDRadixSelector.PathSlice points,\n                     IndexOutput out,\n                     BKDRadixSelector radixSelector,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs) throws IOException {\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n      HeapPointWriter heapSource;\n      if (points.writer instanceof HeapPointWriter == false) {\n        // Adversarial cases can cause this, e.g. merging big segments with most of the points deleted\n        heapSource  = switchToHeap(points.writer);\n      } else {\n        heapSource = (HeapPointWriter) points.writer;\n      }\n\n      int from = Math.toIntExact(points.start);\n      int to = Math.toIntExact(points.start + points.count);\n\n      //we store common prefix on scratch1\n      computeCommonPrefixLength(heapSource, scratch1);\n\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n      FixedBitSet[] usedBytes = new FixedBitSet[numDataDims];\n      for (int dim = 0; dim < numDataDims; ++dim) {\n        if (commonPrefixLengths[dim] < bytesPerDim) {\n          usedBytes[dim] = new FixedBitSet(256);\n        }\n      }\n      //Find the dimension to compress\n      for (int dim = 0; dim < numDataDims; dim++) {\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int offset = dim * bytesPerDim;\n          for (int i = 0; i < heapSource.count(); ++i) {\n            PointValue value = heapSource.getPackedValueSlice(i);\n            BytesRef packedValue = value.packedValue();\n            int bucket = packedValue.bytes[packedValue.offset + offset + prefix] & 0xff;\n            usedBytes[dim].set(bucket);\n          }\n          int cardinality = usedBytes[dim].cardinality();\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      // sort the chosen dimension\n      radixSelector.heapRadixSort(heapSource, from, to, sortedDim, commonPrefixLengths[sortedDim]);\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = to - from;\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, from, count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          PointValue value = heapSource.getPackedValueSlice(from + i);\n          return value.packedValue();\n        }\n      };\n      assert valuesInOrderAndBounds(count, sortedDim, minPackedValue, maxPackedValue, packedValues,\n          heapSource.docIDs, from);\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numIndexDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      assert nodeID < splitPackedValues.length : \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = points.count / 2;\n      long leftCount = points.count - rightCount;\n\n      int commonPrefixLen = Arrays.mismatch(minPackedValue, splitDim * bytesPerDim,\n          splitDim * bytesPerDim + bytesPerDim, maxPackedValue, splitDim * bytesPerDim,\n          splitDim * bytesPerDim + bytesPerDim);\n      if (commonPrefixLen == -1) {\n        commonPrefixLen = bytesPerDim;\n      }\n\n      BKDRadixSelector.PathSlice[] pathSlices = new BKDRadixSelector.PathSlice[2];\n\n      byte[] splitValue =  radixSelector.select(points, pathSlices, points.start, points.start + points.count,  points.start + leftCount, splitDim, commonPrefixLen);\n\n      int address = nodeID * (1 + bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      byte[] minSplitPackedValue = new byte[packedIndexBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedIndexBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedIndexBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedIndexBytesLength);\n\n      System.arraycopy(splitValue, 0, minSplitPackedValue, splitDim * bytesPerDim, bytesPerDim);\n      System.arraycopy(splitValue, 0, maxSplitPackedValue, splitDim * bytesPerDim, bytesPerDim);\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, pathSlices[0], out, radixSelector,\n            minPackedValue, maxSplitPackedValue, splitPackedValues, leafBlockFPs);\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, pathSlices[1], out, radixSelector,\n            minSplitPackedValue, maxPackedValue, splitPackedValues, leafBlockFPs);\n    }\n  }\n\n","sourceOld":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to. */\n  private void build(int nodeID, int leafNodeOffset,\n                     BKDRadixSelector.PathSlice points,\n                     IndexOutput out,\n                     BKDRadixSelector radixSelector,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs) throws IOException {\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n      HeapPointWriter heapSource;\n      if (points.writer instanceof HeapPointWriter == false) {\n        // Adversarial cases can cause this, e.g. merging big segments with most of the points deleted\n        heapSource  = switchToHeap(points.writer);\n      } else {\n        heapSource = (HeapPointWriter) points.writer;\n      }\n\n      int from = Math.toIntExact(points.start);\n      int to = Math.toIntExact(points.start + points.count);\n\n      //we store common prefix on scratch1\n      computeCommonPrefixLength(heapSource, scratch1);\n\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n      FixedBitSet[] usedBytes = new FixedBitSet[numDataDims];\n      for (int dim = 0; dim < numDataDims; ++dim) {\n        if (commonPrefixLengths[dim] < bytesPerDim) {\n          usedBytes[dim] = new FixedBitSet(256);\n        }\n      }\n      //Find the dimension to compress\n      for (int dim = 0; dim < numDataDims; dim++) {\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int offset = dim * bytesPerDim;\n          for (int i = 0; i < heapSource.count(); ++i) {\n            PointValue value = heapSource.getPackedValueSlice(i);\n            BytesRef packedValue = value.packedValue();\n            int bucket = packedValue.bytes[packedValue.offset + offset + prefix] & 0xff;\n            usedBytes[dim].set(bucket);\n          }\n          int cardinality = usedBytes[dim].cardinality();\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      // sort the chosen dimension\n      radixSelector.heapRadixSort(heapSource, from, to, sortedDim, commonPrefixLengths[sortedDim]);\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = to - from;\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, from, count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          PointValue value = heapSource.getPackedValueSlice(from + i);\n          return value.packedValue();\n        }\n      };\n      assert valuesInOrderAndBounds(count, sortedDim, minPackedValue, maxPackedValue, packedValues,\n          heapSource.docIDs, from);\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numIndexDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      assert nodeID < splitPackedValues.length : \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = points.count / 2;\n      long leftCount = points.count - rightCount;\n\n      int commonPrefixLen = FutureArrays.mismatch(minPackedValue, splitDim * bytesPerDim,\n          splitDim * bytesPerDim + bytesPerDim, maxPackedValue, splitDim * bytesPerDim,\n          splitDim * bytesPerDim + bytesPerDim);\n      if (commonPrefixLen == -1) {\n        commonPrefixLen = bytesPerDim;\n      }\n\n      BKDRadixSelector.PathSlice[] pathSlices = new BKDRadixSelector.PathSlice[2];\n\n      byte[] splitValue =  radixSelector.select(points, pathSlices, points.start, points.start + points.count,  points.start + leftCount, splitDim, commonPrefixLen);\n\n      int address = nodeID * (1 + bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      byte[] minSplitPackedValue = new byte[packedIndexBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedIndexBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedIndexBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedIndexBytesLength);\n\n      System.arraycopy(splitValue, 0, minSplitPackedValue, splitDim * bytesPerDim, bytesPerDim);\n      System.arraycopy(splitValue, 0, maxSplitPackedValue, splitDim * bytesPerDim, bytesPerDim);\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, pathSlices[0], out, radixSelector,\n            minPackedValue, maxSplitPackedValue, splitPackedValues, leafBlockFPs);\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, pathSlices[1], out, radixSelector,\n            minSplitPackedValue, maxPackedValue, splitPackedValues, leafBlockFPs);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c7f06758793500ca773d0df1037290e6e404fb33","date":1562230223,"type":5,"author":"Ignacio Vera","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextBKDWriter#build(int,int,BKDRadixSelector.PathSlice,IndexOutput,BKDRadixSelector,byte[],byte[],byte[],long[],int[]).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextBKDWriter#build(int,int,BKDRadixSelector.PathSlice,IndexOutput,BKDRadixSelector,byte[],byte[],byte[],long[]).mjava","sourceNew":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to. */\n  private void build(int nodeID, int leafNodeOffset,\n                     BKDRadixSelector.PathSlice points,\n                     IndexOutput out,\n                     BKDRadixSelector radixSelector,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs,\n                     int[] spareDocIds) throws IOException {\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n      HeapPointWriter heapSource;\n      if (points.writer instanceof HeapPointWriter == false) {\n        // Adversarial cases can cause this, e.g. merging big segments with most of the points deleted\n        heapSource  = switchToHeap(points.writer);\n      } else {\n        heapSource = (HeapPointWriter) points.writer;\n      }\n\n      int from = Math.toIntExact(points.start);\n      int to = Math.toIntExact(points.start + points.count);\n\n      //we store common prefix on scratch1\n      computeCommonPrefixLength(heapSource, scratch1);\n\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n      FixedBitSet[] usedBytes = new FixedBitSet[numDataDims];\n      for (int dim = 0; dim < numDataDims; ++dim) {\n        if (commonPrefixLengths[dim] < bytesPerDim) {\n          usedBytes[dim] = new FixedBitSet(256);\n        }\n      }\n      //Find the dimension to compress\n      for (int dim = 0; dim < numDataDims; dim++) {\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int offset = dim * bytesPerDim;\n          for (int i = 0; i < heapSource.count(); ++i) {\n            PointValue value = heapSource.getPackedValueSlice(i);\n            BytesRef packedValue = value.packedValue();\n            int bucket = packedValue.bytes[packedValue.offset + offset + prefix] & 0xff;\n            usedBytes[dim].set(bucket);\n          }\n          int cardinality = usedBytes[dim].cardinality();\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      // sort the chosen dimension\n      radixSelector.heapRadixSort(heapSource, from, to, sortedDim, commonPrefixLengths[sortedDim]);\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = to - from;\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      // Write doc IDs\n      int[] docIDs = spareDocIds;\n      for (int i = 0; i < count; i++) {\n        docIDs[i] = heapSource.getPackedValueSlice(from + i).docID();\n      }\n      writeLeafBlockDocs(out, spareDocIds, 0, count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          PointValue value = heapSource.getPackedValueSlice(from + i);\n          return value.packedValue();\n        }\n      };\n      assert valuesInOrderAndBounds(count, sortedDim, minPackedValue, maxPackedValue, packedValues,\n          docIDs, 0);\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numIndexDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      assert nodeID < splitPackedValues.length : \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = points.count / 2;\n      long leftCount = points.count - rightCount;\n\n      int commonPrefixLen = Arrays.mismatch(minPackedValue, splitDim * bytesPerDim,\n          splitDim * bytesPerDim + bytesPerDim, maxPackedValue, splitDim * bytesPerDim,\n          splitDim * bytesPerDim + bytesPerDim);\n      if (commonPrefixLen == -1) {\n        commonPrefixLen = bytesPerDim;\n      }\n\n      BKDRadixSelector.PathSlice[] pathSlices = new BKDRadixSelector.PathSlice[2];\n\n      byte[] splitValue =  radixSelector.select(points, pathSlices, points.start, points.start + points.count,  points.start + leftCount, splitDim, commonPrefixLen);\n\n      int address = nodeID * (1 + bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      byte[] minSplitPackedValue = new byte[packedIndexBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedIndexBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedIndexBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedIndexBytesLength);\n\n      System.arraycopy(splitValue, 0, minSplitPackedValue, splitDim * bytesPerDim, bytesPerDim);\n      System.arraycopy(splitValue, 0, maxSplitPackedValue, splitDim * bytesPerDim, bytesPerDim);\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, pathSlices[0], out, radixSelector,\n            minPackedValue, maxSplitPackedValue, splitPackedValues, leafBlockFPs, spareDocIds);\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, pathSlices[1], out, radixSelector,\n            minSplitPackedValue, maxPackedValue, splitPackedValues, leafBlockFPs, spareDocIds);\n    }\n  }\n\n","sourceOld":"  /** The array (sized numDims) of PathSlice describe the cell we have currently recursed to. */\n  private void build(int nodeID, int leafNodeOffset,\n                     BKDRadixSelector.PathSlice points,\n                     IndexOutput out,\n                     BKDRadixSelector radixSelector,\n                     byte[] minPackedValue, byte[] maxPackedValue,\n                     byte[] splitPackedValues,\n                     long[] leafBlockFPs) throws IOException {\n\n    if (nodeID >= leafNodeOffset) {\n\n      // Leaf node: write block\n      // We can write the block in any order so by default we write it sorted by the dimension that has the\n      // least number of unique bytes at commonPrefixLengths[dim], which makes compression more efficient\n      HeapPointWriter heapSource;\n      if (points.writer instanceof HeapPointWriter == false) {\n        // Adversarial cases can cause this, e.g. merging big segments with most of the points deleted\n        heapSource  = switchToHeap(points.writer);\n      } else {\n        heapSource = (HeapPointWriter) points.writer;\n      }\n\n      int from = Math.toIntExact(points.start);\n      int to = Math.toIntExact(points.start + points.count);\n\n      //we store common prefix on scratch1\n      computeCommonPrefixLength(heapSource, scratch1);\n\n      int sortedDim = 0;\n      int sortedDimCardinality = Integer.MAX_VALUE;\n      FixedBitSet[] usedBytes = new FixedBitSet[numDataDims];\n      for (int dim = 0; dim < numDataDims; ++dim) {\n        if (commonPrefixLengths[dim] < bytesPerDim) {\n          usedBytes[dim] = new FixedBitSet(256);\n        }\n      }\n      //Find the dimension to compress\n      for (int dim = 0; dim < numDataDims; dim++) {\n        int prefix = commonPrefixLengths[dim];\n        if (prefix < bytesPerDim) {\n          int offset = dim * bytesPerDim;\n          for (int i = 0; i < heapSource.count(); ++i) {\n            PointValue value = heapSource.getPackedValueSlice(i);\n            BytesRef packedValue = value.packedValue();\n            int bucket = packedValue.bytes[packedValue.offset + offset + prefix] & 0xff;\n            usedBytes[dim].set(bucket);\n          }\n          int cardinality = usedBytes[dim].cardinality();\n          if (cardinality < sortedDimCardinality) {\n            sortedDim = dim;\n            sortedDimCardinality = cardinality;\n          }\n        }\n      }\n\n      // sort the chosen dimension\n      radixSelector.heapRadixSort(heapSource, from, to, sortedDim, commonPrefixLengths[sortedDim]);\n\n      // Save the block file pointer:\n      leafBlockFPs[nodeID - leafNodeOffset] = out.getFilePointer();\n      //System.out.println(\"  write leaf block @ fp=\" + out.getFilePointer());\n\n      // Write docIDs first, as their own chunk, so that at intersect time we can add all docIDs w/o\n      // loading the values:\n      int count = to - from;\n      assert count > 0: \"nodeID=\" + nodeID + \" leafNodeOffset=\" + leafNodeOffset;\n      writeLeafBlockDocs(out, heapSource.docIDs, from, count);\n\n      // TODO: minor opto: we don't really have to write the actual common prefixes, because BKDReader on recursing can regenerate it for us\n      // from the index, much like how terms dict does so from the FST:\n\n      // Write the full values:\n      IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n        final BytesRef scratch = new BytesRef();\n\n        {\n          scratch.length = packedBytesLength;\n        }\n\n        @Override\n        public BytesRef apply(int i) {\n          PointValue value = heapSource.getPackedValueSlice(from + i);\n          return value.packedValue();\n        }\n      };\n      assert valuesInOrderAndBounds(count, sortedDim, minPackedValue, maxPackedValue, packedValues,\n          heapSource.docIDs, from);\n      writeLeafBlockPackedValues(out, commonPrefixLengths, count, sortedDim, packedValues);\n\n    } else {\n      // Inner node: partition/recurse\n\n      int splitDim;\n      if (numIndexDims > 1) {\n        splitDim = split(minPackedValue, maxPackedValue);\n      } else {\n        splitDim = 0;\n      }\n\n      assert nodeID < splitPackedValues.length : \"nodeID=\" + nodeID + \" splitValues.length=\" + splitPackedValues.length;\n\n      // How many points will be in the left tree:\n      long rightCount = points.count / 2;\n      long leftCount = points.count - rightCount;\n\n      int commonPrefixLen = Arrays.mismatch(minPackedValue, splitDim * bytesPerDim,\n          splitDim * bytesPerDim + bytesPerDim, maxPackedValue, splitDim * bytesPerDim,\n          splitDim * bytesPerDim + bytesPerDim);\n      if (commonPrefixLen == -1) {\n        commonPrefixLen = bytesPerDim;\n      }\n\n      BKDRadixSelector.PathSlice[] pathSlices = new BKDRadixSelector.PathSlice[2];\n\n      byte[] splitValue =  radixSelector.select(points, pathSlices, points.start, points.start + points.count,  points.start + leftCount, splitDim, commonPrefixLen);\n\n      int address = nodeID * (1 + bytesPerDim);\n      splitPackedValues[address] = (byte) splitDim;\n      System.arraycopy(splitValue, 0, splitPackedValues, address + 1, bytesPerDim);\n\n      byte[] minSplitPackedValue = new byte[packedIndexBytesLength];\n      System.arraycopy(minPackedValue, 0, minSplitPackedValue, 0, packedIndexBytesLength);\n\n      byte[] maxSplitPackedValue = new byte[packedIndexBytesLength];\n      System.arraycopy(maxPackedValue, 0, maxSplitPackedValue, 0, packedIndexBytesLength);\n\n      System.arraycopy(splitValue, 0, minSplitPackedValue, splitDim * bytesPerDim, bytesPerDim);\n      System.arraycopy(splitValue, 0, maxSplitPackedValue, splitDim * bytesPerDim, bytesPerDim);\n\n      // Recurse on left tree:\n      build(2*nodeID, leafNodeOffset, pathSlices[0], out, radixSelector,\n            minPackedValue, maxSplitPackedValue, splitPackedValues, leafBlockFPs);\n\n      // TODO: we could \"tail recurse\" here?  have our parent discard its refs as we recurse right?\n      // Recurse on right tree:\n      build(2*nodeID+1, leafNodeOffset, pathSlices[1], out, radixSelector,\n            minSplitPackedValue, maxPackedValue, splitPackedValues, leafBlockFPs);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c7f06758793500ca773d0df1037290e6e404fb33":["b88a121b875f9ae2ac50f85cf46dcb680f126357"],"b88a121b875f9ae2ac50f85cf46dcb680f126357":["76a51551f05a6c96a115b5a656837ecc8fd0b1ff"],"c2344a1c769566d8c85cffcacc5e55153fa54b86":["a69ebf290ab26d026cc224e517e0d93d931ac87b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a69ebf290ab26d026cc224e517e0d93d931ac87b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"76a51551f05a6c96a115b5a656837ecc8fd0b1ff":["c2344a1c769566d8c85cffcacc5e55153fa54b86"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c7f06758793500ca773d0df1037290e6e404fb33"]},"commit2Childs":{"c7f06758793500ca773d0df1037290e6e404fb33":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"b88a121b875f9ae2ac50f85cf46dcb680f126357":["c7f06758793500ca773d0df1037290e6e404fb33"],"c2344a1c769566d8c85cffcacc5e55153fa54b86":["76a51551f05a6c96a115b5a656837ecc8fd0b1ff"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a69ebf290ab26d026cc224e517e0d93d931ac87b"],"a69ebf290ab26d026cc224e517e0d93d931ac87b":["c2344a1c769566d8c85cffcacc5e55153fa54b86"],"76a51551f05a6c96a115b5a656837ecc8fd0b1ff":["b88a121b875f9ae2ac50f85cf46dcb680f126357"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}