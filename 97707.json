{"path":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","commits":[{"id":"2209af2c265d2258ec4b29c8cc78622d36994a15","date":1440641916,"type":1,"author":"Gregory Chanan","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerProcessor#run().mjava","sourceNew":"  @Override\n  public void run() {\n    log.info(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      log.error(\"Unable to prioritize overseer \", e);\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, 100, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > maxParallelThreads) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n          List<QueueEvent> heads = workQueue.peekTopN(maxParallelThreads, runningZKTasks, 2000L);\n\n          if (heads == null)\n            continue;\n\n          log.debug(\"Got {} tasks from work-queue : [{}]\", heads.size(), heads.toString());\n\n          if (isClosed) break;\n\n          for (QueueEvent head : heads) {\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            String taskKey = messageHandler.getTaskKey(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n\n            if (!checkExclusivity(messageHandler, message, head.getId())) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              continue;\n            }\n\n            try {\n              markTaskAsRunning(messageHandler, head, taskKey, asyncId, message);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n            } catch (InterruptedException e) {\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n            }\n\n            log.info(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            Runner runner = new Runner(messageHandler, message,\n                operation, head);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    log.info(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      log.error(\"Unable to prioritize overseer \", e);\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, 100, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > maxParallelThreads) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n          List<QueueEvent> heads = workQueue.peekTopN(maxParallelThreads, runningZKTasks, 2000L);\n\n          if (heads == null)\n            continue;\n\n          log.debug(\"Got {} tasks from work-queue : [{}]\", heads.size(), heads.toString());\n\n          if (isClosed) break;\n\n          for (QueueEvent head : heads) {\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            String taskKey = messageHandler.getTaskKey(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n\n            if (!checkExclusivity(messageHandler, message, head.getId())) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              continue;\n            }\n\n            try {\n              markTaskAsRunning(messageHandler, head, taskKey, asyncId, message);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n            } catch (InterruptedException e) {\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n            }\n\n            log.info(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            Runner runner = new Runner(messageHandler, message,\n                operation, head);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ff49eed06b46bfc31ddfdd5bbede7cb5766f1ed6","date":1456562107,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","sourceNew":"  @Override\n  public void run() {\n    log.info(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      log.error(\"Unable to prioritize overseer \", e);\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n          List<QueueEvent> heads = workQueue.peekTopN(MAX_PARALLEL_TASKS, runningZKTasks, 2000L);\n\n          if (heads == null)\n            continue;\n\n          log.debug(\"Got {} tasks from work-queue : [{}]\", heads.size(), heads.toString());\n\n          if (isClosed) break;\n\n          for (QueueEvent head : heads) {\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            String taskKey = messageHandler.getTaskKey(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n\n            if (!checkExclusivity(messageHandler, message, head.getId())) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              continue;\n            }\n\n            try {\n              markTaskAsRunning(messageHandler, head, taskKey, asyncId, message);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n            } catch (InterruptedException e) {\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n            }\n\n            log.info(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            Runner runner = new Runner(messageHandler, message,\n                operation, head);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    log.info(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      log.error(\"Unable to prioritize overseer \", e);\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, 100, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > maxParallelThreads) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n          List<QueueEvent> heads = workQueue.peekTopN(maxParallelThreads, runningZKTasks, 2000L);\n\n          if (heads == null)\n            continue;\n\n          log.debug(\"Got {} tasks from work-queue : [{}]\", heads.size(), heads.toString());\n\n          if (isClosed) break;\n\n          for (QueueEvent head : heads) {\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            String taskKey = messageHandler.getTaskKey(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n\n            if (!checkExclusivity(messageHandler, message, head.getId())) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              continue;\n            }\n\n            try {\n              markTaskAsRunning(messageHandler, head, taskKey, asyncId, message);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n            } catch (InterruptedException e) {\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n            }\n\n            log.info(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            Runner runner = new Runner(messageHandler, message,\n                operation, head);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","bugFix":["6f26f74e4969851a019d28f10315cb1c77786f22","cb5af3afeddbb803fb785098176e6e177c34261b"],"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"af2638813028b254a88b418ebeafb541afb49653","date":1456804822,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","sourceNew":"  @Override\n  public void run() {\n    log.info(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      log.error(\"Unable to prioritize overseer \", e);\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n          List<QueueEvent> heads = workQueue.peekTopN(MAX_PARALLEL_TASKS, runningZKTasks, 2000L);\n\n          if (heads == null)\n            continue;\n\n          log.debug(\"Got {} tasks from work-queue : [{}]\", heads.size(), heads.toString());\n\n          if (isClosed) break;\n\n          for (QueueEvent head : heads) {\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            String taskKey = messageHandler.getTaskKey(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n\n            if (!checkExclusivity(messageHandler, message, head.getId())) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              continue;\n            }\n\n            try {\n              markTaskAsRunning(messageHandler, head, taskKey, asyncId, message);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n            } catch (InterruptedException e) {\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n            }\n\n            log.info(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            Runner runner = new Runner(messageHandler, message,\n                operation, head);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    log.info(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      log.error(\"Unable to prioritize overseer \", e);\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, 100, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > maxParallelThreads) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n          List<QueueEvent> heads = workQueue.peekTopN(maxParallelThreads, runningZKTasks, 2000L);\n\n          if (heads == null)\n            continue;\n\n          log.debug(\"Got {} tasks from work-queue : [{}]\", heads.size(), heads.toString());\n\n          if (isClosed) break;\n\n          for (QueueEvent head : heads) {\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            String taskKey = messageHandler.getTaskKey(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n\n            if (!checkExclusivity(messageHandler, message, head.getId())) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              continue;\n            }\n\n            try {\n              markTaskAsRunning(messageHandler, head, taskKey, asyncId, message);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n            } catch (InterruptedException e) {\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n            }\n\n            log.info(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            Runner runner = new Runner(messageHandler, message,\n                operation, head);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"781578ca5d3d2194bc55fc3457c62aef623db6f2","date":1464859077,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","sourceNew":"  @Override\n  public void run() {\n    log.info(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      log.error(\"Unable to prioritize overseer \", e);\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n          List<QueueEvent> heads = workQueue.peekTopN(MAX_PARALLEL_TASKS, runningZKTasks, 2000L);\n\n          if (heads == null)\n            continue;\n\n          log.debug(\"Got {} tasks from work-queue : [{}]\", heads.size(), heads.toString());\n\n          if (isClosed) break;\n\n          taskBatch.batchId++;\n          for (QueueEvent head : heads) {\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.info(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    log.info(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      log.error(\"Unable to prioritize overseer \", e);\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n          List<QueueEvent> heads = workQueue.peekTopN(MAX_PARALLEL_TASKS, runningZKTasks, 2000L);\n\n          if (heads == null)\n            continue;\n\n          log.debug(\"Got {} tasks from work-queue : [{}]\", heads.size(), heads.toString());\n\n          if (isClosed) break;\n\n          for (QueueEvent head : heads) {\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            String taskKey = messageHandler.getTaskKey(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n\n            if (!checkExclusivity(messageHandler, message, head.getId())) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              continue;\n            }\n\n            try {\n              markTaskAsRunning(messageHandler, head, taskKey, asyncId, message);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n            } catch (InterruptedException e) {\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n            }\n\n            log.info(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            Runner runner = new Runner(messageHandler, message,\n                operation, head);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f7fd147f6ec145b926803480f62aa12e2aad258e","date":1465448343,"type":3,"author":"Scott Blum","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","sourceNew":"  @Override\n  public void run() {\n    log.info(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      log.error(\"Unable to prioritize overseer \", e);\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n          List<QueueEvent> heads = workQueue.peekTopN(MAX_PARALLEL_TASKS, runningZKTasks, 2000L);\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          log.debug(\"Got {} tasks from work-queue : [{}]\", heads.size(), heads.toString());\n\n          if (isClosed) break;\n\n          taskBatch.batchId++;\n          for (QueueEvent head : heads) {\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.info(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    log.info(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      log.error(\"Unable to prioritize overseer \", e);\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n          List<QueueEvent> heads = workQueue.peekTopN(MAX_PARALLEL_TASKS, runningZKTasks, 2000L);\n\n          if (heads == null)\n            continue;\n\n          log.debug(\"Got {} tasks from work-queue : [{}]\", heads.size(), heads.toString());\n\n          if (isClosed) break;\n\n          taskBatch.batchId++;\n          for (QueueEvent head : heads) {\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.info(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"77c9633f72998c3815ffe21baa5f16d6e50f1e4a","date":1465453043,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","sourceNew":"  @Override\n  public void run() {\n    log.info(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      log.error(\"Unable to prioritize overseer \", e);\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n          List<QueueEvent> heads = workQueue.peekTopN(MAX_PARALLEL_TASKS, runningZKTasks, 2000L);\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          log.debug(\"Got {} tasks from work-queue : [{}]\", heads.size(), heads.toString());\n\n          if (isClosed) break;\n\n          taskBatch.batchId++;\n          for (QueueEvent head : heads) {\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.info(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    log.info(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      log.error(\"Unable to prioritize overseer \", e);\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n          List<QueueEvent> heads = workQueue.peekTopN(MAX_PARALLEL_TASKS, runningZKTasks, 2000L);\n\n          if (heads == null)\n            continue;\n\n          log.debug(\"Got {} tasks from work-queue : [{}]\", heads.size(), heads.toString());\n\n          if (isClosed) break;\n\n          taskBatch.batchId++;\n          for (QueueEvent head : heads) {\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.info(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1e4014209b24e71d602e579a316f994355596012","date":1465717267,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","sourceNew":"  @Override\n  public void run() {\n    log.info(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      log.error(\"Unable to prioritize overseer \", e);\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasks.size());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasks.size() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.info(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    log.info(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      log.error(\"Unable to prioritize overseer \", e);\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n          List<QueueEvent> heads = workQueue.peekTopN(MAX_PARALLEL_TASKS, runningZKTasks, 2000L);\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          log.debug(\"Got {} tasks from work-queue : [{}]\", heads.size(), heads.toString());\n\n          if (isClosed) break;\n\n          taskBatch.batchId++;\n          for (QueueEvent head : heads) {\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.info(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d4169a39455be564348e51f60d4dc9f77a80c2f9","date":1473707172,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","sourceNew":"  @Override\n  public void run() {\n    log.info(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasks.size());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasks.size() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.info(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    log.info(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      log.error(\"Unable to prioritize overseer \", e);\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasks.size());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasks.size() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.info(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"89424def13674ea17829b41c5883c54ecc31a132","date":1473767373,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","sourceNew":"  @Override\n  public void run() {\n    log.info(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasks.size());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasks.size() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.info(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    log.info(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      log.error(\"Unable to prioritize overseer \", e);\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasks.size());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasks.size() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.info(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"962cd4f5e313777f35da8f521265323e84184929","date":1474533758,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","sourceNew":"  @Override\n  public void run() {\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasks.size());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasks.size() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.debug(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    log.info(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasks.size());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasks.size() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.info(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","sourceNew":"  @Override\n  public void run() {\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasks.size());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasks.size() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.debug(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    log.info(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      log.error(\"Unable to prioritize overseer \", e);\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasks.size());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasks.size() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.info(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","sourceNew":"  @Override\n  public void run() {\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasks.size());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasks.size() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.debug(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    log.info(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      log.error(\"Unable to prioritize overseer \", e);\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n          List<QueueEvent> heads = workQueue.peekTopN(MAX_PARALLEL_TASKS, runningZKTasks, 2000L);\n\n          if (heads == null)\n            continue;\n\n          log.debug(\"Got {} tasks from work-queue : [{}]\", heads.size(), heads.toString());\n\n          if (isClosed) break;\n\n          for (QueueEvent head : heads) {\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            String taskKey = messageHandler.getTaskKey(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n\n            if (!checkExclusivity(messageHandler, message, head.getId())) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              continue;\n            }\n\n            try {\n              markTaskAsRunning(messageHandler, head, taskKey, asyncId, message);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n            } catch (InterruptedException e) {\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n            }\n\n            log.info(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            Runner runner = new Runner(messageHandler, message,\n                operation, head);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"af3e10d8a1fbcc5c79b22f7477e79de467dd326c","date":1515178406,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","sourceNew":"  @Override\n  public void run() {\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasks.size());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasks.size() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            if (operation == null) {\n              log.error(\"Msg does not have required \" + Overseer.QUEUE_OPERATION + \": {}\", message);\n              workQueue.remove(head);\n              continue;\n            }\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.debug(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasks.size());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasks.size() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.debug(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b94236357aaa22b76c10629851fe4e376e0cea82","date":1516710914,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","sourceNew":"  @Override\n  public void run() {\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasks.size());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasks.size() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            if (operation == null) {\n              log.error(\"Msg does not have required \" + Overseer.QUEUE_OPERATION + \": {}\", message);\n              workQueue.remove(head);\n              continue;\n            }\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.debug(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasks.size());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasks.size() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.debug(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"042da0877b8e28fd372a8ed80d11c4506a466ad7","date":1534516670,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","sourceNew":"  @Override\n  public void run() {\n    MDCLoggingContext.setNode(thisNode);\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasks.size());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasks.size() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            if (operation == null) {\n              log.error(\"Msg does not have required \" + Overseer.QUEUE_OPERATION + \": {}\", message);\n              workQueue.remove(head);\n              continue;\n            }\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.debug(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasks.size());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasks.size() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            if (operation == null) {\n              log.error(\"Msg does not have required \" + Overseer.QUEUE_OPERATION + \": {}\", message);\n              workQueue.remove(head);\n              continue;\n            }\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.debug(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bb222a3f9d9421d5c95afce73013fbd8de07ea1f","date":1543514331,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","sourceNew":"  @Override\n  public void run() {\n    MDCLoggingContext.setNode(thisNode);\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (AlreadyClosedException e) {\n      return;\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (AlreadyClosedException e) {\n        return;\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {} #Completed tasks: {}\",  runningTasksSize(), completedTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasksSize() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasksSize());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasksSize() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            synchronized (runningZKTasks) {\n              if (runningZKTasks.contains(head.getId())) continue;\n            }\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            if (operation == null) {\n              log.error(\"Msg does not have required \" + Overseer.QUEUE_OPERATION + \": {}\", message);\n              workQueue.remove(head);\n              continue;\n            }\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.debug(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (AlreadyClosedException e) {\n\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    MDCLoggingContext.setNode(thisNode);\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {}\", runningTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasks.size());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasks.size() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            if (operation == null) {\n              log.error(\"Msg does not have required \" + Overseer.QUEUE_OPERATION + \": {}\", message);\n              workQueue.remove(head);\n              continue;\n            }\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.debug(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","bugFix":["6f26f74e4969851a019d28f10315cb1c77786f22","ff49eed06b46bfc31ddfdd5bbede7cb5766f1ed6","1e4014209b24e71d602e579a316f994355596012","781578ca5d3d2194bc55fc3457c62aef623db6f2"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"94a5e240cfc05f5af141a42c7a36718fce82667f","date":1555350816,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","sourceNew":"  @Override\n  public void run() {\n    MDCLoggingContext.setNode(thisNode);\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (AlreadyClosedException e) {\n      return;\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (AlreadyClosedException e) {\n        return;\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {} #Completed tasks: {}\",  runningTasksSize(), completedTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasksSize() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasksSize());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasksSize() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            synchronized (runningZKTasks) {\n              if (runningZKTasks.contains(head.getId())) continue;\n            }\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            if (operation == null) {\n              log.error(\"Msg does not have required \" + Overseer.QUEUE_OPERATION + \": {}\", message);\n              workQueue.remove(head);\n              continue;\n            }\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.debug(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n          \n          // Prevent free-spinning this loop.\n          try {\n            Thread.sleep(1000);\n          } catch (InterruptedException e1) {\n            Thread.currentThread().interrupt();\n            return;\n          }\n          \n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (AlreadyClosedException e) {\n\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    MDCLoggingContext.setNode(thisNode);\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (AlreadyClosedException e) {\n      return;\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (AlreadyClosedException e) {\n        return;\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {} #Completed tasks: {}\",  runningTasksSize(), completedTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasksSize() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasksSize());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasksSize() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            synchronized (runningZKTasks) {\n              if (runningZKTasks.contains(head.getId())) continue;\n            }\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            if (operation == null) {\n              log.error(\"Msg does not have required \" + Overseer.QUEUE_OPERATION + \": {}\", message);\n              workQueue.remove(head);\n              continue;\n            }\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.debug(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (AlreadyClosedException e) {\n\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6afb0ba86024b96e8b34cfc2e15562239dc36360","date":1579768208,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","sourceNew":"  @Override\n  public void run() {\n    MDCLoggingContext.setNode(thisNode);\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (AlreadyClosedException e) {\n      return;\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (AlreadyClosedException e) {\n        return;\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {} #Completed tasks: {}\",  runningTasksSize(), completedTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasksSize() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasksSize());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasksSize() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            synchronized (runningZKTasks) {\n              if (runningZKTasks.contains(head.getId())) continue;\n            }\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            if (operation == null) {\n              log.error(\"Msg does not have required \" + Overseer.QUEUE_OPERATION + \": {}\", message);\n              workQueue.remove(head);\n              continue;\n            }\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task {} for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.debug(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n          \n          // Prevent free-spinning this loop.\n          try {\n            Thread.sleep(1000);\n          } catch (InterruptedException e1) {\n            Thread.currentThread().interrupt();\n            return;\n          }\n          \n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (AlreadyClosedException e) {\n\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    MDCLoggingContext.setNode(thisNode);\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (AlreadyClosedException e) {\n      return;\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (AlreadyClosedException e) {\n        return;\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {} #Completed tasks: {}\",  runningTasksSize(), completedTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasksSize() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasksSize());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasksSize() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            synchronized (runningZKTasks) {\n              if (runningZKTasks.contains(head.getId())) continue;\n            }\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            if (operation == null) {\n              log.error(\"Msg does not have required \" + Overseer.QUEUE_OPERATION + \": {}\", message);\n              workQueue.remove(head);\n              continue;\n            }\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.debug(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n          \n          // Prevent free-spinning this loop.\n          try {\n            Thread.sleep(1000);\n          } catch (InterruptedException e1) {\n            Thread.currentThread().interrupt();\n            return;\n          }\n          \n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (AlreadyClosedException e) {\n\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","bugFix":["6f26f74e4969851a019d28f10315cb1c77786f22"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fb03700c9690d16b15fb4f56f6ec36b128fd894e","date":1586745995,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","sourceNew":"  @Override\n  public void run() {\n    MDCLoggingContext.setNode(thisNode);\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (AlreadyClosedException e) {\n      return;\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (AlreadyClosedException e) {\n        return;\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new SolrNamedThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {} #Completed tasks: {}\",  runningTasksSize(), completedTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasksSize() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasksSize());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasksSize() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            synchronized (runningZKTasks) {\n              if (runningZKTasks.contains(head.getId())) continue;\n            }\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            if (operation == null) {\n              log.error(\"Msg does not have required \" + Overseer.QUEUE_OPERATION + \": {}\", message);\n              workQueue.remove(head);\n              continue;\n            }\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task {} for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.debug(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n          \n          // Prevent free-spinning this loop.\n          try {\n            Thread.sleep(1000);\n          } catch (InterruptedException e1) {\n            Thread.currentThread().interrupt();\n            return;\n          }\n          \n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (AlreadyClosedException e) {\n\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    MDCLoggingContext.setNode(thisNode);\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (AlreadyClosedException e) {\n      return;\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (AlreadyClosedException e) {\n        return;\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new DefaultSolrThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {} #Completed tasks: {}\",  runningTasksSize(), completedTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasksSize() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasksSize());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasksSize() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            synchronized (runningZKTasks) {\n              if (runningZKTasks.contains(head.getId())) continue;\n            }\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            if (operation == null) {\n              log.error(\"Msg does not have required \" + Overseer.QUEUE_OPERATION + \": {}\", message);\n              workQueue.remove(head);\n              continue;\n            }\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task {} for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.debug(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n          \n          // Prevent free-spinning this loop.\n          try {\n            Thread.sleep(1000);\n          } catch (InterruptedException e1) {\n            Thread.currentThread().interrupt();\n            return;\n          }\n          \n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (AlreadyClosedException e) {\n\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad4957cde742defe6db19689abdc267c5d948066","date":1587990850,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","sourceNew":"  @Override\n  public void run() {\n    MDCLoggingContext.setNode(thisNode);\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (AlreadyClosedException e) {\n      return;\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (AlreadyClosedException e) {\n        return;\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new SolrNamedThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          if (log.isDebugEnabled()) {\n            log.debug(\"Cleaning up work-queue. #Running tasks: {} #Completed tasks: {}\", runningTasksSize(), completedTasks.size());\n          }\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasksSize() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasksSize());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            if (log.isDebugEnabled()) {\n              log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            }\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasksSize() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            synchronized (runningZKTasks) {\n              if (runningZKTasks.contains(head.getId())) continue;\n            }\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            if (operation == null) {\n              log.error(\"Msg does not have required {} : {}\", Overseer.QUEUE_OPERATION, message);\n              workQueue.remove(head);\n              continue;\n            }\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              if (log.isDebugEnabled()) {\n                log.debug(\"Exclusivity check failed for [{}]\", message);\n              }\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              if (log.isDebugEnabled()) {\n                log.debug(\"Marked task [{}] as running\", head.getId());\n              }\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task {} for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            if (log.isDebugEnabled()) {\n              log.debug(\"{}: Get the message id: {} message: {}\", messageHandler.getName(), head.getId(), message);\n            }\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n          \n          // Prevent free-spinning this loop.\n          try {\n            Thread.sleep(1000);\n          } catch (InterruptedException e1) {\n            Thread.currentThread().interrupt();\n            return;\n          }\n          \n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (AlreadyClosedException e) {\n\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    MDCLoggingContext.setNode(thisNode);\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (AlreadyClosedException e) {\n      return;\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (AlreadyClosedException e) {\n        return;\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new SolrNamedThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          log.debug(\"Cleaning up work-queue. #Running tasks: {} #Completed tasks: {}\",  runningTasksSize(), completedTasks.size());\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasksSize() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasksSize());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasksSize() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            synchronized (runningZKTasks) {\n              if (runningZKTasks.contains(head.getId())) continue;\n            }\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            if (operation == null) {\n              log.error(\"Msg does not have required \" + Overseer.QUEUE_OPERATION + \": {}\", message);\n              workQueue.remove(head);\n              continue;\n            }\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              log.debug(\"Exclusivity check failed for [{}]\", message.toString());\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              log.debug(\"Marked task [{}] as running\", head.getId());\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task {} for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            log.debug(messageHandler.getName() + \": Get the message id:\" + head.getId() + \" message:\" + message.toString());\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n          \n          // Prevent free-spinning this loop.\n          try {\n            Thread.sleep(1000);\n          } catch (InterruptedException e1) {\n            Thread.currentThread().interrupt();\n            return;\n          }\n          \n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (AlreadyClosedException e) {\n\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"98b85aeea4d572e94fd5e6ba67043b5b363f1380","date":1592921806,"type":3,"author":"Ilan Ginzburg","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","sourceNew":"  @Override\n  public void run() {\n    MDCLoggingContext.setNode(thisNode);\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (AlreadyClosedException e) {\n      return;\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (AlreadyClosedException e) {\n        return;\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<>(),\n        new SolrNamedThreadFactory(\"OverseerThreadFactory\"));\n\n    // In OverseerCollectionMessageHandler, a new Session needs to be created for each new iteration over the tasks in the\n    // queue. Incrementing this id causes a new session to be created there.\n    long batchSessionId = 0;\n\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          if (log.isDebugEnabled()) {\n            log.debug(\"Cleaning up work-queue. #Running tasks: {} #Completed tasks: {}\", runningTasks.size(), completedTasks.size());\n          }\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasks.size());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            if (log.isDebugEnabled()) {\n              log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            }\n            // heads has at most MAX_BLOCKED_TASKS tasks.\n            heads.addAll(newTasks);\n          } else {\n            // The sleep below slows down spinning when heads is full from previous work dispatch attempt below and no new\n            // tasks got executed (all executors are busy or all waiting tasks require locks currently held by executors).\n            //\n            // When heads is not full but no progress was made (no new work got dispatched in the for loop below), slowing down\n            // of the spinning is done by the wait time in the call to workQueue.peekTopN() above.\n            // (at least in theory because the method eventually called from there is ZkDistributedQueue.peekElements()\n            // and because it filters out entries that have just completed on a Runner thread in a different way than the\n            // predicate based filtering, it can return quickly without waiting the configured delay time. Therefore spinning\n            // can be observed, likely something to clean up at some point).\n            //\n            // If heads is not empty and new tasks appeared in the queue there's no delay, workQueue.peekTopN() above will\n            // return immediately.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          // clear the blocked tasks, may get refilled below. Given blockedTasks can only get entries from heads and heads\n          // has at most MAX_BLOCKED_TASKS tasks, blockedTasks will never exceed MAX_BLOCKED_TASKS entries.\n          // Note blockedTasks can't be cleared too early as it is used in the excludedTasks Predicate above.\n          blockedTasks.clear();\n\n          // Trigger the creation of a new Session used for locking when/if a lock is later acquired on the OverseerCollectionMessageHandler\n          batchSessionId++;\n\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n                tooManyTasks = runningTasks.size() >= MAX_PARALLEL_TASKS;\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            if (operation == null) {\n              log.error(\"Msg does not have required {} : {}\", Overseer.QUEUE_OPERATION, message);\n              workQueue.remove(head);\n              continue;\n            }\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, batchSessionId);\n            if (lock == null) {\n              if (log.isDebugEnabled()) {\n                log.debug(\"Exclusivity check failed for [{}]\", message);\n              }\n              blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              if (log.isDebugEnabled()) {\n                log.debug(\"Marked task [{}] as running\", head.getId());\n              }\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task {} for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            if (log.isDebugEnabled()) {\n              log.debug(\"{}: Get the message id: {} message: {}\", messageHandler.getName(), head.getId(), message);\n            }\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n          \n          // Prevent free-spinning this loop.\n          try {\n            Thread.sleep(1000);\n          } catch (InterruptedException e1) {\n            Thread.currentThread().interrupt();\n            return;\n          }\n          \n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (AlreadyClosedException e) {\n\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    MDCLoggingContext.setNode(thisNode);\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (AlreadyClosedException e) {\n      return;\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (AlreadyClosedException e) {\n        return;\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<Runnable>(),\n        new SolrNamedThreadFactory(\"OverseerThreadFactory\"));\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          if (log.isDebugEnabled()) {\n            log.debug(\"Cleaning up work-queue. #Running tasks: {} #Completed tasks: {}\", runningTasksSize(), completedTasks.size());\n          }\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasksSize() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasksSize());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            if (log.isDebugEnabled()) {\n              log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            }\n            heads.addAll(newTasks);\n          } else {\n            // Prevent free-spinning this loop.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          blockedTasks.clear(); // clear it now; may get refilled below.\n\n          taskBatch.batchId++;\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n              synchronized (runningTasks) {\n                tooManyTasks = runningTasksSize() >= MAX_PARALLEL_TASKS;\n              }\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              if(blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            synchronized (runningZKTasks) {\n              if (runningZKTasks.contains(head.getId())) continue;\n            }\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            if (operation == null) {\n              log.error(\"Msg does not have required {} : {}\", Overseer.QUEUE_OPERATION, message);\n              workQueue.remove(head);\n              continue;\n            }\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, taskBatch);\n            if (lock == null) {\n              if (log.isDebugEnabled()) {\n                log.debug(\"Exclusivity check failed for [{}]\", message);\n              }\n              //we may end crossing the size of the MAX_BLOCKED_TASKS. They are fine\n              if (blockedTasks.size() < MAX_BLOCKED_TASKS)\n                blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              if (log.isDebugEnabled()) {\n                log.debug(\"Marked task [{}] as running\", head.getId());\n              }\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task {} for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            if (log.isDebugEnabled()) {\n              log.debug(\"{}: Get the message id: {} message: {}\", messageHandler.getName(), head.getId(), message);\n            }\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n          \n          // Prevent free-spinning this loop.\n          try {\n            Thread.sleep(1000);\n          } catch (InterruptedException e1) {\n            Thread.currentThread().interrupt();\n            return;\n          }\n          \n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (AlreadyClosedException e) {\n\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c526352db87264a72a7a9ad68c1b769b81e54305","date":1598780188,"type":3,"author":"Ilan Ginzburg","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","sourceNew":"  @Override\n  public void run() {\n    MDCLoggingContext.setNode(thisNode);\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (AlreadyClosedException e) {\n      return;\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (AlreadyClosedException e) {\n        return;\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<>(),\n        new SolrNamedThreadFactory(\"OverseerThreadFactory\"));\n\n    // In OverseerCollectionMessageHandler, a new Session needs to be created for each new iteration over the tasks in the\n    // queue. Incrementing this id causes a new session to be created there.\n    long batchSessionId = 0;\n\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          if (log.isDebugEnabled()) {\n            log.debug(\"Cleaning up work-queue. #Running tasks: {} #Completed tasks: {}\", runningTasks.size(), completedTasks.size());\n          }\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasks.size());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            if (log.isDebugEnabled()) {\n              log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            }\n            // heads has at most MAX_BLOCKED_TASKS tasks.\n            heads.addAll(newTasks);\n          } else {\n            // The sleep below slows down spinning when heads is full from previous work dispatch attempt below and no new\n            // tasks got executed (all executors are busy or all waiting tasks require locks currently held by executors).\n            //\n            // When heads is not full but no progress was made (no new work got dispatched in the for loop below), slowing down\n            // of the spinning is done by the wait time in the call to workQueue.peekTopN() above.\n            // (at least in theory because the method eventually called from there is ZkDistributedQueue.peekElements()\n            // and because it filters out entries that have just completed on a Runner thread in a different way than the\n            // predicate based filtering, it can return quickly without waiting the configured delay time. Therefore spinning\n            // can be observed, likely something to clean up at some point).\n            //\n            // If heads is not empty and new tasks appeared in the queue there's no delay, workQueue.peekTopN() above will\n            // return immediately.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          // clear the blocked tasks, may get refilled below. Given blockedTasks can only get entries from heads and heads\n          // has at most MAX_BLOCKED_TASKS tasks, blockedTasks will never exceed MAX_BLOCKED_TASKS entries.\n          // Note blockedTasks can't be cleared too early as it is used in the excludedTasks Predicate above.\n          blockedTasks.clear();\n\n          // Trigger the creation of a new Session used for locking when/if a lock is later acquired on the OverseerCollectionMessageHandler\n          batchSessionId++;\n\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n                tooManyTasks = runningTasks.size() >= MAX_PARALLEL_TASKS;\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            if (operation == null) {\n              log.error(\"Msg does not have required {} : {}\", Overseer.QUEUE_OPERATION, message);\n              workQueue.remove(head);\n              continue;\n            }\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, batchSessionId);\n            if (lock == null) {\n              if (log.isDebugEnabled()) {\n                log.debug(\"Exclusivity check failed for [{}]\", message);\n              }\n              blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              if (log.isDebugEnabled()) {\n                log.debug(\"Marked task [{}] as running\", head.getId());\n              }\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task {} for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            if (log.isDebugEnabled()) {\n              log.debug(\"{}: Get the message id: {} message: {}\", messageHandler.getName(), head.getId(), message);\n            }\n            Runner runner = new Runner(messageHandler, cloudConfig, message, operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n          \n          // Prevent free-spinning this loop.\n          try {\n            Thread.sleep(1000);\n          } catch (InterruptedException e1) {\n            Thread.currentThread().interrupt();\n            return;\n          }\n          \n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (AlreadyClosedException e) {\n\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    MDCLoggingContext.setNode(thisNode);\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (AlreadyClosedException e) {\n      return;\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (AlreadyClosedException e) {\n        return;\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<>(),\n        new SolrNamedThreadFactory(\"OverseerThreadFactory\"));\n\n    // In OverseerCollectionMessageHandler, a new Session needs to be created for each new iteration over the tasks in the\n    // queue. Incrementing this id causes a new session to be created there.\n    long batchSessionId = 0;\n\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          if (log.isDebugEnabled()) {\n            log.debug(\"Cleaning up work-queue. #Running tasks: {} #Completed tasks: {}\", runningTasks.size(), completedTasks.size());\n          }\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasks.size());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            if (log.isDebugEnabled()) {\n              log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            }\n            // heads has at most MAX_BLOCKED_TASKS tasks.\n            heads.addAll(newTasks);\n          } else {\n            // The sleep below slows down spinning when heads is full from previous work dispatch attempt below and no new\n            // tasks got executed (all executors are busy or all waiting tasks require locks currently held by executors).\n            //\n            // When heads is not full but no progress was made (no new work got dispatched in the for loop below), slowing down\n            // of the spinning is done by the wait time in the call to workQueue.peekTopN() above.\n            // (at least in theory because the method eventually called from there is ZkDistributedQueue.peekElements()\n            // and because it filters out entries that have just completed on a Runner thread in a different way than the\n            // predicate based filtering, it can return quickly without waiting the configured delay time. Therefore spinning\n            // can be observed, likely something to clean up at some point).\n            //\n            // If heads is not empty and new tasks appeared in the queue there's no delay, workQueue.peekTopN() above will\n            // return immediately.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          // clear the blocked tasks, may get refilled below. Given blockedTasks can only get entries from heads and heads\n          // has at most MAX_BLOCKED_TASKS tasks, blockedTasks will never exceed MAX_BLOCKED_TASKS entries.\n          // Note blockedTasks can't be cleared too early as it is used in the excludedTasks Predicate above.\n          blockedTasks.clear();\n\n          // Trigger the creation of a new Session used for locking when/if a lock is later acquired on the OverseerCollectionMessageHandler\n          batchSessionId++;\n\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n                tooManyTasks = runningTasks.size() >= MAX_PARALLEL_TASKS;\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            if (operation == null) {\n              log.error(\"Msg does not have required {} : {}\", Overseer.QUEUE_OPERATION, message);\n              workQueue.remove(head);\n              continue;\n            }\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, batchSessionId);\n            if (lock == null) {\n              if (log.isDebugEnabled()) {\n                log.debug(\"Exclusivity check failed for [{}]\", message);\n              }\n              blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              if (log.isDebugEnabled()) {\n                log.debug(\"Marked task [{}] as running\", head.getId());\n              }\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task {} for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            if (log.isDebugEnabled()) {\n              log.debug(\"{}: Get the message id: {} message: {}\", messageHandler.getName(), head.getId(), message);\n            }\n            Runner runner = new Runner(messageHandler, message,\n                operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n          \n          // Prevent free-spinning this loop.\n          try {\n            Thread.sleep(1000);\n          } catch (InterruptedException e1) {\n            Thread.currentThread().interrupt();\n            return;\n          }\n          \n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (AlreadyClosedException e) {\n\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7b17e79a71117668ecbf8d3417c876e41396565","date":1598973672,"type":3,"author":"Ilan Ginzburg","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerTaskProcessor#run().mjava","sourceNew":"  @Override\n  public void run() {\n    MDCLoggingContext.setNode(thisNode);\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (AlreadyClosedException e) {\n      return;\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (AlreadyClosedException e) {\n        return;\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<>(),\n        new SolrNamedThreadFactory(\"OverseerThreadFactory\"));\n\n    // In OverseerCollectionMessageHandler, a new Session needs to be created for each new iteration over the tasks in the\n    // queue. Incrementing this id causes a new session to be created there.\n    long batchSessionId = 0;\n\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          if (log.isDebugEnabled()) {\n            log.debug(\"Cleaning up work-queue. #Running tasks: {} #Completed tasks: {}\", runningTasks.size(), completedTasks.size());\n          }\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasks.size());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            if (log.isDebugEnabled()) {\n              log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            }\n            // heads has at most MAX_BLOCKED_TASKS tasks.\n            heads.addAll(newTasks);\n          } else {\n            // The sleep below slows down spinning when heads is full from previous work dispatch attempt below and no new\n            // tasks got executed (all executors are busy or all waiting tasks require locks currently held by executors).\n            //\n            // When heads is not full but no progress was made (no new work got dispatched in the for loop below), slowing down\n            // of the spinning is done by the wait time in the call to workQueue.peekTopN() above.\n            // (at least in theory because the method eventually called from there is ZkDistributedQueue.peekElements()\n            // and because it filters out entries that have just completed on a Runner thread in a different way than the\n            // predicate based filtering, it can return quickly without waiting the configured delay time. Therefore spinning\n            // can be observed, likely something to clean up at some point).\n            //\n            // If heads is not empty and new tasks appeared in the queue there's no delay, workQueue.peekTopN() above will\n            // return immediately.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          // clear the blocked tasks, may get refilled below. Given blockedTasks can only get entries from heads and heads\n          // has at most MAX_BLOCKED_TASKS tasks, blockedTasks will never exceed MAX_BLOCKED_TASKS entries.\n          // Note blockedTasks can't be cleared too early as it is used in the excludedTasks Predicate above.\n          blockedTasks.clear();\n\n          // Trigger the creation of a new Session used for locking when/if a lock is later acquired on the OverseerCollectionMessageHandler\n          batchSessionId++;\n\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n                tooManyTasks = runningTasks.size() >= MAX_PARALLEL_TASKS;\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            if (operation == null) {\n              log.error(\"Msg does not have required {} : {}\", Overseer.QUEUE_OPERATION, message);\n              workQueue.remove(head);\n              continue;\n            }\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, batchSessionId);\n            if (lock == null) {\n              if (log.isDebugEnabled()) {\n                log.debug(\"Exclusivity check failed for [{}]\", message);\n              }\n              blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              if (log.isDebugEnabled()) {\n                log.debug(\"Marked task [{}] as running\", head.getId());\n              }\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task {} for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            if (log.isDebugEnabled()) {\n              log.debug(\"{}: Get the message id: {} message: {}\", messageHandler.getName(), head.getId(), message);\n            }\n            Runner runner = new Runner(messageHandler, message, operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n          \n          // Prevent free-spinning this loop.\n          try {\n            Thread.sleep(1000);\n          } catch (InterruptedException e1) {\n            Thread.currentThread().interrupt();\n            return;\n          }\n          \n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (AlreadyClosedException e) {\n\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","sourceOld":"  @Override\n  public void run() {\n    MDCLoggingContext.setNode(thisNode);\n    log.debug(\"Process current queue of overseer operations\");\n    LeaderStatus isLeader = amILeader();\n    while (isLeader == LeaderStatus.DONT_KNOW) {\n      log.debug(\"am_i_leader unclear {}\", isLeader);\n      isLeader = amILeader();  // not a no, not a yes, try ask again\n    }\n\n    String oldestItemInWorkQueue = null;\n    // hasLeftOverItems - used for avoiding re-execution of async tasks that were processed by a previous Overseer.\n    // This variable is set in case there's any task found on the workQueue when the OCP starts up and\n    // the id for the queue tail is used as a marker to check for the task in completed/failed map in zk.\n    // Beyond the marker, all tasks can safely be assumed to have never been executed.\n    boolean hasLeftOverItems = true;\n\n    try {\n      oldestItemInWorkQueue = workQueue.getTailId();\n    } catch (KeeperException e) {\n      // We don't need to handle this. This is just a fail-safe which comes in handy in skipping already processed\n      // async calls.\n      SolrException.log(log, \"\", e);\n    } catch (AlreadyClosedException e) {\n      return;\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n\n    if (oldestItemInWorkQueue == null)\n      hasLeftOverItems = false;\n    else\n      log.debug(\"Found already existing elements in the work-queue. Last element: {}\", oldestItemInWorkQueue);\n\n    try {\n      prioritizer.prioritizeOverseerNodes(myId);\n    } catch (AlreadyClosedException e) {\n        return;\n    } catch (Exception e) {\n      if (!zkStateReader.getZkClient().isClosed()) {\n        log.error(\"Unable to prioritize overseer \", e);\n      }\n    }\n\n    // TODO: Make maxThreads configurable.\n\n    this.tpe = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, MAX_PARALLEL_TASKS, 0L, TimeUnit.MILLISECONDS,\n        new SynchronousQueue<>(),\n        new SolrNamedThreadFactory(\"OverseerThreadFactory\"));\n\n    // In OverseerCollectionMessageHandler, a new Session needs to be created for each new iteration over the tasks in the\n    // queue. Incrementing this id causes a new session to be created there.\n    long batchSessionId = 0;\n\n    try {\n      while (!this.isClosed) {\n        try {\n          isLeader = amILeader();\n          if (LeaderStatus.NO == isLeader) {\n            break;\n          } else if (LeaderStatus.YES != isLeader) {\n            log.debug(\"am_i_leader unclear {}\", isLeader);\n            continue; // not a no, not a yes, try asking again\n          }\n\n          if (log.isDebugEnabled()) {\n            log.debug(\"Cleaning up work-queue. #Running tasks: {} #Completed tasks: {}\", runningTasks.size(), completedTasks.size());\n          }\n          cleanUpWorkQueue();\n\n          printTrackingMaps();\n\n          boolean waited = false;\n\n          while (runningTasks.size() > MAX_PARALLEL_TASKS) {\n            synchronized (waitLock) {\n              waitLock.wait(100);//wait for 100 ms or till a task is complete\n            }\n            waited = true;\n          }\n\n          if (waited)\n            cleanUpWorkQueue();\n\n          ArrayList<QueueEvent> heads = new ArrayList<>(blockedTasks.size() + MAX_PARALLEL_TASKS);\n          heads.addAll(blockedTasks.values());\n\n          //If we have enough items in the blocked tasks already, it makes\n          // no sense to read more items from the work queue. it makes sense\n          // to clear out at least a few items in the queue before we read more items\n          if (heads.size() < MAX_BLOCKED_TASKS) {\n            //instead of reading MAX_PARALLEL_TASKS items always, we should only fetch as much as we can execute\n            int toFetch = Math.min(MAX_BLOCKED_TASKS - heads.size(), MAX_PARALLEL_TASKS - runningTasks.size());\n            List<QueueEvent> newTasks = workQueue.peekTopN(toFetch, excludedTasks, 2000L);\n            if (log.isDebugEnabled()) {\n              log.debug(\"Got {} tasks from work-queue : [{}]\", newTasks.size(), newTasks);\n            }\n            // heads has at most MAX_BLOCKED_TASKS tasks.\n            heads.addAll(newTasks);\n          } else {\n            // The sleep below slows down spinning when heads is full from previous work dispatch attempt below and no new\n            // tasks got executed (all executors are busy or all waiting tasks require locks currently held by executors).\n            //\n            // When heads is not full but no progress was made (no new work got dispatched in the for loop below), slowing down\n            // of the spinning is done by the wait time in the call to workQueue.peekTopN() above.\n            // (at least in theory because the method eventually called from there is ZkDistributedQueue.peekElements()\n            // and because it filters out entries that have just completed on a Runner thread in a different way than the\n            // predicate based filtering, it can return quickly without waiting the configured delay time. Therefore spinning\n            // can be observed, likely something to clean up at some point).\n            //\n            // If heads is not empty and new tasks appeared in the queue there's no delay, workQueue.peekTopN() above will\n            // return immediately.\n            Thread.sleep(1000);\n          }\n\n          if (isClosed) break;\n\n          if (heads.isEmpty()) {\n            continue;\n          }\n\n          // clear the blocked tasks, may get refilled below. Given blockedTasks can only get entries from heads and heads\n          // has at most MAX_BLOCKED_TASKS tasks, blockedTasks will never exceed MAX_BLOCKED_TASKS entries.\n          // Note blockedTasks can't be cleared too early as it is used in the excludedTasks Predicate above.\n          blockedTasks.clear();\n\n          // Trigger the creation of a new Session used for locking when/if a lock is later acquired on the OverseerCollectionMessageHandler\n          batchSessionId++;\n\n          boolean tooManyTasks = false;\n          for (QueueEvent head : heads) {\n            if (!tooManyTasks) {\n                tooManyTasks = runningTasks.size() >= MAX_PARALLEL_TASKS;\n            }\n            if (tooManyTasks) {\n              // Too many tasks are running, just shove the rest into the \"blocked\" queue.\n              blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            if (runningZKTasks.contains(head.getId())) continue;\n            final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n            final String asyncId = message.getStr(ASYNC);\n            if (hasLeftOverItems) {\n              if (head.getId().equals(oldestItemInWorkQueue))\n                hasLeftOverItems = false;\n              if (asyncId != null && (completedMap.contains(asyncId) || failureMap.contains(asyncId))) {\n                log.debug(\"Found already processed task in workQueue, cleaning up. AsyncId [{}]\",asyncId );\n                workQueue.remove(head);\n                continue;\n              }\n            }\n            String operation = message.getStr(Overseer.QUEUE_OPERATION);\n            if (operation == null) {\n              log.error(\"Msg does not have required {} : {}\", Overseer.QUEUE_OPERATION, message);\n              workQueue.remove(head);\n              continue;\n            }\n            OverseerMessageHandler messageHandler = selector.selectOverseerMessageHandler(message);\n            OverseerMessageHandler.Lock lock = messageHandler.lockTask(message, batchSessionId);\n            if (lock == null) {\n              if (log.isDebugEnabled()) {\n                log.debug(\"Exclusivity check failed for [{}]\", message);\n              }\n              blockedTasks.put(head.getId(), head);\n              continue;\n            }\n            try {\n              markTaskAsRunning(head, asyncId);\n              if (log.isDebugEnabled()) {\n                log.debug(\"Marked task [{}] as running\", head.getId());\n              }\n            } catch (KeeperException.NodeExistsException e) {\n              lock.unlock();\n              // This should never happen\n              log.error(\"Tried to pick up task [{}] when it was already running!\", head.getId());\n              continue;\n            } catch (InterruptedException e) {\n              lock.unlock();\n              log.error(\"Thread interrupted while trying to pick task {} for execution.\", head.getId());\n              Thread.currentThread().interrupt();\n              continue;\n            }\n            if (log.isDebugEnabled()) {\n              log.debug(\"{}: Get the message id: {} message: {}\", messageHandler.getName(), head.getId(), message);\n            }\n            Runner runner = new Runner(messageHandler, cloudConfig, message, operation, head, lock);\n            tpe.execute(runner);\n          }\n\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n            log.warn(\"Overseer cannot talk to ZK\");\n            return;\n          }\n          SolrException.log(log, \"\", e);\n          \n          // Prevent free-spinning this loop.\n          try {\n            Thread.sleep(1000);\n          } catch (InterruptedException e1) {\n            Thread.currentThread().interrupt();\n            return;\n          }\n          \n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          return;\n        } catch (AlreadyClosedException e) {\n\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    } finally {\n      this.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"af2638813028b254a88b418ebeafb541afb49653":["2209af2c265d2258ec4b29c8cc78622d36994a15","ff49eed06b46bfc31ddfdd5bbede7cb5766f1ed6"],"b94236357aaa22b76c10629851fe4e376e0cea82":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","af3e10d8a1fbcc5c79b22f7477e79de467dd326c"],"2209af2c265d2258ec4b29c8cc78622d36994a15":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c526352db87264a72a7a9ad68c1b769b81e54305":["98b85aeea4d572e94fd5e6ba67043b5b363f1380"],"bb222a3f9d9421d5c95afce73013fbd8de07ea1f":["042da0877b8e28fd372a8ed80d11c4506a466ad7"],"1e4014209b24e71d602e579a316f994355596012":["77c9633f72998c3815ffe21baa5f16d6e50f1e4a"],"77c9633f72998c3815ffe21baa5f16d6e50f1e4a":["781578ca5d3d2194bc55fc3457c62aef623db6f2","f7fd147f6ec145b926803480f62aa12e2aad258e"],"ad4957cde742defe6db19689abdc267c5d948066":["fb03700c9690d16b15fb4f56f6ec36b128fd894e"],"ff49eed06b46bfc31ddfdd5bbede7cb5766f1ed6":["2209af2c265d2258ec4b29c8cc78622d36994a15"],"89424def13674ea17829b41c5883c54ecc31a132":["1e4014209b24e71d602e579a316f994355596012","d4169a39455be564348e51f60d4dc9f77a80c2f9"],"f7fd147f6ec145b926803480f62aa12e2aad258e":["781578ca5d3d2194bc55fc3457c62aef623db6f2"],"af3e10d8a1fbcc5c79b22f7477e79de467dd326c":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["1e4014209b24e71d602e579a316f994355596012","962cd4f5e313777f35da8f521265323e84184929"],"d4169a39455be564348e51f60d4dc9f77a80c2f9":["1e4014209b24e71d602e579a316f994355596012"],"94a5e240cfc05f5af141a42c7a36718fce82667f":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"e7b17e79a71117668ecbf8d3417c876e41396565":["c526352db87264a72a7a9ad68c1b769b81e54305"],"962cd4f5e313777f35da8f521265323e84184929":["89424def13674ea17829b41c5883c54ecc31a132"],"98b85aeea4d572e94fd5e6ba67043b5b363f1380":["ad4957cde742defe6db19689abdc267c5d948066"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["af2638813028b254a88b418ebeafb541afb49653","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"042da0877b8e28fd372a8ed80d11c4506a466ad7":["b94236357aaa22b76c10629851fe4e376e0cea82"],"781578ca5d3d2194bc55fc3457c62aef623db6f2":["af2638813028b254a88b418ebeafb541afb49653"],"6afb0ba86024b96e8b34cfc2e15562239dc36360":["94a5e240cfc05f5af141a42c7a36718fce82667f"],"fb03700c9690d16b15fb4f56f6ec36b128fd894e":["6afb0ba86024b96e8b34cfc2e15562239dc36360"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e7b17e79a71117668ecbf8d3417c876e41396565"]},"commit2Childs":{"af2638813028b254a88b418ebeafb541afb49653":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","781578ca5d3d2194bc55fc3457c62aef623db6f2"],"b94236357aaa22b76c10629851fe4e376e0cea82":["042da0877b8e28fd372a8ed80d11c4506a466ad7"],"2209af2c265d2258ec4b29c8cc78622d36994a15":["af2638813028b254a88b418ebeafb541afb49653","ff49eed06b46bfc31ddfdd5bbede7cb5766f1ed6"],"c526352db87264a72a7a9ad68c1b769b81e54305":["e7b17e79a71117668ecbf8d3417c876e41396565"],"bb222a3f9d9421d5c95afce73013fbd8de07ea1f":["94a5e240cfc05f5af141a42c7a36718fce82667f"],"1e4014209b24e71d602e579a316f994355596012":["89424def13674ea17829b41c5883c54ecc31a132","17e5da53e4e5bd659e22add9bba1cfa222e7e30d","d4169a39455be564348e51f60d4dc9f77a80c2f9"],"77c9633f72998c3815ffe21baa5f16d6e50f1e4a":["1e4014209b24e71d602e579a316f994355596012"],"ad4957cde742defe6db19689abdc267c5d948066":["98b85aeea4d572e94fd5e6ba67043b5b363f1380"],"ff49eed06b46bfc31ddfdd5bbede7cb5766f1ed6":["af2638813028b254a88b418ebeafb541afb49653"],"89424def13674ea17829b41c5883c54ecc31a132":["962cd4f5e313777f35da8f521265323e84184929"],"f7fd147f6ec145b926803480f62aa12e2aad258e":["77c9633f72998c3815ffe21baa5f16d6e50f1e4a"],"af3e10d8a1fbcc5c79b22f7477e79de467dd326c":["b94236357aaa22b76c10629851fe4e376e0cea82"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["b94236357aaa22b76c10629851fe4e376e0cea82","af3e10d8a1fbcc5c79b22f7477e79de467dd326c","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["2209af2c265d2258ec4b29c8cc78622d36994a15"],"d4169a39455be564348e51f60d4dc9f77a80c2f9":["89424def13674ea17829b41c5883c54ecc31a132"],"94a5e240cfc05f5af141a42c7a36718fce82667f":["6afb0ba86024b96e8b34cfc2e15562239dc36360"],"962cd4f5e313777f35da8f521265323e84184929":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"e7b17e79a71117668ecbf8d3417c876e41396565":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"98b85aeea4d572e94fd5e6ba67043b5b363f1380":["c526352db87264a72a7a9ad68c1b769b81e54305"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"042da0877b8e28fd372a8ed80d11c4506a466ad7":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"781578ca5d3d2194bc55fc3457c62aef623db6f2":["77c9633f72998c3815ffe21baa5f16d6e50f1e4a","f7fd147f6ec145b926803480f62aa12e2aad258e"],"6afb0ba86024b96e8b34cfc2e15562239dc36360":["fb03700c9690d16b15fb4f56f6ec36b128fd894e"],"fb03700c9690d16b15fb4f56f6ec36b128fd894e":["ad4957cde742defe6db19689abdc267c5d948066"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}