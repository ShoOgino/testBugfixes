{"path":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState,FSTLoadMode).mjava","commits":[{"id":"763da4a9605e47013078edc323b9d4b608f0f9e0","date":1555353576,"type":1,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState,FSTLoadMode).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState,Lucene50PostingsFormat.FSTLoadMode).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state, FSTLoadMode defaultLoadMode) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final FSTLoadMode fstLoadMode = getLoadMode(state.readerAttributes, FST_MODE_KEY, defaultLoadMode);\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final FSTLoadMode perFieldLoadMode = getLoadMode(state.readerAttributes, FST_MODE_KEY + \".\" + fieldInfo.name, fstLoadMode);\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,\n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm, state.openedFromWriter, perFieldLoadMode));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state, Lucene50PostingsFormat.FSTLoadMode fstLoadMode) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm, state.openedFromWriter, fstLoadMode));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a6071cfb20c18ef694430df7330bc1aaa5e08667","date":1575293424,"type":3,"author":"Bruno Roustant","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState,FSTLoadMode).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState,FSTLoadMode).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state, FSTLoadMode defaultLoadMode) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final FSTLoadMode fstLoadMode = getLoadMode(state.readerAttributes, FST_MODE_KEY, defaultLoadMode);\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n      fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final FSTLoadMode perFieldLoadMode = getLoadMode(state.readerAttributes, FST_MODE_KEY + \".\" + fieldInfo.name, fstLoadMode);\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fieldMap.put(fieldInfo.name,\n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm, state.openedFromWriter, perFieldLoadMode));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      List<String> fieldList = new ArrayList<>(fieldMap.keySet());\n      fieldList.sort(null);\n      this.fieldList = Collections.unmodifiableList(fieldList);\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state, FSTLoadMode defaultLoadMode) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final FSTLoadMode fstLoadMode = getLoadMode(state.readerAttributes, FST_MODE_KEY, defaultLoadMode);\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final FSTLoadMode perFieldLoadMode = getLoadMode(state.readerAttributes, FST_MODE_KEY + \".\" + fieldInfo.name, fstLoadMode);\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,\n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm, state.openedFromWriter, perFieldLoadMode));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cb77022ef17ff655c519a3f6ecd393747ac88bcf","date":1578579386,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState,FSTLoadMode).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState,FSTLoadMode).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state, FSTLoadMode defaultLoadMode) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final FSTLoadMode fstLoadMode = getLoadMode(state.readerAttributes, FST_MODE_KEY, defaultLoadMode);\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n      fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        if (version < VERSION_META_LONGS_REMOVED) {\n          final int longsSize = termsIn.readVInt();\n          if (longsSize < 0) {\n            throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n          }\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final FSTLoadMode perFieldLoadMode = getLoadMode(state.readerAttributes, FST_MODE_KEY + \".\" + fieldInfo.name, fstLoadMode);\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fieldMap.put(fieldInfo.name,\n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, indexIn, minTerm, maxTerm, state.openedFromWriter, perFieldLoadMode));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      List<String> fieldList = new ArrayList<>(fieldMap.keySet());\n      fieldList.sort(null);\n      this.fieldList = Collections.unmodifiableList(fieldList);\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state, FSTLoadMode defaultLoadMode) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final FSTLoadMode fstLoadMode = getLoadMode(state.readerAttributes, FST_MODE_KEY, defaultLoadMode);\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n      fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final FSTLoadMode perFieldLoadMode = getLoadMode(state.readerAttributes, FST_MODE_KEY + \".\" + fieldInfo.name, fstLoadMode);\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fieldMap.put(fieldInfo.name,\n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm, state.openedFromWriter, perFieldLoadMode));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      List<String> fieldList = new ArrayList<>(fieldMap.keySet());\n      fieldList.sort(null);\n      this.fieldList = Collections.unmodifiableList(fieldList);\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"06ab276a5660cb79daae8c5ede063531c700a03a","date":1578587874,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState,FSTLoadMode).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState,FSTLoadMode).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state, FSTLoadMode defaultLoadMode) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final FSTLoadMode fstLoadMode = getLoadMode(state.readerAttributes, FST_MODE_KEY, defaultLoadMode);\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n      fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final FSTLoadMode perFieldLoadMode = getLoadMode(state.readerAttributes, FST_MODE_KEY + \".\" + fieldInfo.name, fstLoadMode);\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fieldMap.put(fieldInfo.name,\n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm, state.openedFromWriter, perFieldLoadMode));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      List<String> fieldList = new ArrayList<>(fieldMap.keySet());\n      fieldList.sort(null);\n      this.fieldList = Collections.unmodifiableList(fieldList);\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state, FSTLoadMode defaultLoadMode) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final FSTLoadMode fstLoadMode = getLoadMode(state.readerAttributes, FST_MODE_KEY, defaultLoadMode);\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n      fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        if (version < VERSION_META_LONGS_REMOVED) {\n          final int longsSize = termsIn.readVInt();\n          if (longsSize < 0) {\n            throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n          }\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final FSTLoadMode perFieldLoadMode = getLoadMode(state.readerAttributes, FST_MODE_KEY + \".\" + fieldInfo.name, fstLoadMode);\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fieldMap.put(fieldInfo.name,\n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, indexIn, minTerm, maxTerm, state.openedFromWriter, perFieldLoadMode));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      List<String> fieldList = new ArrayList<>(fieldMap.keySet());\n      fieldList.sort(null);\n      this.fieldList = Collections.unmodifiableList(fieldList);\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"08a5168e06e037794c0aba7f94f76ff3c09704d2","date":1579264785,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState,FSTLoadMode).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState,FSTLoadMode).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state, FSTLoadMode defaultLoadMode) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final FSTLoadMode fstLoadMode = getLoadMode(state.readerAttributes, FST_MODE_KEY, defaultLoadMode);\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n      fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        if (version < VERSION_META_LONGS_REMOVED) {\n          final int longsSize = termsIn.readVInt();\n          if (longsSize < 0) {\n            throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n          }\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final FSTLoadMode perFieldLoadMode = getLoadMode(state.readerAttributes, FST_MODE_KEY + \".\" + fieldInfo.name, fstLoadMode);\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fieldMap.put(fieldInfo.name,\n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, indexIn, minTerm, maxTerm, state.openedFromWriter, perFieldLoadMode));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      List<String> fieldList = new ArrayList<>(fieldMap.keySet());\n      fieldList.sort(null);\n      this.fieldList = Collections.unmodifiableList(fieldList);\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state, FSTLoadMode defaultLoadMode) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final FSTLoadMode fstLoadMode = getLoadMode(state.readerAttributes, FST_MODE_KEY, defaultLoadMode);\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n      fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final FSTLoadMode perFieldLoadMode = getLoadMode(state.readerAttributes, FST_MODE_KEY + \".\" + fieldInfo.name, fstLoadMode);\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fieldMap.put(fieldInfo.name,\n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm, state.openedFromWriter, perFieldLoadMode));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      List<String> fieldList = new ArrayList<>(fieldMap.keySet());\n      fieldList.sort(null);\n      this.fieldList = Collections.unmodifiableList(fieldList);\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a4e83191a3e02851a0b67e5335e6922f3e9ea86d","date":1583489709,"type":5,"author":"Bruno Roustant","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState,FSTLoadMode).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n      fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        if (version < VERSION_META_LONGS_REMOVED) {\n          final int longsSize = termsIn.readVInt();\n          if (longsSize < 0) {\n            throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n          }\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fieldMap.put(fieldInfo.name,\n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, indexIn, minTerm, maxTerm, state.openedFromWriter));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      List<String> fieldList = new ArrayList<>(fieldMap.keySet());\n      fieldList.sort(null);\n      this.fieldList = Collections.unmodifiableList(fieldList);\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state, FSTLoadMode defaultLoadMode) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final FSTLoadMode fstLoadMode = getLoadMode(state.readerAttributes, FST_MODE_KEY, defaultLoadMode);\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n      fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        if (version < VERSION_META_LONGS_REMOVED) {\n          final int longsSize = termsIn.readVInt();\n          if (longsSize < 0) {\n            throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n          }\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final FSTLoadMode perFieldLoadMode = getLoadMode(state.readerAttributes, FST_MODE_KEY + \".\" + fieldInfo.name, fstLoadMode);\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fieldMap.put(fieldInfo.name,\n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, indexIn, minTerm, maxTerm, state.openedFromWriter, perFieldLoadMode));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      List<String> fieldList = new ArrayList<>(fieldMap.keySet());\n      fieldList.sort(null);\n      this.fieldList = Collections.unmodifiableList(fieldList);\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"763da4a9605e47013078edc323b9d4b608f0f9e0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cb77022ef17ff655c519a3f6ecd393747ac88bcf":["a6071cfb20c18ef694430df7330bc1aaa5e08667"],"a6071cfb20c18ef694430df7330bc1aaa5e08667":["763da4a9605e47013078edc323b9d4b608f0f9e0"],"08a5168e06e037794c0aba7f94f76ff3c09704d2":["06ab276a5660cb79daae8c5ede063531c700a03a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a4e83191a3e02851a0b67e5335e6922f3e9ea86d"],"a4e83191a3e02851a0b67e5335e6922f3e9ea86d":["08a5168e06e037794c0aba7f94f76ff3c09704d2"],"06ab276a5660cb79daae8c5ede063531c700a03a":["cb77022ef17ff655c519a3f6ecd393747ac88bcf"]},"commit2Childs":{"763da4a9605e47013078edc323b9d4b608f0f9e0":["a6071cfb20c18ef694430df7330bc1aaa5e08667"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["763da4a9605e47013078edc323b9d4b608f0f9e0"],"cb77022ef17ff655c519a3f6ecd393747ac88bcf":["06ab276a5660cb79daae8c5ede063531c700a03a"],"a6071cfb20c18ef694430df7330bc1aaa5e08667":["cb77022ef17ff655c519a3f6ecd393747ac88bcf"],"08a5168e06e037794c0aba7f94f76ff3c09704d2":["a4e83191a3e02851a0b67e5335e6922f3e9ea86d"],"06ab276a5660cb79daae8c5ede063531c700a03a":["08a5168e06e037794c0aba7f94f76ff3c09704d2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"a4e83191a3e02851a0b67e5335e6922f3e9ea86d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}