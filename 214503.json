{"path":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","commits":[{"id":"14d5815ecbef89580f5c48990bcd433f04f8563a","date":1399564106,"type":0,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"/dev/null","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries < maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core \"+coreNeedingRecovery+\" on  \" + recoveryUrl + \n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\");              \n      } else {\n        log.info(\"Asking core \"+coreNeedingRecovery+\" on  \" + recoveryUrl + \" to recover\");              \n      }\n      \n      HttpSolrServer server = new HttpSolrServer(recoveryUrl);\n      try {\n        server.setSoTimeout(60000);\n        server.setConnectionTimeout(15000);\n        try {\n          server.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \"+CoreAdminAction.REQUESTRECOVERY+\n              \" command to core \"+coreNeedingRecovery+\" on \"+recoveryUrl);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      } finally {\n        server.shutdown();\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica \"+coreNeedingRecovery+\n              \" on \"+replicaNodeName+\" because my core container is shutdown.\");\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState(true);\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (continueTrying && collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            String lirState = \n                zkController.getLeaderInitiatedRecoveryState(collection, shardId, coreNeedingRecovery);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica \"+coreNeedingRecovery+\n                  \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (ZkStateReader.RECOVERING.equals(lirState)) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName, coreNeedingRecovery, null, null);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                String replicaState = replicaProps.get(0).getState();\n                if (ZkStateReader.ACTIVE.equals(replicaState)) {\n                  // replica published its state as \"active\", \n                  // which is bad if lirState is still \"down\"\n                  if (ZkStateReader.DOWN.equals(lirState)) {\n                    // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                    // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                    log.warn(\"Replica \"+coreNeedingRecovery+\" set to active but the leader thinks it should be in recovery;\"\n                        + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0));\n                    zkController.ensureReplicaInLeaderInitiatedRecovery(collection, \n                        shardId, replicaUrl, nodeProps, true); // force republish state to \"down\"\n                  }\n                }                    \n              }                    \n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of \"+coreNeedingRecovery+\" due to: \"+ignoreMe);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["22859cb40e09867e7da8de84a31956c07259f82f","3e0300d1df37d7e9662d491269e91b6f66dca8bd","d046deab00d0262b4d2c5b1d704faff97a5803a7","10399eb409ace56e6d66f136f184643f7432371d"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"07c824e7f6927860d366e4888be45e4db8c9e03b","date":1405193679,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries < maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n      \n      HttpSolrServer server = new HttpSolrServer(recoveryUrl);\n      try {\n        server.setSoTimeout(60000);\n        server.setConnectionTimeout(15000);\n        try {\n          server.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      } finally {\n        server.shutdown();\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is shutdown.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState(true);\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (continueTrying && collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            String lirState = \n                zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (ZkStateReader.RECOVERING.equals(lirState)) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName, coreNeedingRecovery, null, null);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                String replicaState = replicaProps.get(0).getState();\n                if (ZkStateReader.ACTIVE.equals(replicaState)) {\n                  // replica published its state as \"active\", \n                  // which is bad if lirState is still \"down\"\n                  if (ZkStateReader.DOWN.equals(lirState)) {\n                    // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                    // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                    log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                        + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                    zkController.ensureReplicaInLeaderInitiatedRecovery(collection, \n                        shardId, replicaUrl, nodeProps, true); // force republish state to \"down\"\n                  }\n                }                    \n              }                    \n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries < maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core \"+coreNeedingRecovery+\" on  \" + recoveryUrl + \n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\");              \n      } else {\n        log.info(\"Asking core \"+coreNeedingRecovery+\" on  \" + recoveryUrl + \" to recover\");              \n      }\n      \n      HttpSolrServer server = new HttpSolrServer(recoveryUrl);\n      try {\n        server.setSoTimeout(60000);\n        server.setConnectionTimeout(15000);\n        try {\n          server.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \"+CoreAdminAction.REQUESTRECOVERY+\n              \" command to core \"+coreNeedingRecovery+\" on \"+recoveryUrl);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      } finally {\n        server.shutdown();\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica \"+coreNeedingRecovery+\n              \" on \"+replicaNodeName+\" because my core container is shutdown.\");\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState(true);\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (continueTrying && collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            String lirState = \n                zkController.getLeaderInitiatedRecoveryState(collection, shardId, coreNeedingRecovery);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica \"+coreNeedingRecovery+\n                  \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (ZkStateReader.RECOVERING.equals(lirState)) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName, coreNeedingRecovery, null, null);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                String replicaState = replicaProps.get(0).getState();\n                if (ZkStateReader.ACTIVE.equals(replicaState)) {\n                  // replica published its state as \"active\", \n                  // which is bad if lirState is still \"down\"\n                  if (ZkStateReader.DOWN.equals(lirState)) {\n                    // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                    // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                    log.warn(\"Replica \"+coreNeedingRecovery+\" set to active but the leader thinks it should be in recovery;\"\n                        + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0));\n                    zkController.ensureReplicaInLeaderInitiatedRecovery(collection, \n                        shardId, replicaUrl, nodeProps, true); // force republish state to \"down\"\n                  }\n                }                    \n              }                    \n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of \"+coreNeedingRecovery+\" due to: \"+ignoreMe);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":null,"bugIntro":["d046deab00d0262b4d2c5b1d704faff97a5803a7"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"be2d2facad861c539d42173b0e4538d64b7fda80","date":1405194900,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries < maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n      \n      HttpSolrServer server = new HttpSolrServer(recoveryUrl);\n      try {\n        server.setSoTimeout(60000);\n        server.setConnectionTimeout(15000);\n        try {\n          server.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      } finally {\n        server.shutdown();\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is shutdown.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState(true);\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (continueTrying && collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            String lirState = \n                zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (ZkStateReader.RECOVERING.equals(lirState)) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                String replicaState = replicaProps.get(0).getState();\n                if (ZkStateReader.ACTIVE.equals(replicaState)) {\n                  // replica published its state as \"active\", \n                  // which is bad if lirState is still \"down\"\n                  if (ZkStateReader.DOWN.equals(lirState)) {\n                    // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                    // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                    log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                        + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                    zkController.ensureReplicaInLeaderInitiatedRecovery(collection, \n                        shardId, replicaUrl, nodeProps, true); // force republish state to \"down\"\n                  }\n                }                    \n              }                    \n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries < maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n      \n      HttpSolrServer server = new HttpSolrServer(recoveryUrl);\n      try {\n        server.setSoTimeout(60000);\n        server.setConnectionTimeout(15000);\n        try {\n          server.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      } finally {\n        server.shutdown();\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is shutdown.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState(true);\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (continueTrying && collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            String lirState = \n                zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (ZkStateReader.RECOVERING.equals(lirState)) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName, coreNeedingRecovery, null, null);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                String replicaState = replicaProps.get(0).getState();\n                if (ZkStateReader.ACTIVE.equals(replicaState)) {\n                  // replica published its state as \"active\", \n                  // which is bad if lirState is still \"down\"\n                  if (ZkStateReader.DOWN.equals(lirState)) {\n                    // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                    // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                    log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                        + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                    zkController.ensureReplicaInLeaderInitiatedRecovery(collection, \n                        shardId, replicaUrl, nodeProps, true); // force republish state to \"down\"\n                  }\n                }                    \n              }                    \n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":null,"bugIntro":["3e0300d1df37d7e9662d491269e91b6f66dca8bd"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries < maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n      \n      HttpSolrServer server = new HttpSolrServer(recoveryUrl);\n      try {\n        server.setSoTimeout(60000);\n        server.setConnectionTimeout(15000);\n        try {\n          server.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      } finally {\n        server.shutdown();\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is close.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState(true);\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (continueTrying && collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            String lirState = \n                zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (ZkStateReader.RECOVERING.equals(lirState)) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                String replicaState = replicaProps.get(0).getState();\n                if (ZkStateReader.ACTIVE.equals(replicaState)) {\n                  // replica published its state as \"active\", \n                  // which is bad if lirState is still \"down\"\n                  if (ZkStateReader.DOWN.equals(lirState)) {\n                    // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                    // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                    log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                        + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                    zkController.ensureReplicaInLeaderInitiatedRecovery(collection, \n                        shardId, replicaUrl, nodeProps, true); // force republish state to \"down\"\n                  }\n                }                    \n              }                    \n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries < maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n      \n      HttpSolrServer server = new HttpSolrServer(recoveryUrl);\n      try {\n        server.setSoTimeout(60000);\n        server.setConnectionTimeout(15000);\n        try {\n          server.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      } finally {\n        server.shutdown();\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is shutdown.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState(true);\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (continueTrying && collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            String lirState = \n                zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (ZkStateReader.RECOVERING.equals(lirState)) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                String replicaState = replicaProps.get(0).getState();\n                if (ZkStateReader.ACTIVE.equals(replicaState)) {\n                  // replica published its state as \"active\", \n                  // which is bad if lirState is still \"down\"\n                  if (ZkStateReader.DOWN.equals(lirState)) {\n                    // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                    // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                    log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                        + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                    zkController.ensureReplicaInLeaderInitiatedRecovery(collection, \n                        shardId, replicaUrl, nodeProps, true); // force republish state to \"down\"\n                  }\n                }                    \n              }                    \n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6e36353d7461af8d2329a78a71457cf8e3c1e88f","date":1411572107,"type":3,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n      \n      HttpSolrServer server = new HttpSolrServer(recoveryUrl);\n      try {\n        server.setSoTimeout(60000);\n        server.setConnectionTimeout(15000);\n        try {\n          server.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      } finally {\n        server.shutdown();\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState(true);\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (continueTrying && collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            String lirState = \n                zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (ZkStateReader.RECOVERING.equals(lirState)) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                String replicaState = replicaProps.get(0).getState();\n                if (ZkStateReader.ACTIVE.equals(replicaState)) {\n                  // replica published its state as \"active\", \n                  // which is bad if lirState is still \"down\"\n                  if (ZkStateReader.DOWN.equals(lirState)) {\n                    // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                    // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                    log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                        + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                    zkController.ensureReplicaInLeaderInitiatedRecovery(collection, \n                        shardId, replicaUrl, nodeProps, true); // force republish state to \"down\"\n                  }\n                }                    \n              }                    \n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries < maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n      \n      HttpSolrServer server = new HttpSolrServer(recoveryUrl);\n      try {\n        server.setSoTimeout(60000);\n        server.setConnectionTimeout(15000);\n        try {\n          server.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      } finally {\n        server.shutdown();\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is close.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState(true);\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (continueTrying && collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            String lirState = \n                zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (ZkStateReader.RECOVERING.equals(lirState)) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                String replicaState = replicaProps.get(0).getState();\n                if (ZkStateReader.ACTIVE.equals(replicaState)) {\n                  // replica published its state as \"active\", \n                  // which is bad if lirState is still \"down\"\n                  if (ZkStateReader.DOWN.equals(lirState)) {\n                    // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                    // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                    log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                        + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                    zkController.ensureReplicaInLeaderInitiatedRecovery(collection, \n                        shardId, replicaUrl, nodeProps, true); // force republish state to \"down\"\n                  }\n                }                    \n              }                    \n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bafca15d8e408346a67f4282ad1143b88023893b","date":1420034748,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n      \n      HttpSolrClient client = new HttpSolrClient(recoveryUrl);\n      try {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      } finally {\n        client.shutdown();\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState(true);\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (continueTrying && collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            String lirState = \n                zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (ZkStateReader.RECOVERING.equals(lirState)) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                String replicaState = replicaProps.get(0).getState();\n                if (ZkStateReader.ACTIVE.equals(replicaState)) {\n                  // replica published its state as \"active\", \n                  // which is bad if lirState is still \"down\"\n                  if (ZkStateReader.DOWN.equals(lirState)) {\n                    // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                    // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                    log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                        + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                    zkController.ensureReplicaInLeaderInitiatedRecovery(collection, \n                        shardId, replicaUrl, nodeProps, true); // force republish state to \"down\"\n                  }\n                }                    \n              }                    \n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n      \n      HttpSolrServer server = new HttpSolrServer(recoveryUrl);\n      try {\n        server.setSoTimeout(60000);\n        server.setConnectionTimeout(15000);\n        try {\n          server.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      } finally {\n        server.shutdown();\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState(true);\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (continueTrying && collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            String lirState = \n                zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (ZkStateReader.RECOVERING.equals(lirState)) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                String replicaState = replicaProps.get(0).getState();\n                if (ZkStateReader.ACTIVE.equals(replicaState)) {\n                  // replica published its state as \"active\", \n                  // which is bad if lirState is still \"down\"\n                  if (ZkStateReader.DOWN.equals(lirState)) {\n                    // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                    // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                    log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                        + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                    zkController.ensureReplicaInLeaderInitiatedRecovery(collection, \n                        shardId, replicaUrl, nodeProps, true); // force republish state to \"down\"\n                  }\n                }                    \n              }                    \n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d046deab00d0262b4d2c5b1d704faff97a5803a7","date":1421937368,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n      \n      HttpSolrClient client = new HttpSolrClient(recoveryUrl);\n      try {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      } finally {\n        client.shutdown();\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState(true);\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            String lirState = \n                zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (ZkStateReader.RECOVERING.equals(lirState)) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  if (replicaCoreNodeName.equals(((Replica) prop.getNodeProps()).getName())) {\n                    String replicaState = prop.getState();\n                    if (ZkStateReader.ACTIVE.equals(replicaState)) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (ZkStateReader.DOWN.equals(lirState)) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                            shardId, replicaUrl, nodeProps, true); // force republish state to \"down\"\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n      \n      HttpSolrClient client = new HttpSolrClient(recoveryUrl);\n      try {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      } finally {\n        client.shutdown();\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState(true);\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (continueTrying && collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            String lirState = \n                zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (ZkStateReader.RECOVERING.equals(lirState)) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                String replicaState = replicaProps.get(0).getState();\n                if (ZkStateReader.ACTIVE.equals(replicaState)) {\n                  // replica published its state as \"active\", \n                  // which is bad if lirState is still \"down\"\n                  if (ZkStateReader.DOWN.equals(lirState)) {\n                    // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                    // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                    log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                        + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                    zkController.ensureReplicaInLeaderInitiatedRecovery(collection, \n                        shardId, replicaUrl, nodeProps, true); // force republish state to \"down\"\n                  }\n                }                    \n              }                    \n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":["07c824e7f6927860d366e4888be45e4db8c9e03b","14d5815ecbef89580f5c48990bcd433f04f8563a"],"bugIntro":["dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc","22859cb40e09867e7da8de84a31956c07259f82f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cc3b13b430571c2e169f98fe38e1e7666f88522d","date":1422446157,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient(recoveryUrl)) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState(true);\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            String lirState = \n                zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (ZkStateReader.RECOVERING.equals(lirState)) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  if (replicaCoreNodeName.equals(((Replica) prop.getNodeProps()).getName())) {\n                    String replicaState = prop.getState();\n                    if (ZkStateReader.ACTIVE.equals(replicaState)) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (ZkStateReader.DOWN.equals(lirState)) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                            shardId, replicaUrl, nodeProps, true); // force republish state to \"down\"\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n      \n      HttpSolrClient client = new HttpSolrClient(recoveryUrl);\n      try {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      } finally {\n        client.shutdown();\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState(true);\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            String lirState = \n                zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (ZkStateReader.RECOVERING.equals(lirState)) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  if (replicaCoreNodeName.equals(((Replica) prop.getNodeProps()).getName())) {\n                    String replicaState = prop.getState();\n                    if (ZkStateReader.ACTIVE.equals(replicaState)) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (ZkStateReader.DOWN.equals(lirState)) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                            shardId, replicaUrl, nodeProps, true); // force republish state to \"down\"\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc","date":1426444850,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient(recoveryUrl)) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState(true);\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            String lirState = \n                zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (ZkStateReader.RECOVERING.equals(lirState)) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  if (replicaCoreNodeName.equals(((Replica) prop.getNodeProps()).getName())) {\n                    String replicaState = prop.getState();\n                    if (ZkStateReader.ACTIVE.equals(replicaState)) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (ZkStateReader.DOWN.equals(lirState)) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        // force republish state to \"down\"\n                        zkController.ensureReplicaInLeaderInitiatedRecovery(collection, shardId, nodeProps, true, leaderCoreNodeName);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient(recoveryUrl)) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState(true);\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            String lirState = \n                zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (ZkStateReader.RECOVERING.equals(lirState)) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  if (replicaCoreNodeName.equals(((Replica) prop.getNodeProps()).getName())) {\n                    String replicaState = prop.getState();\n                    if (ZkStateReader.ACTIVE.equals(replicaState)) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (ZkStateReader.DOWN.equals(lirState)) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                            shardId, replicaUrl, nodeProps, true); // force republish state to \"down\"\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":["d046deab00d0262b4d2c5b1d704faff97a5803a7"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0932eb10135843758b2ca508d5aa2b4798aa07f9","date":1426947197,"type":3,"author":"Ramkumar Aiyengar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient(recoveryUrl)) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState(true);\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            String lirState = \n                zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (ZkStateReader.RECOVERING.equals(lirState)) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  if (replicaCoreNodeName.equals(((Replica) prop.getNodeProps()).getName())) {\n                    String replicaState = prop.getState();\n                    if (ZkStateReader.ACTIVE.equals(replicaState)) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (ZkStateReader.DOWN.equals(lirState)) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        zkController.ensureReplicaInLeaderInitiatedRecovery(\n                            collection, shardId, nodeProps, leaderCoreNodeName,\n                            true /* forcePublishState */, true /* retryOnConnLoss */\n                        );\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient(recoveryUrl)) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState(true);\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            String lirState = \n                zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (ZkStateReader.RECOVERING.equals(lirState)) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  if (replicaCoreNodeName.equals(((Replica) prop.getNodeProps()).getName())) {\n                    String replicaState = prop.getState();\n                    if (ZkStateReader.ACTIVE.equals(replicaState)) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (ZkStateReader.DOWN.equals(lirState)) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        // force republish state to \"down\"\n                        zkController.ensureReplicaInLeaderInitiatedRecovery(collection, shardId, nodeProps, true, leaderCoreNodeName);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":null,"bugIntro":["22859cb40e09867e7da8de84a31956c07259f82f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient(recoveryUrl)) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState(true);\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            String lirState = \n                zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (ZkStateReader.RECOVERING.equals(lirState)) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  if (replicaCoreNodeName.equals(((Replica) prop.getNodeProps()).getName())) {\n                    String replicaState = prop.getState();\n                    if (ZkStateReader.ACTIVE.equals(replicaState)) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (ZkStateReader.DOWN.equals(lirState)) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        zkController.ensureReplicaInLeaderInitiatedRecovery(\n                            collection, shardId, nodeProps, leaderCoreNodeName,\n                            true /* forcePublishState */, true /* retryOnConnLoss */\n                        );\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient(recoveryUrl)) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState(true);\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            String lirState = \n                zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (ZkStateReader.RECOVERING.equals(lirState)) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  if (replicaCoreNodeName.equals(((Replica) prop.getNodeProps()).getName())) {\n                    String replicaState = prop.getState();\n                    if (ZkStateReader.ACTIVE.equals(replicaState)) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (ZkStateReader.DOWN.equals(lirState)) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                            shardId, replicaUrl, nodeProps, true); // force republish state to \"down\"\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a219f1dcad1700e84807666bdbd2b573e8de7021","date":1428130940,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient(recoveryUrl)) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState(true);\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        zkController.ensureReplicaInLeaderInitiatedRecovery(\n                            collection, shardId, nodeProps, leaderCoreNodeName,\n                            true /* forcePublishState */, true /* retryOnConnLoss */\n                        );\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient(recoveryUrl)) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState(true);\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            String lirState = \n                zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (ZkStateReader.RECOVERING.equals(lirState)) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  if (replicaCoreNodeName.equals(((Replica) prop.getNodeProps()).getName())) {\n                    String replicaState = prop.getState();\n                    if (ZkStateReader.ACTIVE.equals(replicaState)) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (ZkStateReader.DOWN.equals(lirState)) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        zkController.ensureReplicaInLeaderInitiatedRecovery(\n                            collection, shardId, nodeProps, leaderCoreNodeName,\n                            true /* forcePublishState */, true /* retryOnConnLoss */\n                        );\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"102da6baafc0f534a59f31729343dbab9d3b9e9a","date":1438410244,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient(recoveryUrl)) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState();\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        zkController.ensureReplicaInLeaderInitiatedRecovery(\n                            collection, shardId, nodeProps, leaderCoreNodeName,\n                            true /* forcePublishState */, true /* retryOnConnLoss */\n                        );\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient(recoveryUrl)) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState(true);\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        zkController.ensureReplicaInLeaderInitiatedRecovery(\n                            collection, shardId, nodeProps, leaderCoreNodeName,\n                            true /* forcePublishState */, true /* retryOnConnLoss */\n                        );\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"22859cb40e09867e7da8de84a31956c07259f82f","date":1441822065,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n\n    String replicaCoreName = nodeProps.getCoreName();\n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient(recoveryUrl)) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState();\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \" + replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        publishDownState(replicaCoreName, replicaCoreNodeName, replicaNodeName, replicaUrl, true);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n        \n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient(recoveryUrl)) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState();\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \"+replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        zkController.ensureReplicaInLeaderInitiatedRecovery(\n                            collection, shardId, nodeProps, leaderCoreNodeName,\n                            true /* forcePublishState */, true /* retryOnConnLoss */\n                        );\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":["14d5815ecbef89580f5c48990bcd433f04f8563a","d046deab00d0262b4d2c5b1d704faff97a5803a7","0932eb10135843758b2ca508d5aa2b4798aa07f9"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3e0300d1df37d7e9662d491269e91b6f66dca8bd","date":1443011762,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n\n    String replicaCoreName = nodeProps.getCoreName();\n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient(recoveryUrl)) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState();\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n          if (!leaderCd.getCloudDescriptor().isLeader()) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader!\");\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String lcnn = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, lcnn);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \" + replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        publishDownState(replicaCoreName, replicaCoreNodeName, replicaNodeName, replicaUrl, true);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n\n    String replicaCoreName = nodeProps.getCoreName();\n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient(recoveryUrl)) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState();\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String leaderCoreNodeName = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, leaderCoreNodeName);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \" + replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        publishDownState(replicaCoreName, replicaCoreNodeName, replicaNodeName, replicaUrl, true);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":["be2d2facad861c539d42173b0e4538d64b7fda80","14d5815ecbef89580f5c48990bcd433f04f8563a"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"10399eb409ace56e6d66f136f184643f7432371d","date":1446743029,"type":3,"author":"Christine Poerschke","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n\n    String replicaCoreName = nodeProps.getCoreName();\n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient(recoveryUrl)) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Exception t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState();\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n          if (!leaderCd.getCloudDescriptor().isLeader()) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader!\");\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String lcnn = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, lcnn);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \" + replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        publishDownState(replicaCoreName, replicaCoreNodeName, replicaNodeName, replicaUrl, true);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n\n    String replicaCoreName = nodeProps.getCoreName();\n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient(recoveryUrl)) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Throwable t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState();\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n          if (!leaderCd.getCloudDescriptor().isLeader()) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader!\");\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String lcnn = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, lcnn);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \" + replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        publishDownState(replicaCoreName, replicaCoreNodeName, replicaNodeName, replicaUrl, true);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":["14d5815ecbef89580f5c48990bcd433f04f8563a"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f","date":1457343183,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n\n    String replicaCoreName = nodeProps.getCoreName();\n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient(recoveryUrl)) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Exception t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n          if (!leaderCd.getCloudDescriptor().isLeader()) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader!\");\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String lcnn = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, lcnn);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \" + replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        publishDownState(replicaCoreName, replicaCoreNodeName, replicaNodeName, replicaUrl, true);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n\n    String replicaCoreName = nodeProps.getCoreName();\n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient(recoveryUrl)) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Exception t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        try {\n          zkStateReader.updateClusterState();\n        } catch (Exception exc) {\n          log.warn(\"Error when updating cluster state: \"+exc);\n        }        \n        \n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n          if (!leaderCd.getCloudDescriptor().isLeader()) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader!\");\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String lcnn = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, lcnn);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \" + replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        publishDownState(replicaCoreName, replicaCoreNodeName, replicaNodeName, replicaUrl, true);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e3c94a8b8bf47db4f968d9ae510ec8bbe1372088","date":1460069869,"type":3,"author":"Anshum Gupta","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n\n    String replicaCoreName = nodeProps.getCoreName();\n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient.Builder(recoveryUrl).build()) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Exception t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n          if (!leaderCd.getCloudDescriptor().isLeader()) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader!\");\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String lcnn = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, lcnn);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \" + replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        publishDownState(replicaCoreName, replicaCoreNodeName, replicaNodeName, replicaUrl, true);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n\n    String replicaCoreName = nodeProps.getCoreName();\n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient(recoveryUrl)) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Exception t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n          if (!leaderCd.getCloudDescriptor().isLeader()) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader!\");\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String lcnn = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, lcnn);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \" + replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        publishDownState(replicaCoreName, replicaCoreNodeName, replicaNodeName, replicaUrl, true);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5bdaf2cee03ff78b0a0cbf23df0095a3590b493b","date":1460110033,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n\n    String replicaCoreName = nodeProps.getCoreName();\n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient.Builder(recoveryUrl).build()) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Exception t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n          if (!leaderCd.getCloudDescriptor().isLeader()) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader!\");\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String lcnn = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, lcnn);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \" + replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        publishDownState(replicaCoreName, replicaCoreNodeName, replicaNodeName, replicaUrl, true);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n\n    String replicaCoreName = nodeProps.getCoreName();\n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient(recoveryUrl)) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Exception t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n          if (!leaderCd.getCloudDescriptor().isLeader()) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader!\");\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String lcnn = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, lcnn);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \" + replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        publishDownState(replicaCoreName, replicaCoreNodeName, replicaNodeName, replicaUrl, true);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"43d1e498704edd2bba13548a189eed4dfccff11b","date":1499143458,"type":3,"author":"Anshum Gupta","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n\n    String replicaCoreName = nodeProps.getCoreName();\n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient.Builder(recoveryUrl)\n          .withConnectionTimeout(15000)\n          .withSocketTimeout(60000)\n          .build()) {\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Exception t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n          if (!leaderCd.getCloudDescriptor().isLeader()) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader!\");\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String lcnn = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, lcnn);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \" + replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        publishDownState(replicaCoreName, replicaCoreNodeName, replicaNodeName, replicaUrl, true);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n\n    String replicaCoreName = nodeProps.getCoreName();\n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient.Builder(recoveryUrl).build()) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Exception t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n          if (!leaderCd.getCloudDescriptor().isLeader()) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader!\");\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String lcnn = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, lcnn);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \" + replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        publishDownState(replicaCoreName, replicaCoreNodeName, replicaNodeName, replicaUrl, true);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2ea161f828a3a7a6eb9410a431aecda6d7ab1065","date":1499213384,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n\n    String replicaCoreName = nodeProps.getCoreName();\n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient.Builder(recoveryUrl)\n          .withConnectionTimeout(15000)\n          .withSocketTimeout(60000)\n          .build()) {\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Exception t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n          if (!leaderCd.getCloudDescriptor().isLeader()) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader!\");\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String lcnn = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, lcnn);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \" + replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        publishDownState(replicaCoreName, replicaCoreNodeName, replicaNodeName, replicaUrl, true);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n\n    String replicaCoreName = nodeProps.getCoreName();\n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient.Builder(recoveryUrl).build()) {\n        client.setSoTimeout(60000);\n        client.setConnectionTimeout(15000);\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Exception t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n          if (!leaderCd.getCloudDescriptor().isLeader()) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader!\");\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String lcnn = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, lcnn);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \" + replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        publishDownState(replicaCoreName, replicaCoreNodeName, replicaNodeName, replicaUrl, true);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29252e837df815b8d01fd6dff973126cced351c5","date":1521709907,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n\n    String replicaCoreName = nodeProps.getCoreName();\n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient.Builder(recoveryUrl)\n          .withConnectionTimeout(15000)\n          .withSocketTimeout(60000)\n          .build()) {\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Exception t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException ||\n                  rootCause instanceof UnknownHostException);\n\n          if (!wasCommError) {\n            continueTrying = false;\n          }\n\n          if (CloudUtil.replicaExists(zkController.getClusterState(), collection, shardId, replicaCoreNodeName)) {\n            SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover, wasCommError:\"+wasCommError, t);\n          } else {\n            log.info(\"Replica {} is removed, hence remove its lir state\", replicaCoreNodeName);\n            removeLIRState(replicaCoreNodeName);\n            break;\n          }\n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n          if (!leaderCd.getCloudDescriptor().isLeader()) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader!\");\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String lcnn = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, lcnn);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \" + replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        publishDownState(replicaCoreName, replicaCoreNodeName, replicaNodeName, replicaUrl, true);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n\n    String replicaCoreName = nodeProps.getCoreName();\n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient.Builder(recoveryUrl)\n          .withConnectionTimeout(15000)\n          .withSocketTimeout(60000)\n          .build()) {\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Exception t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n          if (!leaderCd.getCloudDescriptor().isLeader()) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader!\");\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String lcnn = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, lcnn);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \" + replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        publishDownState(replicaCoreName, replicaCoreNodeName, replicaNodeName, replicaUrl, true);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":null,"bugIntro":["6f6e1db00b87e3b03cb04cfa726108aff894a5b6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6815b5b5d6334b2245dd7be2f8b6cca949bf7f43","date":1521731438,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n\n    String replicaCoreName = nodeProps.getCoreName();\n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient.Builder(recoveryUrl)\n          .withConnectionTimeout(15000)\n          .withSocketTimeout(60000)\n          .build()) {\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Exception t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException ||\n                  rootCause instanceof UnknownHostException);\n\n          if (!wasCommError) {\n            continueTrying = false;\n          }\n\n          if (CloudUtil.replicaExists(zkController.getClusterState(), collection, shardId, replicaCoreNodeName)) {\n            SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover, wasCommError:\"+wasCommError, t);\n          } else {\n            log.info(\"Replica {} is removed, hence remove its lir state\", replicaCoreNodeName);\n            removeLIRState(replicaCoreNodeName);\n            break;\n          }\n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n          if (!leaderCd.getCloudDescriptor().isLeader()) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader!\");\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String lcnn = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, lcnn);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \" + replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        publishDownState(replicaCoreName, replicaCoreNodeName, replicaNodeName, replicaUrl, true);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n\n    String replicaCoreName = nodeProps.getCoreName();\n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient.Builder(recoveryUrl)\n          .withConnectionTimeout(15000)\n          .withSocketTimeout(60000)\n          .build()) {\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Exception t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException);\n\n          SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          \n          if (!wasCommError) {\n            continueTrying = false;\n          }                                                \n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n          if (!leaderCd.getCloudDescriptor().isLeader()) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader!\");\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String lcnn = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, lcnn);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \" + replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        publishDownState(replicaCoreName, replicaCoreNodeName, replicaNodeName, replicaUrl, true);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6f6e1db00b87e3b03cb04cfa726108aff894a5b6","date":1522323487,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n\n    String replicaCoreName = nodeProps.getCoreName();\n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient.Builder(recoveryUrl)\n          .withConnectionTimeout(15000)\n          .withSocketTimeout(60000)\n          .build()) {\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Exception t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException ||\n                  rootCause instanceof UnknownHostException);\n\n          if (!wasCommError) {\n            continueTrying = false;\n          }\n\n          if (rootCause.getMessage().contains(\"Unable to locate core\")) {\n            log.info(\"Replica {} is removed, hence remove its lir state\", replicaCoreNodeName);\n            removeLIRState(replicaCoreNodeName);\n            break;\n          } else {\n            SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover, wasCommError:\"+wasCommError, t);\n          }\n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n          if (!leaderCd.getCloudDescriptor().isLeader()) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader!\");\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String lcnn = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, lcnn);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \" + replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        publishDownState(replicaCoreName, replicaCoreNodeName, replicaNodeName, replicaUrl, true);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n\n    String replicaCoreName = nodeProps.getCoreName();\n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient.Builder(recoveryUrl)\n          .withConnectionTimeout(15000)\n          .withSocketTimeout(60000)\n          .build()) {\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Exception t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException ||\n                  rootCause instanceof UnknownHostException);\n\n          if (!wasCommError) {\n            continueTrying = false;\n          }\n\n          if (CloudUtil.replicaExists(zkController.getClusterState(), collection, shardId, replicaCoreNodeName)) {\n            SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover, wasCommError:\"+wasCommError, t);\n          } else {\n            log.info(\"Replica {} is removed, hence remove its lir state\", replicaCoreNodeName);\n            removeLIRState(replicaCoreNodeName);\n            break;\n          }\n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n          if (!leaderCd.getCloudDescriptor().isLeader()) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader!\");\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String lcnn = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, lcnn);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \" + replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        publishDownState(replicaCoreName, replicaCoreNodeName, replicaNodeName, replicaUrl, true);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":["29252e837df815b8d01fd6dff973126cced351c5"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"43564cbb30b064675027cfb569564e8531096e97","date":1522334265,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n\n    String replicaCoreName = nodeProps.getCoreName();\n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient.Builder(recoveryUrl)\n          .withConnectionTimeout(15000)\n          .withSocketTimeout(60000)\n          .build()) {\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Exception t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException ||\n                  rootCause instanceof UnknownHostException);\n\n          if (!wasCommError) {\n            continueTrying = false;\n          }\n\n          if (rootCause.getMessage().contains(\"Unable to locate core\")) {\n            log.info(\"Replica {} is removed, hence remove its lir state\", replicaCoreNodeName);\n            removeLIRState(replicaCoreNodeName);\n            break;\n          } else {\n            SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover, wasCommError:\"+wasCommError, t);\n          }\n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n          if (!leaderCd.getCloudDescriptor().isLeader()) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader!\");\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String lcnn = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, lcnn);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \" + replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        publishDownState(replicaCoreName, replicaCoreNodeName, replicaNodeName, replicaUrl, true);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n\n    String replicaCoreName = nodeProps.getCoreName();\n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient.Builder(recoveryUrl)\n          .withConnectionTimeout(15000)\n          .withSocketTimeout(60000)\n          .build()) {\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Exception t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException ||\n                  rootCause instanceof UnknownHostException);\n\n          if (!wasCommError) {\n            continueTrying = false;\n          }\n\n          if (CloudUtil.replicaExists(zkController.getClusterState(), collection, shardId, replicaCoreNodeName)) {\n            SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover, wasCommError:\"+wasCommError, t);\n          } else {\n            log.info(\"Replica {} is removed, hence remove its lir state\", replicaCoreNodeName);\n            removeLIRState(replicaCoreNodeName);\n            break;\n          }\n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n          if (!leaderCd.getCloudDescriptor().isLeader()) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader!\");\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String lcnn = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, lcnn);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \" + replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        publishDownState(replicaCoreName, replicaCoreNodeName, replicaNodeName, replicaUrl, true);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180","date":1539076849,"type":4,"author":"Cao Manh Dat","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#sendRecoveryCommandWithRetry().mjava","sourceNew":null,"sourceOld":"  protected void sendRecoveryCommandWithRetry() throws Exception {    \n    int tries = 0;\n    long waitBetweenTriesMs = 5000L;\n    boolean continueTrying = true;\n\n    String replicaCoreName = nodeProps.getCoreName();\n    String recoveryUrl = nodeProps.getBaseUrl();\n    String replicaNodeName = nodeProps.getNodeName();\n    String coreNeedingRecovery = nodeProps.getCoreName();\n    String replicaCoreNodeName = ((Replica) nodeProps.getNodeProps()).getName();\n    String replicaUrl = nodeProps.getCoreUrl();\n    \n    log.info(getName()+\" started running to send REQUESTRECOVERY command to \"+replicaUrl+\n        \"; will try for a max of \"+(maxTries * (waitBetweenTriesMs/1000))+\" secs\");\n\n    RequestRecovery recoverRequestCmd = new RequestRecovery();\n    recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n    recoverRequestCmd.setCoreName(coreNeedingRecovery);\n    \n    while (continueTrying && ++tries <= maxTries) {\n      if (tries > 1) {\n        log.warn(\"Asking core={} coreNodeName={} on \" + recoveryUrl +\n            \" to recover; unsuccessful after \"+tries+\" of \"+maxTries+\" attempts so far ...\", coreNeedingRecovery, replicaCoreNodeName);\n      } else {\n        log.info(\"Asking core={} coreNodeName={} on \" + recoveryUrl + \" to recover\", coreNeedingRecovery, replicaCoreNodeName);\n      }\n\n      try (HttpSolrClient client = new HttpSolrClient.Builder(recoveryUrl)\n          .withConnectionTimeout(15000)\n          .withSocketTimeout(60000)\n          .build()) {\n        try {\n          client.request(recoverRequestCmd);\n          \n          log.info(\"Successfully sent \" + CoreAdminAction.REQUESTRECOVERY +\n              \" command to core={} coreNodeName={} on \" + recoveryUrl, coreNeedingRecovery, replicaCoreNodeName);\n          \n          continueTrying = false; // succeeded, so stop looping\n        } catch (Exception t) {\n          Throwable rootCause = SolrException.getRootCause(t);\n          boolean wasCommError =\n              (rootCause instanceof ConnectException ||\n                  rootCause instanceof ConnectTimeoutException ||\n                  rootCause instanceof NoHttpResponseException ||\n                  rootCause instanceof SocketException ||\n                  rootCause instanceof UnknownHostException);\n\n          if (!wasCommError) {\n            continueTrying = false;\n          }\n\n          if (rootCause.getMessage().contains(\"Unable to locate core\")) {\n            log.info(\"Replica {} is removed, hence remove its lir state\", replicaCoreNodeName);\n            removeLIRState(replicaCoreNodeName);\n            break;\n          } else {\n            SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover, wasCommError:\"+wasCommError, t);\n          }\n        }\n      }\n      \n      // wait a few seconds\n      if (continueTrying) {\n        try {\n          Thread.sleep(waitBetweenTriesMs);\n        } catch (InterruptedException ignoreMe) {\n          Thread.currentThread().interrupt();          \n        }\n        \n        if (coreContainer.isShutDown()) {\n          log.warn(\"Stop trying to send recovery command to downed replica core={} coreNodeName={} on \"\n              + replicaNodeName + \" because my core container is closed.\", coreNeedingRecovery, replicaCoreNodeName);\n          continueTrying = false;\n          break;\n        }\n        \n        // see if the replica's node is still live, if not, no need to keep doing this loop\n        ZkStateReader zkStateReader = zkController.getZkStateReader();\n        if (!zkStateReader.getClusterState().liveNodesContain(replicaNodeName)) {\n          log.warn(\"Node \"+replicaNodeName+\" hosting core \"+coreNeedingRecovery+\n              \" is no longer live. No need to keep trying to tell it to recover!\");\n          continueTrying = false;\n          break;\n        }\n\n        String leaderCoreNodeName = leaderCd.getCloudDescriptor().getCoreNodeName();\n        // stop trying if I'm no longer the leader\n        if (leaderCoreNodeName != null && collection != null) {\n          String leaderCoreNodeNameFromZk = null;\n          try {\n            leaderCoreNodeNameFromZk = zkController.getZkStateReader().getLeaderRetry(collection, shardId, 1000).getName();\n          } catch (Exception exc) {\n            log.error(\"Failed to determine if \" + leaderCoreNodeName + \" is still the leader for \" + collection +\n                \" \" + shardId + \" before starting leader-initiated recovery thread for \" + replicaUrl + \" due to: \" + exc);\n          }\n          if (!leaderCoreNodeName.equals(leaderCoreNodeNameFromZk)) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader! New leader is \" + leaderCoreNodeNameFromZk);\n            continueTrying = false;\n            break;\n          }\n          if (!leaderCd.getCloudDescriptor().isLeader()) {\n            log.warn(\"Stop trying to send recovery command to downed replica core=\" + coreNeedingRecovery +\n                \",coreNodeName=\" + replicaCoreNodeName + \" on \" + replicaNodeName + \" because \" +\n                leaderCoreNodeName + \" is no longer the leader!\");\n            continueTrying = false;\n            break;\n          }\n        }\n\n        // additional safeguard against the replica trying to be in the active state\n        // before acknowledging the leader initiated recovery command\n        if (collection != null && shardId != null) {\n          try {\n            // call out to ZooKeeper to get the leader-initiated recovery state\n            final Replica.State lirState = zkController.getLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName);\n            \n            if (lirState == null) {\n              log.warn(\"Stop trying to send recovery command to downed replica core=\"+coreNeedingRecovery+\n                  \",coreNodeName=\" + replicaCoreNodeName + \" on \"+replicaNodeName+\" because the znode no longer exists.\");\n              continueTrying = false;\n              break;              \n            }\n            \n            if (lirState == Replica.State.RECOVERING) {\n              // replica has ack'd leader initiated recovery and entered the recovering state\n              // so we don't need to keep looping to send the command\n              continueTrying = false;  \n              log.info(\"Replica \"+coreNeedingRecovery+\n                  \" on node \"+replicaNodeName+\" ack'd the leader initiated recovery state, \"\n                      + \"no need to keep trying to send recovery command\");\n            } else {\n              String lcnn = zkStateReader.getLeaderRetry(collection, shardId, 5000).getName();\n              List<ZkCoreNodeProps> replicaProps = \n                  zkStateReader.getReplicaProps(collection, shardId, lcnn);\n              if (replicaProps != null && replicaProps.size() > 0) {\n                for (ZkCoreNodeProps prop : replicaProps) {\n                  final Replica replica = (Replica) prop.getNodeProps();\n                  if (replicaCoreNodeName.equals(replica.getName())) {\n                    if (replica.getState() == Replica.State.ACTIVE) {\n                      // replica published its state as \"active\",\n                      // which is bad if lirState is still \"down\"\n                      if (lirState == Replica.State.DOWN) {\n                        // OK, so the replica thinks it is active, but it never ack'd the leader initiated recovery\n                        // so its state cannot be trusted and it needs to be told to recover again ... and we keep looping here\n                        log.warn(\"Replica core={} coreNodeName={} set to active but the leader thinks it should be in recovery;\"\n                            + \" forcing it back to down state to re-run the leader-initiated recovery process; props: \" + replicaProps.get(0), coreNeedingRecovery, replicaCoreNodeName);\n                        publishDownState(replicaCoreName, replicaCoreNodeName, replicaNodeName, replicaUrl, true);\n                      }\n                    }\n                    break;\n                  }\n                }\n              }\n            }                  \n          } catch (Exception ignoreMe) {\n            log.warn(\"Failed to determine state of core={} coreNodeName={} due to: \"+ignoreMe, coreNeedingRecovery, replicaCoreNodeName);\n            // eventually this loop will exhaust max tries and stop so we can just log this for now\n          }                \n        }\n      }\n    }\n    \n    // replica is no longer in recovery on this node (may be handled on another node)\n    zkController.removeReplicaFromLeaderInitiatedRecoveryHandling(replicaUrl);\n    \n    if (continueTrying) {\n      // ugh! this means the loop timed out before the recovery command could be delivered\n      // how exotic do we want to get here?\n      log.error(\"Timed out after waiting for \"+(tries * (waitBetweenTriesMs/1000))+\n          \" secs to send the recovery request to: \"+replicaUrl+\"; not much more we can do here?\");\n      \n      // TODO: need to raise a JMX event to allow monitoring tools to take over from here\n      \n    }    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc":["cc3b13b430571c2e169f98fe38e1e7666f88522d"],"6815b5b5d6334b2245dd7be2f8b6cca949bf7f43":["43d1e498704edd2bba13548a189eed4dfccff11b","29252e837df815b8d01fd6dff973126cced351c5"],"e3c94a8b8bf47db4f968d9ae510ec8bbe1372088":["7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f"],"6f6e1db00b87e3b03cb04cfa726108aff894a5b6":["6815b5b5d6334b2245dd7be2f8b6cca949bf7f43"],"bafca15d8e408346a67f4282ad1143b88023893b":["6e36353d7461af8d2329a78a71457cf8e3c1e88f"],"b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180":["43564cbb30b064675027cfb569564e8531096e97"],"7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f":["10399eb409ace56e6d66f136f184643f7432371d"],"a219f1dcad1700e84807666bdbd2b573e8de7021":["0932eb10135843758b2ca508d5aa2b4798aa07f9"],"43d1e498704edd2bba13548a189eed4dfccff11b":["5bdaf2cee03ff78b0a0cbf23df0095a3590b493b"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["cc3b13b430571c2e169f98fe38e1e7666f88522d","0932eb10135843758b2ca508d5aa2b4798aa07f9"],"22859cb40e09867e7da8de84a31956c07259f82f":["102da6baafc0f534a59f31729343dbab9d3b9e9a"],"2ea161f828a3a7a6eb9410a431aecda6d7ab1065":["5bdaf2cee03ff78b0a0cbf23df0095a3590b493b","43d1e498704edd2bba13548a189eed4dfccff11b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cc3b13b430571c2e169f98fe38e1e7666f88522d":["d046deab00d0262b4d2c5b1d704faff97a5803a7"],"14d5815ecbef89580f5c48990bcd433f04f8563a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"0932eb10135843758b2ca508d5aa2b4798aa07f9":["dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc"],"6e36353d7461af8d2329a78a71457cf8e3c1e88f":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"43564cbb30b064675027cfb569564e8531096e97":["6815b5b5d6334b2245dd7be2f8b6cca949bf7f43","6f6e1db00b87e3b03cb04cfa726108aff894a5b6"],"3e0300d1df37d7e9662d491269e91b6f66dca8bd":["22859cb40e09867e7da8de84a31956c07259f82f"],"be2d2facad861c539d42173b0e4538d64b7fda80":["07c824e7f6927860d366e4888be45e4db8c9e03b"],"102da6baafc0f534a59f31729343dbab9d3b9e9a":["a219f1dcad1700e84807666bdbd2b573e8de7021"],"d046deab00d0262b4d2c5b1d704faff97a5803a7":["bafca15d8e408346a67f4282ad1143b88023893b"],"29252e837df815b8d01fd6dff973126cced351c5":["43d1e498704edd2bba13548a189eed4dfccff11b"],"07c824e7f6927860d366e4888be45e4db8c9e03b":["14d5815ecbef89580f5c48990bcd433f04f8563a"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["be2d2facad861c539d42173b0e4538d64b7fda80"],"5bdaf2cee03ff78b0a0cbf23df0095a3590b493b":["7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f","e3c94a8b8bf47db4f968d9ae510ec8bbe1372088"],"10399eb409ace56e6d66f136f184643f7432371d":["3e0300d1df37d7e9662d491269e91b6f66dca8bd"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180"]},"commit2Childs":{"dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc":["0932eb10135843758b2ca508d5aa2b4798aa07f9"],"6815b5b5d6334b2245dd7be2f8b6cca949bf7f43":["6f6e1db00b87e3b03cb04cfa726108aff894a5b6","43564cbb30b064675027cfb569564e8531096e97"],"e3c94a8b8bf47db4f968d9ae510ec8bbe1372088":["5bdaf2cee03ff78b0a0cbf23df0095a3590b493b"],"6f6e1db00b87e3b03cb04cfa726108aff894a5b6":["43564cbb30b064675027cfb569564e8531096e97"],"bafca15d8e408346a67f4282ad1143b88023893b":["d046deab00d0262b4d2c5b1d704faff97a5803a7"],"7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f":["e3c94a8b8bf47db4f968d9ae510ec8bbe1372088","5bdaf2cee03ff78b0a0cbf23df0095a3590b493b"],"b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a219f1dcad1700e84807666bdbd2b573e8de7021":["102da6baafc0f534a59f31729343dbab9d3b9e9a"],"43d1e498704edd2bba13548a189eed4dfccff11b":["6815b5b5d6334b2245dd7be2f8b6cca949bf7f43","2ea161f828a3a7a6eb9410a431aecda6d7ab1065","29252e837df815b8d01fd6dff973126cced351c5"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"22859cb40e09867e7da8de84a31956c07259f82f":["3e0300d1df37d7e9662d491269e91b6f66dca8bd"],"2ea161f828a3a7a6eb9410a431aecda6d7ab1065":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["14d5815ecbef89580f5c48990bcd433f04f8563a"],"cc3b13b430571c2e169f98fe38e1e7666f88522d":["dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"14d5815ecbef89580f5c48990bcd433f04f8563a":["07c824e7f6927860d366e4888be45e4db8c9e03b"],"0932eb10135843758b2ca508d5aa2b4798aa07f9":["a219f1dcad1700e84807666bdbd2b573e8de7021","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"6e36353d7461af8d2329a78a71457cf8e3c1e88f":["bafca15d8e408346a67f4282ad1143b88023893b"],"43564cbb30b064675027cfb569564e8531096e97":["b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180"],"3e0300d1df37d7e9662d491269e91b6f66dca8bd":["10399eb409ace56e6d66f136f184643f7432371d"],"be2d2facad861c539d42173b0e4538d64b7fda80":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"102da6baafc0f534a59f31729343dbab9d3b9e9a":["22859cb40e09867e7da8de84a31956c07259f82f"],"d046deab00d0262b4d2c5b1d704faff97a5803a7":["cc3b13b430571c2e169f98fe38e1e7666f88522d"],"29252e837df815b8d01fd6dff973126cced351c5":["6815b5b5d6334b2245dd7be2f8b6cca949bf7f43"],"07c824e7f6927860d366e4888be45e4db8c9e03b":["be2d2facad861c539d42173b0e4538d64b7fda80"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["6e36353d7461af8d2329a78a71457cf8e3c1e88f"],"5bdaf2cee03ff78b0a0cbf23df0095a3590b493b":["43d1e498704edd2bba13548a189eed4dfccff11b","2ea161f828a3a7a6eb9410a431aecda6d7ab1065"],"10399eb409ace56e6d66f136f184643f7432371d":["7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","2ea161f828a3a7a6eb9410a431aecda6d7ab1065","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}